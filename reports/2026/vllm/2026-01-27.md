# 每日更新报告（2026-01-27）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-27 23:55:54 | danielafrimi | Support heterogeneous NemotronHPuzzle model (#32549) |
| 2026-01-27 23:53:26 | danisereb | [LoRA][Spec Decode] Support LoRA for Nemotron-H MTP models (#32265) |
| 2026-01-27 23:18:10 | wang.yuqi | [Frontend] Cleanup api server (#33158) |
| 2026-01-27 23:16:49 | omkhalil | [Metrics][MFU] Fix UnembedMetrics FLOP overcounting for prefill (#33045) (#33045)<br>Fix UnembedMetrics to correctly count FLOPs for the unembedding (LM head) layer.<br>The bug: UnembedMetrics used total_num_tokens() which counts all tokens in the<br>batch for projection flops, vocab projections are run on just the last token for the<br>autoregressive use case. |
| 2026-01-27 23:03:20 | Nicolò Lucchesi | [Bugfix] Fix DeepseekV32 `AssertionError: num_kv_heads == 1` (#33090) |
| 2026-01-27 23:02:51 | Matthew Bonanni | [5/N][Attention] Finish eliminating `vllm/attention` folder (#32064) |
| 2026-01-27 21:46:51 | Nicolò Lucchesi | [Bugfix] Disable CG for Whisper+FA2 (#33164) |
| 2026-01-27 21:34:49 | omerpaz95 | [Metrics] [KVConnector] Add Offloading Connector metrics (#27942)<br>Added queries and hits metrics for the Offloading Connector.<br>Also added timing metrics for store and load operations, which take the<br>average time it takes to load/store, per-token.<br>The metrics are available from Prometheus and from the StatLogger. |
| 2026-01-27 20:30:14 | Harry Mellor | Fix weight mapping test for Transfomers v5 (#33162) |
| 2026-01-27 20:15:43 | wang.yuqi | [Frontend] Frontend will only attach supported tasks corresponding entrypoints. (#33139) |
| 2026-01-27 17:34:13 | Lifan Shen | [AMD][QWEN3-NEXT] FP8 Tunings (#32042) |
| 2026-01-27 15:16:43 | rasmith | [AMD][Kernel][BugFix] Use correct scale in concat_and_cache_ds_mla_kernel when on gfx942 (#32976) |
| 2026-01-27 14:50:31 | Roger Wang | [Models] Kimi-K2.5 (#33131) |
| 2026-01-27 13:26:48 | Andreas Karatzas | [CI][Pooling] Stabilize ModernBERT test (#32909) |
| 2026-01-27 12:57:16 | Ning Xie | [code clean] remove duplicate code (#33135) |
| 2026-01-27 12:47:26 | Cyrus Leung | [Frontend] Cleanup serving engine (#33103) |
| 2026-01-27 12:25:02 | Richard Zou | [torch.compile] Stop assuming 32 bit indexing (#33113) |
| 2026-01-27 11:50:37 | Cyrus Leung | [Frontend] Reduce mixin usage in serving pooling (#33101) |
| 2026-01-27 11:45:45 | Paco Xu | [Perf] avoid duplicate mem_get_info() call in get_current_memory_usage (#33064) |
| 2026-01-27 11:05:02 | Vincent Gimenes | [DOC]: Add warning about max_num_batched_tokens and max_model_len when chunked prefill is disabled (#33109) |
| 2026-01-27 10:33:37 | Strahinja Stamenkovic | Fix IndexError with encoder-decoder models when using Custom Paged Attention (#33112) |
| 2026-01-27 10:22:35 | wangln19 | fix: preserve native tool call ID in multi-turn tool calling (#32768) |
| 2026-01-27 09:28:02 | Robert Shaw | [MoE Refactor] Integrate Naive Prepare Finalize into MK (#32567) |
| 2026-01-27 08:47:35 | Woosuk Kwon | [Model Runner V2] Remove UvaBufferPool for cpu->gpu copy (#33055) |
| 2026-01-27 07:46:11 | XiongfeiWei | [Bugfix][TPU] Return a Default fp8 MoE Backend (#32908) |
| 2026-01-27 07:13:18 | Pengchao Wang | [Bugfix][MXFP4] Call `trtllm_fp4_block_scale_moe` with kwargs (#33104) |
| 2026-01-27 05:59:44 | dolpm | [fix] CPUDNNLGEMMHandler pointer baked into inductor artifact (#32913) |
| 2026-01-27 05:49:03 | Jared Wen | [Logging] add `--disable-access-log-for-endpoints` CLI option (#30011)<br>Add a new CLI option --disable-access-log-for-endpoints to suppress<br>uvicorn access logs for specified endpoints (e.g., /health, /metrics, /ping).<br>This addresses the need to reduce log noise in production environments<br>where health check endpoints are frequently polled by load balancers or<br>monitoring systems, generating excessive log entries that obscure<br>meaningful request logs.<br>Fixes #29982 |
| 2026-01-27 05:06:45 | Wentao Ye | [Refactor] Remove unused `_moe_permute` function (#33108) |
| 2026-01-27 04:28:20 | Kevin H. Luu | [ci] Sync test areas with test-pipeline.yaml and enable new pipeline generator (#33080) |
| 2026-01-27 04:09:32 | Robert Shaw | [Bugfix] Fix Dtypes for Pynccl Wrapper (#33030) |
| 2026-01-27 02:53:22 | Cyrus Leung | [Model] Bump transformers version for test registry (#33100) |
| 2026-01-27 02:40:40 | Nicolò Lucchesi | [Bugfix] Fix Voxtral streaming slot_mapping (#33073) |
| 2026-01-27 02:04:20 | danielafrimi |  [FIX] Always support TP > 4 for FP4 Gemm (#31099) |
| 2026-01-27 01:01:52 | Andy Lo | Remove unused logic in `models/mistral.py` (#33095) |

### 📊 统计摘要
> 本日共 35 个提交 | 🔴高 6 | 🟡中 15 | 🟢低 14
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (6)](#-🔴-高重要度变更-6)
    - [[5/N][Attention] Finish eliminating `vllm/attention` fold...](#a608b4c)
    - [[Frontend] Frontend will only attach supported tasks corr...](#76139d0)
    - [[Models] Kimi-K2.5 (#33131)](#b539f98)
    - [fix: preserve native tool call ID in multi-turn tool call...](#2d70534)
    - [[MoE Refactor] Integrate Naive Prepare Finalize into MK (...](#5a93b91)
    - [[Logging] add `--disable-access-log-for-endpoints` CLI op...](#6ee7f18)
  - [🟡 中重要度变更 (15)](#-🟡-中重要度变更-15)
    - [Support heterogeneous NemotronHPuzzle model (#32549)](#83fb2d0)
    - [[LoRA][Spec Decode] Support LoRA for Nemotron-H MTP model...](#f3a5ee7)
    - [[Frontend] Cleanup api server (#33158)](#7cbbca9)
    - [[Metrics] [KVConnector] Add Offloading Connector metrics ...](#7227d06)
    - [Fix weight mapping test for Transfomers v5 (#33162)](#14385c8)
    - [[AMD][QWEN3-NEXT] FP8 Tunings (#32042)](#da8d0c4)
    - [[Frontend] Cleanup serving engine (#33103)](#e0b005d)
    - [[Frontend] Reduce mixin usage in serving pooling (#33101)](#c831911)
    - [[Model Runner V2] Remove UvaBufferPool for cpu->gpu copy ...](#6d86fde)
    - [[Bugfix][MXFP4] Call `trtllm_fp4_block_scale_moe` with kw...](#8caffd9)
    - [[fix] CPUDNNLGEMMHandler pointer baked into inductor arti...](#58a05b0)
    - [[Refactor] Remove unused `_moe_permute` function (#33108)](#8f98788)
    - [[ci] Sync test areas with test-pipeline.yaml and enable n...](#ebe0ba9)
    - [[Model] Bump transformers version for test registry (#33100)](#c25dbee)
    - [[FIX] Always support TP > 4 for FP4 Gemm (#31099)](#67fe677)
  - [🟢 低重要度变更 (14)](#-🟢-低重要度变更-14)
    - [[Metrics][MFU] Fix UnembedMetrics FLOP overcounting for p...](#5ec4405)
    - [[Bugfix] Fix DeepseekV32 `AssertionError: num_kv_heads ==...](#492a798)
    - [[Bugfix] Disable CG for Whisper+FA2 (#33164)](#1f3a2c2)
    - [[AMD][Kernel][BugFix] Use correct scale in concat_and_cac...](#58996f3)
    - [[CI][Pooling] Stabilize ModernBERT test (#32909)](#6c00645)
    - [[code clean] remove duplicate code (#33135)](#b781eea)
    - [[torch.compile] Stop assuming 32 bit indexing (#33113)](#3b8f0fe)
    - [[Perf] avoid duplicate mem_get_info() call in get_current...](#157caf5)
    - [[DOC]: Add warning about max_num_batched_tokens and max_m...](#0b53bec)
    - [Fix IndexError with encoder-decoder models when using Cus...](#c568581)
    - [[Bugfix][TPU] Return a Default fp8 MoE Backend (#32908)](#510ed1e)
    - [[Bugfix] Fix Dtypes for Pynccl Wrapper (#33030)](#43a013c)
    - [[Bugfix] Fix Voxtral streaming slot_mapping (#33073)](#19ab0f7)
    - [Remove unused logic in `models/mistral.py` (#33095)](#d56afd4)
#### 🔴 高重要度变更 (6)

### [5/N][Attention] Finish eliminating `vllm/attention` folder (#32064)
**SHA**: `a608b4c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a608b4c6c2b9098f7500ed305f47cbe1375df5ee)

**🎯 变更类型**：架构变更 / 重构  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 完成对 `vllm/attention` 包的迁移：所有注意力实现、工具函数以及相关测试均移动至 `vllm/model_executor/layers/attention`。  
- 删除 `vllm/attention` 目录及其 `__init__`、utils 等文件，更新所有内部 import 为新路径。  
- 同步更新 CI（Buildkite）依赖、`CODEOWNERS`、文档、以及旧代码中对 `vllm.attention.layer`、`vllm.attention.utils` 的引用。  
- 在新模块中重新实现 `MLAAttention`，并将原先的 KV‑Sharing 校验逻辑迁入 `attention/attention.py`（保留原功能）。  

**🎯 影响范围**  
- **核心层**：`vllm/model_executor/layers/attention/*`（Attention、MLAAttention、ChunkedLocalAttention、CrossAttention、EncoderOnlyAttention、StaticSinkAttention、MLA等）  
- **模型加载/执行**：所有模型实现（BLOOM、LLAMA、GPT‑J、Gemma 等）通过 `Attention`/`MLAAttention` 实例化的路径被修改。  
- **编译/自定义算子**：`vllm/compilation/*` 中的 `Attention` 引用更新；自定义 ops（`unified_attention_*`）仍指向同一实现。  
- **测试套件**：`tests/*`、`tests/kernels/attention/*`、`tests/compile/*` 均改用新导入。  
- **CI / 构建**：Buildkite CI 步骤的源码依赖更新，为后续删除 `vllm/attention` 做准备。  
- **文档与 CODEOWNERS**：文档示例、贡献指南以及代码所有权声明全部迁移。  

---

## 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 将注意力实现提升到 `model_executor` 层，使其与其他模型执行层（linear、quantization、moe 等）保持同级，提升代码组织一致性。<br>- 移除旧 `vllm/attention` 包后，外部 Import 需要更新；若第三方项目仍依赖旧路径，会出现 `ImportError`。<br>- 新增 `__all__` 导出列表，明确公共 API，便于未来的模块化和插件化。 |
| **性能影响** | - 业务逻辑未改变，所有算子仍使用同一底层 `torch.ops.vllm.*` / FlashAttention / FlashInfer 实现，**运行时性能基本保持不变**。<br>- 迁移过程中对 `MLAAttention` 进行了代码合并（去掉冗余 import、inline KV‑Sharing），可能略微降低模块加载时间（更少的文件系统 I/O）。 |
| **安全考虑** | - 没有引入新的网络、文件或进程交互，仅是内部 import 重构，攻击面未扩大。<br>- 需要确保删除的旧模块不再被打包进发行版，以免泄露未维护的代码。<br>- CI 中的依赖路径修改若遗漏，会导致构建失败或误用旧代码，建议在发布前做完整的包检查。 |
| **兼容性** | - **向后兼容破坏**：`import vllm.attention.layer as Attention` 已失效。<br>- 若项目在运行时动态导入（比如插件、实验性模型）仍引用旧路径，将抛出 `ModuleNotFoundError`。<br>- 兼容层（如在 `vllm/attention/__init__.py` 放置一个 thin wrapper）已被删除，意味着官方不再提供兼容 shim。 |
| **维护性** | - 统一了注意力层与其他执行层的目录结构，新的代码审查路径更清晰（`CODEOWNERS` 已指向新目录）。<br>- 文档示例同步更新，降低新手使用错误的概率。<br>- 大量文件改动（+585 / -527 行）导致潜在的遗漏 import，需通过 CI 强检查。 |
| **依赖/构建** | - Buildkite `source_file_dependencies` 现在指向 `vllm/model_executor/layers/attention`，旧路径已被标记为 TODO 删除，准备在后续 PR 完全清理。<br>- `tools/pre_commit/mypy.py` 检查列表已移除 `vllm/attention`，防止 stale type‑checking。 |

---

## ⚠️ 潜在风险

1. **外部破坏性升级**  
   - 第三方库或用户脚本仍使用 `vllm.attention.*` 将直接报错。  
2. **遗漏导入**  
   - 迁移过程涉及上千处 import，若有遗漏会导致运行时 `ImportError`，尤其在动态 import 场景（如自定义模型、插件）。  
3. **打包误删**  
   - `setup.py`/`pyproject.toml` 中的 `package_data` 需要同步更新，防止旧目录被意外打入发行版或新目录未被包含。  
4. **CI 失效**  
   - Buildkite 步骤中仍保留 `# TODO: remove` 注释，如果后续忘记删除，可能导致不必要的文件依赖检查或误报。  
5. **文档/示例不一致**  
   - 若文档更新不完整，用户快速上手时会遇到导入错误。  

---

## 💡 关注建议

| 建议 | 具体措施 |
|------|----------|
| **兼容层** | - 在 `vllm/attention/__init__.py` 保留一个 **deprecated shim**（如 `from vllm.model_executor.layers.attention import *`)，并在文档里明确标记将在下个次要版本删除，以平滑过渡。 |
| **CI 完整性** | - 增加 **全局 import 检查**（如 `flake8`/`pylint` 的 `import-error`）确保没有残留老路径。<br>- 在 CI 中加入 **二进制包检查**：`python -c "import vllm"`，确保发行版不包含已删除文件。 |
| **发布说明** | - 在 Changelog 中突出 “Attention 包迁移，旧 import 已不再支持”。<br>- 给出迁移指南（示例 `from vllm.model_executor.layers.attention import Attention`）。 |
| **文档同步** | - 确认所有文档、README、API 参考均使用新路径。<br>- 在 `docs/contributing/model/basic.md`、`docs/design/custom_op.md` 等示例代码中加入注释 “旧路径已废弃”。 |
| **测试覆盖** | - 运行 **完整的单元/集成测试**（包括 `tests/kernels/attention`、`tests/compile`、`tests/v1/worker`），确保在不同硬件（CPU、CUDA、ROCm）上均通过。 |
| **代码审查** | - 关注 **`CODEOWNERS`** 改动：确保 `LucasWilkinson` 正在审查所有新注意力实现。 |
| **打包配置** | - 检查 `MANIFEST.in`、`setup.cfg` 或 `pyproject.toml`，确保新目录 `vllm/model_executor/layers/attention/**` 被正确包含。 |
| **回滚准备** | - 若出现大范围 ImportError，准备在下一个补丁版本中临时恢复 `vllm/attention` 的 thin wrapper，防止紧急用户受阻。 |

**总体结论**  
此 PR 完成了对注意力层的结构性迁移，提升了代码组织的一致性与可维护性，对运行时性能和安全性没有直接影响。但因为它是一次破坏性 API 变更，需要在发布时做好兼容性提示、文档同步以及 CI/包装的全方位检查，以防止用户升级后出现导入错误。保持上述建议执行，可将迁移风险降至最低，并为后续注意力相关功能扩展（如新自定义算子或多模态注意力）提供更清晰的基础。

---

### [Frontend] Frontend will only attach supported tasks corresponding entrypoints. (#33139)
**SHA**: `76139d0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/76139d0801e95bf6c54352650a6f42e181b510be)

**🎯 变更类型**：架构变更 / 功能增强  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 将 OpenAI 前端入口点改为仅在运行时根据模型实际支持的任务（`supported_tasks`）动态挂载对应的 API 路由。  
2. 拆分原有的 “generate” 相关实现至 `vllm/entrypoints/openai/generate/`，并在 `api_server.py` 中依据 `supported_tasks` 条件性初始化对应的服务对象（Chat、Completion、Responses、Anthropic、Tokens 等）。  
3. 为 SageMaker 接口重新实现路由 (`vllm/entrypoints/sagemaker/api_router.py`)，同样基于 `supported_tasks` 动态匹配请求类型。  
4. 调整 Pooling、Transcriptions、Translations 等模块的路由和状态初始化签名，使其接受 `supported_tasks` 参数。  
5. 测试层级改为验证 `openai.NotFoundError`（已统一为 404），而非原来的自定义错误结构。  

**🎯 影响范围**  
- **核心 API 服务器**：`vllm/entrypoints/openai/api_server.py`、`vllm/entrypoints/openai/generate/`、`vllm/entrypoints/sagemaker/api_router.py`。  
- **任务相关子系统**：Chat、Completion、Responses、Transcriptions、Translations、Pooling（classify、embed、score、pooling）等。  
- **测试套件**：OpenAI、SageMaker 相关入口点的单元/集成测试。  
- **文档和用户 SDK**：OpenAI 风格的客户端调用方式及错误处理逻辑。  

---

## 🔍 技术洞察  

| 维度 | 影响 |
|------|------|
| **架构影响** | - **模块化任务驱动**：通过 `engine_client.get_supported_tasks()` 在启动时获取模型支持的任务列表，随后 **按需挂载** 路由和服务对象，形成 **插件化** 结构。<br>- **解耦**：`generate`、`transcriptions`、`pooling` 等功能不再在 `api_server.py` 中硬编码，而是分别在独立子包实现，提升可维护性。<br>- **向后兼容风险**：API URL 仍然保持不变，但错误响应从自定义结构（`error.code/message`）改为 FastAPI 默认的 404 (`openai.NotFoundError`)；若有使用旧错误字段的第三方代码，需升级。 |
| **性能影响** | - **启动时间**：省去不必要的服务对象创建（如 `OpenAIServingChat`、`OpenAIServingCompletion`）以及对应的模型加载路径，显著降低冷启动延迟。<br>- **运行时开销**：FastAPI 路由表更小，匹配成本下降；请求体校验仅针对实际支持的请求类型进行，避免无效的 Pydantic 验证。<br>- **内存占用**：未使用的 `ToolServer`、`Transcription`、`Translation` 等实例不再实例化，减小进程常驻内存。 |
| **安全考虑** | - **攻击面缩小**：仅暴露模型真实支持的端点，防止攻击者尝试调用未实现或未授权的 API（如调用不存在的 `chat/completions`），可直接返回 404。<br>- **错误信息一致性**：使用统一的 OpenAI 错误模型 (`NotFoundError`) 减少信息泄露风险。<br>- **依赖变更**：引入 `model_hosting_container_standards.sagemaker`，需要确保该外部库的安全性与兼容性。 |
| **可维护性** | - **代码分离**：每个任务的路由与状态初始化放在对应子包，后续新增任务只需要在 `SUPPORTED_TASKS` 列表中添加并实现对应的 `api_router` 与 `serving`，不必改动主服务器入口。<br>- **类型检查**：大量使用 `TYPE_CHECKING` 防止运行时循环导入，提升 IDE/静态分析体验。<br>- **文档同步**：需要更新 README/Docs 中的 “Supported Endpoints” 表格，说明哪些模型会返回 404。 |

---

## ⚠️ 潜在风险  

1. **`get_supported_tasks()` 返回不完整**  
   - 若底层引擎在某些异常情况下未能正确报告全部支持的任务，相关路由将缺失，导致合法请求被误判为 404。  
   - **对策**：在 `init_app_state` 前后加入日志/监控，确保返回集合与模型配置一致；在关键路径捕获异常并回退到 “全部开放” 模式（可通过配置开关）。  

2. **向后兼容性**  
   - 旧版客户端期望错误结构 `{error: {code, message}}`，现在会收到 `openai.NotFoundError`。  
   - **对策**：在发布说明中明确标记为 **breaking change**；提供可选的兼容层（如 `--legacy-error-format` 标志）在后续版本中缓冲迁移。  

3. **SageMaker 路由的动态匹配**  
   - `get_invocation_types` 按任务顺序构造，若任务列表顺序变化可能导致优先级不一致（例如 `embedding` 与 `score` 同时接受相同字段）。  
   - **对策**：在 `SUPPORTED_TASKS` 中保持固定的优先级顺序，或在 `get_invocation_types` 中显式排序。  

4. **工具服务器（ToolServer）初始化路径变更**  
   - `init_generate_state` 中仍保留原有 `ToolServer` 初始化逻辑，但 `init_app_state` 已删除；若用户未通过 `generate` 任务使用工具功能，可能出现未初始化的 `tool_server`。  
   - **对策**：在 `init_generate_state` 中确保即使 `generate` 不在 `supported_tasks` 时，也能安全地创建 (或跳过) `tool_server`，防止潜在的 `NoneType` 调用。  

5. **依赖同步**  
   - 新增 `model_hosting_container_standards.sagemaker` 可能在不同运行环境（如非 SageMaker）下不可用，导致启动失败。  
   - **对策**：在 `requirements` 中标记为可选依赖，并在 `api_router` 中捕获 `ImportError`，提供友好的错误提示。  

---

## 💡 关注建议  

1. **发布前回归测试**  
   - 重点覆盖以下场景：<br>• 模型仅支持 `generate` → 确认 `chat/completions`、`completions` 正常；<br>• 模型仅支持 `transcription` → 确认 `audio/transcriptions` 正常、其余端点返回 404；<br>• SageMaker 部署下的 `invocations` 能正确路由多种请求类型。  

2. **监控与报警**  
   - 在生产环境记录 `supported_tasks` 与实际路由命中率的指标；若出现 404 错误率异常升高，快速定位是否是任务列表不完整导致。  

3. **文档与 SDK 同步**  
   - 更新官方文档的“Supported Endpoints”章节，明确说明 **“If a model does not support a task, the endpoint returns 404 (NotFoundError)”**。<br>• 同步改动至 OpenAI 官方 Python SDK 示例，捕获 `openai.NotFoundError`。  

4. **兼容性开关**  
   - 考虑在 `vllm.entrypoints.openai.api_server` 中加入环境变量或 CLI 参数（如 `--legacy-error-format`），在需要平滑迁移的场景下仍返回原始错误结构。  

5. **代码审计**  
   - 对 `vllm/entrypoints/sagemaker/api_router.py` 中的 `pydantic.TypeAdapter` 使用进行审计

---

### [Models] Kimi-K2.5 (#33131)
**SHA**: `b539f98` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b539f988e1eeffe1c39bebbeaba892dc529eefaf)

**🎯 变更类型**：功能增强 / 架构变更 / 多模态扩展  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 为 vLLM 新增 **Kimi‑K2.5** 模型实现（`kimi_k25`），包括 Vision Tower、MM Projector、权重加载、推理与多模态前向。  
2. 引入统一的 **`vision_chunk`** 多模态类型，统一处理图片与视频块；在 `BaseMultiModalItemTracker`、解析器、渲染器等全链路加入相应适配。  
3. 新增 **VideoLoader‑identity**（返回原始字节），并在渲染阶段完成 **video placeholder → video‑chunk prompt** 的替换。  
4. 扩展配置体系（`KimiK25Config`、`KimiK25VisionConfig`），并在模型注册表、Reasoning Parser、`supported_models.md` 中完成文档与入口更新。  

**🎯 影响范围**  
- **模型目录**：`vllm/model_executor/models/kimi_k25*`（Vision Tower、Processor、模型类）。  
- **多模态核心**：`vllm/entrypoints/chat_utils.py`、`vllm/multimodal/inputs.py`、`vllm/multimodal/parse.py`、`vllm/multimodal/video.py`、`vllm/renderers/hf.py`。  
- **配置/注册**：`vllm/transformers_utils/config.py`、`registry.py`、`reasoning/*`。  
- **测试**：`tests/models/registry.py`（新增模型注册检查）。  

---

## 🔍 技术洞察

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | - 新增 **统一视觉块 (`vision_chunk`)**，取代原有 `image`/`video` 两条分支，导致 `BaseMultiModalItemTracker`、`_resolve_items`、`_parse_*` 全链路改动。 <br>- `use_unified_vision_chunk` 成为模型配置开关，只有开启时才走统一路径，保持向后兼容。 <br>- `KimiK25MultiModalProcessor` 在 `split_video_chunks` 中完成视频切块，返回 `VisionChunkVideo` 列表；`build_video_prompts_from_mm_data` 在渲染阶段把这些块的提示拼接回原始占位符。 |
| **性能影响** | - 视频块拆分与 **Temporal‑Pooling** 在 Vision Tower (`MoonViT3dPretrainedModel`) 中实现，计算量随 **`merge_kernel_size`**、**帧数**线性增长。 <br>- 新增的 `IdentityVideoLoader` 直接返回原始字节，避免 OpenCV 解码开销，但后续在 `media_processor.split_video_chunks` 中仍需进行帧抽取与预处理，CPU + GPU 负载会增加。 <br>- `run_dp_sharded_mrope_vision_model` 支持数据并行，可在多卡上分片处理，提升大批量视频的吞吐。 |
| **安全考虑** | - **`trust_remote_code=True`** 在模型加载（`tests/models/registry.py`、`MoonshotKimiVAutoProcessor`）仍然保持，仍然存在远程代码执行风险。建议在生产环境显式禁用或使用 sandbox。 <br>- `IdentityVideoLoader` 直接返回原始字节，若未对输入大小做限制，可能导致 OOM 或 DoS（恶意超大视频）。应在入口处加入 **文件大小上限**（如 200 MiB）。 |
| **可维护性** | - 通过 **TypedDict** (`VisionChunkImage`/`VisionChunkVideo`) 明确数据结构，类型检查友好。 <br>- `ModalityStr` 扩展为 `vision_chunk`，但所有旧模型仍保持原有 `image/video`，代码路径清晰。 <br>- 新增的 `rebuild_mm_uuids_from_mm_data` 与 `build_video_prompts_from_mm_data` 放在渲染器中，职责单一，易于单元测试。 |
| **兼容性** | - 只在模型的 **`use_unified_vision_chunk`** 为 `True` 时激活新路径，默认依旧为 `False`，旧模型不受影响。 <br>- 对已有 `supports_multi_modal` 接口的模型无需改动，只需在配置里显式开启。 |

---

## ⚠️ 潜在风险

1. **占位符替换不匹配**  
   - 当 `prompt_raw` 中的 `video_placeholder` 数量 ≠ 实际处理得到的 `vision_chunk`（视频块）数时，会出现 `logger.warning`，但仍会返回未替换的占位符，可能导致模型生成错误的 token 序列。  
2. **权重映射错误**  
   - `load_weights` 中针对 `vision`、`mm_projector` 的映射依赖层名严格匹配，若上游模型改动层前缀或参数命名，加载会 silently跳过或抛异常。  
3. **资源耗尽**  
   - 视频切块后会产生 **`t * h * w`** 个嵌入，每个嵌入向量经 `mm_projector` 放大 `merge_h * merge_w`，大分辨率或长视频容易导致显存 OOM。  
4. **安全漏洞**  
   - `trust_remote_code=True` 对任何第三方模型都允许运行自定义代码，可能被恶意模型植入后门。  
   - `IdentityVideoLoader` 未限制上传字节大小，可能被用于 **DoS** 攻击。  
5. **多模态序列长度冲突**  
   - `mm_data` 中的 `vision_chunk` 与文本 token 合并后，若总长度超过模型的 `max_position_embeddings`，会触发 **position overflow**，目前没有显式校验。  

---

## 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | 1. 为 `KimiK25Config.use_unified_vision_chunk` 设置 **默认 False**，并在模型仓库的 README 中说明开启方法。<br>2. 在 `load_weights` 前加入 **层名校验**，在缺失关键权重时抛出明确异常。<br>3. 为 `IdentityVideoLoader.load_bytes` 添加 **最大字节数检查**（如 `max_video_bytes = 200 * 1024**2`），并在超过时返回 `None` + 警告。 |
| **测试** | - 为 `vision_chunk` 路径新增 **单元测试**：<br>  * 视频块切分数量、uuid 重建、prompt 替换的正确性。<br>  * OOM 场景的提前检测（模拟大分辨率/长视频）。 |
| **性能** | - 在 `MoonViT3dPretrainedModel.forward` 前后加入 **profile hooks**，收集 GPU/CPU 用时，以便在未来调优 `merge_kernel_size` 或 `num_frames_per_chunk`。<br>- 考虑对 `vision_chunk` 使用 **torch.compile**（已在 `get_rope_shape` 中使用），可进一步提升推理速度。 |
| **安全** | - 在生产部署环境强制 **`trust_remote_code=False`**，仅对已经审计过的模型使用 `trust_remote_code=True`（可通过环境变量控制）。<br>- 对上传的媒体文件实行 **MIME 类型和尺寸白名单

---

### fix: preserve native tool call ID in multi-turn tool calling (#32768)
**SHA**: `2d70534` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2d7053438a112e2be55cf6d2bde9deb8a169d0a4)

**🎯 变更类型**：功能增强 / Bug修复  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 在多轮工具调用场景中，新增对模型原生工具调用 ID（尤其是 Kimi K2） 的保留与传递，避免因统一随机 ID 生成导致的 ID 丢失或格式不匹配。  
- 为 `FunctionCall` 增加内部 `id` 字段（序列化时排除），并在请求/响应处理、工具解析、以及 ID 生成逻辑中统一使用该字段或根据模型类型生成合适的 ID。

**🎯 影响范围**：  
- `vllm/entrypoints/openai/chat_completion/serving.py`（流式与完整生成路径）  
- `vllm/entrypoints/openai/engine/protocol.py`（`FunctionCall` 模型）  
- `vllm/entrypoints/openai/engine/serving.py`（工具调用解析）  
- `vllm/entrypoints/openai/responses/serving.py`（Responses API 输出）  
- `vllm/tool_parsers/kimi_k2_tool_parser.py`（Kimi K2 工具调用解析）  
- 相关单元测试文件的 Mock `ModelConfig` 扩展  

**🔍 技术洞察**  

- **架构影响**  
  - **模型抽象层**：在 `ModelConfig` 中通过 `hf_text_config` 与 `hf_overrides` 判断模型类型，从而决定 `tool_call_id_type`（`kimi_k2` vs `random`），保持对不同模型的兼容性。  
  - **数据流统一**：所有生成工具调用 ID 的路径（流式 `DeltaToolCall`、完整 `ToolCall`、Responses API `ResponseFunctionToolCall`）均改为统一使用 `make_tool_call_id` 或直接使用原生 ID，确保前后端、日志、监控等全链路一致。  
  - **序列化安全**：`FunctionCall.id` 使用 Pydantic `Field(..., exclude=True)`，在对外返回的 OpenAI 兼容 JSON 中不出现该字段，避免破坏现有 API 合约。  

- **性能影响**  
  - 新增的 ID 判定、少量条件分支以及 `isinstance(tokenizer, MistralTokenizer)` 检查带来的 CPU 开销极低（微秒级），对整体吞吐与延迟基本没有可感知影响。  
  - 代码路径中加入的 `assert tokenizer is not None` 以及 import `MistralTokenizer` 仅在首次调用时执行，略微增加模块加载时间，可接受。  

- **安全考虑**  
  - 公开原生工具调用 ID（如 Kimi K2 的 `call_12345`）在大多数场景下属于内部标识，不泄露业务敏感信息。  
  - `FunctionCall.id` 被排除在序列化外，防止意外暴露给 OpenAI 客户端或第三方。  
  - 仍需确保在日志记录、监控平台等内部系统中对该 ID 的存储符合公司内部数据治理要求。  

**⚠️ 潜在风险**  

1. **兼容性回归**  
   - 对外接口仍需返回符合 OpenAI schema 的 `function` 对象，若 `exclude=True` 未生效或在自定义序列化路径中被手动加入，可能导致客户端解析错误。  
2. **模型类型判断失效**  
   - 依赖 `hf_text_config.model_type` 与 `hf_overrides["model_type"]`，若未来出现新模型的配置方式或拼写错误，ID 生成将回退为 `random`，导致 Kimi K2 等模型的原生 9‑字符 ID 被错误替换。  
3. **工具调用历史计数错误**  
   - 在多个分支中对 `history_tool_call_cnt` 的递增逻辑不完全一致（如流式 vs 完整生成），极端情况下可能出现 ID 冲突或不连续。  
4. **类型注解与运行时不匹配**  
   - 代码中大量使用 `# type: ignore[arg-type]`，在严格类型检查或未来迁移到 `pydantic v2` 时可能暴露隐藏的接口不匹配。  

**💡 关注建议**  

- **测试覆盖**：  
  - 增加针对 `kimi_k2`、`random` 两种 `tool_call_id_type` 的单元测试，验证多轮对话中工具调用 ID 的一致性与格式（9 字符 vs UUID）。  
  - 对 `FunctionCall` 序列化进行回归测试，确保 `id` 字段始终被排除。  

- **代码规范**：  
  - 将 ID 判定与生成抽象为统一的内部工具函数，例如 `resolve_tool_call_id(parser_result, tokenizer, cfg)`，减少分散的条件分支。  
  - 移除不必要的 `# type: ignore`，在必要处添加明确的类型转换或 `cast`，提升可维护性。  

- **监控/审计**：  
  - 在日志中记录原生 ID 与生成的随机 ID（若有）对应关系，便于后续追踪问题。  
  - 若业务对工具调用 ID 有审计需求，建议在内部统一使用 UUID 并在必要时映射回原生 ID。  

- **文档更新**：  
  - 在 `vLLM` 的 OpenAI API 文档中说明：对 Kimi K2 及未来可能的模型，工具调用 ID 会保留原生值；对其他模型则采用随机生成的兼容 ID。  

- **未来兼容**：  
  - 预留 `tool_call_id_type` 的枚举实现路径，便于在后续支持更多模型（如 `gemma_tool`）时无代码侵入式改动。  

---

---

### [MoE Refactor] Integrate Naive Prepare Finalize into MK (#32567)
**SHA**: `5a93b91` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5a93b9162bfef6be6714ba70d3191e1e7ab7d3e4)

**🎯 变更类型**：架构变更 / 重构 / 性能优化  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
此 PR 完成了 MoE（Mixture‑of‑Experts）模块的大幅重构，核心工作是将 **Naive Prepare‑Finalize**（原本在 `moe_prepare_finalize.py` 中的实现）迁移到统一的 **Modular‑Kernel (MK)** 接口，并用新的 `flashinfer_a2a_prepare_finalize.py` 替代了原有的 `flashinfer_cutlass_prepare_finalize.py`。同时扩展了 All2All 通信层的接口（`dispatch_router_logits` 与 `dispatch`），统一了不同后端（naive、AllGather、All2All、FlashInfer）下的调度路径。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/*` 目录下的所有 MoE 相关实现（配置、并行、专家、调度、准备/完成、共享 Expert、DeepEP、DeepPE、PPLX、Mori、FlashInfer 等）  
- 分布式通信层 `vllm/distributed/device_communicators/*`（CPU、CUDA、XPU）  
- 构建脚本、CI 测试配置 (`.buildkite/*.yaml`)  
- 文档 `docs/design/moe_kernel_features.md`  
- 各类 Quantization 方法（FP8、NVFP4、MFXP4）以及对应的 Oracle 实现  
- 单元测试与基准测试代码  

---

## 🔍 技术洞察  

| 维度 | 影响描述 |
|------|----------|
| **架构** | 1. **统一 Modular‑Kernel 接口**：所有 MoE 计算现在通过 `mk.FusedMoEModularKernel`（内部持有 `prepare_finalize` 与 `experts`）统一调度，去除了旧的 `make_fp8_moe_kernel_for_mkm`、`make_nvfp4_moe_kernel_for_mkm` 等辅助函数。<br>2. **All2All 接口抽象化**：在 `BaseDeviceCommunicator` 中新增 `dispatch_router_logits`（用于原 `dispatch` 接收路由 logits）以及 `dispatch`（用于 hidden‑states、top‑k weights/ids），所有后端实现（Naive、AgRs、Pplx、FlashInfer 等）均实现这两套接口，实现了 **路由 logits 与专家数据的解耦**。<br>3. **新 Prepare‑Finalize 实现**：`flashinfer_a2a_prepare_finalize.py` 负责 FlashInfer‑All2All 的准备/完成；`MoEPrepareAndFinalizeNaiveEP` 负责 Naive All2All（支持 `defer_input_quant`）。<br>4. **配置扩展**：`FusedMoEParallelConfig` 新增 `is_sequence_parallel` 与 `use_fi_all2allv_kernels` 标识，统一在 `config.make` 中传递。 |
| **性能** | 1. **All2All 直通**：FlashInfer‑All2All 采用 `MnnvlMoe.mnnvl_moe_alltoallv`（一次性 All‑to‑All），省去原先的两次 AllGather/ReduceScatter，预计在 DP/EP+SP 场景下显著降低通信延迟。<br>2. **输入量化延迟**：对 `defer_input_quant=True` 的路径（例如 DeepSeek‑FP8 block‑scale）在 `prepare` 中不再提前量化，而交由专家 kernel 自行完成，避免了在 All2All 前的额外拷贝/scale 处理。<br>3. **统一调度**：通过 `dispatch_router_logits` 与 `dispatch` 的统一实现，避免了在 DP/EP 模式下对不同后端分别维护 `dispatch` 逻辑导致的分支开销。<br>4. **缓存/工作空间**：FlashInfer All2All 必须预先 `ensure_alltoall_workspace_initialized`，在大模型/多卡部署下可一次性分配大块工作空间，减少运行时多次 `cudaMalloc`。 |
| **安全** | - 新增的 `dispatch` 接口在 `BaseDeviceCommunicator` 中对 `extra_tensors` 参数做了显式检查，若后端不支持会立刻 `NotImplementedError`，避免了隐式错误。<br>- `flashinfer_a2a_prepare_finalize` 在 `quantization` 路径中对 `nvfp4` 的 scale 进行 `nvfp4_block_scale_interleave` 后才传入 All2All，确保不会出现未对齐的张量导致 GPU 访问错误。 |
| **可维护性** | - 大量 **老代码 (cutlass_prepare_finalize, make_*_for_mkm)** 被删除，接口统一后代码重复度显著下降。<br>- `maybe_make_prepare_finalize` 成为统一入口，所有 MoE 量化方法（FP8、NVFP4、MFXP4）只需要一次性调用即可获得对应 `PrepareAndFinalize` 实例。<br>- 文档已同步更新，显示新 kernel 名称 `FlashInferA2APrepareAndFinalize`。 |
| **向后兼容** | - 旧的 `MoEPrepareAndFinalizeNoEP` 仍保留（但已被 `MoEPrepareAndFinalizeNaiveEP` 替代），因此在不启用 All2All 的情况下仍可工作。<br>- 通过 `allow_new_interface` 参数在 `maybe_make_prepare_finalize` 中保持向后兼容：如果新接口不被支持，则返回 `None` 并保持原有 `make_prepare_finalize` 流程。<br>- 现有测试大幅修改以适配新接口，说明已有回归测试覆盖大部分路径。 |

---

## ⚠️ 潜在风险  

| 风险点 | 说明 | 可能的影响 |
|--------|------|------------|
| **All2All 通信路径不完整** | `dispatch` 与 `dispatch_router_logits` 在 `BaseDeviceCommunicator` 默认实现里直接返回 `extra_tensors`，但某些后端（如 `NaiveAll2AllManager`) 仍未实现 `dispatch`（只实现 `dispatch_router_logits`)。若未来在这些后端使用 `dispatch`（如专家权重/ids），会触发 `NotImplementedError`。 | 在混合使用 `dispatch` 与 `dispatch_router_logits` 时，可能出现运行时异常。 |
| **Defer‑Input‑Quant 逻辑不统一** | 部分 MoE kernel（DeepEP、PPLX）在 `expects_unquantized_inputs` 为 `True` 时仍在 `prepare` 中强制量化（如 `MoEPrepareAndFinalizeNaiveEP` 的 `defer_input_quant` 参数未被传递），导致误量化或 double‑quant。 | 量化精度下降，或出现数值溢出。 |
| **`is_sequence_parallel` 处理** | `dispatch_router_logits` 中的 `is_sequence_parallel` 仅在 `NaiveAll2AllManager` 与 `AgRsAll2AllManager` 中使用，其他后端（如 `CudaDeviceCommunicator`）仍未考虑 SP。若用户开启 `use_sequence_parallel_moe=True` 并使用 FlashInfer A2A，SP 的 token‑count 计算可能错误。 | 产生错误的 token 分配，导致结果错误或 Crash。 |
| **共享 Expert 与 All2All 交叉** | `SharedFusedMoE` 中的判定条件现在依据 `moe_parallel_config.use_fi_all2allv_kernels`，但在 `FlashInferA2APrepareAndFinalize.finalize` 中仍使用 `self.all2all_manager`（可能为 `None`）进行 `combine`。若 `use_fi_all2allv_kernels` 为 `True` 而 `all2all_manager` 未初始化，会触发 `AttributeError`。 | 启用 FlashInfer A2A 时，可能出现未初始化的 All2All 管理器。 |
| **文档/示例未同步** | 部分公共 API（如 `maybe_make_prepare_finalize`）在文档、示例中未出现，导致开发者无法快速找到入口。 | 使用门槛提升，易导致误用。 |
| **旧的 `prepare_finalize.expects_unquantized_inputs` API 被替换为属性**：部分自定义插件或外部依赖可能仍调用旧的 static 方法，导致 `AttributeError`。 | 外部扩展兼容性受影响。 |
| **单

---

### [Logging] add `--disable-access-log-for-endpoints` CLI option (#30011)
**SHA**: `6ee7f18` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6ee7f18f33bd83df657fa0a3aec9b448e71e4781)

**🎯 变更类型**：功能增强 / 日志过滤  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
在 vLLM 的 OpenAI 接口服务器中新增 `--disable-access-log-for-endpoints` CLI 参数，配合实现的 `UvicornAccessLogFilter` 与 `create_uvicorn_log_config`，可以在 uvicorn 的访问日志层面对指定路径（如 `/health`、`/metrics`、`/ping`）进行精准过滤，显著降低生产环境中健康检查等高频调用产生的日志噪声。  
**🎯 影响范围**：  
- `vllm/entrypoints/openai/api_server.py`（日志配置入口）  
- `vllm/entrypoints/openai/cli_args.py`（CLI 参数定义）  
- `vllm/logging_utils/`（新增过滤器与配置生成函数、`__init__` 导出）  
- 文档 `examples/others/logging_configuration.md`（使用示例）  
- 新增单元测试 `tests/test_access_log_filter.py`  

**🔍 技术洞察**  

- **架构影响**  
  - **解耦**：日志过滤实现为独立的 `logging.Filter`，通过 `logging.config` 动态注入到 uvicorn 的 `access` handler，保持与核心推理逻辑的完全解耦。  
  - **入口统一**：通过 `get_uvicorn_log_config` 统一处理三种日志配置来源（文件 → endpoint 过滤 → 默认），避免在 `run_server_worker` 中出现冗余逻辑。  
  - **向后兼容**：如果用户已经使用 `--log-config-file` 或 `--disable-uvicorn-access-log`，新增参数不会影响已有行为。  

- **性能影响**  
  - **过滤开销**：在每条 access log 记录生成时触发一次 `filter` 调用，主要工作是一次 `urlparse` 与集合成员判断，CPU 开销在微秒级，几乎可以忽略。  
  - **日志量降低**：对高频 endpoint（如健康检查）过滤后，stdout/stderr I/O 大幅减少，有助于提升整体服务吞吐和降低日志聚合成本。  

- **安全考虑**  
  - **信息泄露**：过滤仅针对 **访问日志**（不影响业务日志），不会改变请求/响应的实际处理，仅抑制日志记录，不会导致安全功能失效。  
  - **误删风险**：若误将业务关键路径加入排除列表，相关访问日志将被隐藏，可能影响审计与故障排查。需在文档与 CLI 帮助中强调慎重选择。  

**⚠️ 潜在风险**  

1. **配置冲突**：用户自行提供 `log_config_file` 时仍会覆盖过滤器。如果文件中已显式配置 `uvicorn.access` handler 且未加入 `access_log_filter`，则 `--disable-access-log-for-endpoints` 将失效。  
2. **路径匹配限制**：过滤使用 **精确路径匹配**（不考虑前缀或通配符），可能与用户预期不符；例如 `/healthz` 不会被 `/health` 排除。  
3. **异常日志记录**：当 `record.args` 不符合 uvicorn 的预期结构（如自定义日志格式），过滤器会默认放行，可能导致仍有噪声产生。  
4. **日志级别不一致**：`create_uvicorn_log_config` 默认使用 `log_level` 参数的 **大写** 形式，如果用户在 CLI 传入非标准值（如 `Info`），可能导致 uvicorn 报错或使用默认级别。  
5. **对 `disable_uvicorn_access_log` 的交互**：若同时开启 `--disable-uvicorn-access-log`（全部关闭）和 `--disable-access-log-for-endpoints`，两者的先后顺序未显式说明，可能导致配置冲突或误导用户。  

**💡 关注建议**  

1. **文档与提示**  
   - 在 CLI `--help` 中补充示例与注意事项，明确提示“仅过滤 uvicorn.access 日志，业务日志不受影响”。  
   - 强调 “路径匹配为完整路径，查询参数会被忽略”。  

2. **配置合并策略**  
   - 若检测到用户提供的 `log_config_file`，在加载后自动检查是否已注入 `access_log_filter`，若未注入可给出警告或自动合并，以防用户误以为过滤生效。  

3. **增强健壮性**  
   - 在 `UvicornAccessLogFilter.filter` 中加入对 `record.args` 非元组或长度不足的防御性日志（如 `logger.debug("Malformed uvicorn access log args")`），避免因异常格式导致异常抛出。  
   - 将路径匹配改为支持前缀或通配符（如 `fnmatch`），并通过配置开关保持向后兼容，可提升灵活性。  

4. **测试覆盖**  
   - 已加入丰富单元测试，建议再增加：  
     - 与自定义 `log_config_file` 共存的集成测试。  
     - 大写/小写 `log_level` 参数兼容性测试。  
     - 多线程/并发请求场景下过滤器的线程安全验证（当前仅使用集合，只读操作，安全，但可加上显式注释）。  

5. **监控与审计**  
   - 对于生产环境，建议在监控平台上新增 “被过滤的 access log 条数” 指标（可在 filter 中计数），帮助运维评估过滤效果与潜在误删风险。  

通过上述评估，整体变更对系统架构的侵入极小，性能提升显著，且安全风险可控。建议在正式发布前完成配置合并检测及更完整的兼容性测试，以确保用户在自定义日志配置时仍能获得预期的过滤行为。

---

#### 🟡 中重要度变更 (15)

### Support heterogeneous NemotronHPuzzle model (#32549)
**SHA**: `83fb2d0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/83fb2d09e8f6f70e99c68521b5a0218119cf5cfb)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交为 *NemotronH* 系列新增 “Puzzle” 异构模型的支持。核心改动包括：在模型注册表中加入 `NemotronHPuzzleForCausalLM`，在配置校验中复用 `NemotronHForCausalLMConfig`；在 `nemotron_h.py` 中实现层级化配置读取、可选 sliding‑window 参数、以及在 MoE 权重映射时依据所有层的 `n_routed_experts` 取最大的值；在模型架构转换工具中新增 `get_num_experts_from_block_configs` 用于从 `block_configs` 中解析专家数量。测试用例也更新以声明该模型不在线可用。  

**🎯 影响范围**：  
- `vllm/model_executor/models/registry.py`（模型映射）  
- `vllm/model_executor/models/config.py`（配置校验）  
- `vllm/model_executor/models/nemotron_h.py`（层级化配置、注意力、MoE 权重加载）  
- `vllm/transformers_utils/model_arch_config_convertor.py`（专家数解析）  
- `tests/models/registry.py`（新增测试入口）  

**💡 关注建议**：  
1. **模型配置**：使用 Puzzle 模型时请在 HF 配置对象里实现 `get_nemotron_h_config_for_layer`，或直接在 `block_configs` 中声明每层的 `n_routed_experts` 与 `sliding_window`。  
2. **权重加载**：`_get_max_n_routed_experts` 会取所有层的最大专家数，确保所有 MoE 权重被读取，但可能导致显存峰值高于单层配置的预期，需在部署前评估。  
3. **兼容性**：对非异构 NemotronH 仍保持向后兼容，因默认使用完整配置；若旧版配置缺少 `block_configs`，会回退到原有属性。  
4. **测试覆盖**：新增的注册条目标记为 `is_available_online=False`，确保 CI 不尝试下载模型；若未来上线，请同步更新。  
5. **文档 & 示例**：建议在 README 或模型使用指南中说明异构模型的配置方式（per‑layer config、sliding window、专家数上限），避免使用者因配置缺失导致运行时错误。  

整体而言，改动为 Puzzle 异构模型提供了灵活的层级配置路径，提升了 vLLM 对多样化 MoE 结构的兼容性，需关注显存占用与配置完整性。

---

### [LoRA][Spec Decode] Support LoRA for Nemotron-H MTP models (#32265)
**SHA**: `f3a5ee7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f3a5ee705fa9d655c34c476dfed45e55293ab4f9)

**核心变更**  
1. 在 `vllm/lora/lora_model.py` 新增 `skip_prefixes` 参数及 `_should_skip_module` 静态方法，实现基于模块名前缀的过滤。  
2. LoRA 加载流程（`from_lora_tensors`、`from_local_checkpoint`、`check_unexpected_modules`）全部接入该过滤逻辑，避免把模型内部不参与推理的层（如 Nemotron‑H 的 MTP 层）加载进来。  
3. `vllm/lora/worker_manager.py` 在创建 LoRA 实例时读取模型属性 `lora_skip_prefixes` 并传递给 `LoRAModel`。  
4. `SupportsLoRA` 协议新增 `lora_skip_prefixes` ClassVar，默认空列表。  
5. 在 `NemotronHForCausalLM` 中声明 `lora_skip_prefixes = ["mtp."]`，实现对 MTP 层的专门跳过。

**影响范围**  
- LoRA 相关模块 (`lora_model.py`, `worker_manager.py`)  
- 所有实现 `SupportsLoRA` 协议的模型，尤其 Nemotron‑H 系列  
- 可能影响自定义模型的 LoRA 加载逻辑（如自行实现 `lora_skip_prefixes`）。

**关注建议**  
- **兼容性**：新增参数默认 `None`，对已有调用保持向后兼容；但若已有模型中误用了与前缀冲突的名称，可能导致合法层被误删，建议在文档中明确前缀匹配规则。  
- **测试**：新增单元测试，覆盖：① 正常加载全部 LoRA 权重；② `skip_prefixes` 生效后对应模块被跳过；③ 多前缀、前缀包含点的情况。  
- **文档**：在 LoRA 使用说明里加入 `lora_skip_prefixes` 配置的使用场景及示例，特别说明对 MTP、Mixture‑of‑Experts 等结构的必要性。  
- **实现细节**：`_should_skip_module` 只检查 `".{prefix}"` 或 `startswith`，如果模块名中出现相同子串但不是前缀（如 `some_mtp_layer`），可能误判。可以考虑使用 `module_name.split('.')` 的前几段进行更严格匹配。  
- **性能**：过滤逻辑非常轻量，对加载时延几乎无影响，反而可显著降低内存占用和初始化时间。  

**对开发者**：在自定义模型实现 `SupportsLoRA` 时，若模型拥有不参与推理的子模块（MTP、MoE 等），请通过 `lora_skip_prefixes` 明确声明，以避免无效权重占用显存。  
**对用户**：加载使用 Nemotron‑H LoRA 适配器时不需要额外操作，框架已自动跳过 MTP 层；若自行调优或迁移其他模型，请确认是否需要自定义前缀过滤。

---

### [Frontend] Cleanup api server (#33158)
**SHA**: `7cbbca9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7cbbca9aaa5fd5c42efd22994b14e7b3336fe3da)

**🎯 变更类型**：重构 & 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原 `api_server.py` 中大量繁杂的启动、异常处理、日志、身份认证等代码抽离至新模块 `server_utils.py`，并拆分出 `basic/api_router.py` 负责最小化的公共路由（/load、/version 等）。`api_server` 只保留构建 FastAPI 实例的逻辑，统一使用 `lifespan`、`log_response`、`AuthenticationMiddleware`、`XRequestIdMiddleware` 等实现。SageMaker 路由入口改为引用新的 `basic.api_router.base`，并提供 `sagemaker_standards_bootstrap` 包装函数。

**🎯 影响范围**  
- `vllm/entrypoints/openai/api_server.py`（核心入口）  
- 新增 `vllm/entrypoints/openai/server_utils.py`（异常、日志、生命周期、middleware、uvicorn 配置）  
- 新增 `vllm/entrypoints/openai/basic/api_router.py`（公共路由）  
- `vllm/entrypoints/sagemaker/api_router.py`（引用路径调整）  
- 相关 imports 在其他模块（如 `serve`, `elastic_ep`）中可能仍指向旧实现。

**💡 关注建议**  
1. **兼容性检查**：确认项目中仍有直接 `from vllm.entrypoints.openai.api_server import …` 的代码已更新为 `server_utils` 或 `basic.api_router`，避免 `ImportError`。  
2. **日志配置**：`get_uvicorn_log_config` 与 `load_log_config` 迁至 `server_utils`，若外部调用旧函数，需要同步更新。  
3. **GC 相关**：`freeze_gc_heap` 现在在 `server_utils.lifespan` 中调用，确保 `vllm.utils.gc_utils` 仍在依赖列表，否则启动时可能报错。  
4. **中间件顺序**：`AuthenticationMiddleware`、`XRequestIdMiddleware` 与 `log_response` 的添加顺序与原实现保持一致，否则会影响请求 ID、访问日志或鉴权。  
5. **单元/集成测试**：重点跑包含 `/load`、`/version`、SageMaker `/invocations` 的接口测试，检查异常返回结构（`ErrorResponse`）是否与旧版本保持兼容。  
6. **文档更新**：API 文档（Swagger）已被 `disable_fastapi_docs` 控制，若用户依赖默认文档，需要在新版说明中提示对应开关。  

总体而言，此次重构提升代码可维护性和复用度，但需确保所有旧入口已迁移并通过回归测试后再正式发布。

---

### [Metrics] [KVConnector] Add Offloading Connector metrics (#27942)
**SHA**: `7227d06` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7227d06156e12b6f6ac2d04f34d8316d2f60e0f5)

**变更核心**  
1️⃣ 为 **OffloadingConnector** 引入完整的 KV‑offload 统计体系：新增 `OffloadingConnectorStats`、`OffloadingOperationMetrics`，实现 `reset/aggregate/reduce/is_empty/record_transfer`。  
2️⃣ 扩展 Prometheus 埋点：`OffloadPromMetrics` 记录每次转移的字节数、耗时以及 size‑histogram，支持 per‑engine 标签。  
3️⃣ 将 `TransferResult` 从 `(job_id, bool)` 扩展为 dataclass，携带 `transfer_size、transfer_time、transfer_type`，并在 `cpu_gpu` 工作线程中实际测量并填充这些字段。  
4️⃣ `OffloadingConnector` 与调度层加入获取/汇总统计的接口 (`get_kv_connector_stats`, `build_kv_connector_stats` 等)。  
5️⃣ 大量单元测试覆盖 stats 的构造、聚合、reduce、reset 以及新返回结构的正确性。

**影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`、`metrics.py`、`worker/cpu_gpu.py`、`worker/worker.py`  
- 相关的 KV‑offload 调度与统计抽象层  
- Prometheus 监控端点及 `StatLogger` 输出  
- 所有使用 OffloadingConnector 的推理进程（GPU‑CPU、CPU‑GPU）  

**关注建议**  
- **兼容性**：旧版代码仍可能期待 `(int, bool)` 元组，务必检查所有调用点已迁移到 `TransferResult`。  
- **性能**：新增 CUDA event 计时和统计聚合会有微小开销，建议在生产环境监测 CPU/GPU 利用率，必要时通过配置关闭 stats。  
- **监控**：Prometheus label `transfer_type` 采用大写 `"GPU","CPU"`，与历史 metric 区分，需在监控仪表盘同步更新。  
- **序列化**：`KVConnectorStats.reduce` 只返回合计值，若后续需要平均/分位数，请在 `OffloadPromMetrics.observe` 中自行计算。  
- **文档**：补全 metrics 说明以及 `TransferResult` 字段含义，给用户提供查询示例（Prometheus、StatLogger）。  

> 开发者在提交新代码前，请运行新增的 `tests/v1/kv_connector/unit/test_offloading_connector.py` 与 `tests/v1/kv_offload/test_cpu_gpu.py`，确保统计、计时均能恢复并正确聚合。用户在升级后，可通过 `vllm --metrics` 查看新指标，若不需要可在配置中关闭 stats 收集。

---

### Fix weight mapping test for Transfomers v5 (#33162)
**SHA**: `14385c8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/14385c80fc78c92e789b12fd4cbd1be2dc5df653)

**🎯 变更类型**：Bug 修复 / 兼容性增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Transformers 5.x 中 `tie_word_embeddings` 可能缺失或在 meta 设备上未自动绑定的情况添加容错处理。测试中在 meta 设备创建模型后显式调用 `model.tie_weights()`，并在运行时通过 `getattr(..., "tie_word_embeddings", False)` 安全访问该属性，避免 AttributeError。  

**🎯 影响范围**：  
- `tests/models/multimodal/test_mapping.py`（新增模型权重绑定逻辑）  
- `vllm/model_executor/models/transformers/base.py`（pipeline‑parallel 分支检查改为安全读取）  
- `vllm/model_executor/models/transformers/causal.py`（加载权重时跳过 `lm_head`、权重绑定改为使用安全变量）  

**💡 关注建议**：  
1. 检查项目中其他直接使用 `config.tie_word_embeddings` 的位置，统一改为 `getattr(..., "tie_word_embeddings", False)`，防止未来升级导致崩溃。  
2. 当 Transformers 正式在 meta 设备上实现正确的权重绑定后，可删除测试里的手动 `model.tie_weights()`，并相应清理 `TODO`。  
3. 运行全套单元测试，特别是多卡 pipeline‑parallel 场景，确认 `lm_head` 权重仍被正确共享且不会被重复加载。  
4. 留意 Transformers 的后续发布说明，若 `tie_word_embeddings` 行为再次变化，需相应调整 vLLM 的权重映射逻辑。

---

### [AMD][QWEN3-NEXT] FP8 Tunings (#32042)
**SHA**: `da8d0c4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/da8d0c441aadcf30dba787d31746c38e5648b70c)

**🎯 变更类型**：功能增强（为 AMD Instinct MI300X/ROCm 加入 FP8 w8a8 量化内核的调优配置）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. `benchmark_w8a8_block_fp8.py` 的平台检测从 “只能 CUDA” 放宽为 “CUDA 或 ROCm”。  
2. 在 `vllm/model_executor/layers/quantization/utils/configs/` 下新增 5 组 JSON 配置文件，覆盖 N∈{1024,12288,2048,9216,2048}、K∈{2048,4096} 的多种块大小，专为 `device_name=AMD_Instinct_MI300X、dtype=fp8_w8a8、block_shape=[128,128]` 设计。  

**🎯 影响范围**  
- **Benchmark 子模块**：跑 FP8 w8a8 基准时可在 ROCm 环境直接执行。  
- **量化配置加载**：`vllm/model_executor/layers/quantization/utils` 会在运行时读取上述 JSON，影响 `FP8W8A8` 版 MatMul 的调度参数（BLOCK_SIZE、GROUP_SIZE、warps、stages）。  
- **文档/CI**：若 CI 未覆盖 ROCm，可能遗漏新配置的有效性检查。  

**💡 关注建议**  
- **平台兼容**：确认其他代码路径（如 kernel 编译、`torch.cuda`‑风格的调用）均已抽象为 “is_cuda or is_rocm”。若仍硬编码 `cuda`，在 ROCm 上会抛异常。  
- **配置文件命名**：文件名中包含逗号、等号等字符，确保 `ConfigLoader` 使用 `os.path.join` 而非手动 split，防止路径解析错误。  
- **性能回归**：在 MI300X 上跑一次完整基准（`benchmark_w8a8_block_fp8.py`），对比 CUDA 结果，检查 `num_warps`、`num_stages` 是否在实际硬件上达到预期吞吐。  
- **文档更新**：在 README/Quantization 部分加入 “AMD/ROCm 支持” 章节，说明如何在 `vllm` 启动参数中指定 `device=rocm` 并使用 `fp8_w8a8`。  
- **回退机制**：若用户在 ROCm 环境下仍使用旧的 CUDA‑only 配置，系统应回退到默认调度而不是崩溃。可以在加载配置前检测 `current_platform.is_rocm()`。  

整体来看，此次提交为 AMD Instinct MI300X 引入了针对 FP8 w8a8 的细粒度调优，实现了跨平台兼容，需在测试、文档以及潜在硬编码路径上做好收尾。

---

### [Frontend] Cleanup serving engine (#33103)
**SHA**: `e0b005d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e0b005d9cf9aff4d2f45d9bc9de401c807544e03)

**🎯 变更类型**：重构 / 清理  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原先在各入口类中实现的 `_get_prompt_components` 抽取为公共函数 `vllm.inputs.parse.get_prompt_components`，统一调用方式。  
- `get_max_tokens` 从只接受 `input_length` 改为直接接受完整的 `Prompt`（`TokensPrompt` / `EmbedsPrompt`），内部使用 `length_from_prompt_token_ids_or_embeds` 计算长度，并兼容 `ChatCompletionRequest`、`CompletionRequest`、`ResponsesRequest`。  
- 删除了多处冗余的 `self.default_sampling_params` 初始化，相关逻辑迁移到 `utils.get_max_tokens`。  
- 在工具调用、response 生成等路径上同步更新 `max_tokens` 的计算方式，保证与新函数签名一致。

**🎯 影响范围**  
- `vllm.entrypoints.openai.chat_completion.serving`、`completion.serving`、`engine.serving`、`responses.serving`  
- `vllm.entrypoints.utils`（新增 `get_max_tokens` 逻辑）  
- `vllm.inputs.parse` 与 `vllm.utils.length_from_prompt_token_ids_or_embeds`  

**💡 关注建议**  
1. **兼容性**：若项目中有自定义的子类覆盖 `_get_prompt_components`，需改为调用公共函数或保持同名方法兼容。  
2. **单元测试**：运行完整的 CI，尤其是涉及 `max_tokens` 计算、工具调用和流式响应的测试，用以捕捉因签名变化导致的遗漏。  
3. **文档更新**：在 OpenAI 接口文档中说明 `max_tokens` 的默认行为已统一到 `utils.get_max_tokens`，并列出对 `ResponsesRequest` 的新支持。  
4. **性能检查**：新实现通过一次性 `length_from_prompt_token_ids_or_embeds` 计算长度，理论上比原来多次手动 `len()` 更高效，建议在大批量请求场景下做基准对比，确保没有回归。  

总体而言，此次改动提升了代码复用度和可维护性，但需确保所有入口路径都已迁移到新函数，以避免运行时 `TypeError`。如果项目中有外部插件直接调用旧的私有方法，请尽早适配。

---

### [Frontend] Reduce mixin usage in serving pooling (#33101)
**SHA**: `c831911` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c831911be29179bdc5042428556e6a43d191d169)

**🎯 变更类型**：重构 / 代码清理  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 OpenAI Serving 层大幅削减了 Mixin 的使用，将原先的 `RequestProcessingMixin`、`ResponseGenerationMixin` 合并进统一的 `ServeContext`，并为分类、embedding 两类服务各自声明了专属的 `ServeContext`（`ClassificationServeContext`、`EmbeddingServeContext`）。相应地，分类 (`ServingClassification`) 与向量化 (`OpenAIServingEmbedding`) 的实现从继承 Mixin 改为直接继承 `OpenAIServing`，并在预处理、响应构造等关键路径上移除了 `@override`、冗余 `cast`，统一了类型签名与错误返回。  
同时，响应类型扩展了 `EmbeddingBytesResponse`，并对 chunk‑processing、Pooling 参数创建等细节做了更严格的类型约束和错误检查。

**🎯 影响范围**  
- `vllm/entrypoints/openai/engine/serving.py`：ServeContext 重构、响应类型合并。  
- `vllm/entrypoints/pooling/classify/serving.py`：分类服务实现改为 `ServingClassification`，去除旧 Mixin。  
- `vllm/entrypoints/pooling/embed/serving.py`：embedding 服务实现改为 `OpenAIServingEmbedding`，同样去除 Mixin，调整了 chunk 处理逻辑。  
- 相关协议/类型文件（`protocol.py`）的导入路径也随之更新。  

**💡 关注建议**  
1. **兼容性检查**：外部代码若直接引用旧的 `ClassificationServeContext`、`EmbeddingServeContext` 或 Mixin 类（如 `RequestProcessingMixin`）需要更新为新的 `ServeContext` 子类或直接使用 `OpenAIServing`。  
2. **单元/集成测试**：重点跑通分类 (`/v1/classifications`) 与 embedding (`/v1/embeddings`) 接口的完整路径，验证 chunk‑processing、聊天模板、LoRA 适配等场景仍保持预期行为。  
3. **性能监控**：虽然 Mixin 删除本质上不影响运行时性能，但重构后的生成器类型和 `final_res_batch` 结构变化可能影响内存占用，建议在高并发场景下观察 GC 与 latency。  
4. **错误处理**：新增的 `self.create_error_response` 调用在多个分支统一返回，确保返回的 HTTP 状态码与旧实现保持一致，防止 API 调用方误判。  
5. **文档与示例更新**：在开发者文档中注明 `request_id_prefix`、`chat_template` 参数的来源已从 Mixin 移至服务实例，提供相应的初始化代码示例。  

通过上述检查与验证，可确保本次大幅度的结构简化不会引入回归，同时为后续功能扩展提供更清晰的代码基线。

---

### [Model Runner V2] Remove UvaBufferPool for cpu->gpu copy (#33055)
**SHA**: `6d86fde` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6d86fde09c8b8ef10f3b085a1be93ecf6b7c0411)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 将原来的 **UvaBufferPool**（基于 UVA 的 CPU‑>GPU 零拷贝缓冲池）全部剔除。  
2. 新增通用函数 **async_copy_to_gpu**，在需要时将 CPU（普通）张量或 `numpy` 数组 **pin‑memory** 后以 `non_blocking=True` 的方式复制到目标 GPU 张量。  
3. 相关调用点（`encoder_runner.py`、`model_runner.py`）改用 `async_copy_to_gpu` 或直接 `tensor.to(..., non_blocking=True)`，并把 `pin_memory` 参数统一为 `True`。

**🎯 影响范围**  
- `vllm/v1/worker/gpu/buffer_utils.py`（新增函数）  
- `vllm/v1/worker/gpu/mm/encoder_runner.py`（去除 `UvaBufferPool.tmp_is_mm_embed`，改为 `torch.to`）  
- `vllm/v1/worker/gpu/model_runner.py`（删除三个 `UvaBufferPool` 成员，改为 `async_copy_to_gpu`）  

**💡 关注建议**  

1. **功能等价性**：`UvaBufferPool` 通过统一的 pinned‑memory 缓冲区实现多次复用，新的实现每次都会 `pin_memory()` 并立刻 `copy_`. 在高并发、频繁调度的场景下可能产生额外的 pin / unpin 开销。建议在压力测试中对比两者的吞吐和延迟，确保没有显著回退。  

2. **内存泄漏风险**：`torch.from_numpy` 创建的张量共享原始 `numpy` 内存，随后 `pin_memory()` 会产生临时 pinned 缓冲区。如果大量短生命周期的 `np.ndarray` 被复制，可能导致 pinned 内存累积。可在关键路径加入 `torch.cuda.empty_cache()` 或显式 `del tmp` 以帮助回收。  

3. **异常路径**：`async_copy_to_gpu` 现在假设输入始终是 CPU 张量且未 pinned。若外部已经传入 pinned 张量或 GPU 张量，会触发断言或异常。建议在调用前做更宽松检查或补充文档说明。  

4. **多模态编码器**：`encoder_runner` 中把 `is_mm_embed` 的 `pin_memory` 改为 `True` 并直接 `to(..., non_blocking=True)`，这与之前的 copy‑to‑gpu 行为保持一致，但需要确认 `torch.bool` 张量的复制在不同显卡上仍能保持非阻塞。  

5. **向后兼容**：删除 `UvaBufferPool` 相关属性后，任何仍引用 `self.tmp_*` 的旧插件或自定义代码都会报 AttributeError。更新文档并在发布说明中明确该 API 已废弃。  

6. **测试覆盖**：  
   - 单元测试：验证 `async_copy_to_gpu(np_array, device)` 与 `torch.tensor(np_array).pin_memory().to(device, non_blocking=True)` 的数值一致性。  
   - 性能基准：在多请求并发、不同 batch 大小下测量 CPU‑>GPU 复制时延，确保新实现不劣于旧实现超过 5%。  
   - 内存压力：使用 `torch.cuda.memory_summary()` 检查 pinned 内存峰值是否出现异常增长。  

总体来看，此次重构简化了代码路径，去除了 UVA 依赖，降低了维护成本。但需重点关注 pinned‑memory 的频繁分配带来的性能和内存波动，在正式发布前完成上述验证。

---

### [Bugfix][MXFP4] Call `trtllm_fp4_block_scale_moe` with kwargs (#33104)
**SHA**: `8caffd9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8caffd92dfb011733b3c71c76756a687f64eb3ae)

**🎯 变更类型**：Bug 修复（提升 API 调用安全性）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/model_executor/layers/quantization/mxfp4.py` 中，将对底层函数 `trtllm_fp4_block_scale_moe` 的调用由位置参数改为关键字参数。这样可以防止参数顺序错误、提升可读性，并保证在函数签名变化时不会导致运行时异常。  

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/mxfp4.py`（MXFP4 量化层）  
- 可能间接影响使用 MXFP4 量化的模型执行路径、单元测试以及任何直接或间接依赖该层的自定义插件。  

**💡 关注建议**  
1. **功能验证**：运行 MXFP4 相关的单元/集成测试，确保数值结果与改动前保持一致，尤其是 `router_logits` 的 `bfloat16` 转换和 `routing_method_type` 的取值。  
2. **兼容性检查**：搜索仓库中是否还有对 `trtllm_fp4_block_scale_moe` 的位置参数调用，若有需同步改为关键字调用或更新函数签名。  
3. **文档/注释**：在代码或函数注释中标明关键字参数含义，防止后续维护时产生混淆。  
4. **性能监控**：虽然改动本身不影响算子实现，但建议对比改动前后的吞吐量，确认没有引入隐藏的复制或类型转换开销。  

整体来看，此次改动提升了代码的可维护性和安全性，只要通过现有测试即可放心合并。

---

### [fix] CPUDNNLGEMMHandler pointer baked into inductor artifact (#32913)
**SHA**: `58a05b0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/58a05b0ca184048e11ea1a54759732ad634ce34a)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
将 `CPUDNNLGEMMHandler` 中的原生指针（`int64_t`）封装为 `torch.Tensor`，并相应修改 C++ kernel（`csrc/cpu/dnnl_kernels.cpp`）以及 Python 绑定（`csrc/cpu/torch_bindings.cpp`、`vllm/_custom_ops.py`）的签名和实现，防止指针在 TorchInductor 编译阶段被常量折叠。另在 `vllm/envs.py` 中简化 AOT 编译默认值的判断，去除对 CPU 平台的特殊限制。

**🎯 影响范围**  
- **CPU oneDNN GEMM 实现**：`dnnl_kernels.cpp`、`torch_bindings.cpp`  
- **Python 自定义算子层**：`_custom_ops.py` 中 `CPUDNNLGEMMHandler`、`create_onednn_*`、`*_mm` 调用  
- **运行时配置**：`envs.py` 中 AOT compile 开关默认值逻辑  

**💡 关注建议**  
1. 确保 `handler_tensor` 始终是 CPU 上的标量张量（`dtype=int64`），否则 `.item()` 可能触发设备同步或报错。  
2. 在 C++ 中使用 `handler_tensor.item<int64_t>()` 前加入 `TORCH_CHECK(handler_tensor.is_cpu() && handler_tensor.numel() == 1)` 以捕获错误使用。  
3. 释放时仍需调用 `torch.ops._C.release_dnnl_matmul_handler(handler_tensor.item())`，避免内存泄漏。  
4. 迁移后，搜索项目中所有 `*_handler`（int）调用，确保已全部改为张量形式，防止残留编译错误。  
5. 由于去掉了对 CPU 平台的 AOT 编译限制，建议在 CI 中加入针对 CPU 环境的性能/兼容性回归测试，防止意外开启 AOT 编译导致的错误。  

总体而言，此次改动提升了 OneDNN GEMM handler 在 TorchInductor 中的可携带性，风险主要在张量类型与设备一致性的校验上。建议在发布前加入相应单元/集成测试。

---

### [Refactor] Remove unused `_moe_permute` function (#33108)
**SHA**: `8f98788` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8f987883cbc0a562f253fda79783b0f22179e5c6)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 删除 `vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py` 中未使用的私有函数 `_moe_permute` 与 `_moe_unpermute_and_reduce`（约 64 行代码）。  
- 同步更新 `benchmarks/kernels/benchmark_moe_permute_unpermute.py`：去除 `use_customized_permute` 参数，直接调用公开的 `moe_permute` / `moe_unpermute`，简化准备与运行逻辑，删除多余的中间变量。  

**🎯 影响范围**  
- **核心层**：`vllm.model_executor.layers.fused_moe.moe_permute_unpermute`（私有实现被清理）。  
- **基准测试**：`benchmarks/kernels/benchmark_moe_permute_unpermute.py`（命令行参数、返回结构改动）。  
- **依赖**：仅内部调用 `_moe_permute` 的旧代码会报错，但项目中没有其他引用，属于死代码。  

**💡 关注建议**  
1. **兼容性检查**：确保外部插件或用户脚本未显式导入 `_moe_permute`／`_moe_unpermute_and_reduce`，如有需改为调用 `moe_permute`/`moe_unpermute`。  
2. **文档同步**：更新 README / API 文档，移除关于 “customized permute” 的说明，并说明 `moe_permute` 已统一实现。  
3. **基准脚本**：修改 CI/benchmark 调用，去除已删除的 `--use-customized-permute` 参数，以免脚本启动失败。  
4. **测试覆盖**：运行完整单元/集成测试，确认 `moe_permute`/`moe_unpermute` 在 FP8、INT8 场景下行为不变。  

整体来看，此次提交仅裁剪死代码、精简基准脚本，对功能没有影响，风险低。持续关注后续合并是否仍有残留的私有实现引用。

---

### [ci] Sync test areas with test-pipeline.yaml and enable new pipeline generator (#33080)
**SHA**: `ebe0ba9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ebe0ba91db7ebc2bcefb7f8d72a0825de1fd9241)

**🎯 变更类型**：功能增强 / CI 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 CI 配置同步到 `test‑pipeline.yaml`，新建 `hardware_tests` 目录并为 AMD、Arm、Ascend NPU、GH200、Intel 系列硬件分别编写 Buildkite 步骤。  
- `image_build.sh` 大幅改写：加入构建器统一管理、缓存标签自动清理、镜像存在检查、CI 配置文件自动下载、日志信息丰富等；脚本入口改为只接受 `IMAGE_TAG`（或 `IMAGE_TAG_LATEST`）。  
- 大量 YAML 中原 `gpu`/`num_gpus` 字段改为统一的 `device`/`num_devices`，并对部分测试做了软失败、可选、插件等标记。  
- 更新量子化、量化 MoE、Kernels、LM‑Eval、插件等子模块的硬件指定，统一使用新字段。  

**🎯 影响范围**  
- **CI 基础设施**：`.buildkite/*` 目录全部受影响，新的硬件测试会在每次 CI 中启动；旧的镜像构建脚本被替换，构建缓存策略和日志更改。  
- **测试矩阵**：几乎所有测试区域（attention、compile、distributed、e2e、kernels、lm_eval、quantization、weight_loading 等）均被改写为 `device`/`num_devices`，若自定义脚本仍使用旧字段会失效。  
- **开发者本地调试**：`image_build.sh` 现在需要 `set -euo pipefail`，并依赖 `docker buildx`、`curl`、AWS 元数据服务等，未在 AWS 环境下运行可能缺少实例信息。  

**💡 关注建议**  
1. **CI 维护**：确认所有自定义 Buildkite 插件或脚本已同步到新 `device`/`num_devices` 名称，否则会出现 “unknown field” 错误。  
2. **缓存标签**：`clean_docker_tag` 会截断并替换非法字符，确保分支名、PR 编号符合此规则，避免因标签冲突导致缓存失效。  
3. **镜像构建**：若项目已有手动镜像构建流程，需要改为调用 `image_build.sh <...> <IMAGE_TAG>`（或 `IMAGE_TAG_LATEST`），否则会跳过构建。  
4. **硬件依赖**：新增的 AMD/Arm/Ascend/Intel/GH200 测试在非对应硬件机器上会被标记为 `soft_fail`，但仍会占用 CI 资源，建议在 CI 配置中适当限制并发数。  
5. **本地调试**：在非 AWS 环境运行 `image_build.sh` 时，实例信息获取会返回 “Not running on EC2”，不影响功能；若依赖 `BUILDKIT_SOCKET`，请确保本地已启动 `buildkitd` 或使用默认 `docker‑container` 驱动。  

> **总体评价**：本次改动显著提升了 CI 的可维护性与硬件覆盖，统一了硬件描述字段并加入更健壮的镜像构建逻辑。开发者在本地或自定义 CI 中需同步字段与脚本参数，以避免构建/测试中断。

---

### [Model] Bump transformers version for test registry (#33100)
**SHA**: `c25dbee` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c25dbee40dda0803e24482edc70f3c1bda39a060)

**🎯 变更类型**：功能增强 / 兼容性升级  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交将项目中所有针对 `transformers` 5.0.0.dev* 版本的最低要求统一提升至正式版 `5.0.0`（部分提升至 `5.1.0`），并相应更新了测试用例、模型注册表以及内部版本检查逻辑。  

**🎯 影响范围**  
- `tests/models/registry.py`、`tests/models/test_transformers.py`、`tests/v1/e2e/test_spec_decode.py`：注册表和跳过条件的版本阈值调整。  
- `vllm/model_executor/models/transformers/` 目录下的 `base.py`、`moe.py`：内部 `check_version` 调用从 `5.0.0.dev0` 改为 `5.0.0`，确保 encoder、Eagle3、MoE 等特性在正式版 Transformers 中才被激活。  
- `vllm/transformers_utils/config.py`：配置兼容性判断从 `<5.0.0.dev0` 改为 `<5.0.0`。  

**💡 关注建议**  
1. **依赖升级**：确保部署环境已升级到 `transformers>=5.0.0`（或 `5.1.0` 对应模型），否则相关测试会被跳过或报错。  
2. **回归验证**：在升级后运行完整的单元/集成测试，尤其是涉及 MoE、Eagle3、Encoder‑only 模型的路径，确认功能仍然正常。  
3. **文档同步**：更新 README/CHANGELOG 中对最低 `transformers` 版本的说明，防止用户在旧版本环境下误使用。  
4. **兼容性监控**：关注 `transformers` 未来的 minor/patch 变动，若出现 API 回退，及时在 `check_version` 中调整对应的最小版本。  

整体来说，此次改动是一次向正式版 `transformers` 的兼容性迁移，风险主要在于依赖未同步升级的环境，建议在 CI/CD 流程中加入 `transformers` 版本校验。

---

### [FIX] Always support TP > 4 for FP4 Gemm (#31099)
**SHA**: `67fe677` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/67fe677c53e7a5f5d27a4f07edd66f44bea0495c)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交为 FP4 GemM 添加了对 **Tensor‑Parallel > 4** 的通用支持。核心做法是：在使用 Cutlass/FlashInfer 后端时，对 packed NVFP4 权重与激活进行 32‑对齐的 padding（行 N 与列 K），并在前向结束后将多余的 N‑维度切回原尺寸。为此在 `quant_utils.py` 新增了对齐、padding 与切片的工具函数，并在权重加载 (`process_weights_after_loading`) 与矩阵乘法前置处理 (`apply_weights`) 中分别调用。

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py`  
- `vllm/model_executor/layers/quantization/modelopt.py`（FP4 Quantizer）  
- `vllm/model_executor/layers/quantization/utils/quant_utils.py`（新工具函数）  
- 依赖 Cutlass / FlashInfer FP4 kernel 的模型推理路径（包括 TP > 4 场景）

**💡 关注建议**  

1. **兼容性校验**：`pad_nvfp4_weight_for_cutlass` 只在 `backend in ("cutlass", "flashinfer-cutlass")` 时触发，确保其它后端（如 `fbgemm`、原生 CUDA）不受意外 padding 影响；可以在函数入口加断言或日志提醒。  

2. **属性一致性**：`layer.weights_padding_cols` 与 `layer.output_size_per_partition` 在所有 FP4 层实现中均已定义，否则在后续调用 `slice_nvfp4_output` 时会抛出属性错误。建议在基类或统一注册点添加默认值（0）。  

3. **内存与性能**：对权重和激活的 padding 会额外占用 ≤31 列的 4‑bit 存储（≈1 byte/列），在大模型、TP > 4 场景下仍可接受，但建议在文档中注明额外的显存开销。  

4. **单元/集成测试**：  
   - 覆盖 TP 1、2、4、8、16 的 FP4 模型推理，验证输出与未 padding 前一致（误差在数值容限内）。  
   - 对比 `cutlass` 与 `flashinfer-cutlass` 两个后端的执行时间，确认 padding 后不会导致显著回退。  

5. **文档更新**：在量化章节说明 FP4 后端现在需要 32‑对齐的 K、N，说明在使用 `tensor_parallel_size > 4` 时无需手动调整。  

6. **错误处理**：`slice_nvfp4_output` 目前仅在最后一维不匹配时切片，若后续加入多维输出（如 2‑D KV 缓冲）需扩展该函数的维度检测逻辑。  

总体来看，改动聚焦在对齐 padding，解决了之前在 TP > 4 时切分不完整导致的非法矩阵尺寸问题。只要确保上述兼容性检查与测试覆盖到位，此补丁即可安全合入。

---

#### 🟢 低重要度变更 (14)

### [Metrics][MFU] Fix UnembedMetrics FLOP overcounting for prefill (#33045) (#33045)
**SHA**: `5ec4405` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5ec44056f7f1ce81a8cd089249449f2fd4256fdb)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `num_logits_tokens` 方法，改正 UnembedMetrics 在预填阶段对 LM head FLOPs 的统计错误，使预填仅计最后一个 token 的 logits，解码阶段仍计全部 token。相关 FLOP、读写字节计算均改用该方法。

---

### [Bugfix] Fix DeepseekV32 `AssertionError: num_kv_heads == 1` (#33090)
**SHA**: `492a798` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/492a7983dd71fe0869aa5f77dcd050db1e37e31c)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 DeepseekV32 KV 缓存形状检查中硬编码的 `num_kv_heads=4` 调整为 `1`，修复 “AssertionError: num_kv_heads == 1”。

---

### [Bugfix] Disable CG for Whisper+FA2 (#33164)
**SHA**: `1f3a2c2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1f3a2c2944f240ff043c3b7e01c4ae81461b09ed)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 FlashAttention2 后端新增对 CUDA 图的检测；若模型为 encoder‑decoder（如 Whisper）且使用 FlashAttention2，则因已知精度问题禁用 CUDA 图，并给出警告。这样避免了 #33091 中的错误。

---

### [AMD][Kernel][BugFix] Use correct scale in concat_and_cache_ds_mla_kernel when on gfx942 (#32976)
**SHA**: `58996f3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/58996f3589434d99c320e6ee2460a231135f9641)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正gfx942上Fp8缩放比例，新增 `kFp8ScaleDivisor` 常量区分 224 与 448，确保正确计算 tile 与 quant 缩放。

---

### [CI][Pooling] Stabilize ModernBERT test (#32909)
**SHA**: `6c00645` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6c006457123f802d78e0570471ee8ea2d2a87dfb)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 token‑classification 测试添加统一的随机种子 fixture 以保证可复现；对随机初始化的 ModernBERT 模型加入 `@flaky(reruns=3)` 重试标记并打印提示，减轻数值波动导致的偶发失败。

---

### [code clean] remove duplicate code (#33135)
**SHA**: `b781eea` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b781eeaa15c0d6daeb5cdb067bdcdf54d14c6fc1)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/v1/structured_output/__init__.py` 中删除未使用的 `reasoning_parser` 变量赋值，去除冗余代码以提升代码整洁度。

---

### [torch.compile] Stop assuming 32 bit indexing (#33113)
**SHA**: `3b8f0fe` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3b8f0fe59e67318c2c6bffb20eb54c6648082c04)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `DynamicShapesConfig.assume_32_bit_indexing` 默认值改为 `False`，并在注释中说明 `True` 需要 PyTorch 2.10+，以避免在 torch.compile 场景下错误假设 32‑bit 索引。

---

### [Perf] avoid duplicate mem_get_info() call in get_current_memory_usage (#33064)
**SHA**: `157caf5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/157caf511b5264c1b93610e5dcde04f70fd9d676)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/platforms/rocm.py` 中将两次 `torch.cuda.mem_get_info` 调用合并为一次，直接解包为 `free_mem, total_mem`，提升性能并避免重复查询。

---

### [DOC]: Add warning about max_num_batched_tokens and max_model_len when chunked prefill is disabled (#33109)
**SHA**: `0b53bec` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0b53bec60b9b41c9e16cefa2db367afb6a60628d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/configuration/optimization.md` 中新增警告，说明若关闭 chunked prefill，`max_num_batched_tokens` 必须大于 `max_model_len`，否则可能在服务器启动时崩溃。

---

### Fix IndexError with encoder-decoder models when using Custom Paged Attention (#33112)
**SHA**: `c568581` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c568581ff38af13924627fdd5c2cd9d9282fbe3d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在跨注意力场景下，若 `key`/`value` 为 `None` 会导致 IndexError。新增空值检查并使用已缓存的 KV，修复 ROCm 注意力实现及分页解码函数的相关错误。

---

### [Bugfix][TPU] Return a Default fp8 MoE Backend (#32908)
**SHA**: `510ed1e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/510ed1e8d3b1da814d3f2b516008b8a0bc2ee464)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fp8.py` 中引入 `current_platform`，当平台非 CUDA/ROCM 时不再抛 `NotImplementedError`，而返回默认的 `Fp8MoeBackend.NONE`，解决 TPU 上的 FP8 MoE 部署问题。

---

### [Bugfix] Fix Dtypes for Pynccl Wrapper (#33030)
**SHA**: `43a013c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/43a013c3a29194f7b88b1b611b3b0067592b8c67)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `pynccl_wrapper` 中新增对 `torch.float8_e4m3fn` 的 NCCL dtype 映射并更新枚举；在 `flashinfer_cutlass_prepare_finalize` 中对 `a1q_scale` 为 1‑byte 类型时转为 `torch.uint8`，防止类型错误。

---

### [Bugfix] Fix Voxtral streaming slot_mapping (#33073)
**SHA**: `19ab0f7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/19ab0f7ce56af548d54632cfc278aefa894d797e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 Whisper Causal Attention 添加自定义实现，使用 Builder 计算的 slot_mapping 替代原始映射，并在 forward 中手动更新 KV 缓存，修复 Voxtral 流式卡槽映射错误。

---

### Remove unused logic in `models/mistral.py` (#33095)
**SHA**: `d56afd4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d56afd45fd4efee581129c401613be356b95350d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `models/mistral.py` 中删除未使用的量化融合相关逻辑和变量，简化初始化代码，未影响功能。

---

