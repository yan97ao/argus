# 每日更新报告（2026-01-10）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-10 22:44:35 | Michael Goin | [Quant] Support MXFP4 W4A16 for compressed-tensors dense models (#31926) |
| 2026-01-10 17:52:53 | Jeremy Teboul | [Bugfix] Fix integer overflow in Gemma3n audio processing (#31657) |
| 2026-01-10 15:50:57 | Ning Xie | [Bugfix] fix offline chat output prompt (#32076) |
| 2026-01-10 15:11:03 | Cyrus Leung | [Benchmark][1/2] Generalize SLA criterion validation from binary flags to margins (#32075) |
| 2026-01-10 14:27:58 | Frelam | [Bugfix] fix encoder cache leak of waiting requests in scheduler to solve stuck in CPU scheduling (#31857) |
| 2026-01-10 13:06:44 | Lucas Wilkinson | [Misc] Delay deprecation of CommonAttentionMetadata properties (#32074) |
| 2026-01-10 13:02:35 | Andreas Karatzas | [ROCm][CI] Fix flaky `test_function_calling_with_stream` and reduce schema test examples (#32063) |
| 2026-01-10 12:54:13 | roikoren755 | Update modelopt KV cache quantization resolution to new scheme (#31895) |
| 2026-01-10 12:53:24 | Cyrus Leung | [Refactor] Separate sequence and token pooling types (#32026) |
| 2026-01-10 12:19:49 | maang | [Core] Refactor ColumnParallelLinear: remove unused parameter and optimize forward (#31939) |
| 2026-01-10 11:57:38 | Kevin McKay | [Bugfix][Hardware][AMD] Use dynamic WARP_SIZE in sampler vectorized_process (#31295) |
| 2026-01-10 11:18:37 | PatrykSaffer | Fuse RoPE and MLA KV-cache write (#25774) |
| 2026-01-10 11:18:05 | Akshat Shrivastava | feature/issac 0.2 (#31550) |
| 2026-01-10 11:01:38 | Lucas Kabela | [Misc][LLaMa4] Compile LLaMa Vision Encoder (#30709) |
| 2026-01-10 10:41:27 | Ning Xie | resolve pydantic error in startup benchmark (#31348) |
| 2026-01-10 10:39:22 | Kevin McKay | [Bugfix] Narrow broad exceptions in compilation backends (#31616) |
| 2026-01-10 10:10:47 | Micah Williamson | [CI] Allow Deprecated Quantization For LM Eval Tests (#32065) |
| 2026-01-10 08:46:11 | Wentao Ye | [Perf] Optimize async scheduling placeholder using empty (#32056) |
| 2026-01-10 08:28:57 | Russell Bryant | [Core] Use weights_only=True with torch.load (#32045) |
| 2026-01-10 08:27:15 | Matthew Bonanni | [2/N][Attention] Fix pre-commit errors (#32052) |
| 2026-01-10 07:30:38 | Lucas Kabela | [Misc][BE] Type coverage for vllm/compilation [2/3] (#31744) |
| 2026-01-10 07:09:34 | Nick Hill | [Misc] Enable async scheduling by default with spec decoding (#31998) |
| 2026-01-10 05:24:51 | zhrrr | [perf][async] support non cpu sync get logprob tensors for spec (#31336) |
| 2026-01-10 05:22:19 | Chendi.Xue | [NIXL] refine decoder side post process for heterogeneous BlockSize and kv_layout (#30275) |
| 2026-01-10 05:10:24 | Matthew Bonanni | [1/N][Attention] Restructure attention: move files (#31916) |
| 2026-01-10 05:00:57 | Andrew Xia | [responsesAPI] fix incomplete_messages for simple/parsable context (#31836) |
| 2026-01-10 04:49:27 | Lucas Wilkinson | [Quant] Make static quant support all group shapes (#30833) |
| 2026-01-10 04:03:02 | jiahanc | [fix] add cutedsl to global sf (#32001) |
| 2026-01-10 03:58:39 | Runkai Tao | Add unpermute-aware fused MoE path and small-batch fallback (#29354) |
| 2026-01-10 03:34:51 | Jeremy Teboul | [Fix] Introduce audio channels spec (#31595) |
| 2026-01-10 03:13:43 | Wentao Ye | [Perf] Optimize cutlass moe problem size calculation, 5.3% E2E Throughput improvement, 2.2% TTFT improvement (#31830) |
| 2026-01-10 03:09:02 | Wentao Ye | [Refactor] Remove numpy split in async scheduling (#32034) |
| 2026-01-10 03:03:57 | Andrew Xia | [Frontend][gpt-oss] Allow system message to overwrite model identity (#31737) |
| 2026-01-10 02:53:20 | Yifan Qiao | [Feat][Core] Support multiple KV cache groups in Hybrid KV Coordinator (#31707) |
| 2026-01-10 01:12:35 | Michael Goin | [UX] Add vLLM model inspection view (#29450) |
| 2026-01-10 00:21:11 | Shanshan Shen | [Doc] Add developer guide for CustomOp (#30886) |

### 📊 统计摘要
> 本日共 36 个提交 | 🔴高 3 | 🟡中 18 | 🟢低 15
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[Refactor] Separate sequence and token pooling types (#32...](#583a90e)
    - [[1/N][Attention] Restructure attention: move files (#31916)](#2612ba9)
    - [[Fix] Introduce audio channels spec (#31595)](#657e9c0)
  - [🟡 中重要度变更 (18)](#-🟡-中重要度变更-18)
    - [[Quant] Support MXFP4 W4A16 for compressed-tensors dense ...](#e6c6f2c)
    - [[Bugfix] Fix integer overflow in Gemma3n audio processing...](#07286ec)
    - [[Benchmark][1/2] Generalize SLA criterion validation from...](#5f2385a)
    - [Fuse RoPE and MLA KV-cache write (#25774)](#80fead8)
    - [feature/issac 0.2 (#31550)](#e45946b)
    - [[Misc][LLaMa4] Compile LLaMa Vision Encoder (#30709)](#ea6d067)
    - [[2/N][Attention] Fix pre-commit errors (#32052)](#0308901)
    - [[Misc][BE] Type coverage for vllm/compilation [2/3] (#31744)](#aaf4b70)
    - [[perf][async] support non cpu sync get logprob tensors fo...](#97ba96f)
    - [[NIXL] refine decoder side post process for heterogeneous...](#9457812)
    - [[responsesAPI] fix incomplete_messages for simple/parsabl...](#1f8b7c5)
    - [[Quant] Make static quant support all group shapes (#30833)](#0a0aa07)
    - [Add unpermute-aware fused MoE path and small-batch fallba...](#a4d5d66)
    - [[Perf] Optimize cutlass moe problem size calculation, 5.3...](#308feab)
    - [[Frontend][gpt-oss] Allow system message to overwrite mod...](#f32c629)
    - [[Feat][Core] Support multiple KV cache groups in Hybrid K...](#cd4a95e)
    - [[UX] Add vLLM model inspection view (#29450)](#d5ec6c0)
    - [[Doc] Add developer guide for CustomOp (#30886)](#08d954f)
  - [🟢 低重要度变更 (15)](#-🟢-低重要度变更-15)
    - [[Bugfix] fix offline chat output prompt (#32076)](#14fc7a6)
    - [[Bugfix] fix encoder cache leak of waiting requests in sc...](#a01a1c0)
    - [[Misc] Delay deprecation of CommonAttentionMetadata prope...](#da6709c)
    - [[ROCm][CI] Fix flaky `test_function_calling_with_stream` ...](#d83becd)
    - [Update modelopt KV cache quantization resolution to new s...](#0c96148)
    - [[Core] Refactor ColumnParallelLinear: remove unused param...](#52d4282)
    - [[Bugfix][Hardware][AMD] Use dynamic WARP_SIZE in sampler ...](#c60578d)
    - [resolve pydantic error in startup benchmark (#31348)](#abd9224)
    - [[Bugfix] Narrow broad exceptions in compilation backends ...](#4dc0d60)
    - [[CI] Allow Deprecated Quantization For LM Eval Tests (#32...](#ac0675f)
    - [[Perf] Optimize async scheduling placeholder using empty ...](#e18464a)
    - [[Core] Use weights_only=True with torch.load (#32045)](#1963245)
    - [[Misc] Enable async scheduling by default with spec decod...](#3adffd5)
    - [[fix] add cutedsl to global sf (#32001)](#f9e2a75)
    - [[Refactor] Remove numpy split in async scheduling (#32034)](#28ae32a)
#### 🔴 高重要度变更 (3)

### [Refactor] Separate sequence and token pooling types (#32026)
**SHA**: `583a90e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/583a90e0055f3fa1fc464b2dd62ea26a7d389ae1)

**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
本次 PR 将原先 `PoolerConfig.pooling_type`（一次性兼容序列与 token 级别的池化方式）拆分为 `seq_pooling_type` 与 `tok_pooling_type` 两个字段，实现序列池化与 token 池化的显式区分。为保持向后兼容，仍保留 `pooling_type` 并在 `__post_init__` 中自动映射至对应的子字段。代码中所有对 `pooling_type` 的使用已改为对应的 `*_pooling_type`，并相应更新默认值、装饰器、属性查询以及 chunked‑prefill / prefix‑caching 判定逻辑。大量单元测试同步更新，覆盖模型加载、配置解析、任务验证等场景。

**🎯 影响范围**：  
- `vllm/config/pooler.py`、`vllm/config/model.py`、`vllm/model_executor/models/*`（Bert、Roberta、Clip、Siglip、ModernBert、InternLM2、GritLM、Qwen2…）  
- 相关工具入口 `vllm/entrypoints/llm.py`、`vllm/pooling_params.py`、`vllm/tasks.py`  
- 注册中心、接口基类 `vllm/model_executor/models/interfaces_base.py`、`registry.py`  
- 全套测试文件（`tests/*`）  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **配置层解耦**：将序列池化 (`CLS/MEAN/LAST`) 与 token 池化 (`ALL/STEP`) 明确分离，使模型配置更具可读性、可扩展性。<br>- **向后兼容**：`pooling_type` 仍保留，仅在实例化时自动填充对应子字段；若同时设置 `seq_pooling_type`/`tok_pooling_type` 与 `pooling_type`，会抛出明确的 `ValueError`，防止模糊配置。<br>- **默认值机制**：`ModelConfig.__post_init__` 现在分别读取模型信息的 `default_seq_pooling_type` 与 `default_tok_pooling_type`，避免原有 “单一 default_pooling_type” 的歧义。 |
| **性能影响** | - 变更仅涉及配置对象的属性读取与初始化逻辑，运行时几乎无额外开销（一次属性赋值、一次类型检查）。<br>- 对 `is_chunked_prefill_supported` 与 `is_prefix_caching_supported` 的判定改为同时检查序列和 token 池化类型，逻辑分支略有增加，但在模型加载阶段执行，对推理吞吐量无感知影响。 |
| **安全考虑** | - 未引入网络交互或外部 I/O，安全面直接影响有限。<br>- 通过显式校验防止 **配置冲突**（同时设置 `pooling_type` 与子字段）导致运行时未定义行为，提升安全性和可维护性。 |
| **可维护性** | - API 更加明确，后续若要新增序列或 token 池化方式，只需在对应 `Literal` 中添加即可，无需改动混合 `PoolingTypeStr`。<br>- 代码中大量 `pooler_config.pooling_type` 已全部迁移，降低误用概率。<br>- 文档/注释已同步更新（如 `default_pooling_type` 改为 `default_seq_pooling_type`/`default_tok_pooling_type`），提升开发者可读性。 |
| **兼容性** | - 对外直接使用 `PoolerConfig(pooling_type=…)` 的老代码仍可运行，内部会自动映射。<br>- 但 **显式使用子字段** 的新代码必须提供 `seq_pooling_type` 或 `tok_pooling_type`（或两者），否则 `ModelConfig` 的默认填充逻辑会补足。<br>- 对第三方库（如自行构造 `PoolerConfig` 并序列化为 JSON）若仅包含 `pooling_type`，仍可被 `vllm` 读取；若混用两者，将触发 `ValueError`，需相应调整。 |

---

### ⚠️ 潜在风险

1. **向后兼容性遗漏**  
   - 某些自定义模型加载路径可能直接读取 `pooler_config.pooling_type` 并在业务层使用，若未经过 `ModelConfig` 的 `__post_init__`（如手动实例化 `PoolerConfig` 后直接访问），可能得到 `None` 并触发断言错误。  
2. **默认填充值不一致**  
   - 新增 `default_tok_pooling_type` 为 `"ALL"`（原 `default_pooling_type` 的 `"ALL"`），但部分模型（如 `Qwen2` 的 reward/step 模型）只适用于 token‑level 池化，若忘记在模型实现上使用对应装饰器，可能导致默认不匹配，进而影响 `is_chunked_prefill_supported` 判断。  
3. **序列/Token 池化混用错误**  
   - 如果用户错误地在 `PoolerConfig` 中同时指定 `seq_pooling_type` 与 `tok_pooling_type`（两者都非 `None`），当前实现不会报错，但业务层可能误以为两者对应同一种池化，从而产生语义错误。  
4. **文档/示例未及时同步**  
   - downstream 项目或教程仍然使用 `pooling_type` 关键字示例，若不提示兼容警告，用户在升级后可能产生困惑。  

---

### 💡 关注建议

| 受众 | 建议 |
|------|------|
| **开发者** | - 在 `PoolerConfig` 的 `__post_init__` 中已实现冲突检测，建议在项目的 CI 中加入 **配置冲突测试**（确保同时出现 `pooling_type` 与子字段时报错）。<br>- 为防止遗漏，建议在 `vllm/config/pooler.py` 添加 `__repr__` 或 `dict` 方法，明确展示三个字段的实际值，便于调试。 |
| **用户** | - 文档/README 中补充 **“从 pooling_type 到 seq_pooling_type / tok_pooling_type 的迁移指南”**，并在示例代码中使用新字段。<br>- 对已有模型配置文件（如 `config.json`）若只包含 `pooling_type`，无需改动；若需要细粒度控制，请显式添加 `seq_pooling_type` / `tok_pooling_type`。 |
| **测试/运维** | - 在 CI 中加入 **向后兼容性回归用例**：加载仅含 `pooling_type` 的旧配置并验证默认值是否正确填充。<br>- 对 `ModelConfig` 的默认填充路径增加日志级别 DEBUG，确保在模型加载时能够追踪到 `default_seq_pooling_type` 与 `default_tok_pooling_type` 的来源。 |
| **后续演进** | - 考虑在未来将 `pooling_type` 完全移除或标记为 **已废弃**（deprecation），并在一次 major 版本中彻底删除，以避免长期维护双套字段。<br>- 若计划引入更多序列或 token 池化方式（例如 `MAX`, `MIN`），只需在对应 `Literal

---

### [1/N][Attention] Restructure attention: move files (#31916)
**SHA**: `2612ba9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2612ba9285d828f3179801334b6d2db0046b9b6c)

**🎯 变更类型**：架构变更 / 重构  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
此次提交对 vLLM 的注意力（Attention）子系统进行了大规模重构：将原有的 `vllm/attention` 包整体迁移到 `vllm/v1/attention`，并同步更新了所有内部引用、文档、CI 配置以及插件系统相关的路径。核心实现、后端注册表、selector、ops、backend 抽象等全部搬迁至新命名空间，旧路径仅保留少量兼容性占位或已标记为废弃。此举旨在为后续的 v1 系列特性（如更细粒度的后端插件、统一的 KV‑Cache 接口）提供更清晰的代码组织，并为未来的多版本共存奠定基础。

**🎯 影响范围**  
- **核心模块**：`vllm/attention/*` → `vllm/v1/attention/*`（包括 `backend.py、selector.py、ops、backends`）  
- **模型实现层**：所有 `vllm/model_executor/layers/attention/*` 以及相关模型文件的导入路径均被改为新路径。  
- **配置、平台、编译器**：`vllm/config/*.py、vllm/platforms/*、vllm/engine/*` 中的 `AttentionBackendEnum`、`AttentionBackend` 等导入全部切换。  
- **测试、示例、文档**：所有 test、example、docs 均已更新 import。  
- **CI/CD**：Buildkite、GitHub CODEOWNERS、Mergify 规则同步到新路径。  
- **插件系统**：插件指南中对后端基类的引用改为 `vllm.v1.attention.backend.AttentionBackend`。  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 将注意力相关抽象统一到 `vllm.v1.attention`，使得 **v1 系列** 与 **旧 v0.x** 可以并行演进。<br>- 新的 `backend.py` 抽象（`AttentionBackend、AttentionImpl、AttentionLayer`）取代旧的 `abstract.py`，实现更细粒度的后端实现（如 flashinfer、triton、mla 等）。<br>- `selector.py` 重新实现，绑定 `AttentionBackendEnum` 与后端实现的映射，统一了插件注册入口。<br>- KV‑Cache 接口 (`vllm.v1.kv_cache_interface`) 与注意力后端解耦，便于跨平台（CUDA、ROCm、CPU）统一调度。 |
| **性能影响** | - 代码路径本身并未改变算法实现，性能基准应保持不变。<br>- 因为大量 import 从旧路径迁移，**启动时间**（module import）在首次加载时可能略有提升，后续缓存后影响可忽略。<br>- 迁移后统一使用 `vllm.v1.attention.ops.*`，有助于后期对 custom op 的注册与 JIT 编译进行统一调度，潜在的 **优化空间** 更大。 |
| **安全考虑** | - 重构未引入外部依赖，安全面影响主要是 **路径泄露** 与 **误用旧接口**：如果外部插件仍依赖已废弃的 `vllm.attention.*`，可能在运行时触发 `ImportError` 或加载不匹配的后端实现。<br>- CI 中已更新 CODEOWNERS 与 Mergify 规则，确保对新路径的审查责任明确，降低未经审查的安全变更风险。 |
| **可维护性** | - 通过统一 `v1` 前缀，代码组织更清晰，后期可以在同一层级下推出 `v2`、`v3` 等新版本而不与旧实现冲突。<br>- 文档同步完成，示例已指向新路径，降低新手学习成本。<br>- 迁移涉及 400+ 行改动，改动范围广，潜在的 **遗漏**（未更新的 import）风险仍在，需要全面回归测试。 |
| **兼容性** | - 项目内部已完成全部 import 替换，**主线代码** 运行无兼容性问题。<br>- 对外部使用者（自定义插件、第三方项目）仍可能引用旧路径，官方已在文档中标记 `vllm.attention` 为 **已废弃**，未来 0.13+ 版本将删除。若外部未同步，将在运行时抛出 `ImportError`。建议插件维护者尽快迁移到 `vllm.v1.attention`。 |
| **测试影响** | - 所有单元测试、集成测试均已更新 import，且 CI 配置同步。<br>- 大量文件改动可能导致 **缓存失效**（pytest 缓存）、**Cython 编译** 或 **torch script** 的缓存失效，需要在 CI 中确保 `--cache-clear` 或全量重新编译。 |

**⚠️ 潜在风险**  

1. **遗留旧路径残留**：如果某些未被搜索到的文件仍引用 `vllm.attention.*`，运行时会导致 `ImportError`，尤其是第三方插件或历史模型权重的加载脚本。  
2. **循环导入风险**：在迁移 selector 与 backend 注册表时，出现了交叉引用（如 `selector` 需要 `backend`，而 `backend` 中的某些实现又导入 `selector` 用于调度）。若未使用 `TYPE_CHECKING` 进行延迟导入，可能在解释阶段触发循环导入错误。  
3. **CI 失效**：CI 中的 `buildkite` 脚本、`mergify` 规则及 `CODEOWNERS` 已同步，但若有硬编码路径（例如在 `benchmarks`、`examples` 中的相对路径），可能在 CI 运行时找不到文件。  
4. **向后兼容性**：旧的 `vllm.attention` 包在本次提交中被删除，仅保留空的 `__init__`，如果用户仍通过 `import vllm.attention` 访问 `AttentionBackendEnum` 等，将收到 ModuleNotFoundError。  
5. **文档/示例不完整**：虽然大多数文档已更新，但仍有少量代码块（如嵌入在 README、外部博客）可能指向旧路径，导致用户困惑。  

**💡 关注建议**  

| 建议 | 说明 |
|------|------|
| **回归完整性测试** | 在 CI 上执行全套 **unit + integration + performance** 测试，确保所有 import 已被覆盖，尤其是 **插件相关**（`vllm.plugin.*`）的加载。 |
| **提供兼容层** | 为平滑迁移，可在 `vllm/attention/__init__.py` 中加入 **旧路径到新路径的再导出**（如 `from vllm.v1.attention import *`），并在文档中明确标记为 **deprecated**，直至下个 major 版本删除。 |
| **更新插件 SDK** | 若项目提供插件 SDK，务必同步 SDK 中的导入路径，并在发布说明中提醒插件作者升级至 `vllm.v1.attention`。 |
| **防止循环导入** | 检查 `selector.py` 与 `backend.py`、`registry.py` 之间的依赖链，使用 `TYPE_CHECKING` 或延迟导入 (`import importlib`) 降低循环导入概率。 |
| **监控启动时的 ImportError** | 在 `vllm/__init__.py` 或平台初始化阶段捕获 `ImportError`，给出友好的提示：“已迁移至 vllm.v1.attention，请更新您的代码”。 |
| **文档统一搜索** | 使用全局搜索工具（如 `ripgrep`）确保仓库内不再出现 `vllm.attention.backends`、`vllm.attention.selector` 等字符串，防止遗漏。 |
| **版本发布说明** | 在 0.13/0.14 的发布日志中明确标注：“Attention 子系统已迁移至 v1 命名空间，旧路径将在未来版本中删除”。提供迁移指南与自动脚本（如 `sed`）帮助用户批量修改。 |
| **监控性能回归** | 通过

---

### [Fix] Introduce audio channels spec (#31595)
**SHA**: `657e9c0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/657e9c0e183d4bdb157b07db987644d2d9d36245)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
本次提交在 vLLM 的多模态音频处理路径中引入了 **音频通道规格（Audio Channel Spec）**。核心改动包括：

1. 新增 `ChannelReduction` 枚举、`AudioSpec` 数据类以及 `normalize_audio` 通用归一化函数，实现对多通道音频的自动 **单声道化**（或其他通道目标）以及通道降维策略（均值、首通道、最大值、求和）。  
2. `MultiModalDataParser` 增加 `target_channels` 参数，并在音频解析 (`_parse_audio_data`) 中调用 `normalize_audio` 完成通道规范化。  
3. 所有已实现的音频模型（Whisper、Qwen2‑Audio、Qwen2.5‑Omni、Ultravox）实现 `get_target_channels()` 并在创建 `MultiModalDataParser` 时传入对应的 `target_channels=1`（默认 mono）。  
4. 文档 `multimodal_inputs.md` 补充 “Automatic Audio Channel Normalization” 章节，说明 vLLM 会根据模型自动将立体声、5.1 环绕声等转换为单声道。  
5. 大量单元测试覆盖：`normalize_audio` 各种输入格式、通道降维方式、异常路径；以及完整管线（Parser → Processor → Model）在不同音频格式和采样率下的 E2E 行为。  

**🎯 影响范围**  
- `vllm/multimodal/audio.py`（核心音频规格 & 归一化实现）  
- `vllm/multimodal/parse.py`（Parser 增加 `target_channels` 与调用）  
- 各模型实现文件 `whisper.py、qwen2_audio.py、qwen2_5_omni_thinker.py、ultravox.py`（新增 `get_target_channels` 与 parser 参数）  
- 文档 `docs/features/multimodal_inputs.md`  
- 测试目录 `tests/models/multimodal/processing/`, `tests/multimodal/`, `tests/audio/...`  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 引入 **AudioSpec** 作为跨模型统一的音频格式描述，使得音频前处理不再散落在各个模型实现中，提升可维护性。<br>- `MultiModalDataParser` 通过 `target_channels` 参数完成通道归一化，形成 **输入层统一化**（采样率 + 通道数），下游模型只需关注内容而不必自行处理通道。<br>- `ChannelReduction` 为未来可能的 **多通道→多通道**（例如立体声→立体声）提供扩展点。 |
| **性能影响** | - 归一化主要是 **均值/取首/取最大/求和** 的一次矩阵运算，时间复杂度 O(C·T)（C 为通道数，T 为采样点数），在常见 1‑2 通道、采样率 ≤48kHz 的音频上几乎可以忽略不计。<br>- 仅在 **音频需转置**（time‑>channel）时做一次拷贝；转置本身是视图操作（`numpy.T`/`torch.T`），不产生额外内存拷贝。<br>- 通过 `target_channels=None` 可保持 **Passthrough**，对已有模型性能无回归。 |
| **安全考虑** | - `normalize_audio` 对输入形状进行严格校验，若出现 **3D 或更高维度**、**通道扩张**（mono → 多声道）会抛出 `ValueError`，防止潜在的内存溢出或错误的特征抽取。<br>- 所有异常路径在单元测试中得到覆盖，确保在异常输入下不会导致服务崩溃。 |
| **可维护性** | - 通过 `AudioSpec` 与 `ChannelReduction` 把“**“如何把 X 通道转为 Y 通道**”的策略集中管理，后续若需支持 **立体声模型**（target_channels=2）或 **自定义降噪策略**，只需在模型实现 `get_target_channels` 返回相应值或在 `AudioSpec` 中自定义 `channel_reduction`。<br>- 文档同步更新，开发者不必自行查源码即可了解自动归一化行为。 |
| **兼容性** | - 仅对 **实现了 `get_target_channels` 的模型**（目前四个）产生影响，其他模型保持原有行为。<br>- `target_channels` 参数默认 `None`，对未显式指定的模型保持 **向后兼容**。 |

---

### ⚠️ 潜在风险

1. **误用 `target_channels`**：如果第三方模型误实现 `get_target_channels` 返回非 `None`（例如 2）但特征提取器仅支持单声道，仍会产生 **通道数不匹配** 错误。建议模型实现者在 `get_feature_extractor` 与 `get_target_channels` 保持一致性。  
2. **通道降维策略变更**：现默认使用 `MEAN`（均值），但某些模型可能对 **相位信息** 较为敏感，需改为 `FIRST` 或 `MAX`。若未来更改默认行为，需在模型文档或迁移指南中说明。  
3. **跨库张量类型兼容**：`normalize_audio` 支持 `numpy.ndarray` 与 `torch.Tensor`，但未对 **JAX**、**TensorFlow** 等其他张量库做适配，若 vLLM 在未来扩展到这些框架，可能需要额外实现。  
4. **异常路径未被捕获**：在生产环境中如果用户传入 **3D 音频**（如多段音频拼接），当前会抛 `ValueError`，若未在上层捕获会导致请求失败。建议在 API 层统一包装异常返回友好错误信息。  

---

### 💡 关注建议

| 对象 | 建议 |
|------|------|
| **模型开发者** | - 在实现新音频模型时实现 `get_target_channels` 并确保特征提取器的 `sampling_rate` 与 `target_channels` 对齐。<br>- 如需支持 **立体声** 或 **多声道**，在 `AudioSpec` 中自行指定 `channel_reduction=ChannelReduction.FIRST`（或其他）并返回对应 `target_channels`。 |
| **库维护者** | - 为 `AudioSpec` 增加 `__post_init__` 校验：`target_channels` 为正整数或 `None`，防止误配。<br>- 将 `normalize_audio` 的通道检测逻辑（`shape[0] > shape[1]`）抽取为独立函数，以便在其它模块复用。<br>- 在 CI 中保留 **异常路径测试**，防止未来改动意外放宽维度检测。 |
| **用户/使用者** | - 当使用自定义音频加载库（如 `soundfile`）时，无需自行转置或降维，直接将 `(audio, sr)` 交给 `LLM.generate` 即可。<br>- 若使用 **非标准音频格式**（3D、batch 等），请在调用前自行预处理或捕获 `ValueError`。 |
| **文档/培训** | - 更新 API 文档中 `MultiModalDataParser`、模型 `get_target_channels` 的说明，明确 “默认 mono”。<br>- 在示例代码（`examples/offline_inference/audio_language.py`）中加入 **立体声 → mono** 的演示，帮助新手快速了解自动归一化。 |

--- 

**整体结论**：此改动为 vLLM 引入了系统化的音频通道规范化机制，显著提升了多模态音频模型的 **易用性** 与 **一致性**，对已有功能几乎无性能负担且保持向后兼容。唯一需要关注的是 **模型与特征提取器之间的 target_channels 对齐** 以及 **异常输入的错误处理**，在合理的代码审查与文档说明下，这些风险是可控的。

---

#### 🟡 中重要度变更 (18)

### [Quant] Support MXFP4 W4A16 for compressed-tensors dense models (#31926)
**SHA**: `e6c6f2c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e6c6f2c79d2c1e0765d17b4dec83a4a8283342e4)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `compressed_tensors` 体系中新增对 **MXFP4 (W4A16‑Mxfp4)** 的支持。  
- 新增检测函数 `_is_mxfp4`、对应的方案类 `CompressedTensorsW4A16Mxfp4`，并在方案工厂 `_get_scheme_from_parts` 中加入分支。  
- 将新方案暴露在 `schemes/__init__.py`，并实现权重加载、Marlin 前处理以及前向计算的全部逻辑。

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`  
- `vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py`  
- 新增文件 `schemes/compressed_tensors_w4a16_mxfp4.py`  
- 相关的 Marlin 工具 (`marlin_utils_fp4.py`) 以及参数封装类。

**💡 关注建议**  

1. **检测逻辑完整性**  
   - `_is_mxfp4` 仅检查 `group_size == 32`、`num_bits == 4`、`symmetric`、`strategy == GROUP`、`type == FLOAT`。若未来出现非对称或不同分组大小的 MXFP4 实现，需同步更新。建议将这些约束抽成常量或文档说明。  

2. **参数 dtype 与硬件兼容**  
   - `weight_scale` 使用 `torch.uint8`（E8M0），但在后续 `apply_fp4_marlin_linear` 中可能会被转为 `torch.float16/32`。请确认 `marlin_utils_fp4` 能正确处理 `uint8` 作为 scale，或在 `process_weights_after_loading` 中显式转换。  

3. **workspace 属性的前置声明**  
   - `apply_weights` 直接引用 `layer.workspace`，但在 `create_weights` 中没有创建该属性。若其他方案依赖 `prepare_fp4_layer_for_marlin` 自动创建，则保持不变；否则应在 `create_weights` 中显式 `layer.workspace = ...`，防止运行时 `AttributeError`。  

4. **模型序列化/保存**  
   - 新增的 `weight_packed` 与 `weight_scale` 仍然以自定义 `Parameter` 注册，确保 `vllm` 的 checkpoint/loader 能正确序列化这两个张量；若已有的 `ModelWeightParameter`/`GroupQuantScaleParameter` 只在 `dtype` 为 `torch.float16` 时注册，需要检查兼容性。  

5. **单元测试 & CI**  
   - 建议在 `tests/quantization` 新增 MXFP4 端到端测试：① 通过 `compressed-tensors` 生成 Mxfp4‑packed 权重并加载；② 验证前向输出与原始 FP16 的数值误差在预期范围；③ 在不同 GPU 架构（如 A100、H100）下跑一次 Marlin kernel，确认 `get_min_capability` 与实际硬件匹配。  

6. **文档同步**  
   - 更新 `docs/source/quantization.md`（或相应章节），说明 MXFP4 的使用方式、适用模型、所需 GPU 计算能力（≥80）以及与 NVFP4 的区别。  

7. **性能监控**  
   - MXFP4 采用分组尺度（无全局尺度），与现有 NVFP4 实现略有不同。建议在基准测试中记录 kernel 启动次数、显存占用以及吞吐率，以验证新增方案的预期加速。  

**总体评价**  
本次 PR 为 vLLM 引入了重要的重量级量化格式——MXFP4，代码结构清晰、检测与工厂路径统一。只需关注参数 dtype、workspace 初始化以及测试覆盖，即可确保在生产环境中平滑使用。祝开发顺利！

---

### [Bugfix] Fix integer overflow in Gemma3n audio processing (#31657)
**SHA**: `07286ec` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/07286ec5a6edf3a7a3623ddfc8db6a24b1d3c70a)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增 `gemma3n_audio_utils.adjust_audio_features_to_expected_length`，对 Gemma3n 音频特征进行 **填充或截断**，防止 `audio_seq_len > expected_tokens` 时出现负数扩展导致的整数溢出。  
- 在 `gemma3n_mm.py` 中调用该工具函数，并在截断时加入日志警告。  
- 完善类型注解、返回值统一为 `| None`，并新增一套 **CPU 单元测试**（包括短音、长音、恰好长度、批处理及负数扩展验证），确保逻辑在各种序列长度下不再崩溃。  

**🎯 影响范围**  
- `vllm/model_executor/models/gemma3n_audio_utils.py`（新模块）  
- `vllm/model_executor/models/gemma3n_mm.py`（音频预处理路径）  
- 相关测试 `tests/models/multimodal/processing/test_gemma3.py`  
- 运行时日志 (`logger.warning`)  

**💡 关注建议**  
1. **上线前验证**：在含音频的 Gemma3n 推理任务上跑一次完整的生成，确认不会出现 “numel: integer multiplication overflow”。  
2. **监控截断**：关注日志中的 `tokens_truncated` 警告，若频繁出现说明输入音频超出模型预期长度，可能需要在前端做时长限制或切片。  
3. **兼容性**：新函数已抽离，保持与原有 `embed_multimodal` 接口兼容，第三方插件无需改动。  
4. **性能**：填充/截断的额外 `cat` 与 `expand` 操作成本极低，基本不影响吞吐，可在高并发场景直接使用。  

总体而言，此次修改消除了 Gemma3n 音频处理的整数溢出风险，提升了模型在异常音频长度下的鲁棒性，建议合并后在多机部署环境做一次回归测试。

---

### [Benchmark][1/2] Generalize SLA criterion validation from binary flags to margins (#32075)
**SHA**: `5f2385a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5f2385a4c8daac7e15142ebf96bc9a91acd61283)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原先 SLA 判断的二元 “通过/不通过” 逻辑改为基于 *margin* 的连续数值判定，新增 `compute_margin` 与 `print_and_compute_margin`，并在搜索、估算流程中记录每一步的 margin，以便二分搜索时更精确地定位阈值。新增单元测试覆盖 `estimate_sla_bounds` 与 `find_sla_value` 在不同比较符号、边界及超出范围的行为。  

**🎯 影响范围**  
- `vllm/benchmarks/sweep/sla_sweep.py`（核心 SLA 判定抽象）  
- `vllm/benchmarks/sweep/serve_sla.py`（估算、二分搜索实现）  
- `vllm/benchmarks/sweep/param_sweep.py`（返回值类型微调）  
- 新增/修改测试 `tests/benchmarks/sweep/test_serve_sla.py`  

**💡 关注建议**  
1. **向后兼容**：公开的 `SLACriterionBase.validate` 已被删除，若外部代码仍调用，需要改为 `compute_margin` 并自行判断 `<=0`。建议在文档中说明迁移步骤。  
2. **数值容差**：`SLA_EPS = 1e-8` 用于处理等号比较，确保浮点误差不会导致误判。若有更宽松的容差需求，可通过环境变量或参数暴露。  
3. **搜索性能**：`_estimate_sla_bounds` 与 `_find_sla_value` 现在返回 `history`，对外部调用无影响，但若在生产脚本中不再使用，可考虑删除或标记为内部实现，以避免不必要的内存占用。  
4. **测试覆盖**：新增的 200+ 行测试验证了边界、超界和不同比较符的行为，建议在 CI 中保留并确保在未来的优化/重构中不被误删。  

总体来说，此次改动提升了 SLA 评估的可解释性和搜索精度，对核心 benchmark 流程影响有限，兼容性只需在外部使用者处做好迁移即可。

---

### Fuse RoPE and MLA KV-cache write (#25774)
**SHA**: `80fead8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/80fead8bf6508b7907a565d05cb0a6aed9101a3d)

**🎯 变更类型**：功能增强（RoPE 与 MLA KV‑cache 写入融合）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：新增 `concat_and_cache_mla_rope_fused` 接口及对应 CUDA kernel，实现一次性完成 Q/K 的旋转位置编码（RoPE）并写入 MLA KV‑cache。CMake、头文件、Python 绑定及单元测试同步更新。  
**🎯 影响范围**：  
- `csrc/cache_kernels_fused.cu`（新核）  
- `csrc/cache.h` 与 `csrc/torch_bindings.cpp`（接口声明/实现）  
- `vllm/_custom_ops.py`（Python 包装）  
- 测试 `test_rotary_embedding_mla_cache_fused.py`  
- CMake 编译列表  

**💡 关注建议**  

1. **正确性**：核内部对 `positions`、`slot_mapping` 为 `-1` 的情况已提前返回，仍需在上层确保 padding token 不写入 KV‑cache，防止越界。  
2. **数据类型**：模板参数覆盖了 FP16/BF16/FP32 以及 FP8 量化路径，建议在 CI 中加入不同 `kv_cache_dtype`（auto、fp8）与 `kv_cache_scale` 的组合测试，验证量化误差在可接受范围。  
3. **性能**：线程块大小取 `max(rope_block_size, mla_block_size)`，但若 `kv_lora_rank` 与 `rot_dim` 差距悬殊可能导致低占用。可在实际推理时采集 occupancy，适当调节 `rope_block_size` 上限。  
4. **跨平台**：代码已兼容 ROCm（通过宏切换），但 `__nv_bfloat16` 别名仅在 AMD 编译时生效，建议在 CI 中加入 ROCm 环境跑一次完整测试。  
5. **回滚兼容**：原有 `concat_and_cache_mla` 仍保留，避免现有模型因调用新接口而失效；但文档需明确两者区别和使用场景。  
6. **后续维护**：kernel 文件体积较大，建议拆分出 RoPE‑apply 与 KV‑write 两个内部子核，以便后续单独优化或在不需要 KV‑write 时复用。  

整体来看，此次融合可显著降低两次 CUDA launch 的开销，对长序列推理和大模型 KV‑cache 使用场景有正向提升。请在推理基准中对比前后 latency、GPU utilization，确保收益符合预期。

---

### feature/issac 0.2 (#31550)
**SHA**: `e45946b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e45946bd91946e257d809d910c10d770e251e703)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Isaac 系列模型新增 0.2‑2B‑Preview 版本的支持；完善模型配置的拆分（vision 与 text），并在模型初始化时修正 RoPE 参数的拼接逻辑；更新测试、注册表与权重映射以兼容新版本。  

**🎯 影响范围**  
- `tests/models/multimodal/generation/test_common.py`、`tests/models/registry.py`（新增模型标识）  
- `vllm/model_executor/models/isaac.py`（权重映射、`rope_scaling`/`rope_parameters` 处理）  
- `vllm/transformers_utils/configs/isaac.py`（`IsaacConfig` 拆分子配置、`text_config` 构造）  
- 相关的 `vllm/transformers_utils/config.py`（`patch_rope_parameters` 调用）  

**💡 关注建议**  
1. **权重映射**：新增了 `model.text_model.` 前缀的映射以及 `model.lm_head.`，需确认 HF checkpoint 中对应键名是否一致，防止加载失效。  
2. **RoPE 参数**：对 `rope_scaling` 与 `rope_parameters` 的兼容处理较为复杂，建议在单元测试中覆盖 `mrope_section`、`mrope_interleaved` 两种情况，防止出现未覆盖的配置路径。  
3. **向后兼容**：`IsaacConfig` 现在接受 `text_config` 为 dict、Config 实例或 None，确保旧版加载（仅 `vision_config`）仍可正常实例化。  
4. **注册表 Extras**：`extras` 新增键值对，使用 `IsaacForConditionalGeneration` 时若显式指定 `extras`，应在文档中说明调用方式。  
5. **文档与示例**：更新模型使用说明，列出 `Isaac-0.2-2B-Preview` 与 `Isaac-0.1` 的区别及推荐的 `rope_scaling` 参数。  

总体来看，改动为 Isaac 多模态模型的版本迭代提供了必要的配置与权重映射支持，但涉及配置拼接与 RoPE 参数的细节较多，务必通过 CI 中的多版本加载与推理测试验证兼容性。

---

### [Misc][LLaMa4] Compile LLaMa Vision Encoder (#30709)
**SHA**: `ea6d067` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ea6d067a2aebc32976f4ebfc0ab2546fbaa91c42)

**🎯 变更类型**：功能增强（为 LLaMa‑4‑Scout 添加多模态编码器编译支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. `CompilationConfig.compile_mm_encoder` 文档扩展，声明已可用于 `mLLaMa4`。  
2. 为 `Llama4VisionModel` 与其子模块加入 `@support_torch_compile`（在满足 `should_torch_compile_mm_vit` 时开启），并在模型构造时使用 `set_model_tag` 标记为 encoder。  
3. `mm_encoder_attention.py`、`llama4_vision_rope.py` 以及 `vit_attn_wrappers.py` 的参数顺序/默认值做了细微调整，以兼容 Torch‑compile。  
4. `LlamaModel` 引入 `shape_invariants` 兼容层并保留 TODO。  
5. 增加 `test_mllama4_vit_compilation`（forked、skip）用于 CI 验证 Vision 子模块可被编译。  

**🎯 影响范围**：  
- `vllm/config/compilation.py`  
- `vllm/model_executor/layers/attention/mm_encoder_attention.py`  
- `vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py`  
- `vllm/model_executor/models/mllama4.py`（VisionModel、MultiModalProjector、forward 逻辑）  
- `vllm/v1/attention/ops/vit_attn_wrappers.py`  
- 测试目录 `tests/compile/fullgraph/test_multimodal_compile.py`  

**💡 关注建议**  
- **开发者**：打开 `CompilationConfig.compile_mm_encoder` 并在 Tensor‑Parallel=8 以上的环境下跑完整图编译，留意 `compilation_counter` 受 TP 影响的偏差；必要时在 CI 中恢复 `forked` 测试。  
- **用户**：在使用 LLaMa‑4‑Scout 时，确保平台满足 `should_torch_compile_mm_vit` 条件（CUDA ≥ 12、支持 Torch‑compile），否则编译开关会自动失效。  
- **文档/维护**：同步更新 README/Config 文档，说明 Vision 编码器的编译前置条件和已知限制；后续可在 `TODO[#32068]` 中完善 `LlamaModel` 的重编译策略。  

通过以上改动，vLLM 已基本支持 mLLaMa‑4 的视觉编码器编译，为多模态推理提供更高的图执行效率。

---

### [2/N][Attention] Fix pre-commit errors (#32052)
**SHA**: `0308901` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/03089019759b4e9af69c160ad0c516e3dcdb1420)

**🎯 变更类型**：Bug 修复（pre‑commit mypy 错误）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `tools/pre_commit/mypy.py` 中删除了已失效的忽略路径 `vllm/v1/attention/backends/fa_utils.py`，避免无意义的 “TODO”。  
2. `vllm/v1/attention/backends/fa_utils.py` 与 `vllm/v1/attention/ops/paged_attn.py` 的导入方式改为直接从模块取得符号，去掉了中间的 `ops` 别名包装，保持统一的 `from … import …` 风格。  

**🎯 影响范围**  
- `vllm/v1/attention` 子包（包括 `fa_utils.py`、`paged_attn.py`）  
- Mypy 静态检查配置（`tools/pre_commit/mypy.py`）  

**💡 关注建议**  
- **运行时行为**：导入路径的改动在功能上等价，但仍需在不同硬件平台（CUDA、XPU、ROCM）下跑一次单元测试，确保 `reshape_and_cache_flash`、`flash_attn_varlen_func`、`get_scheduler_metadata` 能被正确加载。  
- **类型检查**：检查其他文件是否仍然使用被删除的 ignore‑path；若有，及时更新 `mypy` 配置，避免未来的 “unused ignore” 警告。  
- **文档/注释**：在 `fa_utils.py` 顶部补充简要说明为何采用直接导入，以免后续维护者误以为缺少别名实现。  
- **兼容性**：确保 `vllm/_custom_ops` 与 `vllm/_ipex_ops` 在未安装对应加速库时仍能安全导入（例如使用 `try/except ImportError`），防止 CI 在非 CUDA 环境下报错。  

总体而言，此次提交清理了无效的 mypy 忽略并统一了导入写法，提升了代码可维护性和静态检查的准确性。建议在 CI 中保留 `mypy --strict` 以及跨平台的单元测试以捕获潜在的导入回退问题。

---

### [Misc][BE] Type coverage for vllm/compilation [2/3] (#31744)
**SHA**: `aaf4b70` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aaf4b70aae5976115eb5f8caead3d4eeb3657136)

**🔧 变更类型**：功能增强（类型覆盖/可维护性提升）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/compilation` 相关子包中集中加入或完善了 **PEP‑526/PEP‑484** 类型注解，并对部分函数签名做了细粒度的 `Callable[..., Any]`、`ParamSpec`、`Literal` 等修正。同步添加 `__all__`、`type: ignore[misc]` 注释，重新导出 `CustomGraphPass`，并将若干内部工具改写为返回 `Generator[None, None, None]` 的上下文管理器。整体代码可读性、IDE 自动完成及静态检查得到提升。

**🗂 影响范围**  
- **编译后端**：`backends.py`、`piecewise_backend.py`、`wrapper.py`、`cudagraph.py` 等文件的函数签名被细化，涉及 runtime 调用路径。  
- **缓存、序列化**：`caching.py` 中 `VllmSerializableFunction`、`serialize_compile_artifacts` 均加入完整的 `Callable[..., Any]`、`Sequence[Any]` 注解。  
- **装饰器与 Pass 管理**：`decorators.py`、`pass_manager.py`、`inductor_pass.py`、`fix_functionalization.py`、`noop_elimination.py` 等实现了 `-> None`、`-> str`、`-> Any` 等明确返回类型。  
- **配置与接口导出**：`config/compilation.py` 新增 `__all__ = ["Range"]`，`caching.py`、`wrapper.py` 新增 `type: ignore[misc]` 以兼容 `torch` 动态类。  
- **分布式工具**：`pynccl_allocator.py` 参数类型声明微调。

**💡 关注建议**  
1. **运行时兼容性**：虽然类型注解本身不影响执行，但修改了 `Callable` 的可变参数签名（如 `Callable[..., Any]`），请确认外部插件或用户自定义后端仍能接受旧的 `Callable` 类型，避免因 `isinstance(..., Callable)` 检查失效。  
2. **测试覆盖**：新增/改动的 `ContextManager`（返回 `Generator`）以及 `__call__` 方法的参数/返回类型需在单元测试中加入对应的调用路径，确保未引入 `TypeError`（如 `*args` 被误当作 `tuple` 传递）。  
3. **静态检查**：项目已引入 `mypy`、`ruff` 等工具，建议在 CI 中加入 `--strict` 检查，确认所有新加入的 `Any`、`Literal`、`ParamSpec` 在实际实现里得到合理使用。  
4. **文档同步**：部分公开 API（`CustomGraphPass`、`Range`、`VllmSerializableFunction`）的参数说明已变更，建议更新 README / API 文档，防止用户因类型签名变化产生误解。  
5. **兼容旧版 Torch**：对 `torch>=2.7` / `2.8` 的分支做了条件导入与 `type: ignore`，请在最低支持的 Torch 版本上进行一次完整的编译‑运行‑推理链路验证，确保 `torch._dynamo` 的内部调用未因签名改动而失效。  

总体来看，此次提交通过细化类型注解提升了代码可维护性和 IDE 支持，对功能行为没有实质性改变，只要通过上述兼容性检查，即可安全合并。

---

### [perf][async] support non cpu sync get logprob tensors for spec (#31336)
**SHA**: `97ba96f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/97ba96fbe9a53416b973bbe05eb3bfefd6408a6c)

**🎯 变更类型**：性能优化 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `LogprobsTensors` 新增 `filter` 方法，实现基于 bool mask 的张量过滤。  
2. `RejectionSampler._get_logprobs_tensors` 现在直接生成所有 draft token 的索引（包括被拒的），后续在 `parse_output` 中统一过滤，避免 CPU‑GPU 同步。  
3. `parse_output` 接口从返回 `cu_num_tokens` 改为返回 `LogprobsLists`，并在 `GPUModelRunner` 中相应调整。  
4. 相关路径（`gpu_model_runner.py`）同步使用新 API，保持异步调度逻辑不变。

**🎯 影响范围**  
- `vllm/v1/outputs.py`（新增过滤功能）  
- `vllm/v1/sample/rejection_sampler.py`（日志概率计算与结果解析）  
- `vllm/v1/worker/gpu_model_runner.py`（输出包装、异步/同步路径）  
- 可能波及到使用 `SamplerOutput.logprobs_tensors` 的上层调用方及测试用例。

**💡 关注建议**  
1. **正确性**：`filter` 直接对 `self.logprob_token_ids`、`self.logprobs`、`self.selected_token_ranks` 进行掩码切片，确保输入 mask 为 1‑D 展平后与张量长度一致。建议在单元测试中加入 mask 长度不匹配的异常检查。  
2. **性能**：新实现通过一次 gather + 统一过滤，消除了原先的 `accepted_mask.nonzero` 触发的同步。请在加入 `async_scheduling` 场景下对比 GPU/CPU 时间，确认实际提升。  
3. **向后兼容**：原接口返回 `cu_num_tokens` 已被移除，若外部代码仍依赖该字段需同步迁移到 `LogprobsLists`。建议在文档或迁移指南中注明。  
4. **异常路径**：`parse_output` 中 `valid_mask` 用于过滤 `logprobs_tensors`，但若 `logprobs_tensors` 为 `None`（如未请求 logprobs）仍会正常返回。请确认在 `SamplerOutput` 里 `logprobs_tensors` 为 `None` 时不触发不必要的计算。  
5. **测试覆盖**：加入异步调度下的完整对比测试（包括占位 token、拒绝 token、最终 logprobs 对齐），并覆盖 `filter` 边界情况（全 false、全 true）。  

总体来看，此次改动在保持功能不变的前提下显著降低了 GPU‑CPU 同步开销，提升了异步调度的吞吐。只要做好上述兼容与测试检查，即可安全合并。

---

### [NIXL] refine decoder side post process for heterogeneous BlockSize and kv_layout (#30275)
**SHA**: `9457812` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/94578127a488d186c38665dd82116767fc34fa6d)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `kv_connector/utils.py` 中新增 three 辅助函数：  
   - `kv_postprocess_blksize_on_receive`：在本地 block‑size 大于远端时，将 KV 块按照 block‑size 比例展开。  
   - `kv_postprocess_layout_on_receive`：仅做 HND↔NHD 的维度置换。  
   - `kv_postprocess_blksize_and_layout_on_receive`：同时完成 block‑size 扩展和布局置换。  
2. `nixl_connector.py` 中原有的 `permute_device_kv` 与 `blocksize_post_process` 被合并为统一的 `post_process_device_kv_on_receive`，根据 `enable_permute_local_kv` 与 `block_size_ratio` 动态选择上述三个函数。  
3. 调度完成后改为遍历 `block_ids_for_blocksize_post_process`，直接调用统一后处理流程，去除原来的两段独立逻辑。  

**🎯 影响范围**  
- KV 缓存传输层：`vllm/distributed/kv_transfer/kv_connector/*`（尤其是 `utils.py` 与 `v1/nixl_connector.py`）。  
- 相关的 KV 布局/块大小配置 (`kv_topo`, `kv_cache_layout`, `enable_permute_local_kv`)。  
- 可能影响使用 MLA（Multi‑Level Attention）或非块首布局的老旧运行时。  

**💡 关注建议**  
- **正确性**：新增函数在 `permute` 前已做 `permute(0,2,1,3)` 取得物理顺序，随后 reshape/flatten 再次 permute，确保与原 `blocksize_post_process` 的数学等价。建议在多 GPU/多机器的异构 block‑size 场景下增加单元测试，验证 KV 内容一致性。  
- **性能**：`index_select` + `index_copy_` 会产生临时张量，若 block‑size 比例很大，可能增加显存碎片。可考虑在同一张量上原地 view/permute（`as_strided`）来降低拷贝。  
- **兼容性**：`post_process_device_kv_on_receive` 现在默认走 layout+blocksize 组合路径；若用户仅需要布局置换而 `block_size_ratio==1`，仍会走 `kv_postprocess_layout_on_receive`，保持兼容。但 `enable_permute_local_kv=False` 时仍会走 blocksize-only，确认老版本在 `kv_cache_layout=="HND"` 与 `block_size_ratio==1` 的路径仍不触发不必要的拷贝。  
- **日志**：新增的 `logger.debug` 信息已覆盖三种情况，建议在 CI 中开启 DEBUG 级别跑一次完整的 KV 传输，检查日志是否与实际走的分支对应。  

总体而言，此次提交将原来分散的后处理逻辑集中、抽象为可组合的工具函数，提升了代码可维护性和可扩展性；只要通过对应的 KV 内容一致性测试，即可安全合并。

---

### [responsesAPI] fix incomplete_messages for simple/parsable context (#31836)
**SHA**: `1f8b7c5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1f8b7c536be40975573eeebf36204286cfb4e4e9)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 OpenAI Responses API 中，原有实现未在 `simple` 与 `parsable` 上下文下正确处理因 `max_output_tokens` 触发的截断，导致 `status`、`incomplete_details` 信息缺失。通过在 `ResponsesParser` 中记录 `finish_reason`，并在 `serving_responses` 的生成流里依据 `"length"` 将 `status` 设为 `"incomplete"`，同时补充 `incomplete_details.reason=="max_output_tokens"` 的校验。新增两组针对该场景的单元测试。

**🎯 影响范围**：  
- `vllm/entrypoints/openai/parser/responses_parser.py`（新增 `finish_reason` 成员及赋值）  
- `vllm/entrypoints/openai/serving_responses.py`（在 `responses_full_generator` 中依据 `finish_reason` 设置 `status`）  
- 测试模块 `tests/entrypoints/openai/test_response_api_simple.py`、`tests/entrypoints/openai/test_response_api_parsable_context.py`

**💡 关注建议**  
1. 确认其他 `finish_reason`（如 `stop`, `tool_calls`, `error`）仍保持原有行为，防止误把成功或错误响应标记为 incomplete。  
2. `ResponsesParser.finish_reason` 为新增公共属性，若外部代码直接访问，请检查兼容性或提供只读属性。  
3. `incomplete_details` 目前只在 `max_output_tokens` 场景填充，若后续还有其他截断原因（如 `max_input_tokens`），应统一加入对应 `reason`。  
4. 运行全量 CI，关注与旧版交叉兼容的测试，尤其是流式返回与工具调用的路径。  

开发者可参考新增测试以验证新逻辑；用户在调用 `client.responses.create(..., max_output_tokens=…)` 时，可通过 `response.status` 与 `response.incomplete_details.reason` 判断是否因 token 限制被截断。

---

### [Quant] Make static quant support all group shapes (#30833)
**SHA**: `0a0aa07` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0a0aa07747cf851228598334008366ff2a8fd760)

**变更概览**  
本次提交在静态 FP8 量化路径上加入了 **任意 2‑维分组（group‑shape）** 的支持。核心改动包括：

1. **C++/CUDA 接口**  
   * `static_scaled_fp8_quant` 新增 `std::optional<std::tuple<int64_t,int64_t>> group_shape` 参数，默认 `nullopt`。  
   * 新增 `scaled_fp8_quant_kernel_strided_group_shape`，通过模板参数 `STRIDE_I_ZERO / STRIDE_J_ZERO` 在编译期分支不同的尺度 stride 情形，实现了 per‑tensor、per‑channel、per‑token 以及任意 2‑D 组尺度的统一路径。  
   * 计算尺度行/列 stride、组大小 (`group_m`, `group_n`) 的逻辑在 `static_scaled_fp8_quant` 中完成，并对 0‑D、1‑D、2‑D 三种 scale 形状做了严格校验。

2. **Python 接口**  
   * `torch.ops._C.static_scaled_fp8_quant` 暴露新签名 `(result, input, scale, (int,int)? group_shape=None)`。  
   * `vllm._custom_ops.scaled_fp8_quant` 增加 `group_shape` 参数，文档中说明 1‑D scale 必须配合 `group_shape` 区分 per‑channel 与 per‑token。  
   * `QuantFP8` 在动态 group 量化时保持原行为，静态路径转交 `ops.scaled_fp8_quant` 并传递 `group_shape`。

3. **工具函数 & 计算逻辑**  
   * `group_broadcast` 改进以兼容维度缺失的情况（符合 PyTorch broadcasting）。  
   * `scaled_quantize` 增加 `compute_dtype` 参数，内部先转为高精度（默认保持原 dtype），并使用 new `group_shape` 完成 reshape/permute。

4. **测试**  
   * 新增大量基于 `scaled_quantize` 的 2‑D 分组量化单元测试，覆盖 per‑tensor、per‑channel、per‑token、以及多种非平凡组形（如 (128,128)、(1,16) 等），并验证 1‑D scale 的兼容性。

**影响范围**  
- **核心运算库** (`csrc/ops.h`, `csrc/quantization/.../common.cu`)：所有调用 `static_scaled_fp8_quant` 的内部或外部模块需要兼容新的函数签名。  
- **Python 绑定层** (`csrc/torch_bindings.cpp`, `_custom_ops.py`)：用户代码若未显式传入 `group_shape`，仍保持旧行为（默认 `nullopt` → 兼容 per‑tensor）。  
- **模型执行层** (`input_quant_fp8.py`)：动态 group‑quant 仍不使用 scale；静态路径已加入 `group_shape` 参数，确保在不需要分组时仍可工作。  
- **测试及工具**：新增测试依赖 `scaled_quantize`，对 `group_broadcast` 的修改也可能影响其它使用该函数的代码。

**建议**  

1. **向后兼容**：确保在所有调用点（包括第三方插件）仍能使用旧签名；若有显式 `group_shape` 参数的调用，建议在文档中标注“仅在 static 模式下生效”。  
2. **文档同步**：更新 vLLM 使用手册及 API 参考，明确 1‑D scale 必须配合 `group_shape`，并给出常见组合示例（(‑1,1) → per‑channel、(1,‑1) → per‑token）。  
3. **性能评估**：对 `STRIDE_I_ZERO / STRIDE_J_ZERO` 的编译期分支进行基准，确保在常见 per‑tensor、per‑channel 场景下不引入额外开销。  
4. **异常路径**：当前对 1‑D scale 的检查较为严格，若 future 需要支持 “automatic 推断” 建议实现更宽松的 fallback。  
5. **代码审查**：确认 `group_broadcast` 在其它地方（如 `fp8_utils`) 被调用时仍满足预期的广播行为，防止因维度缺失导致的隐藏 bug。  
6. **测试覆盖**：已新增大量组合测试，建议再加一个大规模随机组合（例如 2‑D scale 与不整除的组形）确保错误路径被捕获。

整体而言，此次改动为 vLLM 的 FP8 量化提供了更灵活的分组尺度支持，提升了在多种模型（如 DeepSeek、Per‑Head 量化）上的适用性。只要注意向后兼容性和文档同步，风险相对可控。

---

### Add unpermute-aware fused MoE path and small-batch fallback (#29354)
**SHA**: `a4d5d66` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a4d5d663e295e3b90a515c79df74048bc715e804)

**🎯 变更类型**：功能增强（为 fused MoE 新增 unpermute‑aware 路径并提供小批量回退）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `fused_moe.py` 中引入 `naive_block_assignment` 标志，支持在 `sorted_token_ids` 为 `None` 时采用逐 token 的块分配方式，解决小批量（batch < BLOCK_SIZE_M）时的对齐与划分问题。  
2. 调整 kernel、调用层以及 `fused_experts_impl` 的逻辑：依据 `SPARSITY_FACTOR`、`expert_map` 与块量化条件自动切换普通或 naive 路径，并在后者手动构造 `expert_ids`、`num_tokens_post_padded`。  
3. 对测试 `tests/kernels/moe/test_moe.py` 增加小规模 (`m ≤ 2`) 的参数化用例，验证 Naive 路径的功能与数值一致性。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/fused_moe.py`（kernel、dispatch、invoke、实现入口）  
- `tests/kernels/moe/test_moe.py`（新增小批量测试）  
- 可能影响依赖 fused MoE 的上层模型执行与调度逻辑。  

**💡 关注建议**  
1. **正确性**：Naive 路径使用 `tl.where` 生成 `offs_token`，请确保 `num_valid_tokens` 与 `pid_m` 对齐，避免越界或多余填充值导致算子错误。  
2. **性能阈值**：`SPARSITY_FACTOR = 4` 以及 `tokens_in_chunk * top_k_num * SPARSITY_FACTOR <= global_num_experts` 的 heuristics 可能在某些配置下触发不必要的回退，建议提供配置开关或更细粒度的日志以便调参。  
3. **兼容性**：`sorted_token_ids` 现为可选参数，所有调度入口应在调用前显式传 `None`（如小批量或未排列情形），防止旧代码误以为仍需排好序。  
4. **测试覆盖**：当前仅覆盖 `dtype = bfloat16`、`topk ≤ 2`、`chunk_size=8192` 的情况，建议再加入 `float16`、不同 `BLOCK_SIZE_M`、`use_int8_w8a8` 等量化路径的回退验证。  
5. **文档**：在 README/代码注释中说明 “unpermute‑aware fused MoE” 的使用场景与环境变量 `VLLM_FUSED_MOE_CHUNK_SIZE` 对小批量行为的影响，帮助用户快速定位。  

整体而言，此次改动为小批量推理提供了可靠的 fallback，降低了对齐成本，只要注意上述细节即可平稳上线。

---

### [Perf] Optimize cutlass moe problem size calculation, 5.3% E2E Throughput improvement, 2.2% TTFT improvement (#31830)
**SHA**: `308feab` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/308feab33f3b386a6701cc5b28337bc04fe13a9b)

**🟢 变更类型**：性能优化（Perf）  
**⚡ 重要程度**：🟡 中（提升整体吞吐 5.3%、首个 token 时间 2.2%）  

**📋 变更摘要**  
1. 新增 `get_cutlass_moe_mm_problem_sizes_from_expert_offsets` 系列接口，直接从 `expert_first_token_offset`（MoE permute 的输出）计算每个专家的 GEMM problem size，避免对 `topk_ids` 的二次遍历。  
2. 在 CUDA 实现中加入模板化 `compute_problem_sizes_from_expert_offsets`，并使用 `VLLM_DISPATCH_BOOL` 实现 `swap_ab` 编译期分支，提升 kernel 启动效率。  
3. Python 层 `_custom_ops.py` 与 `cutlass_moe.py` 相应改写：  
   - 取消对 `expert_map → topk_ids` 的映射与复制，改为直接使用原始 `topk_ids`。  
   - `problem_sizes` 的张量维度从 `(global_num_experts, 3)` 缩小到 `(local_E, 3)`，仅为实际参与计算的专家分配空间。  
   - `swap_ab` 逻辑搬到 C++/CUDA 侧，根据 `a1q.size(0) ≤ 64` 自动决定。  

**🎯 影响范围**  
- **csrc/ops.h、cutlass**：新增/修改 C++/CUDA 接口与 kernel。  
- **csrc/quantization/w8a8/cutlass**：实现新 kernel、封装调用。  
- **torch_bindings、_custom_ops.py**：暴露 Python 调用入口。  
- **vllm/model_executor/layers/fused_moe/cutlass_moe.py**：使用新接口并删减旧的 `topk_ids` 处理路径。  

**💡 关注建议**  
1. **兼容性检查**：新 kernel 只在 SM 90/100/120 编译时可用，运行在低于这些 compute capability 的 GPU 将触发 `TORCH_CHECK_NOT_IMPLEMENTED`。建议在发布说明中明确最低硬件要求，或在 Python 层加入运行时检测并回退到旧实现。  
2. **参数校验**：虽然 C++ 已加入多项 `TORCH_CHECK`，仍需确保 Python 调用时 `expert_first_token_offset` 与 `problem_sizes` 的维度、dtype、contiguity 正确，否则会在 CUDA kernel 前抛异常。  
3. **测试覆盖**：请补充单元测试，验证 `swap_ab` 为 `True/False` 时 problem size 完全匹配旧实现的输出，尤其在 `a1q` 行数恰好为 64 边界处。  
4. **性能基准**：建议在不同专家数量、token 长度、不同 GPU（A100、H100）上跑完整的 E2E throughput 与 TTFT 基准，以确认 5%+ 提升在所有场景下都成立。  

总体来看，此次改动通过在 CUDA 层直接利用已有的 token‑offset 信息，省去一次 `topk_ids` 计算与拷贝，显著降低了 CPU‑GPU 同步和内存开销，是一次安全且有价值的性能提升。

---

### [Frontend][gpt-oss] Allow system message to overwrite model identity (#31737)
**SHA**: `f32c629` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f32c629eb4cc2bbe85d321a6b382e30e33a32f7d)

**🎯 变更类型**：功能增强（允许 system 消息覆盖模型身份）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `test_system_prompt_override`，验证在 OpenAI 兼容层中 `system` 消息可直接覆盖模型默认的 system prompt（如 “You are a pirate…”）。  
2. `harmony_utils.parse_response_input` 逻辑调整：仅在 `role == "developer"` 时添加 “Instructions:\n” 前缀，去掉旧的将 `system` 自动改为 `developer` 的行为。  
3. `serving_responses.py` 新增 `_extract_system_message_from_request`，在请求中抽取首个 `system` 消息内容并传递给 `get_system_message`，实现模型身份的自定义。  
4. 在 `_construct_input_messages_with_harmony` 中过滤掉 `system` 消息，使其不再被当作普通聊天消息加入 `messages` 列表。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/serving_responses.py`（请求解析、系统提示构造）  
- `vllm/entrypoints/openai/parser/harmony_utils.py`（消息解析规则）  
- OpenAI 接口的单元测试 `tests/entrypoints/openai/test_response_api_with_harmony.py`  

**💡 关注建议**  
1. **兼容性**：之前将 `system` 自动转为 `developer` 的实现会影响已有使用 `system` 消息的场景，升级后应在发布说明中明确该行为已改为“仅作模型身份覆盖”。  
2. **安全/滥用**：允许用户随意重写系统提示可能被用于规避安全约束，建议在 `get_system_message` 中加入长度或内容校验，或在配置中提供开关。  
3. **错误处理**：`_extract_system_message_from_request` 当 `system` 消息缺失时返回 `None`，确保下游 `get_system_message` 能安全处理 `None`（目前已如此）。  
4. **文档**：在 OpenAI 接口文档中补充 `system` 消息的使用方式、限制以及与 `developer` 角色的区别。  
5. **测试覆盖**：当前仅在 Harmony 模式下测试，建议补充非‑Harmony（普通 OpenAI）路径的单元测试，防止意外回归。  

总体代码清晰，改动集中在系统提示的抽取与解析，若上述安全与兼容性注意点得到确认，可放心合并。

---

### [Feat][Core] Support multiple KV cache groups in Hybrid KV Coordinator (#31707)
**SHA**: `cd4a95e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cd4a95e3aa0478d441964fa9afc062febffd65bf)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 为 HybridKVCacheCoordinator 引入对 *多组* KV‑cache 的支持，去除了仅能同时容纳两类（FullAttention + 其它）缓存的限制。  
- 新增 `MambaSpec` 在测试用例中的使用，实现 “full + sliding_window + mamba” 等复合模型的前缀缓存验证。  
- 重构 `verify_and_split_kv_cache_groups` 与 `find_longest_cache_hit`：按 **spec 类型** 分组、统一 LCM 计算、采用迭代固定点算法逐步收敛最长命中长度。  
- 扩充单元测试，覆盖 2‑4 组、不同 spec 组合、交叉排列等七十余种情形。

**🎯 影响范围**：  
- `vllm/v1/core/kv_cache_coordinator.py`（HybridKVCacheCoordinator 核心逻辑）  
- 相关的单类型管理器（`SingleTypeKVCacheManager`）  
- `tests/v1/core/test_prefix_caching.py`（大量新测试）  
- 可能波及 `KVCacheManager`、`KVCacheConfig`、`KVCacheGroupSpec` 的序列化/调度路径。

**💡 关注建议**：  
1. **向后兼容**：部分外部代码仍可能假设只会出现 “full + other” 两类分组，建议在文档或 `HybridKVCacheCoordinator.__init__` 中明确说明新行为，并在必要时保持旧实现的兼容入口。  
2. **性能验证**：新的迭代收敛式 `find_longest_cache_hit` 在组数较多时会多次遍历所有管理器，务必在真实模型（>8 组）上跑基准，确认未出现显著回退。  
3. **LCM 计算**：`self.lcm_block_size = lcm(*block_sizes)` 已改为支持任意数量的块大小，检查 `lcm` 实现是否在大数情况下仍然高效且不会溢出。  
4. **规格相等判断**：`assert manager_cls is existing_cls` 与 `existing_spec == spec` 依赖 `KVCacheSpec.__eq__` 正确实现，确保 `MambaSpec` 等自定义 spec 能可靠比较。  
5. **测试覆盖**：新加入的组合测试已相当全面，建议在 CI 中保留这些参数化用例，防止以后对 `KVCacheSpec` 增添新字段导致不匹配。  
6. **日志/错误信息**：在 `verify_and_split_kv_cache_groups` 与 `find_longest_cache_hit` 中加入更具可读性的异常信息，帮助用户定位因块大小不匹配导致的 cache miss。  

总体来看，此次提交显著提升了 vLLM 对混合注意力模型的适配能力，但也引入了更复杂的分组与命中计算逻辑，建议在正式发布前进行跨模型、跨硬件的压测并完善文档说明。

---

### [UX] Add vLLM model inspection view (#29450)
**SHA**: `d5ec6c0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d5ec6c056f2f806ea119b9129b07994f1d4fef4e)

**🎯 变更类型**：功能增强（模型可视化/调试）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `LLM` 实例新增 `__repr__`，在首次调用时通过 `collective_rpc("get_model_inspection")` 获取一次模型层级结构，并缓存结果，避免后续的 RPC 开销。  
2. 新增环境变量 `VLLM_LOG_MODEL_INSPECTION`，开启后在模型加载完成后自动打印同样的层级视图。  
3. 引入 `vllm/model_inspection.py`，实现对 `nn.Module` 的递归遍历、相同子层合并、量化信息展示等，输出类似 Transformers 的层级字符串。  
4. `WorkerBase` 增加 `get_model_inspection` 接口，供 RPC 调用。  
5. 小幅格式化修正（`RotaryEmbedding.extra_repr` 逗号、`vllm/entrypoints/llm.py` 缓存字段、`envs.py` 新变量）。  

**🎯 影响范围**  
- `vllm/entrypoints/llm.py`（`LLM` 对象）  
- `vllm/envs.py`（新增 env 变量）  
- `vllm/model_loader/base_loader.py`（加载阶段日志）  
- `vllm/model_inspection.py`（全新模块）  
- `vllm/v1/worker/worker_base.py`（RPC 接口）  
- 相关依赖：`torch.nn`、分布式 RPC 框架  

**💡 关注建议**  
1. **性能**：首次调用 `repr(llm)` 会触发一次 `collective_rpc`，在大规模集群上可能产生网络抖动。建议在文档中提示：仅在调试/分析时使用 `repr`，生产代码可关闭 `VLLM_LOG_MODEL_INSPECTION`。  
2. **缓存一致性**：`_cached_repr` 在进程生命周期内保持不变，即使模型被重新加载（如 `reload`）仍会返回旧结构。若支持热更新，请在模型重新加载后手动清除该缓存或在 `load_model` 中复位。  
3. **异常安全**：`collective_rpc` 可能抛异常或返回空列表。当前实现已做空检查，但若 RPC 超时或返回非字符串，会导致 `repr` 返回默认信息。可考虑捕获异常并记录警告。  
4. **兼容性**：新方法 `get_model_inspection` 通过 RPC name，需要在所有 worker 实例实现该接口；已有自定义 worker 子类若未实现会导致 RPC 调用失败。确认所有自定义 worker 继承 `WorkerBase` 或自行实现此方法。  
5. **日志量**：开启 `VLLM_LOG_MODEL_INSPECTION=1` 会在模型加载时产生大量日志（尤其对大模型），可能影响日志存储和阅读。建议在 CI/单元测试中默认关闭，仅在本地调试时手动打开。  
6. **测试覆盖**：新增的字符串格式化逻辑（层级合并、量化信息）应加入单元测试，确保不同模型（Llama、Mistral、量化/未量化）输出符合预期，防止因模块实现变化导致误报。  

总体而言，此次改动为调试/可视化提供了便利，但在生产环境需要慎重开启，并关注首次 RPC 带来的延迟以及缓存失效的边界情况。

---

### [Doc] Add developer guide for CustomOp (#30886)
**SHA**: `08d954f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/08d954f03659cb08148b77cd2e0d33b77f6bd6ef)

**变更类型**：文档/轻量功能说明（新增 `docs/design/custom_op.md`，在多处源码中加入 `--8<--` 标记以支撑文档抽取；同时对 `compilation.py` 与 `custom_op.py` 做了两行注释/行为微调）。

**核心变更**  
1. **文档**：完整的 CustomOp 开发者手册加入仓库，并在 30+ 业务实现文件中添加 **代码块提取标记**（`--8<-- [start:…]` / `--8<-- [end:…]`），用于自动生成 API 文档。仅为注释，不影响运行时逻辑。  
2. **编译配置**：`CompilationConfig` 中判断 Inductor‑禁用 custom‑op 的条件从 `mode>=VLLM_COMPILE` 改为 `mode>CompilationMode.NONE`，细化了 “非 NONE” 与 “NONE” 的区分。  
3. **dispatch_forward**：将 “enforce enable” 相关说明迁至注释，并加上即将废弃的提示。  
4. 其余改动均为 **代码注释/标记**（如 `# --8<-- [start:fatrelu_and_mul]`），不涉及功能实现。

**影响范围**  
- 文档生成、示例抽取流程。  
- `vllm/config/compilation.py` 的默认行为在 **Inductor + torch‑compile** 场景下会略有变化：只要 `mode` 为 `CompilationMode.NONE`（即不使用 `torch.compile`），custom‑op 仍会保持“默认启用”。此改动可能导致已有脚本在 `mode=NONE` 时仍看到 custom‑op 被禁用的情况，需要在 CI 中验证。  
- 其它模块仅添加了标记，保持原有执行路径不变。

**建议**  
1. **功能回归**：在不同 `CompilationMode`（NONE、FULL、...）以及 `backend="inductor"` 的组合下跑一次完整的模型推理基准，确认 custom‑op 启停行为与预期一致。  
2. **标记安全**：确保文档抽取工具在解析 `--8<--` 标记时不会误删或截断代码，建议在 CI 中加入一次 “doc‑build” 步骤。  
3. **废弃提示**：`enforce_enable` 将被移除，最好在 `CustomOp.__init__` 中加入 `DeprecationWarning`，并在 README/CHANGELOG 中明确迁移路径。  
4. **单元测试**：为 `CustomOp.register`、`register_oot`、以及 `dispatch_forward` 的平台分支各写一条最小化测试，防止未来改动误删注册表或分支逻辑。  

整体来看，此次提交主要提升可读性与文档可维护性，对运行时影响极小，只需注意 **Inductor‑mode 判断** 的细微行为变更以及未来的 `enforce_enable` deprecation。

---

#### 🟢 低重要度变更 (15)

### [Bugfix] fix offline chat output prompt (#32076)
**SHA**: `14fc7a6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/14fc7a68c7d671d7bcea951a7edc6ac718ced964)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在离线聊天示例中，引入 `RequestOutput` 类型并修改 `print_outputs` 为接受 `outputs` 与对应 `prompts`，确保输出的 Prompt 与生成文本一一匹配，修复原有提示错误。

---

### [Bugfix] fix encoder cache leak of waiting requests in scheduler to solve stuck in CPU scheduling (#31857)
**SHA**: `a01a1c0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a01a1c0d691f89fa4788102eb0bd9b15607c1cca)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `scheduler.schedule` 中，当请求因缺少可用块而无法调度时，若其包含 encoder 输入，则调用 `self.encoder_cache_manager.free(request)` 释放 encoder 缓存，防止缓存泄漏导致 CPU 调度卡死。

---

### [Misc] Delay deprecation of CommonAttentionMetadata properties (#32074)
**SHA**: `da6709c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/da6709c9fe6965b7348692576ffadeee8439388e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `CommonAttentionMetadata` 中已废弃属性的删除时间从 v0.14.0 推迟至 v0.15.0，更新相应注释和文档提示。

---

### [ROCm][CI] Fix flaky `test_function_calling_with_stream` and reduce schema test examples (#32063)
**SHA**: `d83becd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d83becd503660fb876ea42beaa9f63217b857b99)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 OpenAI schema 测试中限制示例数量；在流式函数调用测试中修复偶发失败，确保 `get_weather` 被调用并验证结果。

---

### Update modelopt KV cache quantization resolution to new scheme (#31895)
**SHA**: `0c96148` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0c9614876e17680474c240cf9f3138a408a6ec5b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：更新 modelopt KV 缓存量化解析，新增对 `kv_cache_scheme` 的读取，支持 dict 配置的解析并在未知情况下回退到 `auto`。

---

### [Core] Refactor ColumnParallelLinear: remove unused parameter and optimize forward (#31939)
**SHA**: `52d4282` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/52d428295d2256cde1b4a0e781a77083679463ff)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 ColumnParallelLinear 中未使用的 `output_sizes` 参数，调整 bias 返回位置，简化实现并提升前向性能。

---

### [Bugfix][Hardware][AMD] Use dynamic WARP_SIZE in sampler vectorized_process (#31295)
**SHA**: `c60578d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c60578de0a389d261fcb4d3323c75387b98b1fbb)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `csrc/sampler.cu` 引入 `cuda_compat.h`，使用动态 `WARP_SIZE`（`kWarpSize`）替代硬编码 32，以兼容 AMD GPU 的 Wave64 与 Wave32 结构，相关静态断言也相应更新。

---

### resolve pydantic error in startup benchmark (#31348)
**SHA**: `abd9224` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/abd9224280784b5496f24138b3f84cadefc47c5d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增启动基准测试用例；修改 `startup.py` 将子进程函数参数从字典改为 `EngineArgs` 实例，去除 `EngineArgs` 的字典序列化，解决 Pydantic 初始化错误，提升代码可靠性。

---

### [Bugfix] Narrow broad exceptions in compilation backends (#31616)
**SHA**: `4dc0d60` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4dc0d606b78eb8dbb5e833ab45d3fbb1378edf35)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将编译后端读取文件时的异常捕获从宽泛的 `Exception` 收窄为只捕获 `OSError`，避免误捕其他异常导致隐藏错误。

---

### [CI] Allow Deprecated Quantization For LM Eval Tests (#32065)
**SHA**: `ac0675f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ac0675ff6b40768293ef4b87741b161f6cf4518b)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 LM‑Eval 测试脚本中加入 `allow_deprecated_quantization=True` 标志，以支持旧版量化模型的评估。

---

### [Perf] Optimize async scheduling placeholder using empty (#32056)
**SHA**: `e18464a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e18464a57d145872899463838ca07050ea15141b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `output_processor.py` 中新增全局 `EMPTY_CPU_TENSOR`，用其取代 `torch.randn(0, device="cpu")` 作为占位 tensor，减少重复创建，提升异步调度的性能。

---

### [Core] Use weights_only=True with torch.load (#32045)
**SHA**: `1963245` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1963245ed15a9f232ac3b763c7a03351d77ae799)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tensorizer.py` 中加载 `.bin` 权重文件时，加入 `weights_only=True` 参数，避免加载不必要的梯度/元数据，提高安全性和加载效率。

---

### [Misc] Enable async scheduling by default with spec decoding (#31998)
**SHA**: `3adffd5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3adffd5b9038c9ea94cb764c5cfe9f67d0207ec8)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：默认启用异步调度；在 pipeline_parallel >1、与不兼容的 speculative 解码或不支持的 executor 后端等情况下，通过 `logger.warning_once` 给出提示并关闭 async 调度；同时改进相关错误提示信息。

---

### [fix] add cutedsl to global sf (#32001)
**SHA**: `f9e2a75` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f9e2a75a1ee1d339ec5d885f842b3cfc27d71e02)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `flashinfer_utils.py` 中将 `FlashinferMoeBackend.CUTEDSL` 新增至支持全局尺度因子（global sf）的后端列表。

---

### [Refactor] Remove numpy split in async scheduling (#32034)
**SHA**: `28ae32a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/28ae32a5d3717d4d5fc73c2338cca909775910c0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：去除 `numpy.array_split`，改为基于 `VLLM_V1_OUTPUT_PROC_CHUNK_SIZE` 的手动切片循环，简化异步输出调度并删除不再使用的 `numpy` 与 `cdiv` 依赖。

---

