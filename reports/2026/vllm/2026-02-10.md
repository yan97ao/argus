# 每日更新报告（2026-02-10）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-10 23:57:11 | Vadim Gimpelson | [BUGFIX] Fix accuracy bugs in Qwen3-Next MTP (#34077) |
| 2026-02-10 23:46:24 | junuxyz | [Core][BugFix] Fix PP KV cache sharding memory validation (#33698) |
| 2026-02-10 23:29:52 | Roberto L. Castro | [Perf][Kernel] Add faster topKperRow decode kernel for DeepSeek-V3.2 sparse attention (#33680) |
| 2026-02-10 23:24:42 | Zhengxu Chen | [compile] Enable AOT compile with 2.10 in trunk. (#34155) |
| 2026-02-10 23:08:05 | xuebwang-amd | [ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations  (#29008) |
| 2026-02-10 23:04:16 | mgazz | Support benchmarking of Geospatial models  (#33922) |
| 2026-02-10 22:23:52 | Fan Yang | add --insecure arg to the vllm bench to skip TLS (#34026) |
| 2026-02-10 21:46:01 | Harry Mellor | Bump `mamba-ssm` version in CI for Transformers v5 compatibility (#34233) |
| 2026-02-10 21:05:32 | Krish Gupta | [V1][BugFix] Fix EAGLE3 encoder cache miss with disable_chunked_mm_input (#34220) |
| 2026-02-10 20:08:20 | Harry Mellor | Stop testing for slow tokenizers as they will not exist soon (#34235) |
| 2026-02-10 19:16:21 | Phúc H. Lê Khắc | [Misc] allow specify is_mm_prefix_lm in hf_config (#34215) |
| 2026-02-10 18:51:48 | tc-mb | Add flagos in MiniCPM-o (#34126) |
| 2026-02-10 18:41:24 | Roger Wang | [Bugfix] Fix FI kernel`chunk_gated_delta_rule` output shape for Qwen3.5 (#34219) |
| 2026-02-10 18:16:26 | zzaebok | [Docs] Fix format error in KV load failure recovery doc (#34137) |
| 2026-02-10 16:29:10 | Cyrus Leung | [Bugfix] Fix `--trust-remote-code` conflict (#34218) |
| 2026-02-10 16:22:03 | Zetong Li | [Bugfix] Fix memory inconsistency in cross-process shared memory (#32022) |
| 2026-02-10 15:59:04 | Cyrus Leung | Revert #34208 (#34216) |
| 2026-02-10 15:54:41 | Wentao Ye | [Perf] Optimize detokenizer python logic (#32975) |
| 2026-02-10 15:41:16 | Chen Zhang | [BugFix] Avoid prefix cache hit in the same schedule step for mamba layers (#29387) |
| 2026-02-10 15:30:19 | wang.yuqi | [Frontend][CI]  Consolidate instrumentator entrypoints (#34123) |
| 2026-02-10 15:06:30 | Balaxxe | [Bugfix] Sort hf_weights_files in fastsafetensors_weights_iterator to match #33491 (#34190) |
| 2026-02-10 14:53:07 | Andrew Xia | [responsesAPI] fix simpleContext streaming output_messages (#34188) |
| 2026-02-10 14:37:50 | Cyrus Leung | [Bugfix] Add `--trust-remote-code` to dataset bench args (#34208) |
| 2026-02-10 13:29:39 | Lucas Wilkinson | [Bugfix] Fix DP Attention Padding in Dummy Run (#34187) |
| 2026-02-10 13:18:57 | Cyrus Leung | [CI/Build] Relax `test_mcp_tool_call` (#34204) |
| 2026-02-10 13:12:13 | Cyrus Leung | [Doc] Update usage of `--limit-mm-per-prompt` (#34148) |
| 2026-02-10 13:03:32 | Roger Wang | [Bugfix][Core] Fix CPU memory leak from Request reference cycle in prefix caching (#34183) |
| 2026-02-10 12:50:20 | Andreas Karatzas | [ROCm][Bugfix] Resolve Dynamo tracing crash from amdsmi calls in on_gfx* arch detection (#34108) |
| 2026-02-10 11:47:54 | Roger Wang | [Bugfix] Adopt `ChunkGatedDeltaRule` for Qwen3.5 (#34198) |
| 2026-02-10 09:18:42 | Yuwei An | [LMCache] Token Base IPC API (#34175) |
| 2026-02-10 07:49:09 | Ning Xie | [structured output] validate unsupported json features first (#33233) |
| 2026-02-10 07:38:54 | Gregory Shtrasberg | [Bugfix][ROCm][GPT-OSS] Use old triton_kernels implementation on ROCm if the new API is not available (#34153) |
| 2026-02-10 07:33:43 | Michael Goin | [Doc] Add DCP support to attention backend doc (#33936) |
| 2026-02-10 05:47:17 | Nick Hill | [ModelRunner V2][BugFix] Fix `max_query_len` calculation (#34167) |
| 2026-02-10 05:15:43 | Charlie Fu | [torch.compile][Fusion] Fix attention fusion pass removing kv_udpate op. (#33945) |
| 2026-02-10 03:36:30 | Hongxia Yang | [ROCm] update triton branch to support gpt-oss models for gfx11xx devices (#34032) |
| 2026-02-10 03:30:38 | Artus Krohn-Grimberghe | [Bugfix] Voxtral prompt/audio placeholder alignment (#34140) |
| 2026-02-10 03:17:44 | Artus Krohn-Grimberghe | [Bugfix] Avoid duplicate k-proj weight emission in helper (#34142) |
| 2026-02-10 01:17:25 | Jiangyun Zhu | [Kernel] use flashinfer for gdn prefill (#32846) |
| 2026-02-10 00:44:18 | TomerBN-Nvidia | [Bugfix] Fix shared expert input for latent MoE in EP+DP (Nemotron-H) (#34087) |

### 📊 统计摘要
> 本日共 40 个提交 | 🔴高 2 | 🟡中 16 | 🟢低 22
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[Perf][Kernel] Add faster topKperRow decode kernel for De...](#afdce12)
    - [[ROCm][Quantization] GPT_OSS in amd-quark format model lo...](#b129136)
  - [🟡 中重要度变更 (16)](#-🟡-中重要度变更-16)
    - [[Core][BugFix] Fix PP KV cache sharding memory validation...](#c5a66d1)
    - [Support benchmarking of Geospatial models  (#33922)](#599e433)
    - [add --insecure arg to the vllm bench to skip TLS (#34026)](#a194657)
    - [[V1][BugFix] Fix EAGLE3 encoder cache miss with disable_c...](#748625c)
    - [[BugFix] Avoid prefix cache hit in the same schedule step...](#97fa8f6)
    - [[Frontend][CI]  Consolidate instrumentator entrypoints (#...](#dab1de9)
    - [[responsesAPI] fix simpleContext streaming output_message...](#9608844)
    - [[Doc] Update usage of `--limit-mm-per-prompt` (#34148)](#25e48a3)
    - [[Bugfix][Core] Fix CPU memory leak from Request reference...](#8a5e0e2)
    - [[ROCm][Bugfix] Resolve Dynamo tracing crash from amdsmi c...](#4cde2e0)
    - [[LMCache] Token Base IPC API (#34175)](#e94ec59)
    - [[Bugfix][ROCm][GPT-OSS] Use old triton_kernels implementa...](#c60f8e3)
    - [[Doc] Add DCP support to attention backend doc (#33936)](#5e75a14)
    - [[ModelRunner V2][BugFix] Fix `max_query_len` calculation ...](#e7e5278)
    - [[Kernel] use flashinfer for gdn prefill (#32846)](#285bab4)
    - [[Bugfix] Fix shared expert input for latent MoE in EP+DP ...](#995bbf3)
  - [🟢 低重要度变更 (22)](#-🟢-低重要度变更-22)
    - [[BUGFIX] Fix accuracy bugs in Qwen3-Next MTP (#34077)](#000214c)
    - [[compile] Enable AOT compile with 2.10 in trunk. (#34155)](#82e1197)
    - [Bump `mamba-ssm` version in CI for Transformers v5 compat...](#d0bc520)
    - [Stop testing for slow tokenizers as they will not exist s...](#6141397)
    - [[Misc] allow specify is_mm_prefix_lm in hf_config (#34215)](#94de871)
    - [Add flagos in MiniCPM-o (#34126)](#e042d7e)
    - [[Bugfix] Fix FI kernel`chunk_gated_delta_rule` output sha...](#ae4e280)
    - [[Docs] Fix format error in KV load failure recovery doc (...](#cbea11c)
    - [[Bugfix] Fix `--trust-remote-code` conflict (#34218)](#2c32558)
    - [[Bugfix] Fix memory inconsistency in cross-process shared...](#5f97012)
    - [Revert #34208 (#34216)](#998e2d9)
    - [[Perf] Optimize detokenizer python logic (#32975)](#e1060a7)
    - [[Bugfix] Sort hf_weights_files in fastsafetensors_weights...](#8d48d0a)
    - [[Bugfix] Add `--trust-remote-code` to dataset bench args ...](#f69b903)
    - [[Bugfix] Fix DP Attention Padding in Dummy Run (#34187)](#81e217f)
    - [[CI/Build] Relax `test_mcp_tool_call` (#34204)](#ab97bcf)
    - [[Bugfix] Adopt `ChunkGatedDeltaRule` for Qwen3.5 (#34198)](#047a457)
    - [[structured output] validate unsupported json features fi...](#1339784)
    - [[torch.compile][Fusion] Fix attention fusion pass removin...](#bb9f973)
    - [[ROCm] update triton branch to support gpt-oss models for...](#4d39650)
    - [[Bugfix] Voxtral prompt/audio placeholder alignment (#34140)](#8fd31f6)
    - [[Bugfix] Avoid duplicate k-proj weight emission in helper...](#eadb4e8)
#### 🔴 高重要度变更 (2)

### [Perf][Kernel] Add faster topKperRow decode kernel for DeepSeek-V3.2 sparse attention (#33680)
**SHA**: `afdce12` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/afdce12c89555ce7b7bd4f3215b5d844de0a32ed)

**🎯 变更类型**：性能优化 / 功能增强  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 在 `csrc/topk.cu` 中实现了面向 DeepSeek‑V3.2 稀疏注意力的 **large_context_topk** CUDA kernel，专为序列长度 ≥ 8 K、batch ≤ 128 的场景提供更高的吞吐与更低的延迟。  
2. 将新 kernel 暴露至 Python，新增 `large_context_topk` 接口并在 `sparse_attn_indexer` 与 `mla/indexer` 中加入自动选择逻辑（依据 batch 大小与最大序列长切换到新 kernel）。  
3. 更新 CMake、ops.h、torch_bindings 与单元测试，以确保新路径的正确性与回归检测。  

**🎯 影响范围**  
- 核心 CUDA 扩展：`csrc/topk.cu`、`csrc/ops.h`、`csrc/torch_bindings.cpp`  
- 编译系统：`CMakeLists.txt`  
- 注意力稀疏索引器：`vllm/model_executor/layers/sparse_attn_indexer.py`、`vllm/v1/attention/backends/mla/indexer.py`  
- 单元测试：`tests/kernels/test_top_k_per_row.py`  

**🔍 技术洞察**  

- **架构影响**  
  - 引入了新的 `FastTopKParams` 结构和 `large_context_topk` 调度入口，属于 **算子层** 的扩展，不会影响上层模型调度或分布式通信逻辑。  
  - 索引器元数据 (`DeepSeekV32IndexerDecodeMetadata`) 增加 `use_large_context_topk` 与 `offsets` 字段，保持向后兼容，仅在构建阶段填充。  

- **性能影响**  
  - 共享内存从 128 KB 降至 **32 KB**（或 ROCm 48 KB），提升 occupancy，降低每块的寄存器需求，使得在 **长序列** 场景下每个 block 能够同时运行更多 warps。  
  - 采用 **分段 radix‑histogram** + **ping‑pong 缓冲** 的多轮细化策略，避免全局排序，理论复杂度从 `O(N log K)` 降至 `O(N)`（常数系数受 256‑radix 影响），在官方基准中对 batch≤128、seq_len>8K 时可达 **30%‑45%** 的加速。  
  - 对短序列仍使用原有 `top_k_per_row_decode`，避免不必要的调度开销，保证整体吞吐不倒退。  

- **安全考虑**  
  - 所有入口均使用 `TORCH_CHECK` 验证张量位于 CUDA 设备、维度与连贯性，防止非法内存访问。  
  - 动态共享内存大小通过 `cudaFuncSetAttribute` 在模块初始化时一次性设置，若设置失败即抛异常，避免隐式的 OOM。  
  - 未引入外部库或系统调用，无已知安全漏洞。  

**⚠️ 潜在风险**  

1. **边界条件**：`seq_len <= TopK` 时直接返回 `naive_topk_cuda`，但该实现仅写入前 `seq_len` 位置，未清零余下的 `TopK‑seq_len` 项，可能在后续使用未初始化索引。  
2. **共享内存上限**：虽然设为 32 KB，但在显存较小的 GPU（例如 RTX 3050）上可能因 **每块共享内存需求** 超过硬件上限导致 kernel 启动失败。  
3. **调度阈值硬编码**：`batch_size <= 128 && max_seq_len > 8192` 的判断在不同硬件或未来模型（如更大 batch）上可能产生回退或性能倒挂。  
4. **row_starts / offsets 未经完整测试**：在 `large_context_topk` 中 `row_starts` 仍为可选，但未在测试里覆盖非空路径，潜在的偏移错误未被捕获。  
5. **编译兼容性**：对 ROCm 使用 `hipcub`，但在某些旧版 ROCm 环境中 `cudaFuncSetAttribute` 替代实现可能缺失，导致编译或运行时错误。  

**💡 关注建议**  

- **完善边界清理**：在 `naive_topk_cuda` 中将 `output_indices[i] = -1`（或填充合法占位）以避免后续使用未定义值。  
- **动态阈值调优**：提供可配置的 `large_context_topk` 启用阈值（如通过环境变量或运行时参数），让用户根据硬件自行调节。  
- **跨 GPU 验证**：在 CI 中加入 **低显存 GPU**（如 6 GB）以及 **不同算力**（3.5、7.0、8.6）上的跑分，确保共享内存设定不会导致启动失败。  
- **扩展测试**：补充 `row_starts` 与 `offsets` 非空的单元测试，覆盖 `next_n > 1` 的 speculative decoding 场景。  
- **监控回退路径**：在 `large_context_topk` 调用前后捕获 `cudaGetLastError`，如出现 `cudaErrorInvalidConfiguration`，自动回退到 `top_k_per_row_decode`，并记录日志。  
- **文档更新**：在项目 README 或 kernel 文档中注明新 kernel 的适用场景、显存需求以及调参建议，帮助用户做出正确的选择。  

---  
整体来看，此次改动为 **DeepSeek‑V3.2 稀疏注意力** 引入了专门的长序列 Top‑K kernel，能够在关键解码阶段显著提升吞吐，架构上保持向后兼容。只要妥善处理上述风险并在多硬件环境中验证，预计对 vLLM 整体性能将产生 **正向且显著** 的提升。

---

### [ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations  (#29008)
**SHA**: `b129136` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b129136c7a7389133c923123a1ebd76c4401c94d)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 为 GPT‑OSS 系列模型加入对 AMD Quark（MXFP4 / MXFP6）量化格式的完整加载与推理支持，包括权重、偏置、激活尺度的处理。  
2. 新增 `mxfp4_w4a8_moe_quant_config`、`mxfp4_w4a16_moe_quant_config` 等量化配置函数，扩展 `FusedMoEQuantConfig` 的可配置范围。  
3. 通过 `maybe_roundup_hidden_size` 与层级实现的修改，实现对 MXFP 系列模型在不同 Tensor‑Parallel (TP) 环境下自动对隐藏维度向上对齐，确保 OCP‑MX 权重量化的对齐要求得到满足。  
4. 大量内部 API 调整：  
   - `QuantizationConfig.is_mxfp4_quant` 抽象化，统一在不同量化实现（Mxfp4、Quark）中检测。  
   - `QuarkMoEMethod` 与 `QuarkOCP_MX_MoEMethod` 添加对 bias、激活尺度、权重尺度的加载与后处理逻辑。  
   - `utils.moe_kernel_quantize_input` 支持 OCP‑MX 方案的 Q‑D‑Q（Quant‑Dequant）仿真。  
5. 新增端到端量化准确性测试 `tests/models/quantization/test_gpt_oss.py`，并移除旧的仅针对 attention 量化的测试。  

**🎯 影响范围**  
- **模型加载层**：`vllm/model_executor/models/gpt_oss.py`、`vllm/model_executor/layers/fused_moe/*`、`vllm/model_executor/layers/quantization/*`  
- **量化配置**：`vllm/model_executor/layers/fused_moe/config.py`  
- **通用工具**：`vllm/model_executor/layers/quantization/utils/ocp_mx_utils.py`、`vllm/model_executor/layers/quantization/utils/moe_kernel_quantize_input`  
- **测试套件**：新增 `tests/models/quantization/test_gpt_oss.py`、修改 `tests/kernels/moe/test_gpt_oss_triton_kernels.py`，删除旧测试文件。  

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | - 增加了 *Quark* 与 *MXFP4/MXFP6* 两条专门支路，涉及权重加载、偏置、尺度统一抽象。<br>- `FusedMoE` 在实例化前通过 `maybe_roundup_hidden_size` 进行隐藏维度对齐，使得后续 `FusedMoEConfig` 能够直接使用对齐后的尺寸，避免在算子内部再次处理对齐。<br>- 引入 `model_type == "gpt_oss"` 的特化路径，以兼容 GPT‑OSS ckpt 中 “gate_up_proj” 已经融合的结构。 |
| **性能影响** | - 对 **MXFP4** 权重量化在支持硬件（AMD GPU）上走原生 `Mxfp4Backend`，若平台不支持则走 *emulation*（先解压再 FP8 仿真），可能导致显著的内存与计算开销。<br>- `maybe_roundup_hidden_size` 在 TP 大于 1 时会向上对齐到 64/256（取决于平台），会略增显存占用，但能避免因对齐错误导致的崩溃。<br>- 新的 `QuarkOCP_MX_MoEMethod` 在 **ROCm‑AIter** 路径下仍然保持原有的 fused‑MoE kernel 调用，只有在不支持 MXFP4 时才切换到 `fused_experts`（纯 Python/Triton）实现，性能回退可预期。 |
| **安全考虑** | - 代码中加入了对外部模型（`huggingface.co/amd`）访问的检测，只在 Hub 可达且 `amd-quark>=0.9` 时才运行新测试，避免意外下载未知模型。<br>- 新增的 `default_weight_loader`、`maybe_remap_kv_scale_name` 均保持原有的安全检查（如 shape、numel 校验），未出现新的路径注入或文件系统写入风险。 |
| **可维护性** | - 把 “是否是 MXFP4 量化” 的判定抽离到 `QuantizationConfig.is_mxfp4_quant`，统一了不同量化实现的检查，降低未来新增量化格式的改动成本。<br>- `QuarkMoEMethod` 与 `QuarkOCP_MX_MoEMethod` 中大量条件分支（`self.model_type == "gpt_oss"`、`self.ocp_mx_scheme.startswith(...)`）可能导致阅读与调试难度提升，建议后续抽象为策略类。 |
| **兼容性** | - 对已有非 GPT‑OSS、非 MXFP4 模型的加载路径没有任何行为变更，所有新路径都受 `if self.model_type == "gpt_oss"` 限制。<br>- 仅在 `quant_method == "quark"` 时走新逻辑，旧的 `quant_method == "mxfp4"` 仍保持原有实现。 |

**⚠️ 潜在风险**  

1. **显存激增**：`maybe_roundup_hidden_size` 对齐可能导致在极大模型（如 120B）与高 TP（≥8）时出现显存不足。  
2. **回退路径未覆盖**：当平台不支持 MXFP4 且 `use_rocm_aiter_moe` 为 `True`，仍会走 `self._emulate = True`，但部分代码（如 `Mxfp4Backend` 初始化）仍假设后端可用，可能触发未捕获的 `None` 调用。  
3. **Quark 权重映射错误**：`_load_weights_quark` 中对 `expert_id`、`shard_id` 的计算相当复杂，若出现模型 ckpt 结构差异（如多余的层前缀）可能导致权重错位而不易发现。  
4. **测试依赖外部网络**：CI 环境若无法访问 HuggingFace 或缺少 `amd-quark` 包，会直接跳过关键量化准确性测试，导致回归失效检测。  
5. **Triton 路径未实现**：在 `QuarkOCP_MX_MoEMethod.apply` 中明确抛出 `NotImplementedError`，若用户强制开启 `mxfp4` + `TRITON` 后端并使用 GPT‑OSS，系统会在运行时异常。  

**💡 关注建议**  

| 关注对象 | 建议 |
|----------|------|
| **开发者** | - 在提交新模型（特别是自研 Quark ckpt）前，使用 `tests/models/quantization/test_gpt_oss.py` 本地跑一次，确保权重映射、bias、scale 全部加载成功。<br>- 为 `QuarkMoEMethod` 与 `QuarkOCP_MX_MoEMethod` 添加单元测试，覆盖 `self.model_type=="gpt_oss"` 与非 GPT‑OSS 两条分支。 |
| **运维 / CI** | - 为 CI 增加 `amd-quark` 包与网络连通性的前置检查，若缺失则自动标记为 “skip”。<br>-

---

#### 🟡 中重要度变更 (16)

### [Core][BugFix] Fix PP KV cache sharding memory validation (#33698)
**SHA**: `c5a66d1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c5a66d16970fbbc4633761d30f12ec1fc98a9523)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为流水线并行（PP）场景加入 KV‑cache 分片的内存校验逻辑。  
2. 新增 `_project_kv_cache_groups_to_worker`，把全局 KV‑cache 分组投射到单个 worker 的层集合上，并在 `UniformTypeKVCacheSpecs` 中只保留该 worker 拥有的子规格。  
3. `_auto_fit_max_model_len` 由全局分组改为使用 per‑worker 投射分组，逐 worker 计算可容纳的最大模型长度，并记录限制该长度的 worker 所用的可用内存。  
4. `get_kv_cache_configs` 相应改造：先投射分组、再在每个 worker 上执行自动适配与内存检查，防止出现 “worker 2 内存更多却把 worker 1 拒绝”的误判。  
5. 为上述改动添加单元测试，覆盖对称/非对称内存、投射逻辑以及统一/非统一 KV‑cache 规格。

**🎯 影响范围**  
- `vllm/v1/core/kv_cache_utils.py`（核心 KV‑cache 配置与自动适配实现）  
- `tests/v1/core/test_kv_cache_utils.py`（新增测试）  

**💡 关注建议**  
1. **兼容性**：`_project_kv_cache_groups_to_worker` 目前仅在 `get_kv_cache_configs` 中使用，若以后在其它路径（如运行时重新分配）直接调用 `KVCacheGroupSpec`，需要确认投射逻辑保持一致。建议在模块顶部增加对外的 `project_kv_cache_groups_to_worker` 接口或在文档中明确使用范围。  
2. **UniformType 处理**：投射过程中会重新构造 `UniformTypeKVCacheSpecs`，确保子字典键值完整且未意外丢失。如果层在全局组里出现多次（跨 worker），当前实现仍会生成多个仅包含该层的子规格，需确认这不会导致重复计数。  
3. **日志与错误信息**：在 `_auto_fit_max_model_len` 中记录了限制 worker 的可用内存，建议在日志中同时输出受限 worker 的索引，以便调试大型 PP 拓扑。  
4. **性能**：投射与逐 worker 计算在模型非常大、层数上千时会产生一定的 O(num_layers × num_workers) 开销。可以考虑在第一次投射后缓存结果，或在 `VllmConfig` 中加入一次性 “projected_groups” 字段。  
5. **测试覆盖**：当前测试已覆盖对称/非对称内存和 uniform 与 non‑uniform 两种规格，建议再加一次混合 hybrid 模型（不同层使用不同 block_size）下的自动适配，以确保 `_estimate_max_model_len_from_groups` 在投射后仍能正确计算。  

总体来看，此次改动解决了 PP 场景下 KV‑cache 内存校验的误判问题，逻辑清晰且已配套测试，风险不大。按照上述建议进行细化，可进一步提升稳健性与可维护性。

---

### Support benchmarking of Geospatial models  (#33922)
**SHA**: `599e433` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/599e4335a42bbb6f2cad75ac0b4be81272a77aa3)

**🎯 变更类型**：功能增强 / 兼容性提升  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为自定义数据集在 `sample()` 中加入对缺失 tokenizer 的容错，默认 `prompt_len=1`，并在未提供 `output_len` 时强制读取并校验 `output_tokens`。  
2. 在 `benchmarks/lib/endpoint_request_func.py` 中实现 `async_request_vllm_pooling`，并将其注册到 `ASYNC_REQUEST_FUNCS`，支持 vLLM Pooling API。  
3. `benchmarks/serve.py` 多处加入对 `tokenizer` 可能为 `None` 的分支，避免在只需统计 token 数的基准中强制初始化 tokenizer；新增 CLI 参数 `--skip-tokenizer-init` 以显式关闭 tokenizer/ detokenizer 初始化。  

**🎯 影响范围**  
- `vllm/benchmarks/datasets.py`（数据抽样逻辑）  
- `vllm/benchmarks/lib/endpoint_request_func.py`（新增 pool‑ing 请求函数）  
- `vllm/benchmarks/serve.py`（指标计算、日志输出、CLI 参数）  

**💡 关注建议**  
- **兼容性**：`skip-tokenizer-init` 关闭 tokenizer 时，所有依赖 token 长度的统计都会返回硬编码的 `1`，这会显著低估输入/输出 token 数，建议在文档中明确说明仅适用于 **Geospatial/向量相似度** 场景。  
- **错误信息**：`sample()` 对 `output_tokens` 的校验已经完善，建议在异常信息中加入请求索引，方便定位错误数据。  
- **测试覆盖**：为 `async_request_vllm_pooling` 增加单元测试，验证 `_validate_api_url`、payload 合并以及 header 处理的路径；同时为 `--skip-tokenizer-init` 的全流程（从 CLI 到指标打印）增加集成测试。  
- **性能**：在 `calculate_metrics` 中对 `output_len` 的回退逻辑已改为仅在 `tokenizer` 为 `None` 时返回 `1`，避免不必要的 tokenizer 调用，建议在基准报告中注明此 “硬编码 token 长度” 的假设，以免误读吞吐量结果。  

总体而言，此次提交为地理空间模型基准提供了必要的 “无 tokenizer” 支持，并引入 Pooling API，改动局部且向后兼容。后续请关注文档同步和对应测试的完整性。

---

### add --insecure arg to the vllm bench to skip TLS (#34026)
**SHA**: `a194657` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a1946570d80c1bef78063e84b097951d8e8d4e6a)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `vllm bench serve` 添加 `--insecure` 参数，允许在使用自签证书的 HTTPS 服务器时关闭 TLS 证书校验。  
- 通过 `ssl_context` 参数在内部统一传递 SSL 配置，默认对 `https://` URL 启用 TLS，对 `--insecure` 时设为 `False`（禁用验证）。  
- 新增测试 `test_bench_serve_insecure`：在临时目录生成自签证书，启动支持 SSL 的 `RemoteOpenAIServerSSL` 并验证该 flag 的可用性。  

**🎯 影响范围**  
- **vllm/benchmarks/serve.py**：CLI 参数、SSL 上下文处理、连接器创建逻辑。  
- **tests/benchmarks/test_serve_cli.py**：引入 `openssl` 调用、`urllib3` 警告抑制以及 `RemoteOpenAIServerSSL` 子类。  
- 相关导入 (`ssl`, `urllib3`, `requests`) 以及类型提示。  

**💡 关注建议**  
1. **安全审计**：`--insecure` 只应在测试或受信环境使用，建议在帮助信息中明确警示，避免在生产脚本中误用。  
2. **兼容性**：确认所依赖的 `aiohttp` 版本在 `TCPConnector(ssl=bool|SSLContext)` 上行为一致，避免在旧版出现类型错误。  
3. **异常处理**：`get_first_model_from_server` 与 `benchmark` 中若 `ssl_context=False`（即禁用校验）仍会触发 `aiohttp` 的默认 SSL 检查吗，需在 CI 中覆盖 `https://` + `--insecure` 场景。  
4. **测试环境**：`openssl` 必须预装，否则 CI 会因证书生成失败而中断；可在 `setup.cfg` 或 CI 脚本中声明依赖。  
5. **文档更新**：在 README/CLI 手册中加入 `--insecure` 说明、使用示例以及安全风险提示。  

总体上，本次改动实现了对自签 TLS 的便捷支持，代码结构清晰，但需加强安全警示和兼容性验证，以防在非受控环境误用。

---

### [V1][BugFix] Fix EAGLE3 encoder cache miss with disable_chunked_mm_input (#34220)
**SHA**: `748625c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/748625cdafd7898b163115d8c33c7c5521a708e8)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 在 `scheduler._try_schedule_encoder_inputs` 中加入对 **EAGLE‑3**（即 `shift_computed_tokens`）的考虑，避免在回滚调度范围时把需要的 encoder 输入遗漏，从而导致 “Encoder cache miss”。  
- 新增 `test_eagle3_mm_encoder_cache_with_shift`，复现并校验在 `disable_chunked_mm_input=True`、MM 区间与 shift 重叠的边界场景下，encoder 输入能够被正确调度。  

**🎯 影响范围**：  
- `vllm/v1/core/sched/scheduler.py`（调度核心逻辑）  
- `tests/v1/core/test_scheduler.py`（新增单元测试）  

**💡 关注建议**：  
1. **代码路径**：确认 `shift_computed_tokens` 在所有调度入口都有默认值（通常为 `self.spec_decode_config.shift_computed_tokens`），防止未初始化时出现 `NameError`。  
2. **兼容性**：该修改仅在 `disable_chunked_mm_input=True` 且启用 EAGLE（`num_speculative_tokens>0`）时生效，建议在文档中注明此行为的前置条件。  
3. **性能**：`max(0, …)` 的额外计算开销极低，但建议跑一次完整的性能基准，确保在高并发场景下不引入意外的调度延迟。  
4. **回归测试**：除新增测试外，执行全部 `v1` 与 `v2` 的测试套件，确保其他调度路径（比如普通 Chunked‑MM、无 MM 场景）仍保持原有行为。  
5. **后续审计**：若项目后续引入更多 “shift” 相关的优化（如不同的 speculative token 策略），请同步检查此处的边界计算是否仍然满足 `num_computed_tokens + shift_computed_tokens <= start_pos` 的约束。  

整体来看，此次修改精准定位了 EAGLE‑3 与 Chunked‑MM 组合下的调度缺陷，影响范围局限在调度器核心，风险有限，建议合并后立即跑全量 CI 与性能回归。

---

### [BugFix] Avoid prefix cache hit in the same schedule step for mamba layers (#29387)
**SHA**: `97fa8f6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/97fa8f65909d4d8f2eb0edc2137fb22f576a5b25)

**🎯 变更类型**：Bug 修复（防止同一步调度中 Mamba 层的前缀缓存被错误命中）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 KV‑Cache 协调器、管理器以及调度器中新增 `new_step_starts` 接口，调度一次后会清空本步已使用的 Mamba 缓存块集合。  
- `MambaManager.get_num_blocks_to_allocate` 中加入判断：若新计算的块正好是本步已缓存的块，则强制返回「不足块」让调度器把该请求推到下一个步。  
- 相应的单元测试 `test_apc_common_prefix_same_batch` 验证在同一批次、相同前缀的两条请求不会因本步缓存而产生错误的前缀复用，输出仍然正确。  

**🎯 影响范围**  
- `vllm/v1/core/kv_cache_coordinator.py`、`kv_cache_manager.py`、`single_type_kv_cache_manager.py`（Mamba 相关路径）  
- 调度器 `vllm/v1/core/sched/scheduler.py` 中调用新接口  
- 测试套件 `tests/models/language/generation/test_hybrid.py`、`tests/v1/core/test_prefix_caching.py`  

**💡 关注建议**  
1. **接口向后兼容**：`new_step_starts` 只在内部调用，目前未对外文档暴露，仍需在未来的发布说明中注明此行为，以免用户自行在自定义调度器中遗漏调用导致缓存错误。  
2. **性能评估**：强制返回 “块不足” 会导致本步请求被推迟，可能增加调度延迟。建议在 CI 中加入基准测试，评估在高并发、长序列情况下的吞吐影响。  
3. **Mamba 之外的模型**：当前实现仅针对 `MambaManager`，若以后引入其他需要类似“本步不可复用” 的层，请抽象出通用机制，避免重复代码。  
4. **测试覆盖**：新增的 `test_apc_common_prefix_same_batch` 使用了真实 Falcon‑Mamba‑7B，请确保 CI 环境可拉取该模型或提供离线镜像，以防 CI 失效。  

整体来看，改动定位明确，逻辑清晰，能够消除同一步骤中前缀缓存的误命中问题。后续关注兼容性说明与性能回归即可。

---

### [Frontend][CI]  Consolidate instrumentator entrypoints (#34123)
**SHA**: `dab1de9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dab1de9f3895a153a7bc2ce7ef7782ba7818a146)

**🎯 变更类型**：重构 / 其他  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
将原先散落在 `vllm.entrypoints.openai.basic`、`vllm.entrypoints.sagemaker` 等模块的 “instrumentator”（基础 API、健康检查、指标、离线文档、服务器信息）入口点统一到 `vllm.entrypoints.serve.instrumentator` 包。相应地删掉了 `basic/__init__`、`tests/entrypoints/sleep`，并在 CI 配置中把旧的 `instrumentator` 测例路径换成新路径。`vllm.entrypoints.openai.api_server` 也同步去除对旧 `basic` 路由的直接注册，仅通过统一的 `register_instrumentator_api_routers` 注入。

**🎯 影响范围**：  
- `vllm/entrypoints/openai/api_server.py`  
- `vllm/entrypoints/sagemaker/api_router.py`  
- `vllm/entrypoints/serve/__init__.py`（路由注册逻辑）  
- `vllm/entrypoints/serve/instrumentator/*`（新入口实现）  
- CI/YAML 配置文件及对应测试目录  
- 删除的 `vllm/entrypoints/openai/basic`、`tests/entrypoints/sleep`  

**💡 关注建议**  
1. **兼容性**：确认外部用户（如自定义 FastAPI 项目）仍能通过 `register_vllm_serve_api_routers` 获得所有 instrumentator 接口；如有旧路径的 import，建议保留兼容 shim 或在文档中明确迁移步骤。  
2. **导入顺序**：`api_server.py` 里在 `if "instrumentator" in supported_tasks:` 块中多次 `import`，注意避免循环依赖；可以在模块顶部统一导入或使用局部 `import`。  
3. **测试完整性**：CI 已改为运行 `entrypoints/instrumentator`，但仍保留对 `entrypoints/openai` 的 `--ignore=instrumentator`。确保所有新路由的单元/集成测试覆盖 health、metrics、offline_docs 等关键路径。  
4. **文档更新**：README / API 文档需要同步说明 `instrumentator` 已迁移到 `vllm.entrypoints.serve.instrumentator`，以及在服务器启动参数中如何开启 `VLLM_SERVER_DEV_MODE` 查看 dev‑only 路由。  
5. **代码整洁**：`vllm/entrypoints/serve/instrumentator/basic.py` 已去除 `register_basic_api_routers`，保持文件仅定义 router；对应的 `register_instrumentator_api_routers` 中已经 `include_router`，无需再保留旧函数。  
6. **环境变量**：`server_info` 路由仅在 `VLLM_SERVER_DEV_MODE` 为真时注册，确认该变量在 CI/生产环境中的默认值，避免意外暴露内部信息。  

整体来看，改动把 instrumentator 相关入口点集中管理，提升可维护性，但需注意向后兼容和文档同步，确保 CI 与实际运行环境保持一致。

---

### [responsesAPI] fix simpleContext streaming output_messages (#34188)
**SHA**: `9608844` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9608844f96e0e739bead72520b7710f1b6f82b65)

**🎯 变更类型**：Bug 修复（修正 SimpleContext 在流式模式下 `output_messages` 的行为）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- `HarmonyContext` 中原有的 `output_messages` 列表被移除，改为通过只读属性在需要时返回 **合并后的单条消息**，解决了流式请求产生大量碎片化输出的问题。  
- 为 `SimpleContext` 添加了对应的合并逻辑，并在 `append_output` 中仅记录第一次出现的 `prompt`，防止重复写入 `input_messages`。  
- 新增大量单元测试覆盖非流式、流式、多 delta、空文本等场景，确保合并行为的正确性。  
- 在 `benchmarks/datasets.py` 增加 `--trust-remote-code` 参数，提升数据集加载的可选安全选项。  

**🎯 影响范围**  
- **核心**：`vllm/entrypoints/openai/responses/context.py`（`HarmonyContext` 与 `SimpleContext` 的实现）。  
- **测试**：`tests/entrypoints/test_context.py`（新增 250 行测试）。  
- **工具**：`vllm/benchmarks/datasets.py`（CLI 参数变更）。  

**💡 关注建议**  
- **开发者**：检查是否有外部代码仍直接访问 `output_messages` 成员变量；若有，需要改为属性调用或更新兼容层。  
- **性能**：合并后只返回单条 `ResponseRawMessageAndToken`，避免在大量 delta 场景下频繁列表创建，建议在高并发流式服务中进行基准测试。  
- **文档**：更新 OpenAI 兼容层的接口说明，明确 `output_messages` 为 **只读合并视图**，并在 CLI 文档中加入 `--trust-remote-code` 的用途与风险提示。  
- **回归**：在部署前运行完整的 `pytest -q`，确保新加入的测试全部通过，且旧的 API 行为未被意外改变。  

总体来说，此次改动提升了流式返回的可读性和效率，影响范围局限在响应上下文层，风险可控。

---

### [Doc] Update usage of `--limit-mm-per-prompt` (#34148)
**SHA**: `25e48a3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/25e48a3aae35849fd777f8a48c3c494337c11d83)

**🎯 变更类型**：功能增强 / 文档同步  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将原先通过 `--limit-mm-per-prompt '{"image":N}'`（JSON 字符串）限制多模态输入的方式，统一改为 `--limit-mm-per-prompt.image N` 的点号语法，同时在文档、示例以及 `MultiModalConfig` 中同步描述。针对仅文本模式的模型，新增 `--language-model-only`（等价于把所有模态上限设为 0）并在说明中加入 Qwen‑3.5。

**🎯 影响范围**  
- `vllm/config/multimodal.py`（CLI 参数解析、配置注释）  
- 文档 `docs/features/multimodal_inputs.md`、`docs/models/supported_models.md`  
- 多个示例脚本（offline_inference、online_serving、pooling）  
- 可能的自动化测试/CI 脚本（仍使用旧 JSON 格式的地方）  

**💡 关注建议**  
1. **向后兼容**：保留旧的 `--limit-mm-per-prompt '{"..."}'` 解析路径，并在 `argparse` 中标记为已废弃，避免已有用户因升级报错。  
2. **参数注册**：确认 `vllm/engine/arg_parser.py`（或相应模块）已注册 `--limit-mm-per-prompt.<modality>`，并对未知 modality 给出友好错误提示。  
3. **文档一致性**：更新 README 与其他使用示例，确保所有入口均使用新语法；若仍保留旧语法，需要在文档中注明废弃期限。  
4. **测试覆盖**：新增或调整单元测试，验证 `--language-model-only` 与 `--limit-mm-per-prompt.<modality> 0` 的等价性，并检测新语法在不同模型（Phi‑3.5‑vision、Qwen‑3.5 等）下的生效。  
5. **用户迁移指南**：在发布说明中提供“一键迁移”脚本或示例，帮助用户将旧 JSON 参数批量替换为点号语法。  

整体来看，此次改动提升了 CLI 参数的可读性和一致性，但务必做好兼容层和测试支撑，防止因参数解析变化导致的运行时错误。

---

### [Bugfix][Core] Fix CPU memory leak from Request reference cycle in prefix caching (#34183)
**SHA**: `8a5e0e2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8a5e0e2b2bb925d162328927b7565514fa355da1)

**🎯 变更类型**：Bug 修复（核心）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 通过改写 `Request` 中的 block‑hasher 逻辑，去除 `partial(block_hasher, self)` 引入的 **Request → partial → Request** 循环引用，从而在对象销毁时能够立即回收 CPU 内存。  
- 新增 `Request.update_block_hashes()` 方法，并在创建、追加 token、以及调度器合并会话时统一调用。  
- 相应地更新了调度器 `_update_request_as_session` 与单元测试的调用方式。

**🎯 影响范围**  
- `vllm/v1/request.py`（核心数据结构）  
- `vllm/v1/core/sched/scheduler.py`（会话合并逻辑）  
- `tests/v1/core/test_async_scheduler.py`（单元测试）  

**💡 关注建议**  
1. **内存回收**：在使用 `Request` 时，确认已不再持有外部对 `Request` 的强引用，否则循环仍会阻碍 GC。  
2. **兼容性**：原先公开的属性 `get_hash_new_full_blocks` 已被私有化为 `_block_hasher`，若外部代码曾直接访问该属性，需要迁移到 `update_block_hashes()` 或通过 `block_hasher` 参数初始化。  
3. **性能检查**：`update_block_hashes()` 每次调用都会重新计算新块的哈希，建议在高吞吐场景下跑基准测试，确保没有引入额外开销。  
4. **测试覆盖**：新增的 `update_block_hashes` 逻辑已在 `test_prefix_caching_for_multi_turn` 中得到验证，建议在本地完整运行所有核心测试，确保不会出现遗漏的引用泄漏。  

总体来看，此次修改消除了 CPU 内存泄漏的根本原因，影响范围局限在请求对象与调度器，风险有限。后续关注是否有旧版插件或自定义 `block_hasher` 仍依赖原来的 `partial` 形式。

---

### [ROCm][Bugfix] Resolve Dynamo tracing crash from amdsmi calls in on_gfx* arch detection (#34108)
**SHA**: `4cde2e0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4cde2e015944495e6bd650a4415cfb342bd73cfb)

**🎯 变更类型**：Bug 修复（兼容 Torch‑Dynamo）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：原先在 *vllm/platforms/rocm.py* 中的 `on_gfx*` 系列函数每次调用都会通过 `amdsmi` 查询 GCN 架构，导致在 TensorRT‑Dynamo 跟踪阶段触发 CUDA 初始化并引发崩溃。此次改动将架构查询抽取为一次性在模块加载时执行的 `_get_gcn_arch()`，结果保存到全局变量 `_GCN_ARCH`，并用布尔常量（`_ON_GFX1X`、`_ON_MI3XX` 等）取代原来的 `@cache` 动态查询，所有后续判断改为直接使用这些常量。

**🎯 影响范围**  
- `vllm/platforms/rocm.py` 中的架构检测相关函数：`on_gfx1x、on_mi3xx、on_gfx9、on_gfx942、on_gfx950`。  
- `use_rocm_custom_paged_attention`、`DeviceCommunicator` 相关类方法（`supports_mx、supports_fp8、is_fp8_fnuz、use_custom_allreduce、is_navi`）均改为基于 `_GCN_ARCH`。  
- 依赖这些函数的任何模块（如模型分配、注意力实现）在使用 Torch‑Dynamo 或 Ray‑worker 时将不再触发 CUDA 初始化。

**💡 关注建议**  
1. **兼容性验证**：在没有 `amdsmi`（如纯 CPU 环境或非 AMD GPU）机器上确保回退到 `torch.cuda.get_device_properties` 能正常返回空字符串或抛出可捕获的异常。  
2. **线程安全**：虽然全局常量在模块导入时仅计算一次，但如果在多进程/子解释器环境下加载模块，仍建议保持 `_GCN_ARCH` 为不可变字符串，以防意外修改。  
3. **文档更新**：说明 `on_gfx*` 系列已不再是懒加载函数，调用开销几乎为零，同时提醒用户若需要在进程启动后更改 `CUDA_VISIBLE_DEVICES`，仍需在导入 vLLM 前完成。  
4. **回归测试**：添加针对 `amdsmi` 调用失败的单元测试，验证所有 `*_ON_*` 布尔值在 fallback 场景下为 `False`（或符合实际 GPU）。  
5. **性能基准**：在大量模型实例化的工作流中对比前后 `on_gfx*` 的调用延迟，确保改动已彻底消除 Dynamo 跟踪时的崩溃风险。  

总体来说，此次重构通过一次性查询并缓存体系结构信息，既解决了 Dynamo‑trace 崩溃，又降低了运行时查询成本，对使用 ROCm 的用户影响积极。请在多平台 CI（含无 AMD GPU）上验证回退路径，确保发布的稳定性。

---

### [LMCache] Token Base IPC API (#34175)
**SHA**: `e94ec59` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e94ec597334d9a3e9b0d04bc17152e2747c83d51)

**变更概览**  
本次提交为 LMCache 增加了 **Token‑mode IPC API**，在原有基于块哈希的缓存交互之外，引入了以 token 列表为键的路径。核心改动集中在 `vllm/distributed/kv_transfer/kv_connector/v1` 两个文件：

| 文件 | 关键改动 |
|------|----------|
| `multi_process_adapter.py` | 1. `wrap_kv_caches` 参数改为 `torch.Tensor`（与 LMCache 直接交互）。<br>2. 新增 `striding_block_hashes`、`send_lmcache_request`、`get_lmcache_chunk_size` 辅助函数。<br>3. `LoadStoreOp` 结构体扩展：加入 `token_ids`、`start`、`end`，并把 `block_hashes` 设为可选。<br>4. `LMCacheMPSchedulerAdapter` 与 `LMCacheMPWorkerAdapter` 统一支持 **hash‑mode** 与 **token‑mode**，并新增 `end_session`、`maybe_submit_lookup_request`、`check_lookup_result` 等接口。<br>5. 移除废弃的警告、旧的 `convert_block_hashes_to_bytes`。<br>6. 对请求的键生成分别使用 `_create_key`（token） 与 `_create_hash_key`（hash）。|
| `lmcache_mp_connector.py` | 1. 去掉 `convert_block_hashes_to_bytes`，直接在业务层完成 token/哈希切分。<br>2. `GetStoreMetadata`、`GetRetrieveMetadata` 现在构造 `LoadStoreOp` 时填入 `token_ids`、`start`、`end`（基于 `vllm_block_size`），而不是仅提供块哈希。<br>3. `maybe_submit_lookup_request` 调用改为传入 `token_ids`。<br>4. `request_finished` 增加 `scheduler_adapter.end_session`，确保 LMCache 会话及时关闭。<br>5. `get_finished` 接口签名及逻辑调整，加入对已完成请求的去重处理。|

**受影响模块**  
- `vllm.distributed.kv_transfer.kv_connector.v1.lmcache_*`（调度器 & 工作器）  
- 与 LMCache 交互的上层 `LMCacheMPConnector`（包括 request_tracker、metadata 生成）  
- 任何直接使用 `LoadStoreOp`、`IPCCacheEngineKey` 或 `MessageQueueClient` 的自定义插件或测试代码。

**潜在风险**  
1. **向后兼容性**：`LoadStoreOp` 字段改为可选，旧代码若只提供 `block_hashes` 并未适配 `token_ids` 可能在运行时触发断言。  
2. **键生成错误**：`_create_key` 现在需要 `token_ids`，但在某些路径（如 `batched_submit_*`）如果 `op.token_ids` 为 `None`，会产生 `assert`，建议在入口提前校验。  
3. **并发清理**：`end_session` 在 `request_finished` 执行，若异常提前返回，可能导致 LMCache 会话泄漏。  
4. **性能**：在 token‑mode 下，需要在每次请求时把全部 token 列表复制到 `LoadStoreOp`，若 token 数量巨大（数万），会产生额外的 Python 对象创建及序列化开销。  
5. **类型不一致**：`send_lmcache_request` 返回 `MessagingFuture[Any]`，但后续代码把结果直接当作 `int`、`bool`、`list[bool]` 处理，缺少显式类型检查，易出现运行时错位。

**建议**  
- 在 `LoadStoreOp.__post_init__` 中加入 **互斥校验**（只能提供 `block_hashes` 或 `token_ids`），并抛出明确异常，以防误用。  
- 为新接口编写 **单元测试**：覆盖 hash‑mode、token‑mode、混合错误路径、`end_session` 的调用时序。  
- 更新文档/README，明确 **两种模式的使用场景** 与参数要求。  
- 考虑在 `maybe_submit_lookup_request` 中对 `token_ids` 进行 **chunk‑aligned** 截断前的长度检查，避免产生空键列表。  
- 为 `send_lmcache_request` 增加 **异常捕获** 并在 `future.result()` 前返回可读错误信息，防止因服务器侧错误导致的卡死。  
- 若 LMCache 仍需兼容旧的 block‑hash 路径，建议保留一个包装层 `LegacyAdapter`，在项目迁移期间提供平滑过渡。  

总体来看，此次改动为 LMCache 引入了更灵活的 token‑level 缓存键，提升了与 vLLM 的耦合度和潜在复用场景。但需要确保类型校验、异常处理以及文档同步到位，以防在生产环境引入难以定位的错误。

---

### [Bugfix][ROCm][GPT-OSS] Use old triton_kernels implementation on ROCm if the new API is not available (#34153)
**SHA**: `c60f8e3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c60f8e3b49eced1a17ba0e11da3f8c107b309df9)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 ROCm 环境下，如果新版 `triton_kernels` API（`SparseMatrix`、`make_ragged_tensor_metadata` 等）不可用，代码会回退到旧实现。通过 `current_platform.is_rocm()` 检测平台并设置 `use_legacy_triton_kernels`，进而在路由、稀疏矩阵构造以及激活函数的调用处切换到旧接口。

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py`（MOE 融合层）  
- 受影响的内部模块：`triton_kernels.routing`、`triton_kernels.tensor`、`triton_kernels.swiglu`  
- 与平台检测 (`vllm.platforms`) 以及 `has_triton_kernels` 相关的初始化路径。

**💡 关注建议**  

1. **平台检测可靠性**  
   - `use_legacy_triton_kernels` 为模块级全局变量，首次导入时即确定。若运行时切换平台（unlikely），需要确保不会产生状态冲突。建议在函数内部直接判断 `current_platform.is_rocm()`，或在模块初始化后将其设为 `readonly`（如 `use_legacy_triton_kernels = bool(...)`）。

2. **异常路径**  
   - 当非 ROCm 平台缺少新版 API 时会直接抛异常，这符合预期。但在 ROCm 回退路径上，仍然使用 `triton_kernels.routing`、`triton_kernels.swiglu` 的旧实现；若这些旧实现也缺失会导致运行时错误。建议在回退前追加一次 `hasattr` 检查并给出更明确的错误信息。

3. **性能影响**  
   - 旧实现的 `FusedActivation` 调用方式略有差异，参数顺序改变可能影响 JIT 编译缓存。应在 ROCm CI 中对比性能基准，确保回退不会导致显著的吞吐下降。

4. **单元/集成测试**  
   - 增加针对 ROCm 环境（模拟 `current_platform.is_rocm()` 为 True 且缺少新 API） 的测试，验证路由、稀疏矩阵以及激活函数均走旧路径且结果保持数值一致。  
   - 保持原有 GPU（CUDA）路径的回归测试不受影响。

5. **文档更新**  
   - 在 `README` 或平台兼容性说明中标注：在 ROCm 10.x/11.x 上若使用新版 `triton_kernels` 失败，库会自动回退到旧实现。

总体而言，此次改动提供了 ROCm 环境的兼容性保障，逻辑清晰且影响范围局限于 MOE 融合层。只要注意上述异常处理与测试覆盖，即可安全发布。

---

### [Doc] Add DCP support to attention backend doc (#33936)
**SHA**: `5e75a14` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5e75a14a667dccf7f48781568f19f1a6b9c8014a)

**🎯 变更类型**：功能增强（文档层面的特性展示）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `docs/design/attention_backends.md` 增加了 **DCP（Decode Context Parallelism）** 列，展示各后端是否支持 `--decode-context-parallel-size`。  
- `tools/pre_commit/generate_attention_backend_docs.py` 大幅重构：  
  - 抽取公共常量、路径、跳过列表，实现更清晰的模块化。  
  - 新增 AST 辅助函数，能够在实现类(`*_impl`)中递归查找 `can_return_lse_for_decode`（即 DCP 支持标志），并通过继承链向上解析。  
  - 为 FlashAttention、FlashInfer、MLA 等后端分别增加了 `supports_dcp` 字段，并在生成表格时加入 **DCP** 列。  
  - 采用新表格渲染方式（`TableColumn`、`_build_columns`）提升代码可维护性。  

**🎯 影响范围**  
- 文档生成工具 `tools/pre_commit/generate_attention_backend_docs.py`（所有注意力后端的文档）。  
- `docs/design/attention_backends.md`（新增列，表头及部分后端行被更新）。  
- 依赖该脚本的 CI 步骤（pre‑commit）将重新生成文档。  

**💡 关注建议**  
1. **功能完整性**：当前仅在文档层面展示 DCP 支持，未对实际后端代码做检查。若后端实现 `can_return_lse_for_decode` 与文档不一致，易造成误导。建议在 CI 中加入对实现类的单元测试或运行时校验。  
2. **跨文件继承解析**：实现类可能跨模块导入，`_resolve_import_to_file` 只处理相对/绝对导入的简单情况，若使用 `import … as …` 或 `from .module import (A, B)`，解析可能失效。可在解析失败时给出警告，避免漏报。  
3. **性能与可靠性**：文档生成依赖大量 AST 解析，新增递归解析继承链可能导致解析时间稍增。可以在文件变更检测后仅解析受影响的后端，或缓存解析结果。  
4. **向后兼容**：新列的加入对已有文档或使用者不产生功能影响，但 `SKIP_BACKENDS` 中排除的自定义后端仍会缺失 DCP 信息，若用户自行扩展后端，请在相应实现类中添加 `can_return_lse_for_decode` 标识。  

总体而言，此次改动提升了文档的可读性与完整性，并为后续实现 DCP 特性的后端提供了明确的可视化入口。若能在实现层同步检查，就能进一步避免文档与代码不一致的风险。

---

### [ModelRunner V2][BugFix] Fix `max_query_len` calculation (#34167)
**SHA**: `e7e5278` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e7e52781ff636bf772301c9282a7601c73b8b905)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交修正了 `ModelRunner V2` 中 `max_query_len` 的计算方式。原实现使用 `query_start_loc_cpu.max()` 作为最大查询长度，导致在多请求并发时误把累计 token 数当作查询长度，进而产生错误的注意力掩码或 OOM。改为显式传入 `num_tokens_per_req`（或 `num_scheduled_tokens.max()`）来得到真实的单请求最大查询长度，并在所有调用点同步更新参数签名。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/attn_utils.py`（注意力元数据构造）  
- `vllm/v1/worker/gpu/cudagraph_utils.py`（捕获图输入准备）  
- `vllm/v1/worker/gpu/model_runner.py`（调度、dummy 元数据生成）  
- `vllm/v1/worker/gpu/spec_decode/eagle.py`（Spec‑Decode 路径）  

**💡 关注建议**  
1. **功能回归**：确保所有使用 `build_attn_metadata` 的代码路径（prefill、decode、eagle）已被更新为传递 `max_query_len`，避免遗漏导致运行时异常。  
2. **边界检测**：`num_scheduled_tokens.max()` 在 batch 为 0 时会抛异常，建议在调用前添加 guard（如 `if num_reqs>0 else 0`）。  
3. **单元/集成测试**：新增含不同请求长度的多批次测试，验证注意力 mask 与 KV cache 大小保持一致，防止因误算长度引发显存泄漏。  
4. **文档/注释**：在 `build_attn_metadata` 参数列表中说明 `max_query_len` 的意义与来源，降低后续维护风险。  
5. **性能监控**：本次改动为纯数据传递，理论不影响吞吐量，但建议在大模型/高并发场景下跑一次基准，确认没有意外的 CUDA Graph 重新编译开销。  

整体来看，此 BUG 修复对注意力计算的正确性至关，影响范围有限且已覆盖核心调度路径，建议合并后进行完整回归。

---

### [Kernel] use flashinfer for gdn prefill (#32846)
**SHA**: `285bab4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/285bab47526cbc4d4e26c61d831eaeb17b253d0f)

**🎯 变更类型**：功能增强 → 在 CUDA 9.0（sm90）上使用 FlashInfer 实现的 GDN‑prefill 加速  
**⚡ 重要程度**：🟡 中（提升性能但不会破坏已有路径）  

**📋 变更摘要**  
1. 为 `chunk_gated_delta_rule` 添加 `CustomOp` 包装，运行时检测 CUDA 计算能力为 90 时切换到 FlashInfer (`flashinfer.gdn_prefill.chunk_gated_delta_rule`)；否则回落到原有实现 `vllm.model_executor.layers.fla.ops.chunk_gated_delta_rule`。  
2. 在 `Qwen3Next` 模型的前向路径中改为通过 `self.chunk_gated_delta_rule` 调用，从而自动使用加速 kernel。  
3. 新增 `l2norm_fwd` 前置归一化、数据类型转换（`float32`）以及 `torch.exp` 处理 `g`。

**🎯 影响范围**  
- `vllm/model_executor/models/qwen3_next.py`（核心模型前向）  
- `vllm/model_executor/custom_op.py`（注册机制）  
- `vllm/model_executor/layers/fla/ops/*.py`（原实现保持不变）  

**💡 关注建议**  
1. **依赖检查**：确保生产环境已安装 `flashinfer`（>=0.2）并匹配的 CUDA 12.x+，否则会在 sm90 检测时回退；建议在 `requirements.txt` 或 CI 中加入显式检查。  
2. **兼容性**：`fi_chunk_gated_delta_rule` 固定把 `q/k` 归一化并转为 `float32`，若模型在 bfloat16/float16 下运行，需要确认数值差异在容忍范围；可在 `use_qk_l2norm_in_kernel=False` 时保留原 dtype。  
3. **性能基准**：建议在不同 GPU（A100, H100）上对比前后吞吐，防止在非 sm90 环境误触发并产生额外拷贝开销。  
4. **单元测试**：加入针对 `CustomOp` 的装配测试，验证 `forward_cuda` 与 `forward_native` 输出一致（相对误差 < 1e‑3）。  
5. **日志与回退**：当前仅在 sm90 打印一次信息，若用户希望强制使用原实现，可提供环境变量或参数开关。  

总体来说，此次改动为 Qwen‑3‑Next 引入了可选的 FlashInfer 加速，结构清晰且保持向后兼容。只要做好依赖、数值与性能验证，即可安全合入。

---

### [Bugfix] Fix shared expert input for latent MoE in EP+DP (Nemotron-H) (#34087)
**SHA**: `995bbf3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/995bbf38f114a0e1bd7e34d6fd92d255ac2efca7)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次 PR 解决了在 EP+DP（数据并行+专家并行）下，使用潜在（latent）MoE 的模型在共享专家（shared expert）中出现的输入维度不匹配问题。通过在 MoE 前向路径中显式传递 `shared_experts_input`（原始 hidden‑state），并在最终化阶段使用该张量而不是被投射的 latent 张量，保证共享专家能够得到正确的全维度特征。与此同时，将 `FlashInfer` 相关的并行配置检查放宽为始终返回 `True`，避免误报。

**🎯 影响范围**：  
- `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`（并行配置检查）  
- `vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py`、`modular_kernel.py`（MoE 前向/后处理）  
- 多个量化实现层：`compressed_tensors_moe.py`、`fp8.py`、`modelopt.py`（均加入 `shared_experts_input` 参数）  

**💡 关注建议**  
1. **测试覆盖**：在 EP+DP + latent‑MoE 场景下新增单元/集成测试，验证共享专家输出与预期保持数值一致。  
2. **后向兼容**：`shared_experts_input` 参数默认 `None`，对已有模型不产生影响，但请确认在旧版本的 checkpoint 中不会出现未保存的额外张量。  
3. **性能检查**：传递额外张量会增加一次拷贝或视图操作，建议在大规模推理时测量额外的内存占用与吞吐率影响。  
4. **文档更新**：在 MoE 使用说明中标明 “latent MoE 需要 `shared_experts_input`” 的限制，并解释 `FlashInfer` 并行配置已被简化。  

总体而言，改动定位准确、影响模块明确，若配合相应测试与性能评估，可安全合并。

---

#### 🟢 低重要度变更 (22)

### [BUGFIX] Fix accuracy bugs in Qwen3-Next MTP (#34077)
**SHA**: `000214c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/000214c4bb3f4fb61989eea19c625aedd0559ace)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Qwen3‑Next MTP 的准确性问题，新增对零长填充序列的排除、调整 prefill 计数、过滤 block_table 以去除填充、修正 query_start_loc 切片并加入断言，确保 decode 与 spec‑decode 不冲突。

---

### [compile] Enable AOT compile with 2.10 in trunk. (#34155)
**SHA**: `82e1197` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/82e11973cc07909de895a1309ce0f6a2144c576a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 AOT 编译默认开启的 Torch 版本要求从 `2.11.0.dev` 降至 `2.10.0`，提升对更早 Torch 版本的兼容性。

---

### Bump `mamba-ssm` version in CI for Transformers v5 compatibility (#34233)
**SHA**: `d0bc520` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d0bc52056915e108c347aa4b5520e163e5c5b726)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 CI 流水线中的 `mamba-ssm` 依赖版本从 v2.2.5 升级至 v2.3.0，以确保在 Transformers v5 环境下的兼容性。

---

### Stop testing for slow tokenizers as they will not exist soon (#34235)
**SHA**: `6141397` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/61413973e83b9ca07f3c894a90ddecca0a39d2b6)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除对慢速（非 fast）分词器的测试代码及相关导入，仅保留 fast tokenizer 的协议检查，降低测试负担。

---

### [Misc] allow specify is_mm_prefix_lm in hf_config (#34215)
**SHA**: `94de871` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/94de871546e8da687c08ed8a7e0a26531500d4bd)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `ModelConfig.is_mm_prefix_lm` 属性中加入对 `hf_config.is_mm_prefix_lm` 的检测，允许用户在 HuggingFace 配置中显式指定是否使用多模态前缀语言模型，若未指定则回退到原有模型名单。

---

### Add flagos in MiniCPM-o (#34126)
**SHA**: `e042d7e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e042d7e685daacfa9d4df92cc7d330060327a32b)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `MiniCPM-O` 模型实现中引入 `flag_gems` 加速库，依据环境变量 `USE_FLAGOS=1` 动态加载并仅启用指定的算子列表。

---

### [Bugfix] Fix FI kernel`chunk_gated_delta_rule` output shape for Qwen3.5 (#34219)
**SHA**: `ae4e280` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ae4e280602f3c91d322a449f33f5aebbdd59ccc1)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Qwen3.5 FI kernel `chunk_gated_delta_rule` 的输出形状，改为接收 `chunk_gated_delta_rule_fi` 返回的 `(output, final_state)`，并在返回前再 `unsqueeze` 为 4D，以匹配后续模块的期望格式。

---

### [Docs] Fix format error in KV load failure recovery doc (#34137)
**SHA**: `cbea11c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cbea11c9f0ddeef8f5e31449b2e6a37d08e4e653)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `examples/offline_inference/kv_load_failure_recovery/README.md` 中补全缺失的代码块结束符 `````，修复文档格式错误。

---

### [Bugfix] Fix `--trust-remote-code` conflict (#34218)
**SHA**: `2c32558` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2c32558a3c467253161e32203584c1ecb33bb584)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：统一使用 `FlexibleArgumentParser`，删除冗余导入并移除 `--trust-remote-code` 参数定义，修复该参数在不同模块间的冲突。

---

### [Bugfix] Fix memory inconsistency in cross-process shared memory (#32022)
**SHA**: `5f97012` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5f970120f06daab162b8692cfce39b0f366b9b47)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `shm_broadcast.py` 写入共享内存时加入 `memory_fence()`，确保缓冲区内容与标记的写入顺序一致，修复跨进程内存不一致的 bug。

---

### Revert #34208 (#34216)
**SHA**: `998e2d9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/998e2d91f84e2b30dc40c8543b879d4e412d6f14)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `serve.py` 中恢复 `--trust-remote-code` 参数的 CLI 支持，同时在 `datasets.py` 中移除该参数的声明，统一对远程代码信任选项的处理。

---

### [Perf] Optimize detokenizer python logic (#32975)
**SHA**: `e1060a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e1060a71a1bb96103ce9ca98345184dcdc982467)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Detokenizer 中新增 `num_output_tokens` 方法，统一计数逻辑并替换多处 `len(self.output_token_ids)` 用法，简化代码并提升性能。

---

### [Bugfix] Sort hf_weights_files in fastsafetensors_weights_iterator to match #33491 (#34190)
**SHA**: `8d48d0a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8d48d0a9d9edfc2eb9cee6bb941be20211eb8282)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fastsafetensors_weights_iterator` 中对 `hf_weights_files` 进行自然排序，确保权重文件顺序与 #33491 的期望保持一致。

---

### [Bugfix] Add `--trust-remote-code` to dataset bench args (#34208)
**SHA**: `f69b903` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f69b903b4c70716224b3936cb8503e562e25388e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/benchmarks/datasets.py` 新增 `--trust-remote-code` 参数，以在数据集基准中支持 HuggingFace 远程代码；同时在 `vllm/benchmarks/serve.py` 中移除该参数，避免冲突。

---

### [Bugfix] Fix DP Attention Padding in Dummy Run (#34187)
**SHA**: `81e217f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/81e217fe6b5a3030aa5c4d859a2125b81979bee4)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `gpu_model_runner.py` 的 `_dummy_run` 中新增 `num_tokens_padded` 参数，仅在开启完整 CUDAGraph 模式时传递，修复了 DP Attention 的填充错误。

---

### [CI/Build] Relax `test_mcp_tool_call` (#34204)
**SHA**: `ab97bcf` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ab97bcf66295fca10a892bd14090e902b4b3c317)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：放宽 `test_mcp_tool_call` 对结果数值的断言，支持“56088”或带千位分隔符的“56,088”，并使用 `importlib.util` 进行导入。

---

### [Bugfix] Adopt `ChunkGatedDeltaRule` for Qwen3.5 (#34198)
**SHA**: `047a457` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/047a457fa4af2010303ba775ae6f3ee9c1852c2c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `qwen3_5.py` 中引入 `ChunkGatedDeltaRule`，并在解码层初始化时创建 `self.chunk_gated_delta_rule` 实例，以修复 Qwen3.5 模型的 gated‑delta 规则错误。

---

### [structured output] validate unsupported json features first (#33233)
**SHA**: `1339784` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/13397841ab469cecf1ed425c3f52a9ffc38139b5)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `validate_xgrammar_grammar` 中，将对 JSON schema 不支持特性的检查提前到 `Grammar.from_json_schema` 调用前，以便更早抛出明确错误，代码顺序调整，功能未变。

---

### [torch.compile][Fusion] Fix attention fusion pass removing kv_udpate op. (#33945)
**SHA**: `bb9f973` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bb9f97308d0b88c5ad2d64c217b22866e40c79df)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在注意力融合 Pass 中补入 `kv_cache_dummy_dep` 参数及其占位 tensor，修复因删除 kv_update 导致的融合异常；相应测试新增断言，确保前后该属性保持一致。

---

### [ROCm] update triton branch to support gpt-oss models for gfx11xx devices (#34032)
**SHA**: `4d39650` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4d3965096164328451538988824d72ab03593c04)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docker/Dockerfile.rocm_base` 中将 `TRITON_BRANCH` 从 `57c693b6` 更新为 `f332c492`，以支持 gfx11xx 设备上的 gpt-oss 模型。

---

### [Bugfix] Voxtral prompt/audio placeholder alignment (#34140)
**SHA**: `8fd31f6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8fd31f62452960efdd6dd7b912c388f487536b3c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Voxtral 模型中 prompt/audio 占位符对齐问题，新增 `target_channels=1`，改为从 `out_mm_kwargs` 获取音频长度并支持多音频列表返回，统一音频提示格式为 `(audio_array, sample_rate)` 列表。

---

### [Bugfix] Avoid duplicate k-proj weight emission in helper (#34142)
**SHA**: `eadb4e8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/eadb4e868bae8acd2e9b764f5827c0500ec44c34)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Whisper 模型中 `_create_fake_bias_for_k_proj` 的生成器逻辑，避免重复输出 k‑proj 权重导致的错误。

---

