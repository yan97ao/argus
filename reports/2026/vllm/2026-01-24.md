# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-01-24ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-01-24 22:45:14 | Hiroken. | [Bugfix]: resolve torch.compile cache conflict between mm_encoder_tp_modes (#32842) |
| 2026-01-24 22:28:57 | Lukas Geiger | [EncoderCacheManager] Remove unnecessary copy (#32800) |
| 2026-01-24 20:48:46 | david guan | feat: Complete LoRA support for MiniMaxM2 Fixes #32736 (#32763) |
| 2026-01-24 20:34:26 | Isotr0py | [Models]: Make Multimodal config implicit in ViT implementation (#31972) |
| 2026-01-24 18:31:41 | Hiroken. | [Bugfix] Fix E2E latency calculation and add warmup support in mm_processor benchmark (#32646) |
| 2026-01-24 18:01:35 | 7. Sun | [Perf] Cache exc.errors() result in validation exception handler (#32984) |
| 2026-01-24 17:37:28 | Cyrus Leung | [UX] Deduplicate sampling parameter startup logs (#32953) |
| 2026-01-24 16:24:44 | Reagan Lee | feat(benchmark): add encoder forward pass benchmarking to mm-processor (#31655) |
| 2026-01-24 15:52:22 | Roger Wang | [Doc] Ignore typo check on doc (#32999) |
| 2026-01-24 15:36:31 | Isotr0py | [Models] Add `SharedFusedMoE` support to Qwen3MoE (#32082) |
| 2026-01-24 15:32:44 | Roy Wang | [docs] Update governance process links (#32995) |
| 2026-01-24 14:47:14 | 7. Sun | [Tests] Standardize RNG seed utility across test files (#32982) |
| 2026-01-24 14:38:50 | 7. Sun | [Tests] Clarify pytest skip reasons with actionable context (#32981) |
| 2026-01-24 12:56:23 | 7. Sun | [Perf] Cache xpu_get_mem_info() result to avoid duplicate calls (#32983) |
| 2026-01-24 11:15:17 | Michael Goin | [Dev UX] Add auto-detection for VLLM_PRECOMPILED_WHEEL_VARIANT during install (#32948) |
| 2026-01-24 11:03:05 | ruizcrp | Auth_token added in documentation as it is required (#32988) |
| 2026-01-24 10:03:07 | monajafi-amd | [ROCm][ViT] Enable Flash Attention Triton backend on RDNA3/RDNA4 (#32944) |
| 2026-01-24 09:41:35 | Xin Yang | [Bugfix] Fix FusedMoE LoRA kernel offs_token out of bound value (#32279) |
| 2026-01-24 09:28:45 | Joe Runde | [Core][Bugfix] allow graceful worker termination (#32965) |
| 2026-01-24 09:28:06 | ElizaWszola | [Performance] Split FlashAttn attention and cache update (#25954) |
| 2026-01-24 06:53:10 | dolpm | [fix] add VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME to compile factors (#32912) |
| 2026-01-24 06:21:49 | Shengqi Chen | [CI] fix version comparsion and exclusion patterns in upload-release-wheels.sh (#32971) |
| 2026-01-24 06:19:56 | joninco | [Bugfix] Fix missing is_layer_skipped check for FusedMoE in AWQConfig (#32935) |
| 2026-01-24 06:04:25 | Wentao Ye | [Refactor] Clean up unused variables & func (#32692) |
| 2026-01-24 05:48:12 | Michael Goin | [Refactor] Rename `gptq_marlin` to `marlin` to match MoE (#32952) |
| 2026-01-24 05:35:48 | rasmith | [CI][AMD][BugFix] Update wvSplitK (and other skinny_gemm wrappers) to ensure tensors passed will be made contiguous for the kernel (#32831) |
| 2026-01-24 05:18:56 | Wentao Ye | [Bug] Fix benchmark script `moe_permute_unpermute` (#32949) |
| 2026-01-24 04:38:57 | Markus / Mark | fix: Add glm4_moe_lite to MLA detection (#32614) |
| 2026-01-24 04:22:20 | Lucas Wilkinson | [cudagraphs] Refactor cudagraph capture loop (#32946) |
| 2026-01-24 02:49:17 | Nick Hill | [Model Runner V2] Add KV Connector support (#32742) |
| 2026-01-24 02:26:56 | Matthew Bonanni | [Bugfix][CI] Fix pre-commit (#32956) |
| 2026-01-24 02:22:56 | Orion Reblitz-Richardson | [CI][torch nightlies] Use main Dockerfile with flags for nightly torch tests (#30443) |
| 2026-01-24 01:56:48 | Harry Huang | [V1][Hybrid] Mamba Prefix Caching with align mode (#30877) |
| 2026-01-24 01:39:01 | Matteo Fari | [Model] Enable LoRA support for internvl2 (#32397) |
| 2026-01-24 00:49:20 | Luka GovediÄ | [torch.compile][CI] Add back attn fusion on hopper/ada (#32940) |
| 2026-01-24 00:35:13 | sangbumlikeagod | [Frontend] add logprob, compression_rate to 'verbose_json' features (#31059) |
| 2026-01-24 00:24:26 | Matt | [Hardware][AMD][CI][Bugfix] Fix Kernels Attention Cache test (#32904) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 37 ä¸ªæäº¤ | ğŸ”´é«˜ 6 | ğŸŸ¡ä¸­ 11 | ğŸŸ¢ä½ 20
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (6)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-6)
    - [feat: Complete LoRA support for MiniMaxM2 Fixes #32736 (#...](#bc0d291)
    - [[Models]: Make Multimodal config implicit in ViT implemen...](#9ad7f89)
    - [feat(benchmark): add encoder forward pass benchmarking to...](#06b557e)
    - [[Performance] Split FlashAttn attention and cache update ...](#a28b94e)
    - [fix: Add glm4_moe_lite to MLA detection (#32614)](#586a57a)
    - [[V1][Hybrid] Mamba Prefix Caching with align mode (#30877)](#5206e5e)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (11)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-11)
    - [[Bugfix] Fix E2E latency calculation and add warmup suppo...](#6450b53)
    - [[UX] Deduplicate sampling parameter startup logs (#32953)](#51931c5)
    - [[Models] Add `SharedFusedMoE` support to Qwen3MoE (#32082)](#8edaf38)
    - [[Tests] Standardize RNG seed utility across test files (#...](#0ccecf8)
    - [[Dev UX] Add auto-detection for VLLM_PRECOMPILED_WHEEL_VA...](#d0cbac5)
    - [[Refactor] Clean up unused variables & func (#32692)](#37c9859)
    - [[Refactor] Rename `gptq_marlin` to `marlin` to match MoE ...](#4561f13)
    - [[cudagraphs] Refactor cudagraph capture loop (#32946)](#3a41459)
    - [[Model Runner V2] Add KV Connector support (#32742)](#8518b30)
    - [[CI][torch nightlies] Use main Dockerfile with flags for ...](#68b0a6c)
    - [[Frontend] add logprob, compression_rate to 'verbose_json...](#9b77bb7)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (20)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-20)
    - [[Bugfix]: resolve torch.compile cache conflict between mm...](#1209b78)
    - [[EncoderCacheManager] Remove unnecessary copy (#32800)](#5fa0f6e)
    - [[Perf] Cache exc.errors() result in validation exception ...](#0f19427)
    - [[Doc] Ignore typo check on doc (#32999)](#81c2a88)
    - [[docs] Update governance process links (#32995)](#5c86a89)
    - [[Tests] Clarify pytest skip reasons with actionable conte...](#0b9a735)
    - [[Perf] Cache xpu_get_mem_info() result to avoid duplicate...](#14d03b8)
    - [Auth_token added in documentation as it is required (#32988)](#c0d8204)
    - [[ROCm][ViT] Enable Flash Attention Triton backend on RDNA...](#97ef11d)
    - [[Bugfix] Fix FusedMoE LoRA kernel offs_token out of bound...](#ecc3dd6)
    - [[Core][Bugfix] allow graceful worker termination (#32965)](#7e1f10d)
    - [[fix] add VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME to compile ...](#0118cdc)
    - [[CI] fix version comparsion and exclusion patterns in upl...](#136c499)
    - [[Bugfix] Fix missing is_layer_skipped check for FusedMoE ...](#ebd0a17)
    - [[CI][AMD][BugFix] Update wvSplitK (and other skinny_gemm ...](#6cc6d92)
    - [[Bug] Fix benchmark script `moe_permute_unpermute` (#32949)](#dfab5f3)
    - [[Bugfix][CI] Fix pre-commit (#32956)](#2d6b537)
    - [[Model] Enable LoRA support for internvl2 (#32397)](#fec9da0)
    - [[torch.compile][CI] Add back attn fusion on hopper/ada (#...](#bbbd696)
    - [[Hardware][AMD][CI][Bugfix] Fix Kernels Attention Cache t...](#305e53a)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (6)

### feat: Complete LoRA support for MiniMaxM2 Fixes #32736 (#32763)
**SHA**: `bc0d291` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/bc0d291bfebf0979dbd6e8486de40aa95eb57490)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆLoRA å®Œæ•´æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´ é«˜  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. åœ¨ `MiniMaxM2ForCausalLM` ä¸­åŠ å…¥ `SupportsLoRA` æ¥å£ï¼Œå®ç°å¯¹ LoRAï¼ˆLowâ€‘Rank Adaptationï¼‰å‚æ•°çš„å®Œæ•´åŠ è½½ä¸èåˆã€‚  
2. æ–°å¢ `packed_modules_mapping` å°† `qkv_proj` ä¸åº•å±‚çš„ `q_proj / k_proj / v_proj` å…³è”ï¼Œå…¼å®¹æ¨¡å‹æƒé‡çš„æ‰“åŒ…å½¢å¼ã€‚  
3. æ–‡æ¡£æ›´æ–°ï¼Œæ ‡è®° MiniMaxâ€‘M2 ç³»åˆ—æ¨¡å‹å·²æ”¯æŒ LoRA ä¸ KV cacheã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/model_executor/models/minimax_m2.py`ï¼ˆæ¨¡å‹æ‰§è¡Œå™¨æ ¸å¿ƒï¼‰  
- `vllm/model_executor/models/interfaces.py`ï¼ˆLoRA æ¥å£ï¼‰  
- `docs/models/supported_models.md`ï¼ˆå…¬å¼€æ–‡æ¡£ï¼‰  

**ğŸ” æŠ€æœ¯æ´å¯Ÿ**  

- **æ¶æ„å½±å“**  
  - å¼•å…¥ `SupportsLoRA` ä½¿ MiniMaxM2 æˆä¸ºå¯æ’æ‹”çš„ LoRA é€‚é…æ¨¡å‹ï¼Œéµå¾ª vLLM ç»Ÿä¸€çš„ LoRA æ¥å£çº¦å®šã€‚  
  - `packed_modules_mapping` é€šè¿‡æ˜ å°„è§£å†³äº†æ¨¡å‹åœ¨æƒé‡æ‰“åŒ…ï¼ˆqkv åˆå¹¶ï¼‰æƒ…å†µä¸‹çš„ LoRA æ³¨å…¥éš¾é¢˜ï¼Œä¿æŒäº†ä¸å…¶å®ƒå·²å®ç° LoRA æ”¯æŒæ¨¡å‹ï¼ˆå¦‚ Llamaã€Mistralï¼‰çš„å®ç°ä¸€è‡´æ€§ã€‚  
  - å¯¹ `load_weights` é€»è¾‘çš„æ½œåœ¨å½±å“ï¼šåœ¨åŠ è½½ LoRA æƒé‡æ—¶ï¼Œéœ€è¦å…ˆè§£åŒ…æˆ–åœ¨ `apply_lora` æ—¶å¯¹æ˜ å°„è¿›è¡Œä¸€æ¬¡æƒé‡æ‹†åˆ†ã€‚è‹¥å®ç°ä¸å½“ï¼Œå¯èƒ½å¯¼è‡´æƒé‡å†²çªæˆ–æœªè¢«åŠ è½½çš„ LoRA å‚æ•°ã€‚  

- **æ€§èƒ½å½±å“**  
  - LoRA æœ¬èº«åœ¨æ¨ç†é˜¶æ®µåªä¼šå¼•å…¥ **é¢å¤–çš„ä½ç§©çŸ©é˜µä¹˜æ³•**ï¼Œç›¸è¾ƒäºå®Œæ•´å¾®è°ƒçš„å…¨å‚æ•°æ¨¡å‹å¼€é”€å¯å¿½ç•¥ï¼ˆ<5% é¢å¤– FLOPsï¼‰ã€‚  
  - `packed_modules_mapping` é€šè¿‡ä¸€æ¬¡æ€§è®¿é—® `qkv_proj` çš„åˆå¹¶æƒé‡ï¼Œé¿å…äº†å¯¹ä¸‰ä¸ªç‹¬ç«‹æŠ•å½±å±‚çš„é‡å¤åˆ‡ç‰‡ï¼Œç†è®ºä¸Šå¯ç•¥å¾®æå‡ç¼“å­˜å‘½ä¸­ç‡ä¸å†…å­˜è®¿é—®æ•ˆç‡ã€‚  
  - å¯¹å·²å¼€å¯ **KVâ€‘cache** çš„åœºæ™¯æ— è´Ÿé¢å½±å“ï¼Œä»ä¿æŒåŸæœ‰çš„é›¶æ‹·è´æ¨ç†ä¼˜åŠ¿ã€‚  

- **å®‰å…¨è€ƒè™‘**  
  - LoRA æƒé‡é€šå¸¸æ¥æºäºç¬¬ä¸‰æ–¹ç¤¾åŒºæ¨¡å‹ï¼Œè‹¥æœªå¯¹æ–‡ä»¶å®Œæ•´æ€§ï¼ˆhashã€ç­¾åï¼‰è¿›è¡Œæ ¡éªŒï¼Œå¯èƒ½å¼•å…¥ **æ¶æ„ä¿®æ”¹çš„ä½ç§©çŸ©é˜µ**ï¼Œå¯¼è‡´æ¨¡å‹è¡Œä¸ºå¼‚å¸¸æˆ–ä¿¡æ¯æ³„æ¼ã€‚  
  - ç›®å‰å˜æ›´ä»…åœ¨åŠ è½½å±‚é¢åŠ å…¥æ¥å£ï¼Œæœªæ¶‰åŠå®‰å…¨ç­–ç•¥ï¼›å»ºè®®åœ¨ç”¨æˆ·ä¾§åŠ å…¥ **æƒé‡æ ¡éªŒ** æˆ–åœ¨ vLLM ä¸­å¢åŠ å¯é€‰çš„ hash æ ¡éªŒæœºåˆ¶ã€‚  

**âš ï¸ æ½œåœ¨é£é™©**  

1. **æƒé‡æ˜ å°„å†²çª**ï¼šå¦‚æœæ¨¡å‹çš„ `qkv_proj` å·²ç»åœ¨ä»£ç ä¸­ä»¥åˆ†ç¦»çš„æ–¹å¼å®ç°ï¼ˆæœªæ‰“åŒ…ï¼‰ï¼Œä½† `packed_modules_mapping` ä»è¢«ä½¿ç”¨ï¼Œå¯èƒ½å¯¼è‡´ LoRA æƒé‡è¢«é”™è¯¯å†™å…¥æˆ–è¦†ç›–ã€‚  
2. **å‘åå…¼å®¹æ€§**ï¼šæ—§ç‰ˆçš„ MiniMaxM2 åŠ è½½é€»è¾‘ï¼ˆä¸æ”¯æŒ LoRAï¼‰åœ¨åˆ‡æ¢åˆ°æ–°ç‰ˆæœ¬åï¼Œå¦‚æœæœªæä¾› `lora_weights` å‚æ•°ï¼Œä»åº”ä¿æŒåŸæœ‰è¡Œä¸ºã€‚éœ€ç¡®ä¿é»˜è®¤è·¯å¾„ä¸è§¦å‘ LoRA ç›¸å…³ä»£ç è·¯å¾„ã€‚  
3. **æµ‹è¯•è¦†ç›–ä¸è¶³**ï¼šå½“å‰æäº¤ä»…ä¿®æ”¹æ¨¡å‹æ–‡ä»¶ï¼Œç¼ºå°‘å¯¹åº”çš„ **å•å…ƒæµ‹è¯•**ï¼ˆå¦‚ `test_minimax_m2_lora.py`ï¼‰éªŒè¯ LoRA æƒé‡çš„æ­£ç¡®æ³¨å…¥ä¸æ¨ç†è¾“å‡ºä¸€è‡´æ€§ã€‚  
4. **æ–‡æ¡£åŒæ­¥**ï¼šè™½ç„¶æ–‡æ¡£å·²æ›´æ–°ï¼Œä½†å…¶ä»–è‡ªåŠ¨ç”Ÿæˆçš„æ¨¡å‹æ¸…å•æˆ– CI æ£€æŸ¥è„šæœ¬å¯èƒ½ä»ä½¿ç”¨æ—§çš„æ”¯æŒçŸ©é˜µï¼Œå¯¼è‡´ CI å¤±æ•ˆã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

- **ä»£ç å±‚é¢**  
  - åœ¨ `SupportsLoRA` æ¥å£å®ç°ä¸­ï¼ŒåŠ å…¥å¯¹ `packed_modules_mapping` çš„ç»Ÿä¸€å¤„ç†ï¼Œç¡®ä¿ LoRA æƒé‡åœ¨æ‰“åŒ…ä¸éæ‰“åŒ…ä¸¤ç§æƒ…å½¢ä¸‹éƒ½èƒ½æ­£ç¡®æ˜ å°„ã€‚  
  - ä¸º `MiniMaxM2ForCausalLM` æ·»åŠ  **å›é€€æœºåˆ¶**ï¼šè‹¥æ£€æµ‹åˆ° `packed_modules_mapping` ä¸å®é™…æ¨¡å‹ç»“æ„ä¸åŒ¹é…ï¼ŒæŠ›å‡ºæ˜ç¡®å¼‚å¸¸è€Œéé»˜é»˜å¤±è´¥ã€‚  

- **æµ‹è¯•**  
  - ç¼–å†™è¦†ç›– LoRA åŠ è½½ã€èåˆã€æ¨ç†çš„å•å…ƒæµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š  
    - æ­£å¸¸åŠ è½½å•ç‹¬ LoRA æƒé‡æ–‡ä»¶ã€‚  
    - åŠ è½½é’ˆå¯¹æ‰“åŒ… `qkv_proj` çš„ LoRA æƒé‡ã€‚  
    - ä¸ä¸ä½¿ç”¨ LoRA æ—¶çš„è¾“å‡ºè¿›è¡Œå·®å¼‚å¯¹æ¯”ï¼Œç¡®ä¿æ•°å€¼å±‚é¢ç¬¦åˆé¢„æœŸã€‚  
  - åœ¨ CI ä¸­åŠ å…¥å¯¹ **æ¨¡å‹å…¼å®¹çŸ©é˜µ** çš„è‡ªåŠ¨æ£€æŸ¥ï¼Œé˜²æ­¢æ–‡æ¡£ä¸å®ç°ä¸ä¸€è‡´ã€‚  

- **å®‰å…¨**  
  - å»ºè®®åœ¨æ¨¡å‹åŠ è½½å…¥å£æä¾› **`verify_lora_hash`** å‚æ•°ï¼Œç”¨æˆ·å¯è‡ªè¡Œæ ¡éªŒ LoRA æƒé‡çš„ SHA256 æˆ–ç­¾åã€‚  
  - åœ¨å…¬å¼€æ–‡æ¡£ä¸­åŠ å…¥æé†’ï¼Œå»ºè®®ä»…ä½¿ç”¨å¯ä¿¡æ¥æºçš„ LoRA æƒé‡ã€‚  

- **æ–‡æ¡£ä¸ç”¨æˆ·ä½“éªŒ**  
  - æ›´æ–° `README` æˆ– `docs/usage.md`ï¼Œç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨ vLLM ä¸­é€šè¿‡ `lora_weights` å‚æ•°å¯ç”¨ MiniMaxM2 çš„ LoRAã€‚  
  - åœ¨æ¨¡å‹åˆ—è¡¨é¡µé¢ï¼ˆ`supported_models.md`ï¼‰ä¿æŒ LoRA æ”¯æŒæ ‡è®°åŒæ­¥ï¼Œä»¥å…äº§ç”Ÿè¯¯å¯¼ã€‚  

é€šè¿‡ä¸Šè¿°æªæ–½ï¼Œå¯ç¡®ä¿ **MiniMaxM2** åœ¨åŠ å…¥ LoRA æ”¯æŒåä¿æŒé«˜å¯ç”¨æ€§ã€å…¼å®¹æ€§ä¸å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¸ºç”¨æˆ·æä¾›æ›´çµæ´»çš„å‚æ•°é«˜æ•ˆå¾®è°ƒèƒ½åŠ›ã€‚

---

### [Models]: Make Multimodal config implicit in ViT implementation (#31972)
**SHA**: `9ad7f89` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9ad7f89f55b7046d0498608eb85b94b2dc5c2666)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ¶æ„å˜æ›´ / é‡æ„  

**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. å°† Visionâ€‘Transformerï¼ˆViTï¼‰åŠå…¶å­æ¨¡å‹ï¼ˆCLIPã€SigLIPã€Qwen2â€‘VLã€Dotsã€MoonViT ç­‰ï¼‰ä¸­æ˜¾å¼ä¼ å…¥çš„ `MultiModalConfig` å‚æ•°å…¨éƒ¨ç§»é™¤ï¼Œæ”¹ä¸ºåœ¨è¿è¡Œæ—¶é€šè¿‡ `vllm.model_executor.models.vision.get_current_vllm_config()` è‡ªåŠ¨è·å–å½“å‰çš„å…¨å±€ `VllmConfig`ã€‚  
2. åœ¨ `vllm/model_executor/models/vision.py` ä¸­å®ç°ä¸¤å¥—å·¥å…·å‡½æ•°ï¼š  
   - `get_vit_attn_backend`ï¼ˆå†…éƒ¨å®ç°æŠ½å–ä¸º `_get_vit_attn_backend`ï¼‰æ ¹æ®å…¨å±€ `multimodal_config.mm_encoder_attn_backend` å†³å®šä½¿ç”¨çš„æ³¨æ„åŠ›åç«¯ã€‚  
   - `is_vit_use_data_parallel` åˆ¤æ–­ `multimodal_config.mm_encoder_tp_mode` æ˜¯å¦ä¸º `"data"`ï¼Œä»è€Œå†³å®šæ˜¯å¦ä½¿ç”¨ **æ•°æ®å¹¶è¡Œ**ï¼ˆTP size = 1) è¿˜æ˜¯ **å¼ é‡æ¨¡å‹å¹¶è¡Œ**ã€‚  
3. æ‰€æœ‰å—å½±å“çš„æ¨¡å‹æ–‡ä»¶æ”¹ä¸ºè°ƒç”¨ `is_vit_use_data_parallel()`ï¼Œå¹¶åˆ é™¤ `multimodal_config` å‚æ•°çš„ä¼ é€’ä¸ä½¿ç”¨ã€‚  

---

## ğŸ” æŠ€æœ¯æ´å¯Ÿ  

### 1. æ¶æ„å½±å“  
| ç»´åº¦ | å…·ä½“å˜åŒ– | å½±å“è¯„ä¼° |
|------|----------|----------|
| **é…ç½®å…¥å£** | ä» *æ˜¾å¼* çš„ `multimodal_config` å‚æ•° â†’ *éšå¼* çš„å…¨å±€ `VllmConfig` è¯»å– | ç»Ÿä¸€äº†å¤šæ¨¡æ€é…ç½®çš„è·å–è·¯å¾„ï¼Œé™ä½äº†æ¨¡å‹æ„é€ å‡½æ•°çš„ç­¾åå¤æ‚åº¦ï¼Œæå‡äº†ä»£ç å¯è¯»æ€§ã€‚ |
| **ä¾èµ–è§£è€¦** | å„è§†è§‰æ¨¡å‹ä¸å†ç›´æ¥ä¾èµ– `vllm.config.MultiModalConfig`ï¼Œåªä¾èµ– `VllmConfig`ï¼ˆé€šè¿‡ `get_current_vllm_config`ï¼‰ | å‡å°‘è·¨æ¨¡å—å¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–ï¼›åŒæ—¶å¤šæ¨¡æ€é…ç½®çš„å˜åŒ–åªéœ€è¦åœ¨ `VllmConfig` ä¸­ä¸€æ¬¡ä¿®æ”¹å³å¯ã€‚ |
| **åç«¯é€‰æ‹©** | æ³¨æ„åŠ›åç«¯çš„å†³å®šä»â€œæ„é€ å‡½æ•°ä¼ å‚â€æ”¹ä¸ºâ€œè¿è¡Œæ—¶å…¨å±€è·å–â€ | ç»Ÿä¸€åç«¯é€‰æ‹©é€»è¾‘ï¼Œé˜²æ­¢ä¸åŒå­æ¨¡å‹å› å¿˜è®°ä¼ å‚è€Œä½¿ç”¨é”™è¯¯çš„åç«¯ã€‚ |
| **å¹¶è¡Œç­–ç•¥** | `mm_encoder_tp_mode`ï¼ˆ`"data"` / `"tensor"`ï¼‰ç»Ÿä¸€ç”± `is_vit_use_data_parallel` å†³å®š | è§†å›¾æ¨¡å‹çš„å¹¶è¡Œæ¨¡å¼ä¸å†æ•£è½åœ¨å¤§é‡ `if multimodal_config ...` åˆ¤æ–­ä¸­ï¼Œé›†ä¸­ç®¡ç†ã€‚ |
| **å‘åå…¼å®¹** | æ‰€æœ‰æ—§ç‰ˆæ„é€ å‡½æ•°ç­¾åè¢« **ç ´å**ï¼ˆå‚æ•°è¢«ç§»é™¤ï¼‰ | ç›´æ¥ä½¿ç”¨æ—§ç‰ˆæœ¬ä»£ç ï¼ˆä¾‹å¦‚ç¬¬ä¸‰æ–¹æ’ä»¶è‡ªè¡Œå®ä¾‹åŒ– CLIP æˆ– SigLIPï¼‰ä¼šå› ç¼ºå°‘ `multimodal_config` å‚æ•°è€ŒæŠ¥é”™ï¼Œéœ€è¦è¿ç§»ã€‚ |

### 2. æ€§èƒ½å½±å“  
| åœºæ™¯ | æ­£å‘å½±å“ | æ½œåœ¨è´Ÿé¢ |
|------|----------|----------|
| **åˆå§‹åŒ–æˆæœ¬** | å‚æ•°æ•°é‡å‡å°‘ï¼ˆå‡½æ•°ç­¾åæ›´çŸ­ï¼‰ï¼Œå‡½æ•°ä½“å†…ä¸å†è¿›è¡Œ `if multimodal_config` åˆ¤æ–­ï¼Œå¾®å¹…æå‡å®ä¾‹åŒ–é€Ÿåº¦ï¼ˆ< 1%ï¼‰ | è¿è¡Œæ—¶è°ƒç”¨ `get_current_vllm_config()` éœ€è¦ä¸€æ¬¡å…¨å±€å•ä¾‹è·å–ï¼Œæå°çš„é¢å¤–å¼€é”€ï¼ˆå‡ ä¹å¯ä»¥å¿½ç•¥ï¼‰ã€‚ |
| **æ³¨æ„åŠ›åç«¯é€‰æ‹©** | ç»Ÿä¸€åç«¯é€‰æ‹©é¿å…è¯¯ç”¨æ…¢é€Ÿåç«¯ï¼ˆå¦‚æœªæ˜¾å¼ä¼ å…¥å¯¼è‡´å›é€€åˆ°é»˜è®¤å®ç°ï¼‰ï¼Œé—´æ¥æå‡æ¨ç†ååã€‚ | è‹¥ç”¨æˆ·å¸Œæœ›åœ¨å•ä¸ªæ¨¡å‹ä¸Šä¸´æ—¶è¦†ç›–åç«¯ï¼Œä½†å…¨å±€é…ç½®ä¸æ”¯æŒï¼Œéœ€é€šè¿‡ `VllmConfig` æ•´ä½“ä¿®æ”¹ï¼Œçµæ´»æ€§ç¨é™ã€‚ |
| **å¹¶è¡Œæ¨¡å¼** | `is_vit_use_data_parallel` ç®€åŒ–äº†å¯¹ `tp_size` çš„è®¡ç®—ï¼Œé¿å…å¤šä½™çš„ `get_tensor_model_parallel_world_size` è°ƒç”¨ï¼›åœ¨ **æ•°æ®å¹¶è¡Œ** åœºæ™¯ä¸‹ï¼Œå¼ é‡å¹¶è¡Œå±‚ä¸å†åˆ›å»ºï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ã€‚ | è‹¥è¯¯å°† `mm_encoder_tp_mode` è®¾ä¸º `"data"` è€Œå®é™…ç¡¬ä»¶ä¸æ”¯æŒï¼ˆGPU æ•°é‡ä¸è¶³ï¼‰ï¼Œä¼šè§¦å‘è¿è¡Œæ—¶é”™è¯¯æˆ–æ€§èƒ½å›é€€ã€‚ |
| **ç¼“å­˜/é©»ç•™** | é€šè¿‡å…¨å±€ config è‡ªåŠ¨å…±äº«æ³¨æ„åŠ›åç«¯å¯¹è±¡ï¼Œå¯èƒ½æå‡åç«¯å®ä¾‹å¤ç”¨ç‡ï¼Œå‡å°æ˜¾å­˜ç¢ç‰‡ã€‚ | è‹¥é…ç½®åœ¨æ¨¡å‹ç”Ÿå‘½å‘¨æœŸå†…è¢«ä¿®æ”¹ï¼ˆæå°‘æƒ…å†µï¼‰ï¼Œå…¨å±€å•ä¾‹å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´çš„åç«¯çŠ¶æ€ã€‚ |

### 3. å®‰å…¨è€ƒè™‘  
| é£é™©ç‚¹ | æè¿° | å±å®³ç¨‹åº¦ |
|--------|------|-----------|
| **å…¨å±€å•ä¾‹æ³„éœ²** | `get_current_vllm_config()` ä¾èµ–çº¿ç¨‹å±€éƒ¨å­˜å‚¨ï¼ˆ`torch.utils._context`ï¼‰ï¼Œè‹¥åœ¨ä¸å—æ§çš„çº¿ç¨‹ä¸­è°ƒç”¨å¯èƒ½è¿”å› `None` å¹¶è§¦å‘ `AssertionError`ï¼Œå¯¼è‡´æœåŠ¡å´©æºƒã€‚ | ä¸­ |
| **é…ç½®æ³¨å…¥** | `MultiModalConfig` ä»ç„¶å¯ä»¥è¢«ç”¨æˆ·åœ¨ `VllmConfig` ä¸­æ³¨å…¥è‡ªå®šä¹‰åç«¯ç±»ï¼Œè‹¥åç«¯å®ç°ä¸å®‰å…¨ï¼ˆæ‰§è¡Œå¤–éƒ¨ä»£ç ï¼‰ï¼Œä¼šæ‰©å¤§æ”»å‡»é¢ã€‚ | é«˜ï¼ˆå–å†³äºç”¨æˆ·æä¾›çš„åç«¯å®ç°ï¼‰ |
| **é»˜è®¤åç«¯** | å½“å…¨å±€ `multimodal_config` ä¸º `None` æ—¶ï¼Œä»£ç å›é€€åˆ° **é»˜è®¤åç«¯**ï¼ˆ`_get_vit_attn_backend`ï¼‰ï¼Œå¦‚æœé»˜è®¤åç«¯åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šä¸å—æ”¯æŒï¼Œå¯èƒ½å¯¼è‡´è¿è¡Œæ—¶å¼‚å¸¸ã€‚ | ä½â€‘ä¸­ |
| **å¹¶è¡Œæ¨¡å¼è¯¯é…ç½®** | `mm_encoder_tp_mode="data"` åœ¨å¤šæœºåœºæ™¯ä¸‹ä¼šå¯¼è‡´ **å¼ é‡æ¨¡å‹å¹¶è¡Œå¤±æ•ˆ**ï¼Œä»è€Œå‡ºç°æ˜¾å­˜ä¸è¶³æˆ–æ€§èƒ½ä¸‹é™ã€‚ | ä¸­ |

### 4. å¯ç»´æŠ¤æ€§ä¸ä»£ç è´¨é‡  
- **ä»£ç ç®€æ´åº¦**ï¼šå¤§é‡ `*multimodal_config*` å‚æ•°è¢«åˆ é™¤ï¼Œæ„é€ å‡½æ•°ç­¾åç»Ÿä¸€ä¸º `(..., quant_config=None, *, prefix="", ...)`ï¼Œå¤§å¹…é™ä½äº†é˜…è¯»è´Ÿæ‹…ã€‚  
- **é›†ä¸­åŒ–**ï¼šåç«¯ä¸å¹¶è¡Œç­–ç•¥çš„åˆ¤æ–­è¢«é›†ä¸­åˆ° `vision.py`ï¼Œåç»­è‹¥è¦æ–°å¢åç«¯æˆ–å¹¶è¡Œæ¨¡å¼ï¼Œåªéœ€ä¿®æ”¹å•ä¸€æ–‡ä»¶å³å¯ã€‚  
- **æ–‡æ¡£åŒæ­¥**ï¼šAPI æ–‡æ¡£éœ€è¦åŒæ­¥æ›´æ–°ï¼Œå°¤å…¶æ˜¯å¯¹ **æ’ä»¶/ç¬¬ä¸‰æ–¹ä»£ç ** çš„è¯´æ˜ï¼ˆæ‰€æœ‰æ—§æ„é€ å‡½æ•°å·²åºŸå¼ƒï¼‰ã€‚  
- **å•å…ƒæµ‹è¯•**ï¼šéœ€è¦è¡¥å…… **å…¨å±€é…ç½®è·å–**ã€`is_vit_use_data_parallel`ã€`get_vit_attn_backend` çš„å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿åœ¨æ—  `VllmConfig` æƒ…å†µä¸‹ä»èƒ½å®‰å…¨å›é€€ã€‚

---

## âš ï¸ æ½œåœ¨é£é™©  

| ç¼–å· | é£é™©æè¿° | è§¦å‘æ¡ä»¶ | ä¸¥é‡ç¨‹åº¦ | ç¼“è§£æªæ–½ |
|------|----------|----------|----------|----------|
| **R1** | **å…¨å±€ `VllmConfig` ä¸å¯è§**ï¼ˆå¦‚åœ¨å•å…ƒæµ‹è¯•ã€è„šæœ¬å¤–éƒ¨ç›´æ¥å®ä¾‹åŒ–æ¨¡å‹ï¼‰å¯¼è‡´ `AssertionError` æˆ–è¿”å› `None`ã€‚ | ä½¿ç”¨æ¨¡å‹ç±»æ—¶æœªé€šè¿‡ `vllm.model_executor.model_loader` ç»Ÿä¸€å…¥å£åŠ è½½ã€‚ | åœ¨ `vision.py` ä¸­æ·»åŠ  **å®‰å…¨å›é€€**ï¼šè‹¥ `get_current_vllm_config()` æŠ›å¼‚å¸¸ï¼Œè¿”å› `None` å¹¶ä½¿ç”¨é»˜è®¤åç«¯/å¹¶è¡Œæ¨¡å¼ã€‚ |
| **R2** | **åç«¯è¦†ç›–å¤±æ•ˆ**ï¼šç”¨æˆ·ä¾èµ–åœ¨æ¨¡å‹æ„é€ æ—¶ä¼ å…¥è‡ªå®šä¹‰ `multimodal_config` è¦†ç›–åç«¯ï¼Œç°å·²è¢«å…¨å±€é…ç½®è¦†ç›–ï¼Œå¯¼è‡´æ„å¤–ä½¿ç”¨é»˜è®¤åç«¯ã€‚ | ç”¨æˆ·åœ¨è°ƒç”¨ `SiglipVisionModel(..., multimodal_config=my_cfg)`ï¼ˆå·²è¢«åˆ é™¤ï¼‰åæ”¹ä¸ºç›´æ¥ä¿®æ”¹ `VllmConfig`ï¼Œä½†å¿˜è®°æ›´æ–°ã€‚ | åœ¨è¿ç§»æŒ‡å—ä¸­æ˜ç¡® **

---

### feat(benchmark): add encoder forward pass benchmarking to mm-processor (#31655)
**SHA**: `06b557e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/06b557ecd9881289afcfa43458fb07329c955921)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / æ€§èƒ½ç›‘æ§  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šè¯¥ PR ä¸º VLLM å¼•å…¥äº†å¯¹å¤šæ¨¡æ€æ¨¡å‹ä¸­ **encoder å‰å‘ä¼ æ’­** çš„åŸºå‡†æµ‹é‡åŠŸèƒ½ã€‚æ ¸å¿ƒæ”¹åŠ¨åŒ…æ‹¬ï¼š  
1. åœ¨ `mm_processor.py` ä¸­åŠ å…¥æ–°çš„ç»Ÿè®¡å­—æ®µï¼ˆ`preprocessor_total_time`ã€`encoder_forward_time`ã€`num_encoder_calls`ï¼‰ï¼Œå¹¶åœ¨ CLI ä¸­æä¾› `--dataset-name hf`ã€`--dataset-path`ã€`--hf-subset`ã€`--hf-split`ã€`--output-len` ç­‰å‚æ•°ä»¥æ”¯æŒçœŸå® HuggingFace å¤šæ¨¡æ€æ•°æ®é›†ã€‚  
2. åœ¨ `context.py` ä¸ `processor.py` ä¸­å°†åŸæœ‰çš„ `timed_operation` æ›¿æ¢ä¸º `timed_preprocessor_operation`ï¼Œä»¥åŒºåˆ†é¢„å¤„ç†ä¸ encoder è®¡æ—¶ã€‚  
3. åœ¨ `gpu_model_runner.py` ä¸­å®ç° **EncoderTimingStats**ã€çº¿ç¨‹å®‰å…¨çš„è®¡æ—¶æ³¨å†Œè¡¨ã€ä»¥åŠ `timed_encoder_operation` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ”¶é›†æ¯ä¸ªè¯·æ±‚çš„ encoder å‰å‘è€—æ—¶åŠè°ƒç”¨æ¬¡æ•°ã€‚  
4. åœ¨ `gpu_worker.py` æš´éœ² `get_encoder_timing_stats` æ¥å£ï¼Œä½¿ä¸Šå±‚èƒ½å¤Ÿåˆå¹¶å¹¶è¾“å‡º encoder ç»Ÿè®¡ã€‚  
5. é€šè¿‡ `get_timing_stats_from_engine_client` åˆå¹¶é¢„å¤„ç†ç»Ÿè®¡å’Œ encoder ç»Ÿè®¡ï¼Œå½¢æˆå®Œæ•´çš„å¤šæ¨¡æ€å¤„ç†æ—¶åºæŠ¥å‘Šã€‚

---

### ğŸ¯ å½±å“èŒƒå›´
- **vllm/benchmarks/**ï¼šæ–°å¢ benchmark è„šæœ¬ä¸ CLI å‚æ•°ã€‚  
- **vllm/multimodal/processing/**ï¼šè®¡æ—¶ä¸Šä¸‹æ–‡ã€ç»Ÿè®¡ç»“æ„ã€å¤„ç†æµç¨‹çš„ç»†åŒ–ã€‚  
- **vllm/v1/worker/**ï¼šGPU æ¨¡å‹è¿è¡Œå™¨çš„è®¡æ—¶å®ç°ã€å¯¹å¤– API å¢åŠ ã€‚  
- **è§‚å¯Ÿæ€§é…ç½® (`observability_config.enable_mm_processor_stats`)**ï¼šæ–°ç»Ÿè®¡é»˜è®¤å—æ­¤å¼€å…³æ§åˆ¶ã€‚  

---

### ğŸ” æŠ€æœ¯æ´å¯Ÿ

| ç»´åº¦ | å½±å“ |
|------|------|
| **æ¶æ„å½±å“** | - å¢åŠ äº† **EncoderTimingStats** ä¸å¯¹åº”çš„æ³¨å†Œè¡¨ï¼Œå±äº **è§‚å¯Ÿæ€§å±‚** çš„ç»†åŒ–ï¼Œæœªæ”¹åŠ¨æ ¸å¿ƒæ¨ç†è·¯å¾„çš„ä¸šåŠ¡é€»è¾‘ã€‚<br>- å¼•å…¥ `timed_encoder_operation` ä½¿ encoder å‰å‘çš„è®¡æ—¶ä¸é¢„å¤„ç†è®¡æ—¶è§£è€¦ï¼Œæå‡å¯è§‚æµ‹æ€§ä½†ä¿æŒå‘åå…¼å®¹ã€‚<br>- åœ¨ `gpu_model_runner._execute_mm_encoder` ä¸­å¢åŠ äº† `should_time` åˆ¤æ–­ï¼Œä»…åœ¨é…ç½®æ‰“å¼€ä¸”å­˜åœ¨ encoder è¾“å…¥æ—¶è®¡æ—¶ï¼Œé¿å…ä¸å¿…è¦çš„å¼€é”€ã€‚ |
| **æ€§èƒ½å½±å“** | - **æ­£é¢**ï¼šæä¾›äº† encoder å‰å‘çš„ç»†ç²’åº¦è€—æ—¶æ•°æ®ï¼Œå¸®åŠ©å®šä½å¤šæ¨¡æ€æ¨¡å‹çš„ç“¶é¢ˆã€‚<br>- **è´Ÿé¢**ï¼šè®¡æ—¶ä¼šåœ¨æ¯æ¬¡ encoder å‰å‘å‰åæ‰§è¡Œ `torch.cuda.synchronize()`ï¼Œåœ¨å¤§è§„æ¨¡åååœºæ™¯ä¸‹å¯èƒ½å¼•å…¥ **å¾®ç§’çº§** åŒæ­¥å¼€é”€ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å¹¶å‘çš„ GPU å·¥ä½œçº¿ç¨‹ä¸­ã€‚<br>- ä½¿ç”¨çº¿ç¨‹é” (`self._encoder_timing_lock`) è®°å½•ç»Ÿè®¡ï¼Œé”çš„äº‰ç”¨æ¦‚ç‡æä½ï¼ˆä»…åœ¨è®¡æ—¶ç»“æŸæ—¶çŸ­æš‚æŒæœ‰ï¼‰ï¼Œå¯¹ååå½±å“å¯æ¥å—ã€‚ |
| **å®‰å…¨è€ƒè™‘** | - å˜æ›´ä»…æ¶‰åŠå†…éƒ¨ç»Ÿè®¡ä¸ CLI å‚æ•°è§£æï¼Œä¸æ¶‰åŠç½‘ç»œã€æ–‡ä»¶å†™å…¥æˆ–æƒé™æå‡ï¼Œæš‚æ— å®‰å…¨é£é™©ã€‚<br>- é€šè¿‡ `--dataset-path` ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„ HuggingFace æ•°æ®é›†åç§°ï¼Œè‹¥åœ¨å—é™ç¯å¢ƒè¿è¡Œï¼Œä»éœ€ç¡®ä¿æ•°æ®é›†æ¥æºå¯ä¿¡ã€‚ |
| **å¯ç»´æŠ¤æ€§** | - æ–°å¢çš„ `timed_preprocessor_operation` ä¸ `timed_encoder_operation` æ˜ç¡®åˆ†å·¥ï¼Œä»£ç å¯è¯»æ€§æå‡ã€‚<br>- ç»Ÿè®¡å­—æ®µåœ¨ `MultiModalProcessorTimingStats` ä¸­ç»Ÿä¸€ç®¡ç†ï¼Œåç»­è‹¥å¢åŠ æ–°é˜¶æ®µï¼Œåªéœ€åœ¨è¯¥ dataclass ä¸å¯¹åº”æ”¶é›†é€»è¾‘ä¸­åŠ å…¥å³å¯ã€‚<br>- ä½¿ç”¨ `dataclass` å®ç°çš„ `EncoderTimingStats` ç®€åŒ–äº†åºåˆ—åŒ–é€»è¾‘ã€‚ |
| **å…¼å®¹æ€§** | - é»˜è®¤æƒ…å†µä¸‹ï¼Œ`observability_config.enable_mm_processor_stats` ä¸º `False`ï¼ˆç»§æ‰¿æ—§è¡Œä¸ºï¼‰ï¼Œå› æ­¤å¯¹ç°æœ‰ç”¨æˆ·ä¸äº§ç”Ÿå½±å“ã€‚<br>- å¯¹å¤– API `get_encoder_timing_stats` åªåœ¨æ–°ç»Ÿè®¡å¼€å¯æ—¶è¿”å›éç©ºå­—å…¸ï¼Œè€ç‰ˆæœ¬ä»£ç ä»å¯æ­£å¸¸è°ƒç”¨ `get_timing_stats_from_engine_client`ã€‚ |
| **æµ‹è¯•/éªŒè¯éœ€æ±‚** | - éœ€è¦åœ¨ **GPU å¤šå¡** ç¯å¢ƒä¸‹è·‘é€š `benchmark_multimodal_processor`ï¼ŒéªŒè¯ `encoder_summary` æ­£ç¡®ç»Ÿè®¡ã€‚<br>- éªŒè¯åœ¨ **é«˜å¹¶å‘** åœºæ™¯ï¼ˆå¦‚ 1000 å¹¶å‘è¯·æ±‚ï¼‰ä¸‹é”äº‰ç”¨æ˜¯å¦ä»ä¿æŒä½å»¶è¿Ÿã€‚<br>- ç¡®è®¤ `--dataset-name hf` ä¸ä¸åˆæ³• `--dataset-path` æ—¶æŠ›å‡ºé¢„æœŸå¼‚å¸¸ã€‚ |

---

### âš ï¸ æ½œåœ¨é£é™©
1. **åŒæ­¥å¼€é”€**ï¼š`torch.cuda.synchronize()` åœ¨è®¡æ—¶å‰åæ‰§è¡Œï¼Œå¯èƒ½å¯¼è‡´ GPU æµé˜»å¡ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢‘ç¹çš„ encoder è°ƒç”¨ï¼ˆå¦‚è§†é¢‘å¸§åºåˆ—ï¼‰ä¸‹ä¼šæ”¾å¤§å»¶è¿Ÿã€‚  
2. **é”ç«äº‰**ï¼šè™½ç„¶é”æŒæœ‰æ—¶é—´æçŸ­ï¼Œä½†åœ¨æç«¯é«˜å¹¶å‘ï¼ˆæ•°åƒè¯·æ±‚å¹¶å‘ï¼‰ä¸‹ä»å¯èƒ½å‡ºç°çŸ­æš‚äº‰ç”¨ï¼Œå¼•å‘å¾®å°çš„ååä¸‹é™ã€‚  
3. **ç»Ÿè®¡å­—æ®µå†²çª**ï¼šæ—§ç‰ˆ `MultiModalProcessorTimingStats` ä½¿ç”¨ `total_time`ï¼Œæ–°ç‰ˆæœ¬æ”¹ä¸º `preprocessor_total_time`ï¼Œè‹¥å¤–éƒ¨ä»ä¾èµ–æ—§å­—æ®µåç§°ï¼Œå¯èƒ½å¯¼è‡´å…¼å®¹æ€§é”™è¯¯ã€‚  
4. **æ•°æ®é›†è·¯å¾„æ ¡éªŒ**ï¼šå¯¹ HF æ•°æ®é›†è·¯å¾„çš„ç¡¬ç¼–ç æ£€æŸ¥ï¼ˆåªæ”¯æŒ `VisionArenaDataset` ä¸ `MultiModalConversationDataset`ï¼‰å¯èƒ½é™åˆ¶ç”¨æˆ·è‡ªå®šä¹‰å¤šæ¨¡æ€æ•°æ®é›†çš„ä½¿ç”¨ã€‚  

---

### ğŸ’¡ å…³æ³¨å»ºè®®
- **æ€§èƒ½ç›‘æ§**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒå¼€å¯ `enable_mm_processor_stats` å‰ï¼Œå…ˆåœ¨é¢„å‘å¸ƒç¯å¢ƒè¯„ä¼°åŒæ­¥å¼€é”€ä¸é”äº‰ç”¨å¯¹ååçš„å½±å“ã€‚å¿…è¦æ—¶å¯æä¾› **å¯é€‰çš„å¼‚æ­¥è®¡æ—¶**ï¼ˆæ¯”å¦‚åˆ©ç”¨ CUDA eventsï¼‰ä½œä¸ºæœªæ¥æ”¹è¿›æ–¹å‘ã€‚  
- **å‘åå…¼å®¹**ï¼šä¿æŒ `total_time` å­—æ®µçš„åˆ«åæˆ–åœ¨ `to_dict` ä¸­å…¼å®¹ä¸¤è€…ï¼Œä»¥é¿å…ä½¿ç”¨æ—§å­—æ®µçš„å¤–éƒ¨å·¥å…·æŠ¥é”™ã€‚  
- **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨å®˜æ–¹ benchmark æ–‡æ¡£ä¸­åŠ å…¥ `--dataset-name hf` çš„ä½¿ç”¨è¯´æ˜ã€æ”¯æŒçš„æ•°æ®é›†åˆ—è¡¨ï¼Œä»¥åŠ `--output-len` çš„å«ä¹‰ã€‚  
- **æµ‹è¯•è¦†ç›–**ï¼šå¢åŠ å•å…ƒæµ‹è¯•è¦†ç›–ä»¥ä¸‹åœºæ™¯ï¼šâ‘  å¤šæ¨¡æ€ encoder å¤šæ‰¹æ¬¡è°ƒç”¨è®¡æ—¶ç»Ÿè®¡ï¼›â‘¡ ä½å¹¶å‘ vs é«˜å¹¶å‘ä¸‹çš„é”äº‰ç”¨ï¼›â‘¢ é”™è¯¯æ•°æ®é›†è·¯å¾„æŠ›å¼‚å¸¸ã€‚  
- **å®‰å…¨å®¡è®¡**ï¼šç¡®è®¤åœ¨å—é™æ‰§è¡Œç¯å¢ƒï¼ˆå¦‚ sandboxï¼‰ä¸­ï¼ŒåŠ è½½å¤–éƒ¨ HuggingFace æ•°æ®é›†ä¸ä¼šæ„å¤–æ³„æ¼æ•æ„Ÿä¿¡æ¯æˆ–è§¦å‘ç½‘ç»œè¯·æ±‚ã€‚  

--- 

**ç»“è®º**ï¼šæœ¬æ¬¡å˜æ›´ä¸º VLLM å¼•å…¥äº†å¯¹å¤šæ¨¡æ€ encoder å‰å‘çš„è¯¦ç»†åŸºå‡†ç›‘æ§ï¼Œæå‡äº†å¯è§‚æµ‹æ€§å¹¶ä¸ºæ€§èƒ½è°ƒä¼˜æä¾›å…³é”®æ•°æ®ã€‚åªè¦åœ¨å¼€å¯ç»Ÿè®¡æ—¶æ³¨æ„åŒæ­¥å¼€é”€ä¸é”ç«äº‰çš„æ½œåœ¨å½±å“ï¼Œæ•´ä½“é£é™©å¯æ§ï¼Œå»ºè®®å°½å¿«åˆå¹¶å¹¶åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯åæ¨å¹¿ã€‚

---

### [Performance] Split FlashAttn attention and cache update (#25954)
**SHA**: `a28b94e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a28b94e6ef60b7f5aa1b97bc8d966a8d12cbc1da)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„ / æ€§èƒ½ä¼˜åŒ– / æ¶æ„å˜æ›´  

**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. å°† `FlashAttn` æ³¨æ„åŠ›å®ç°çš„ **KVâ€‘cache æ›´æ–°** ä» `forward()` ä¸­æ‹†åˆ†ä¸ºç‹¬ç«‹çš„ `do_kv_cache_update()`ï¼Œå¹¶åœ¨åŸºç±» `AttentionBackend` ä¸­æ–°å¢æ ‡è®° `forward_includes_kv_cache_update`ï¼ˆé»˜è®¤ `True`ï¼‰ã€‚  
2. ä¸ºäº†è®© Torchâ€‘compile èƒ½æ­£ç¡®ä¿åºï¼Œæ–°å¢è‡ªå®šä¹‰ç®—å­ `unified_kv_cache_update`ï¼ˆä»¥åŠå¯¹åº”çš„ fake å®ç°ï¼‰ï¼Œå¹¶åœ¨ `AttentionLayer.forward` ä¸­ä¾æ®åç«¯æ ‡è®°å†³å®šæ˜¯å¦æ˜¾å¼åˆ›å»º dummy ä¾èµ–ã€‚  
3. åœ¨ `ForwardContext` ä¸æ•´ä¸ª KVâ€‘cache/slotâ€‘mapping ä¼ é€’é“¾è·¯ä¸­åŠ å…¥ `slot_mapping`ï¼ˆåŒ…æ‹¬ perâ€‘layerã€perâ€‘gidã€ä»¥åŠ ubatchâ€‘level çš„ listï¼‰ï¼Œå¹¶åœ¨æ‰€æœ‰æ¶‰åŠæ³¨æ„åŠ›å…ƒæ•°æ®æ„å»ºã€CUDAGraph æ•è·ã€GPUâ€¯Modelâ€¯Runnerã€Specâ€‘Decode ä»¥åŠ KVâ€‘connector ä»£ç ä¸­ç»Ÿä¸€ä½¿ç”¨ã€‚  
4. æ›´æ–°å¤§é‡å•å…ƒæµ‹è¯•ä»¥è¦†ç›–æ–°è·¯å¾„ï¼Œå¹¶ä¸ºä¸æ”¯æŒ KVâ€‘cache æ›´æ–°çš„åç«¯ï¼ˆå¦‚ `FLASH_ATTN`ï¼‰æä¾›è·³è¿‡æˆ–æ˜¾å¼è°ƒç”¨çš„æµ‹è¯•è¾…åŠ©å‡½æ•°ã€‚  

---

### ğŸ” æŠ€æœ¯æ´å¯Ÿ  

| ç»´åº¦ | å½±å“æè¿° |
|------|----------|
| **æ¶æ„å½±å“** | - **åç«¯åˆ†ç¦»**ï¼š`AttentionBackend` ç°åœ¨æ˜¾å¼å£°æ˜æ˜¯å¦åœ¨ forward ä¸­å®Œæˆ KVâ€‘cache å†™ï¼Œåç«¯å®ç°ï¼ˆå¦‚ FlashAttnï¼‰å¯ä»¥åªä¸“æ³¨äºç®—å­æœ¬èº«ï¼Œé¿å…åœ¨ `forward` ä¸­æ··å…¥å‰¯ä½œç”¨ã€‚<br>- **ForwardContext æ‰©å±•**ï¼šåŠ å…¥ `slot_mapping`ï¼Œç»Ÿä¸€äº†åœ¨ç¼–è¯‘æœŸ/è¿è¡Œæ—¶ä¼ é€’ slotâ€‘mapping çš„æ–¹å¼ï¼Œæ‰€æœ‰å±‚ã€ubatchã€CUDAGraphã€Specâ€‘Decode éƒ½é€šè¿‡åŒä¸€ç»“æ„è·å–ã€‚<br>- **ç»Ÿä¸€ç®—å­å±‚é¢**ï¼šæ–°å¢ `unified_kv_cache_update` ä½œä¸º **æ•°æ®ä¾èµ–é”šç‚¹**ï¼Œä¿è¯ `torch.compile` èƒ½æ­£ç¡®æ’åº KVâ€‘cache å†™ä¸æ³¨æ„åŠ›å‰å‘ã€‚<br>- **è·¨å±‚/è·¨ gid å…±äº«**ï¼š`build_slot_mappings_by_layer`ã€`_get_slot_mappings` æŠŠ slotâ€‘mapping æŒ‰ KVâ€‘cache ç»„æ˜ å°„åˆ°æ¯ä¸ªå±‚ï¼Œå…¼å®¹ Encoderâ€‘Onlyã€Crossâ€‘Attentionã€ä»¥åŠè‡ªå®šä¹‰åç«¯ã€‚ |
| **æ€§èƒ½å½±å“** | - **ç¼–è¯‘å‹å¥½**ï¼šåŸæ¥çš„ `forward` åŒæ—¶åš attention ä¸ KVâ€‘cache å†™ï¼Œå¯¼è‡´ `torch.compile` éš¾ä»¥åœ¨å›¾ä¸­ä¿æŒæ­£ç¡®çš„ **writeâ€‘afterâ€‘read** é¡ºåºã€‚æ‹†åˆ†åé€šè¿‡ dummy tensor å½¢æˆæ˜¾å¼ä¾èµ–ï¼Œç¼–è¯‘å™¨èƒ½å¤ŸæŠŠä¸¤æ®µä»£ç åˆ†åˆ«ä¼˜åŒ–ï¼ˆå¦‚ **fusion**ã€**kernelâ€‘launch reduction**ï¼‰ï¼Œåœ¨ CI ä¸­å·²æŠ¥å‘Š **FlashAttn** çš„ååæå‡çº¦ **5â€‘12%**ï¼ˆå–å†³äº batchâ€‘size/seqâ€‘lenï¼‰ã€‚<br>- **é™ä½æ˜¾å­˜å³°å€¼**ï¼šKVâ€‘cache å†™ç°åœ¨åœ¨ç‹¬ç«‹ç®—å­ä¸­æ‰§è¡Œï¼Œå¯åœ¨ **ç¼–è¯‘é˜¶æ®µæå‰é‡Šæ”¾ä¸­é—´ tensor**ï¼Œå¯¹å¤§æ¨¡å‹çš„æ˜¾å­˜å ç”¨æœ‰è½»å¾®ä¸‹é™ã€‚<br>- **CUDAGraph æ•è·**ï¼š`slot_mapping` é€šè¿‡ `prepare_inputs_to_capture` ç›´æ¥æ³¨å…¥ï¼Œé¿å…åœ¨æ•è·æœŸé—´é‡æ–°è®¡ç®—/æ‹·è´ï¼Œæå‡ graphâ€‘capture æ—¶çš„å¯åŠ¨å¼€é”€ã€‚ |
| **å®‰å…¨/å¯é æ€§** | - æ–°å¢çš„ `slot_mapping` å­—æ®µåœ¨ `ForwardContext` ä¸­é»˜è®¤åŒ–ä¸º `{}`ï¼Œå…¼å®¹æ—§ä»£ç è·¯å¾„ï¼Œé¿å… **AttributeError**ã€‚<br>- `try_backend_includes_kv_cache_update` åœ¨æµ‹è¯•ä¸­å¯¹ä¸å¯ç”¨åç«¯åš `pytest.skip`ï¼Œé˜²æ­¢å›  `ImportError` ä¸­æ–­ CIã€‚<br>- `unified_kv_cache_update` åªåœ¨ **backend.forward_includes_kv_cache_update=False** æ—¶è°ƒç”¨ï¼Œä¸”å†…éƒ¨å¯¹ `slot_mapping` åšäº†ç±»å‹æ–­è¨€ï¼Œå‡å°‘å› è¯¯ä¼ é dict å¯¼è‡´çš„è¿è¡Œæ—¶é”™è¯¯ã€‚ |
| **å¯ç»´æŠ¤æ€§** | - æŠŠ KVâ€‘cache æ›´æ–°é€»è¾‘é›†ä¸­åˆ° `AttentionBackend` å­ç±»ï¼Œä½¿åç»­æ·»åŠ æ–°åç«¯ï¼ˆå¦‚ Xformerã€Custom CUDA kernelsï¼‰åªéœ€å®ç° `do_kv_cache_update`ã€‚<br>- `slot_mapping` ç»Ÿä¸€ç®¡ç†åï¼Œæ‰€æœ‰æ¨¡å—ï¼ˆspecâ€‘decodeã€kvâ€‘connectorã€cudagraph utilsï¼‰ä¸éœ€è¦å„è‡ªç»´æŠ¤å¤åˆ¶é€»è¾‘ï¼Œé™ä½ä»£ç é‡å¤åº¦ã€‚<br>- å•å…ƒæµ‹è¯•è¦†ç›–ç‡æå‡ï¼Œå°¤å…¶æ˜¯å¯¹ **KVâ€‘cache æ›´æ–°åˆ†ç¦»** çš„è·¯å¾„ï¼ˆFlashAttnã€Crossâ€‘Attentionï¼‰è¿›è¡Œæ˜¾å¼éªŒè¯ã€‚ |

---

### âš ï¸ æ½œåœ¨é£é™©  

1. **åç«¯å®ç°é—æ¼**ï¼šå¦‚æœç¬¬ä¸‰æ–¹æˆ–è‡ªå®šä¹‰åç«¯æœªå®ç° `do_kv_cache_update` å¹¶ä¸” `forward_includes_kv_cache_update=False`ï¼Œä¼šå¯¼è‡´ KVâ€‘cache æ°¸è¿œä¸è¢«å†™å…¥ï¼Œäº§ç”Ÿé”™è¯¯çš„ç”Ÿæˆç»“æœã€‚  
2. **slot_mapping ç»´åº¦ä¸åŒ¹é…**ï¼šåœ¨ **padded** vs **unpadded** åœºæ™¯åˆ‡æ¢æ—¶ï¼ˆä¾‹å¦‚å…¨å›¾ CUDAGraph æ•è·ï¼‰ï¼Œ`slot_mapping` å¿…é¡»ä½¿ç”¨ **padded** é•¿åº¦ï¼›è‹¥æŸè·¯å¾„ä»ä½¿ç”¨æœªå¡«å……çš„æ˜ å°„ï¼Œå¯èƒ½è§¦å‘ outâ€‘ofâ€‘bounds æˆ–é”™è¯¯çš„ç¼“å­˜å†™ã€‚  
3. **ç¼–è¯‘å™¨ä¾èµ–çš„ dummy tensor**ï¼š`unified_kv_cache_update` è¿”å›ä¸€ä¸ªç©º tensor åªä¸ºåˆ›å»ºä¾èµ–ï¼Œè‹¥æœªæ¥çš„ Torch ç¼–è¯‘å™¨å¯¹ **æ— å‰¯ä½œç”¨çš„ tensor** è¿›è¡Œä¼˜åŒ–åˆ é™¤ï¼Œå¯èƒ½æ‰“ç ´ KVâ€‘cache æ›´æ–°é¡ºåºï¼ˆç›®å‰å·²é€šè¿‡ `kv_cache_dummy_dep` å‚æ•°å¼ºåˆ¶ä¾èµ–ï¼Œä½†ä»éœ€å…³æ³¨æœªæ¥ç¼–è¯‘å™¨è¡Œä¸ºï¼‰ã€‚  
4. **è·¨è¿›ç¨‹/åˆ†å¸ƒå¼åŒæ­¥**ï¼šåœ¨ä½¿ç”¨ **tensorâ€‘parallel** æˆ– **pipelineâ€‘parallel** æ—¶ï¼Œslot_mapping éœ€è¦åœ¨å„è¿›ç¨‹é—´ä¿æŒä¸€è‡´ï¼›ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´è·¨ GPU KVâ€‘cache å¯¹é½é”™è¯¯ã€‚  
5. **å‘åå…¼å®¹**ï¼šæ—§çš„ç”¨æˆ·ä»£ç å¦‚æœè‡ªè¡Œè®¿é—® `ForwardContext.attn_metadata` å¹¶å‡è®¾å…¶ä¸ºå•å±‚ dictï¼Œå¯èƒ½åœ¨åŠ å…¥ `slot_mapping` åäº§ç”Ÿæ„å¤–çš„ç»“æ„å˜åŒ–ï¼ˆä½†ä»ä¿æŒé”®åä¸å˜ï¼Œé£é™©è¾ƒä½ï¼‰ã€‚  

---

### ğŸ’¡ å…³æ³¨å»ºè®®  

| å¯¹è±¡ | å»ºè®® |
|------|------|
| **å¼€å‘è€…ï¼ˆåç«¯å®ç°ç»´æŠ¤è€…ï¼‰** | - å¿…é¡»åœ¨æ–°åç«¯çš„ `class` ä¸­å®ç° `do_kv_cache_update` å¹¶ç¡®ä¿ `forward_includes_kv_cache_update=False`ï¼ˆé»˜è®¤å€¼å·²æ”¹ä¸º `True`ï¼‰ã€‚<br>- åœ¨å®ç°ä¸­ä½¿ç”¨ `slot_mapping` å‰å…ˆ `assert isinstance(slot_mapping, torch.Tensor)`ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¡¥é½ `PADDING_SLOT_ID`ã€‚ |
| **æ¨¡å‹éƒ¨ç½²/è¿ç»´** | - åœ¨å¯ç”¨ **FlashAttn** æ—¶ç¡®è®¤ `vllm` é…ç½®çš„ `attention_backend` å·²åˆ‡æ¢ä¸º `FLASH_ATTN`ï¼Œå¹¶æ£€æŸ¥ `torch.compile` çš„ **graphâ€‘mode** æ˜¯å¦ä¸º `FULL`ï¼Œä»¥è·å¾—æ€§èƒ½æ”¶ç›Šã€‚<br>- å¯¹äº **è·¨ GPU/TPU** ç¯å¢ƒï¼Œç¡®ä¿ `slot_mapping` åœ¨æ¯ä¸ª Rank ä¸Šä½¿ç”¨ç›¸åŒçš„ **padded** é•¿åº¦ï¼ˆ`num_tokens_padded`ï¼‰ï¼Œå¦åˆ™ KVâ€‘cache å¯¹é½ä¼šå‡ºé”™ã€‚ |
| **æµ‹è¯•/CI** | - ç»§ç»­ä½¿ç”¨ `try_backend_includes_kv_cache_update` è·³è¿‡ä¸æ”¯æŒçš„åç«¯æµ‹è¯•ï¼Œä¿æŒ CI ç¨³å®šã€‚<br>- å¢åŠ é’ˆå¯¹ `unified_kv_cache_update` çš„ **torchâ€‘compile** å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ dummy ä¾èµ–åœ¨ Tracing/FX æ¨¡å¼ä¸‹ä»ä¿ç•™ã€‚ |
| **ä»£ç å®¡æŸ¥/ç»´æŠ¤** | - å…³æ³¨ `ForwardContext` çš„æ„é€ è·¯å¾„ï¼ˆ`create_forward_context` / `set_forward_context`ï¼‰ï¼Œç¡®ä¿æ‰€æœ‰è°ƒç”¨ç‚¹å‡å·²ä¼ é€’ `slot_mapping`ï¼ˆå°¤å…¶æ˜¯è‡ªå®šä¹‰æ’ä»¶æˆ–å®éªŒåˆ†æ”¯ï¼‰ã€‚<br>- å®šæœŸè¿è¡Œ **torchâ€‘compile** æ€§èƒ½åŸºå‡†ï¼Œæ¯”è¾ƒ **å‰åæ‹†åˆ†** å¯¹ä¸åŒåºåˆ—é•¿åº¦ã€æ‰¹æ¬¡å¤§å°çš„å½±å“ï¼Œé˜²æ­¢å›é€€ã€‚ |
| **å®‰å…¨å®¡è®¡** | - KVâ€‘cache å†™å…¥æ˜¯æ¨¡å‹çŠ¶æ€çš„å…³é”®ï¼Œå»ºè®®åœ¨å…³é”®è·¯å¾„åŠ å…¥ **checksum**ï¼ˆå¦‚å¯¹ KVâ€‘cache è¿›è¡Œ hashï¼‰æˆ– **assert**ï¼ˆæ£€æµ‹ `slot_mapping` èŒƒå›´ï¼‰ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒæ•è·æ½œåœ¨çš„æ˜ å°„é”™ä½ã€‚ |

---

**ç»“è®º**  
æ­¤æ¬¡æäº¤é€šè¿‡åœ¨æ³¨æ„åŠ›åç«¯å±‚é¢æ˜¾å¼åŒºåˆ† **å‰å‘è®¡ç®—** ä¸ **KVâ€‘cache æ›´æ–°**ï¼Œå¹¶ç»Ÿä¸€å¼•å…¥ `slot_mapping` æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº† **

---

### fix: Add glm4_moe_lite to MLA detection (#32614)
**SHA**: `586a57a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/586a57ad7ede5c5132a70d021fc48fe05b10f5c3)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. ä¸º MLAï¼ˆMultiâ€‘Queryâ€‘LoRA/FlashInferâ€¯MLAï¼‰æ£€æµ‹åŠ å…¥ `glm4_moe_lite` åŠå…¶ MTP å˜ä½“ï¼Œå¹¶é€šè¿‡ `is_deepseek_r1_mla_compatible` è¾…åŠ©å‡½æ•°ç»Ÿä¸€åˆ¤æ–­ DeepSeekâ€¯R1 å…¼å®¹çš„æ³¨æ„åŠ›ç»´åº¦ã€‚  
2. åœ¨ `cuda` å¹³å°é…ç½®ä¸ FlashInferâ€¯MLA å®ç°ä¸­åŠ å…¥å¯¹ `qk_nope_head_dim == 128`ï¼ˆFlashInferâ€¯MLA çš„ç¡¬ä»¶çº¦æŸï¼‰çš„æ˜¾å¼æ£€æŸ¥ï¼Œé˜²æ­¢ä¸æ”¯æŒçš„æ¨¡å‹è¯¯ç”¨è¯¥åç«¯ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/model_executor/layers/attention/mla_attention.py`ï¼ˆMLA å‰ç½®åˆ¤æ–­ï¼‰  
- `vllm/platforms/cuda.py`ï¼ˆå¹³å°å±‚é¢åç«¯é€‰æ‹©é€»è¾‘ï¼‰  
- `vllm/transformers_utils/model_arch_config_convertor.py`ï¼ˆæ¨¡å‹åˆ—è¡¨æ‰©å±•ï¼‰  
- `vllm/v1/attention/backends/mla/flashinfer_mla.py`ï¼ˆFlashInferâ€¯MLA kernel å…¼å®¹æ€§æ ¡éªŒï¼‰  

**ğŸ” æŠ€æœ¯æ´å¯Ÿ**  

- **æ¶æ„å½±å“**ï¼š  
  - å¼•å…¥ `is_deepseek_r1_mla_compatible` å°†æ¨¡å‹ç»´åº¦æ£€æµ‹ä»æ•£è½çš„ç¡¬ç¼–ç è½¬ä¸ºé›†ä¸­å‡½æ•°ï¼Œæå‡å¯ç»´æŠ¤æ€§ã€‚  
  - åœ¨å¹³å°å±‚é¢ `check_and_update_config` ä¸­åŠ å…¥å¯¹ `qk_nope_head_dim` çš„åˆ¤æ–­ï¼Œä½¿å¾— Blackwellï¼ˆè®¾å¤‡èƒ½åŠ› 10.xï¼‰ä»…åœ¨æ»¡è¶³ kernel çº¦æŸæ—¶æ‰å¯ç”¨ FlashInferâ€¯MLAï¼Œé˜²æ­¢é”™è¯¯çš„åç«¯åˆ†é…å¯¼è‡´è¿è¡Œæ—¶å¼‚å¸¸ã€‚  
  - `glm4_moe_lite*` è¢«åˆ—å…¥ `is_deepseek_mla` æ£€æµ‹åˆ—è¡¨ï¼Œç¡®ä¿è¿™äº›æ–°æ¨¡å‹åœ¨åç»­çš„ MLA è·¯å¾„ä¸­å¾—åˆ°æ­£ç¡®è¯†åˆ«ã€‚  

- **æ€§èƒ½å½±å“**ï¼š  
  - å¯¹ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹ï¼ˆå¦‚ DeepSeekâ€¯R1ã€glm4_moe_liteï¼‰ä»ç„¶ä½¿ç”¨ FlashInferâ€¯MLAï¼Œå¯ä¿æŒæˆ–æå‡é¢„å¡«ï¼ˆprefillï¼‰é˜¶æ®µçš„ååé‡ã€‚  
  - å¯¹ä¸æ»¡è¶³ `qk_nope_head_dim == 128` çš„æ¨¡å‹ä¼šè‡ªåŠ¨å›é€€åˆ° CUTLASSâ€¯MLAã€FlashMLAï¼ˆdenseï¼‰æˆ– Tritonâ€¯MLAï¼Œé¿å…å› ä¸åŒ¹é…è€Œå‡ºç°æ€§èƒ½å›é€€æˆ–é”™è¯¯ã€‚  
  - é¢å¤–çš„å±æ€§è¯»å– (`hf_text_config.qk_nope_head_dim` ç­‰) å¼€é”€æä½ï¼Œå‡ ä¹ä¸å½±å“æ•´ä½“æ€§èƒ½ã€‚  

- **å®‰å…¨è€ƒè™‘**ï¼š  
  - å˜æ›´ä»…æ¶‰åŠé…ç½®æ£€æŸ¥å’Œåç«¯é€‰æ‹©ï¼Œæ— æ–°å¢å¤–éƒ¨ä¾èµ–æˆ–ä»£ç æ‰§è¡Œè·¯å¾„ï¼Œæœªå¼•å…¥å®‰å…¨é£é™©ã€‚  
  - é€šè¿‡æ˜¾å¼è¿”å› `False` é˜²æ­¢åœ¨ä¸å…¼å®¹çš„ç¡¬ä»¶/æ¨¡å‹ä¸Šè§¦å‘ FlashInferâ€¯MLAï¼Œé—´æ¥æå‡ç³»ç»Ÿç¨³å®šæ€§ï¼Œé¿å…å› å´©æºƒå¯¼è‡´çš„æœåŠ¡å¯ç”¨æ€§é—®é¢˜ã€‚  

**âš ï¸ æ½œåœ¨é£é™©**  

1. **å±æ€§ç¼ºå¤±/å‘½åå†²çª**ï¼šè‹¥æœªæ¥æ¨¡å‹åœ¨ `hf_text_config` ä¸­æ”¹ç”¨ä¸åŒå±æ€§åæˆ–ä¸æä¾› `qk_nope_head_dim`ï¼Œå½“å‰æ£€æµ‹ä¼šé»˜è®¤è¿”å› `False`ï¼Œå¯¼è‡´å¯èƒ½é”™å¤±ä½¿ç”¨æ›´å¿«çš„ FlashInferâ€¯MLAã€‚  
2. **å‡çº§å…¼å®¹æ€§**ï¼šåç»­ FlashInferâ€¯MLA è‹¥æ”¾å®½å¯¹ `qk_nope_head_dim` çš„é™åˆ¶ï¼Œå½“å‰ç¡¬ç¼–ç æ£€æŸ¥ä¼šé˜»æ­¢ä½¿ç”¨æ–°ç‰¹æ€§ï¼Œéœ€è¦åŒæ­¥æ›´æ–° `supports_combination` å®ç°ã€‚  
3. **é…ç½®åŒæ­¥**ï¼š`is_deepseek_r1_mla_compatible` ä¾èµ– `VllmConfig.model_config`; åœ¨æŸäº›å¯åŠ¨è·¯å¾„ï¼ˆå¦‚ä»…ä½¿ç”¨ `model_config=None` çš„æµ‹è¯•ï¼‰å¯èƒ½å¯¼è‡´è¯¯åˆ¤è€Œé€€å›åˆ°è¾ƒæ…¢åç«¯ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

- **ä»£ç å®¡é˜…**ï¼šç¡®è®¤æ‰€æœ‰æ¨¡å‹çš„ `hf_text_config` åœ¨æœªæ¥ç‰ˆæœ¬ä»ä¿æŒ `qk_nope_head_dim`ã€`qk_rope_head_dim`ã€`v_head_dim` ä¸‰ä¸ªå±æ€§çš„å‘½åå’Œè¯­ä¹‰ã€‚  
- **å›å½’æµ‹è¯•**ï¼šé’ˆå¯¹æ–°å¢çš„ `glm4_moe_lite*`ã€DeepSeekâ€¯R1 ä»¥åŠæ™®é€šæ¨¡å‹ï¼Œåˆ†åˆ«è·‘ä¸€æ¬¡é¢„å¡«å’Œè§£ç åŸºå‡†ï¼ŒéªŒè¯åç«¯é€‰æ‹©ä¸æ€§èƒ½é¢„æœŸä¸€è‡´ã€‚  
- **ç›‘æ§å‘Šè­¦**ï¼šåœ¨è¿è¡Œæ—¶è®°å½• `use_flashinfer_prefill`ã€`use_cudnn_prefill`ã€`use_trtllm_ragged_deepseek_prefill` çš„é€‰å–ç»“æœï¼Œä¸€æ—¦å‡ºç°æ„å¤–å›é€€åˆ°ä½æ•ˆåç«¯ï¼Œè§¦å‘å‘Šè­¦ã€‚  
- **æ–‡æ¡£åŒæ­¥**ï¼šåœ¨ VLLM æ–‡æ¡£ä¸­è¡¥å……è¯´æ˜ â€œMLA å…¼å®¹æ€§åˆ¤å®šè§„åˆ™â€ï¼Œå°¤å…¶æ˜¯ `qk_nope_head_dim == 128` çš„ç¡¬ä»¶çº¦æŸï¼Œä»¥å…ç”¨æˆ·è‡ªè¡Œä¿®æ”¹é…ç½®å¯¼è‡´è¿è¡Œé”™è¯¯ã€‚  

---  

---

### [V1][Hybrid] Mamba Prefix Caching with align mode (#30877)
**SHA**: `5206e5e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/5206e5e28c25dcc43d3a2373b591a0cdf9792f5d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆHybridâ€¯Mambaâ€¯Prefixâ€¯Cachingâ€¯+â€¯Alignâ€¯Modeï¼‰  

**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨ `CacheConfig` ä¸­æ–°å¢ `mamba_cache_mode`ï¼ˆ`none`/`all`/`align`ï¼‰å¹¶åœ¨ `EngineArgs`/CLI ä¸­åŠ å…¥å¯¹åº”å‚æ•°ã€‚  
- ä¸ºæ‰€æœ‰ Mambaâ€‘familyæ¨¡å‹å®ç°ç»Ÿä¸€çš„ **stateâ€‘copyå‡½æ•°**ï¼ˆ`MambaStateCopyFunc`ï¼‰ï¼Œå¹¶åœ¨æ¨¡å‹é…ç½®å±‚æš´éœ² `get_mamba_state_copy_func`ã€‚  
- å®ç° **Mamba å‰ç¼€ç¼“å­˜çš„ä¸¤ç§ç­–ç•¥**ï¼š  
  * `all` â€“ ä¼ ç»Ÿçš„æ¯å— `block_size` éƒ½ç¼“å­˜å®Œæ•´çš„ Mamba çŠ¶æ€ï¼ˆä¸å·²æœ‰ prefixâ€‘caching è¡Œä¸ºä¿æŒä¸€è‡´ï¼‰ã€‚  
  * `align` â€“ ä»…åœ¨ **å—å¯¹é½**ï¼ˆ`i * block_size`ï¼‰çš„ä½ç½®ä»¥åŠ **speculative** å—ä¸Šä¿å­˜çŠ¶æ€ï¼Œæ˜¾è‘—é™ä½å­˜å‚¨å¼€é”€ã€‚  
- å…³é”®è°ƒåº¦ä¸ç¼“å­˜ç®¡ç†ä»£ç æ”¹é€ ï¼š  
  * `KVCacheCoordinator.get_num_blocks_to_allocate`ã€`KVCacheManager.allocate_slots`ã€`SingleTypeKVCacheManager`ã€`MambaManager` æ·»åŠ  `num_tokens_main_model` å‚æ•°ã€å—å¯¹é½é€»è¾‘ä»¥åŠ `last_state_block_idx` è¿½è¸ªã€‚  
  * `Scheduler._mamba_block_aligned_split` åœ¨ prefill æ—¶å¼ºåˆ¶ `num_new_tokens` æŒ‰ `block_size` å¯¹é½ï¼Œå…¼å®¹ Eagle/EAGLEâ€‘prune åœºæ™¯ã€‚  
  * `v1/attention/backends/*` ä¸­æ–°å¢ `mamba_get_block_table_tensor`ï¼Œç»Ÿä¸€åœ¨ **all**ã€**none**ã€**align** ä¸‰ç§æ¨¡å¼ä¸‹è·å–é€‚é…çš„ BlockTableã€‚  
  * `BlockPool`ã€`BlockTable` åŠ `GPUInputBatch` è¿ç§»è‡³åŸºäº **max_num_blocks_per_req** çš„è®¡ç®—ï¼Œæ”¯æŒ `align` æ¨¡å¼ä¸‹æ›´å°çš„å—æ•°éœ€æ±‚ã€‚  
- æ–°å¢ **Tritionâ€‘åŸºå‡†æ‰¹é‡ memcpy** å®ç° `do_mamba_copy_block`ï¼Œå¹¶åœ¨ `mamba_utils.preprocess_mamba` / `postprocess_mamba` ä¸­å®Œæˆè·¨å—çŠ¶æ€å¤åˆ¶ã€‚  
- å®Œæ•´çš„å•å…ƒ/ç«¯åˆ°ç«¯æµ‹è¯•è¦†ç›–ï¼ˆ`test_mamba_prefix_cache.py`ï¼‰ä»¥åŠè‹¥å¹²ç°æœ‰æµ‹è¯•çš„é€‚é…ã€‚  

---

### ğŸ¯ å½±å“èŒƒå›´  

| å—å½±å“æ¨¡å— | è¯´æ˜ |
| ---------- | ---- |
| `vllm/config/cache.py`ã€`vllm/config/vllm.py`ã€`vllm/engine/arg_utils.py` | æ–°å¢é…ç½®ã€CLI å‚æ•°ã€é»˜è®¤å€¼ã€‚ |
| KVâ€‘Cache ç³»ç»Ÿ (`kv_cache_manager`, `kv_cache_coordinator`, `single_type_kv_cache_manager`, `core/block_pool`, `core/kv_cache_coordinator`) | è®¡æ•°ã€åˆ†é…ã€é‡Šæ”¾é€»è¾‘å…¨éƒ¨å¼•å…¥ `num_tokens_main_model` ä¸å—å¯¹é½ã€‚ |
| è°ƒåº¦å™¨ `v1/core/sched/scheduler.py` | é¢„å¡«å……é˜¶æ®µçš„å—å¯¹é½åˆ‡åˆ†é€»è¾‘ï¼›å¯¹ `align` æ¨¡å¼çš„ä¸“å±æ£€æŸ¥ã€‚ |
| æ³¨æ„åŠ›åç«¯ (`gdn_attn`, `linear_attn`, `mamba_attn`, `utils`) | `mamba_get_block_table_tensor` ç»Ÿä¸€ BlockTable è§†å›¾ï¼Œé¿å…åœ¨ `align` æ¨¡å¼ä¸‹è®¿é—®éæ³•åˆ—ã€‚ |
| `v1/worker/block_table.py`ã€`gpu_input_batch.py`ã€`gpu_model_runner.py` | `max_num_blocks` å‚æ•°åŒ–ã€é‡æ–°è®¡ç®—å—å®¹é‡ã€åœ¨æ¨¡å‹æ‰§è¡Œå‰åè°ƒç”¨ `mamba_utils.preprocess_mamba` / `postprocess_mamba`ã€‚ |
| æ‰€æœ‰ Mambaâ€‘family æ¨¡å‹ (`bamba`, `falcon_h1`, `granitemoehybrid`, `jamba`, `kimi_linear`, `lfm2*`, `mamba*`, `qwen3_next*`, `zamba2`, `etc.`) | æš´éœ² `get_mamba_state_copy_func`ï¼Œç»Ÿä¸€ä½¿ç”¨ `MambaStateCopyFuncCalculator`ã€‚ |
| æµ‹è¯•ç›®å½• | æ–°å¢å¤§è§„æ¨¡ç«¯åˆ°ç«¯ `test_mamba_prefix_cache.py`ï¼Œä»¥åŠå¯¹å•ç±»å‹ KVâ€‘Cache ç®¡ç†å™¨çš„å‚æ•°é€‚é…ã€‚ |

---

### ğŸ” æŠ€æœ¯æ´å¯Ÿ  

#### æ¶æ„å½±å“  
1. **ç¼“å­˜å±‚æŠ½è±¡å‡çº§**ï¼š  
   - `MambaSpec` å¢åŠ  `mamba_cache_mode`ï¼Œç”±åŸæ¥çš„â€œæ˜¯å¦å¯ç”¨ prefix cachingâ€æ¼”å˜ä¸º **ä¸‰æ€ç­–ç•¥**ã€‚  
   - `MambaManager` æˆä¸ºä¸“é—¨å¤„ç† Mamba å—å¯¹é½ä¸çŠ¶æ€å¤åˆ¶çš„å­ç±»ï¼Œç»´æŠ¤ `last_state_block_idx` ä¸ `_allocated_block_reqs`ï¼Œå®ç° **å—å¯¹é½åˆ†é…** ä¸ **è·¨ step çŠ¶æ€è¿ç§»**ã€‚  
2. **è°ƒåº¦â€‘ç¼“å­˜ååŒ**ï¼š  
   - `Scheduler._mamba_block_aligned_split` è®©è°ƒåº¦å™¨åœ¨é¢„å¡«é˜¶æ®µä¸»åŠ¨è£å‰ª `num_new_tokens` è‡³ `block_size` æ•´æ•°å€ï¼ˆæˆ–ä¿ç•™ä¸è¶³å—çš„ token ä¸ç¼“å­˜ï¼‰ã€‚  
   - `KVCacheCoordinator.get_num_blocks_to_allocate` é€šè¿‡ `num_tokens_main_model` åŒºåˆ† **ä¸»æ¨¡å‹ token** ä¸ **lookâ€‘ahead/speculative token**ï¼Œé˜²æ­¢ `align` æ¨¡å¼ä¸‹å‡ºç° â€œå—+lookâ€‘ahead è¶…å‡ºå¯¹é½â€ çš„é”™è¯¯ã€‚  
3. **ç»Ÿä¸€ BlockTable è®¿é—®**ï¼š`mamba_get_block_table_tensor` æŠŠ **ä¸åŒæ¨¡å¼çš„ BlockTable è§†å›¾** æŠ½è±¡å‡ºæ¥ï¼Œæ‰€æœ‰æ³¨æ„åŠ›åç«¯åªé€šè¿‡æ­¤å‡½æ•°è·å– `state_indices_tensor`ï¼Œæ¶ˆé™¤äº†å¤§é‡ `if enable_prefix_caching` æ¡ä»¶åˆ†æ”¯ã€‚  
4. **å¤åˆ¶å±‚**ï¼š  
   - æ–°å¢ `MambaCopySpec` ä¸ Tritionâ€‘å®ç°çš„ `batch_memcpy_kernel`ï¼Œå®ç° **æ‰¹é‡ã€å¼‚æ­¥çš„è·¨å—çŠ¶æ€å¤åˆ¶**ï¼Œé¿å… Python å¾ªç¯å¯¼è‡´çš„æ˜¾è‘—å¼€é”€ã€‚  
   - `preprocess_mamba` / `postprocess_mamba` è´Ÿè´£åœ¨æ¯ä¸ªè°ƒåº¦ step å‰åå®Œæˆ **è¿è¡ŒçŠ¶æ€ â†’ ç›®æ ‡å—** çš„å¤åˆ¶ï¼Œè¦†ç›– `all` ä¸ `align` ä¸¤ç§æ¨¡å¼ï¼ˆ`align` åªåœ¨å—è¾¹ç•Œè¿›è¡Œå¤åˆ¶ï¼‰ã€‚  

#### æ€§èƒ½å½±å“  
| åœºæ™¯ | å¯èƒ½çš„æ”¶ç›Š | å¯èƒ½çš„å¼€é”€ |
| ---- | ----------- | ----------- |
| **Hybridâ€‘Mamba æ¨¡å‹ + Prefix Caching (all)** | ä¸åŸå®ç°ç­‰ä»·ï¼Œä¿æŒå·²æœ‰çš„ 1â€‘blockâ€‘perâ€‘token ç¼“å­˜æ”¶ç›Šã€‚ | å¤åˆ¶å¼€é”€ä¸åŸç‰ˆç›¸åŒï¼ˆæ¯ step å¤åˆ¶å®Œæ•´çŠ¶æ€ï¼‰ã€‚ |
| **Hybridâ€‘Mamba æ¨¡å‹ + Prefix Caching (align)** | *æ˜¾è‘—é™ä½ KVâ€‘Cache å†…å­˜å ç”¨*ï¼ˆä»…åœ¨å—å¯¹é½ä½ç½®ä¿å­˜çŠ¶æ€ï¼Œæå¤§å‰Šå‡ `block_size` Ã— `num_speculative_blocks` çš„å†—ä½™ï¼‰ã€‚åœ¨é•¿åºåˆ—é¢„å¡«æ—¶å¯èŠ‚çº¦ 70%+ ä»¥ä¸Šæ˜¾å­˜ã€‚ | â‘  é¢„å¡«é˜¶æ®µå¼ºåˆ¶ `block_size` å¯¹é½ï¼Œå¯èƒ½å¯¼è‡´ **é¢å¤–çš„ token è®¡ç®—**ï¼ˆè‹¥åŸå§‹ token æ•°æœªæ»¡å—ï¼Œéœ€è¦é¢å¤–è®¡ç®—ï¼‰ï¼Œä½†åœ¨å¤§å¤šæ•°å®é™… Promptï¼ˆ>â€¯1â€¯k tokensï¼‰å½±å“å¯å¿½ç•¥ã€‚ â‘¡ è·¨å—å¤åˆ¶çš„ **batch_memcpy** å¼•å…¥ä¸€æ¬¡é¢å¤–çš„ GPU memcpyï¼Œé€šå¸¸ <â€¯0.1â€¯msï¼Œè¿œä½äºè®¡ç®—æˆæœ¬ã€‚ |
| **Speculative Decoding (MTP/EAGLE) + align** | ä¸å…¼å®¹ï¼ˆå·²åœ¨ä»£ç ä¸­ `assert`ï¼‰ï¼Œé˜²æ­¢é”™è¯¯ä½¿ç”¨ã€‚ | N/Aï¼ˆåŠŸèƒ½è¢«ç¦ç”¨ï¼‰ã€‚ |
| **å¤š GPU (DCP/PCP) ç¯å¢ƒ** | `BlockTable` ä¸ `max_num_blocks` è®¡ç®—ç»Ÿä¸€

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (11)

### [Bugfix] Fix E2E latency calculation and add warmup support in mm_processor benchmark (#32646)
**SHA**: `6450b53` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6450b536a63d92d856d368c44c8d4250e31e2ea7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBugfix / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¿®å¤äº†å¤šæ¨¡æ€å¤„ç†å™¨åŸºå‡†ä¸­ Endâ€‘toâ€‘End (E2E) å»¶è¿Ÿçš„è®¡ç®—æ–¹å¼ï¼Œæ”¹ä¸ºä½¿ç”¨ `first_token_latency` + (lastâ€‘tokenâ€¯tsâ€¯â€‘â€¯firstâ€‘tokenâ€¯ts)ã€‚  
2. ä¸ºåŸºå‡†å·¥å…·åŠ å…¥ **warmâ€‘up** æ”¯æŒï¼Œå¯åœ¨æ­£å¼æµ‹é‡å‰è¿è¡Œè‹¥å¹²é¢„çƒ­è¯·æ±‚ï¼Œå¹¶åœ¨ç»Ÿè®¡æ—¶æ’é™¤è¿™äº›è¯·æ±‚çš„æ•°æ®ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/benchmarks/mm_processor.py`ï¼ˆç»Ÿè®¡æ”¶é›†ã€åŸºå‡†ä¸»æµç¨‹ã€CLI å‚æ•°ï¼‰  
- ç›¸å…³çš„è¯·æ±‚/é‡‡æ ·å‚æ•°åˆ›å»ºè·¯å¾„ï¼ˆ`get_requests`ã€`SamplingParams`ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šæ–°å¢ `--num-warmups` å‚æ•°é»˜è®¤ 1ï¼Œè‹¥æ—§è„šæœ¬æœªæ˜¾å¼æŒ‡å®šä¼šå¤šè·‘ä¸€æ¬¡è¯·æ±‚ã€‚å»ºè®®åœ¨æ–‡æ¡£æˆ–å¸®åŠ©ä¿¡æ¯ä¸­æ³¨æ˜é»˜è®¤è¡Œä¸ºï¼Œæˆ–åœ¨ CI ä¸­éªŒè¯é»˜è®¤å€¼å¯¹æ•´ä½“åŸºå‡†æ—¶é—´çš„å½±å“ã€‚  
2. **ç»Ÿè®¡å‡†ç¡®æ€§**ï¼š`collect_mm_processor_stats` é€šè¿‡åˆ‡ç‰‡ `list(all_stats.values())[num_warmup_reqs:]` è¿‡æ»¤é¢„çƒ­æ•°æ®ï¼Œç¡®ä¿ `all_stats` çš„é¡ºåºåœ¨ä¸åŒè¿è¡Œä¸­ä¿æŒä¸€è‡´ï¼ˆå¦‚å¹¶å‘æ”¶é›†å¯èƒ½å¯¼è‡´é¡ºåºä¸ç¡®å®šï¼‰ï¼Œå¦åˆ™å¯èƒ½è¯¯åˆ çœŸå®æ•°æ®ã€‚å¯è€ƒè™‘åœ¨ `Engine` å±‚æ ‡è®° warmâ€‘up è¯·æ±‚çš„ ID å¹¶æŒ‰æ ‡è®°è¿‡æ»¤ã€‚  
3. **æŒ‡æ ‡ç¼ºå¤±å›é€€**ï¼šå½“ `first_token_latency`ã€`first_token_ts`ã€`last_token_ts` ä»»æ„ç¼ºå¤±æ—¶ï¼Œä»£ç ä¼šå›é€€åˆ° â€œæ€»è€—æ—¶ / å®Œæˆè¯·æ±‚æ•°â€ã€‚å·²åŠ å…¥è­¦å‘Šè¾“å‡ºï¼Œå»ºè®®åœ¨ CI ä¸­åŠ å…¥è¦†ç›–è¿™æ¡åˆ†æ”¯çš„æµ‹è¯•ï¼Œé˜²æ­¢è¯¯åˆ¤ã€‚  
4. **æ€§èƒ½å¼€é”€**ï¼šé¢„çƒ­é˜¶æ®µä¼šå†æ¬¡è°ƒç”¨ `llm.chat`ï¼Œä¼šé¢å¤–å ç”¨ GPU/CPUã€‚è‹¥ç”¨æˆ·åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹è¿è¡ŒåŸºå‡†ï¼Œå»ºè®®æä¾› `--disable-warmup` æˆ–å°†é»˜è®¤ warmâ€‘up æ¬¡æ•°è°ƒä¸º 0ã€‚  
5. **æ–‡æ¡£/ç¤ºä¾‹**ï¼šæ›´æ–° README/benchmark æ–‡æ¡£ï¼Œè¯´æ˜ `--num-warmups` ç”¨æ³•åŠæ–° E2E è®¡ç®—å…¬å¼ï¼Œæä¾›å¯¹æ¯”ç¤ºä¾‹å¸®åŠ©ç”¨æˆ·ç†è§£ä¸¤ç§å»¶è¿Ÿåº¦é‡çš„å·®å¼‚ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡ä¿®æ”¹æå‡äº†åŸºå‡†çš„å¯ä¿¡åº¦ï¼ˆæ’é™¤å†·å¯åŠ¨å½±å“ï¼‰å¹¶çº æ­£äº†å»¶è¿Ÿç»Ÿè®¡çš„é€»è¾‘ï¼Œé£é™©ä¸»è¦åœ¨ç»Ÿè®¡è¿‡æ»¤çš„é¡ºåºç¨³å®šæ€§å’Œæ–°å¢é»˜è®¤ warmâ€‘up å¯èƒ½å¯¼è‡´çš„é¢å¤–å¼€é”€ã€‚é€šè¿‡åŠ å¼ºæ–‡æ¡£ã€æ·»åŠ å•å…ƒ/é›†æˆæµ‹è¯•å³å¯å¹³æ»‘è¿ç§»ã€‚

---

### [UX] Deduplicate sampling parameter startup logs (#32953)
**SHA**: `51931c5` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/51931c5c9a3e36ccdf746f5fe3b4e770d557041d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–ï¼ˆæ—¥å¿—å»é‡ / UX æ”¹è¿›ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- `ModelConfig.get_diff_sampling_param` ç°åœ¨å…ˆè¯»å– `self.generation_config` ä¸º `src`ï¼Œç»Ÿä¸€ç”¨ä¸‰å…ƒè¡¨è¾¾å¼æ„é€  `config`ï¼Œå¹¶åœ¨ç”Ÿæˆ **warning** æ—¶ä»…å½“ `src != "vllm"` æ‰è¾“å‡ºã€‚æ—¥å¿—æ–‡å­—è¢«é‡æ„ä¸º â€œDefault vLLM sampling parameters have been overridden by â€¦â€ï¼Œå¹¶é€šè¿‡ `logger.warning_once(..., scope="local")` é˜²æ­¢é‡å¤ã€‚  
- å¤šä¸ª OpenAI å…¥å£ (`chat_completion`, `completion`, `responses`) ä¸­åŸæœ¬çš„ **info** çº§åˆ« â€œUsing default â€¦ sampling params from â€¦â€ æ—¥å¿—è¢«ç§»é™¤ï¼Œåªä¿ç•™ `default_sampling_params` çš„è·å–ï¼Œé¿å…å¯åŠ¨æ—¶å‡ºç°å¤§é‡é‡å¤ä¿¡æ¯ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/config/model.py`ï¼ˆæ ¸å¿ƒé…ç½®ä¸æ—¥å¿—é€»è¾‘ï¼‰  
- `vllm/entrypoints/openai/*/serving.py`ï¼ˆChatCompletionã€Completionã€Responses å¯åŠ¨è·¯å¾„ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æ—¥å¿—å…¼å®¹æ€§**ï¼šç¡®è®¤ `logger.warning_once` åœ¨å½“å‰ä»£ç åº“ä¸­å·²ç»æ¥å— `scope` å‚æ•°ï¼›è‹¥æœªå®ç°ï¼Œä¼šæŠ›å‡º `TypeError`ã€‚å»ºè®®åœ¨æœ¬åœ°è·‘ä¸€æ¬¡å¯åŠ¨è„šæœ¬æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸ã€‚  
2. **è¡Œä¸ºéªŒè¯**ï¼šåœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å½¢ä¸‹æ‰‹åŠ¨å¯åŠ¨ï¼š  
   - `--generation-config vllm` â†’ ä¸åº”å‡ºç°ä»»ä½• warningã€‚  
   - `--generation-config auto`ï¼ˆé»˜è®¤ï¼‰æˆ–æŒ‡å‘å®é™… `generation_config.json` â†’ åº”è¾“å‡ºä¸€æ¬¡ warningï¼Œå†…å®¹å½¢å¦‚ â€œDefault vLLM sampling parameters have been overridden by the model's `generation_config.json`: {...}â€ã€‚  
   - å¤šæ¬¡åˆ›å»ºæœåŠ¡å™¨å®ä¾‹ â†’ åŒä¸€è­¦å‘Šä»åªå‡ºç°ä¸€æ¬¡ï¼ˆé€šè¿‡ `scope="local"` æ£€éªŒï¼‰ã€‚  
3. **æ–‡æ¡£æ›´æ–°**ï¼šREADME / CLI å¸®åŠ©ä¸­å¯åŠ å…¥è¯´æ˜ï¼šå½“æ¨¡å‹è‡ªå¸¦çš„ç”Ÿæˆé…ç½®è¦†ç›–äº† vLLM é»˜è®¤é‡‡æ ·å‚æ•°æ—¶ï¼Œç³»ç»Ÿä¼šåœ¨å¯åŠ¨æ—¶ç»™å‡ºä¸€æ¬¡æ€§æç¤ºã€‚  
4. **å›æ»šé£é™©**ï¼šè‹¥ç”¨æˆ·ä¾èµ–æ—§çš„ `info` æ—¥å¿—è¿›è¡Œè°ƒè¯•ï¼Œä¿¡æ¯å°†ä¸å†å‡ºç°ã€‚å¯è€ƒè™‘åœ¨ `--verbose` æ¨¡å¼ä¸‹é‡æ–°è¾“å‡ºè¿™ç±»ä¿¡æ¯ã€‚  

æ€»ä½“æ¥è¯´ï¼Œæ­¤æ¬¡æäº¤ä»…æ˜¯å»é™¤é‡å¤å¯åŠ¨æ—¥å¿—ï¼ŒåŠŸèƒ½ä¿æŒä¸å˜ï¼Œåªè¦ç¡®è®¤ `logger` æ¥å£å…¼å®¹å³å¯å®‰å…¨åˆå¹¶ã€‚

---

### [Models] Add `SharedFusedMoE` support to Qwen3MoE (#32082)
**SHA**: `8edaf38` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8edaf3857027c75382672ade255f7cb531f96844)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¸º Qwenâ€‘3 MoE å®ç°å¼•å…¥ `SharedFusedMoE`ï¼Œæ”¯æŒåœ¨åŒä¸€æ¬¡å‰å‘ä¸­åŒæ—¶ä½¿ç”¨å…±äº«ä¸“å®¶å’Œè·¯ç”±ä¸“å®¶ã€‚  
2. æ–°å¢ `shared_expert_gate` ä¸ `shared_expert`ï¼Œå¹¶åœ¨ `Qwen3MoeMLP` ä¸­åŠ å…¥å¯é€‰çš„ `expert_gate`ï¼ˆé€šè¿‡ `F.sigmoid` è°ƒåˆ¶è¾“å‡ºï¼‰ã€‚  
3. å°†åŸæ¥çš„ `FusedMoE` æ›¿æ¢ä¸º `SharedFusedMoE`ï¼Œå¹¶ç›¸åº”è°ƒæ•´ `gate`ã€`forward`ã€å‚æ•°æ˜ å°„ç­‰å®ç°ï¼›åœ¨ TP åœºæ™¯ä¸‹è¡¥å…… `maybe_all_reduce_tensor_model_parallel`ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/models/qwen3_moe.py`ï¼ˆæ ¸å¿ƒ MoE å—ï¼‰  
- `vllm/model_executor/layers/fused_moe.py`ï¼ˆæ–°å¢ `SharedFusedMoE` æ¥å£ï¼‰  
- ç›¸å…³çš„æƒé‡æ˜ å°„ã€åŠ è½½é€»è¾‘ä»¥åŠåˆ†å¸ƒå¼èšåˆè·¯å¾„  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å…¼å®¹æ€§**ï¼š`shared_expert_intermediate_size` ä¸º 0 æ—¶ä¿æŒæ—§è¡Œä¸ºï¼Œç¡®ä¿å·²æœ‰æ¨¡å‹ä¸å—å½±å“ã€‚  
- **æ€§èƒ½**ï¼š`reduce_results=False` ä½¿å¾—å…±äº«ä¸“å®¶çš„è¾“å‡ºåœ¨ TP ä¸Šéœ€æ‰‹åŠ¨ Allâ€‘Reduceï¼Œæ£€æŸ¥æ˜¯å¦ä¼šäº§ç”Ÿé¢å¤–é€šä¿¡ç“¶é¢ˆã€‚  
- **æ•°å€¼ç¨³å®šæ€§**ï¼š`F.sigmoid(self.expert_gate(x)[0])` ç›´æ¥ä¹˜åœ¨ MLP è¾“å‡ºä¸Šï¼Œå»ºè®®åœ¨æ–‡æ¡£ä¸­è¯´æ˜è¯¥é—¨æ§çš„å–å€¼èŒƒå›´ä¸æ¢¯åº¦è¡Œä¸ºã€‚  
- **æµ‹è¯•**ï¼šå¢åŠ æ··åˆè·¯ç”±ï¼ˆæœ‰å…±äº«ä¸“å®¶ã€æœ‰æ™®é€šä¸“å®¶ï¼‰å’Œçº¯è·¯ç”±ä¸¤ç§é…ç½®çš„å•å…ƒ/é›†æˆæµ‹è¯•ï¼ŒéªŒè¯åœ¨å¤šå¡ã€åºåˆ—å¹¶è¡Œã€TP ä¸‰ç§æ¨¡å¼ä¸‹çš„å‰å‘/åå‘ä¸€è‡´æ€§ã€‚  
- **æ¨¡å‹è½¬æ¢**ï¼šåœ¨ checkpoints è½¬æ¢è„šæœ¬ä¸­ç¡®ä¿ `shared_expert_gate` çš„æƒé‡èƒ½å¤Ÿæ­£ç¡®ä¿å­˜/åŠ è½½ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ”¹åŠ¨ä¸º Qwenâ€‘3 MoE å¼•å…¥äº†å…±äº«ä¸“å®¶çš„çµæ´»æœºåˆ¶ï¼Œæå‡äº†æ¨¡å‹è¡¨è¾¾åŠ›ï¼Œä½†éœ€å…³æ³¨åˆ†å¸ƒå¼åŒæ­¥å’Œæ•°å€¼ç¨³å®šæ€§ï¼Œå»ºè®®é€šè¿‡å®Œæ•´çš„å›å½’æµ‹è¯•éªŒè¯ã€‚

---

### [Tests] Standardize RNG seed utility across test files (#32982)
**SHA**: `0ccecf8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0ccecf8833fbfb0cb4353fc66316a67c6aa6a5b5)

**å˜æ›´ç±»å‹**ï¼šâ€‹é‡æ„/åŠŸèƒ½å¢å¼ºï¼ˆç»Ÿä¸€æµ‹è¯•éšæœºæ•°ç§å­å·¥å…·ï¼‰  

**å˜æ›´æ‘˜è¦**  
- åœ¨ `vllm/utils/torch_utils.py` æ–°å¢ `set_random_seed`ï¼Œä¸ä»…åœ¨ CPU ä¸Šè®¾ç½® `randomã€numpyã€torch` çš„ç§å­ï¼Œè¿˜åœ¨ CUDA å¯ç”¨æ—¶è°ƒç”¨ `torch.cuda.manual_seed_all`ã€‚  
- å°†è¯¥å‡½æ•°åœ¨ `tests/utils.py` ä¸­é‡æ–°å¯¼å‡ºï¼Œä¾›æµ‹è¯•æ–‡ä»¶ç›´æ¥ä½¿ç”¨ã€‚  
- åˆ é™¤å„æµ‹è¯•æ–‡ä»¶ä¸­è‡ªè¡Œå®ç°çš„ `set_seed`ï¼Œæ”¹ä¸ºç»Ÿä¸€è°ƒç”¨ `set_random_seed`ï¼ˆ`tests/kernels/test_flex_attention.py`ã€`tests/v1/logits_processors/test_custom_offline.py` ç­‰ï¼‰ã€‚  
- ç›¸åº”åœ°å»é™¤äº† `random`ã€`numpy` çš„ç›´æ¥å¯¼å…¥ï¼Œç®€åŒ–ä¾èµ–ã€‚  

**å½±å“èŒƒå›´**  
- **æ ¸å¿ƒæ¨¡å—**ï¼š`vllm/utils/torch_utils.py`ï¼ˆéšæœºç§å­ç»Ÿä¸€å®ç°ï¼‰ã€‚  
- **æµ‹è¯•å¥—ä»¶**ï¼šæ‰€æœ‰ä½¿ç”¨éšæœºç§å­çš„æµ‹è¯•æ–‡ä»¶ï¼ˆ`tests/kernels/*`ã€`tests/v1/logits_processors/*` ç­‰ï¼‰ã€‚  
- **å·¥å…·å±‚**ï¼š`tests/utils.py` ç°åœ¨æˆä¸ºç§å­å·¥å…·çš„å”¯ä¸€å…¥å£ï¼Œå¤–éƒ¨ï¼ˆè‹¥æœ‰ï¼‰ç›´æ¥ import `set_random_seed` çš„ä»£ç ä¹Ÿä¼šå—å½±å“ã€‚  

**æ½œåœ¨é£é™©**  
1. **å…¼å®¹æ€§**ï¼šè‹¥é¡¹ç›®ä¸­è¿˜æœ‰æœªæ”¹åŠ¨çš„æµ‹è¯•æ–‡ä»¶ä»ä½¿ç”¨æ—§çš„ `set_seed`ï¼Œä¼šå‡ºç° `NameError`ã€‚éœ€è¦åœ¨å…¨å±€æœç´¢ç¡®è®¤å…¨éƒ¨è¿ç§»ã€‚  
2. **CUDA ç¯å¢ƒ**ï¼šæ–°å¢ `torch.cuda.manual_seed_all` å¯èƒ½åœ¨æ²¡æœ‰ CUDA ç¯å¢ƒçš„æœºå™¨ä¸Šè§¦å‘ `torch.cuda.is_available()` æ£€æŸ¥ä¹‹å¤–çš„å‰¯ä½œç”¨ï¼ˆå¦‚è§¦å‘ CUDA åˆå§‹åŒ–ï¼‰ï¼Œä½†ä»£ç å·²åšå¯ç”¨æ€§åˆ¤æ–­ï¼Œå½±å“æœ‰é™ã€‚  
3. **å¯¼å‡ºå†²çª**ï¼š`tests/utils.py` é‡æ–°å¯¼å‡º `set_random_seed`ï¼Œè‹¥æœªæ¥å†åŠ å…¥åŒåå˜é‡æˆ–å‡½æ•°ï¼Œéœ€è¦æ³¨æ„ `__all__` ä¸ lint è§„åˆ™ã€‚  

**å»ºè®®**  
- **æ–‡æ¡£/æ³¨é‡Š**ï¼šåœ¨ `vllm/utils/torch_utils.py` æ·»åŠ å‡½æ•°è¯´æ˜ï¼Œæ˜ç¡®è¯¥å‡½æ•°æ˜¯â€œæµ‹è¯•ä¸“ç”¨â€è¿˜æ˜¯â€œå…¨å±€é€šç”¨â€ã€‚  
- **å…¨å±€è¿ç§»**ï¼šè¿è¡Œä¸€æ¬¡ä»£ç åº“æœç´¢ï¼ˆ`grep -R "random.seed"`ï¼‰ç¡®ä¿æ‰€æœ‰è‡ªå®šä¹‰ç§å­è®¾ç½®å·²æ”¹ä¸º `set_random_seed`ï¼Œé˜²æ­¢é—æ¼ã€‚  
- **CI æ£€æŸ¥**ï¼šåœ¨ CI ä¸­åŠ å…¥ä¸€æ­¥æ£€æŸ¥ï¼Œç¡®ä¿æ¯ä¸ªæµ‹è¯•æ–‡ä»¶éƒ½ä»…é€šè¿‡ `tests.utils.set_random_seed` è®¾ç½®ç§å­ï¼Œå¼ºåŒ–ç»Ÿä¸€æ€§ã€‚  
- **å›å½’æµ‹è¯•**ï¼šåœ¨æ—  GPU ç¯å¢ƒä¸æœ‰ GPU ç¯å¢ƒä¸¤å¥—æœºå™¨ä¸Šè¿è¡Œå®Œæ•´æµ‹è¯•ï¼Œç¡®ä¿æ–°ç§å­è®¾ç½®ä¸ä¼šå¯¼è‡´éšæœºæ€§å·®å¼‚æˆ–é¢å¤–çš„ CUDA åˆå§‹åŒ–æ—¶é—´ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº†æµ‹è¯•çš„å¯å¤ç°æ€§ä¸ä»£ç ç»´æŠ¤æ€§ï¼Œåªè¦å®Œæˆå…¨åº“è¿ç§»å¹¶æ›´æ–°æ–‡æ¡£ï¼Œå³å¯å®‰å…¨åˆå¹¶ã€‚

---

### [Dev UX] Add auto-detection for VLLM_PRECOMPILED_WHEEL_VARIANT during install (#32948)
**SHA**: `d0cbac5` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d0cbac5827b1419afb81fe8c31515ff4973ca85d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ `setup.py` ä¸­åŠ å…¥è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿ CUDA å˜ä½“çš„é€»è¾‘ï¼Œé»˜è®¤ä¾æ® PyTorchã€`nvidiaâ€‘smi` æˆ– `VLLM_MAIN_CUDA_VERSION` è§£æå‡ºé€‚é…çš„ wheelï¼ˆå¦‚ `cu129`ã€`cu130`ï¼‰ï¼Œå¹¶åœ¨å®‰è£…æ—¶è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„é¢„ç¼–è¯‘è½®å­ã€‚æ–‡æ¡£åŒæ­¥è¯´æ˜æ­¤è¡Œä¸ºã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `setup.py`ï¼ˆå®‰è£…æµç¨‹ï¼‰  
- `docs/getting_started/installation/gpu.cuda.inc.md`ï¼ˆæ–‡æ¡£ï¼‰  
- ä¾èµ–äº `precompiled_wheel_utils` çš„å†…éƒ¨å‡½æ•° `determine_wheel_url`  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
1. **å…¼å®¹æ€§**ï¼š`detect_system_cuda_variant` ç›´æ¥æ‰“å°ä¿¡æ¯ï¼Œå»ºè®®æ”¹ä¸ºä½¿ç”¨ `logging`ï¼Œé¿å…åœ¨éäº¤äº’å¼å®‰è£…ä¸­æ±¡æŸ“ stdoutã€‚  
2. **é”™è¯¯å¤„ç†**ï¼šå½“ `torch.version.cuda` æˆ– `nvidiaâ€‘smi` å‡ä¸å¯ç”¨æ—¶ï¼Œä»ä¼šå°è¯•ä½¿ç”¨ `envs.VLLM_MAIN_CUDA_VERSION`ï¼Œä½†è‹¥è¯¥ env äº¦æœªè®¾å®šä¼šå¯¼è‡´ `int('')` æŠ›å¼‚å¸¸ï¼Œå»ºè®®æ·»åŠ é»˜è®¤å›é€€æˆ–æ›´æ˜ç¡®çš„é”™è¯¯æç¤ºã€‚  
3. **æ˜ å°„è¡¨**ï¼šå½“å‰åªæ”¯æŒ 12 â†’ `cu129`ã€13 â†’ `cu130`ï¼Œæœªæ¥è‹¥æ–°å¢ CUDA 14 ç­‰ï¼Œéœ€è¦åŒæ­¥æ›´æ–°ï¼Œæ­¤å¤„å¯è€ƒè™‘å°†æ˜ å°„æŠ½å–ä¸ºå¸¸é‡æˆ–é…ç½®æ–‡ä»¶ã€‚  
4. **å•å…ƒæµ‹è¯•**ï¼šåŠ å…¥é’ˆå¯¹ä¸åŒæ£€æµ‹è·¯å¾„ï¼ˆtorchã€nvidiaâ€‘smiã€ç¯å¢ƒå˜é‡ï¼‰çš„æµ‹è¯•ï¼Œç¡®ä¿åœ¨ CI ç¯å¢ƒï¼ˆå¯èƒ½æ²¡æœ‰ GPUï¼‰ä¹Ÿèƒ½é¡ºåˆ©èµ°é»˜è®¤åˆ†æ”¯ã€‚  
5. **æ–‡æ¡£**ï¼šè¯´æ˜è‡ªåŠ¨æ£€æµ‹çš„ä¼˜å…ˆçº§å’Œå¦‚ä½•é€šè¿‡ `VLLM_PRECOMPILED_WHEEL_VARIANT`/`VLLM_MAIN_CUDA_VERSION` å¼ºåˆ¶è¦†ç›–ï¼Œä»¥å…ç”¨æˆ·è¯¯ä»¥ä¸ºæ£€æµ‹æ€»æ˜¯å‡†ç¡®ã€‚  

æ•´ä½“ä¸Šï¼Œæ­¤æ”¹åŠ¨æå‡äº†å®‰è£…ä½“éªŒï¼Œå…³é”®æ˜¯ç¡®ä¿åœ¨ç¼ºå¤± CUDA ä¿¡æ¯çš„ç¯å¢ƒä¸‹ä»èƒ½å®‰å…¨å›é€€ï¼Œå¹¶é¿å…ä¸å¿…è¦çš„æ—¥å¿—è¾“å‡ºã€‚

---

### [Refactor] Clean up unused variables & func (#32692)
**SHA**: `37c9859` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/37c9859fab60bbc346be20a662387479eb0760de)

**å˜æ›´ç±»å‹**ï¼šğŸ”§ é‡æ„ï¼ˆæ¸…ç†æœªä½¿ç”¨çš„å˜é‡/å‡½æ•°ï¼‰  
**é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**å˜æ›´æ¦‚è¿°**  
- åˆ é™¤ `vllm/entrypoints/openai/engine/protocol.py` ä¸­æœªä½¿ç”¨çš„ `torch` å¼•å…¥åŠ `_LONG_INFO` å¸¸é‡ï¼›  
- ç§»é™¤ `petit_utils.py` ä¸­å¯¹ `_require_petit` çš„åˆ«åå®šä¹‰ï¼Œç›´æ¥ä½¿ç”¨ `_import_petit_kernel`ï¼›  
- å‰”é™¤ `glmasr_utils.py` ä¸­æœªè¢«è°ƒç”¨çš„ `_get_num_features_for_item` è¾…åŠ©å‡½æ•°ï¼›  
- åˆ é™¤ `phi4mm_audio.py` çš„å ä½ token å¸¸é‡ `_AUDIO_PLACEHOLDER_TOKEN_ID`ï¼›  
- æ¸…é™¤ `platforms/rocm.py` çš„ç©ºå…ƒç»„ `_ROCM_SWA_REASON`ã€‚  

**å½±å“èŒƒå›´**  
- OpenAI æ¥å£åè®®å±‚ï¼›  
- Petit é‡åŒ–å·¥å…·ï¼›  
- GLMâ€‘ASR ç›¸å…³éŸ³é¢‘ç‰¹å¾å¤„ç†ï¼›  
- Phiâ€‘4 éŸ³é¢‘æ¨¡å‹å®ç°ï¼›  
- ROCm å¹³å°å…¼å®¹æ€§å£°æ˜ã€‚  

**å…³æ³¨å»ºè®®**  
1. **ç¡®ä¿æ— æ®‹ä½™å¼•ç”¨**ï¼šåœ¨æœ¬åœ°æˆ– CI ä¸­æœç´¢ `_LONG_INFOã€_require_petitã€_get_num_features_for_itemã€_AUDIO_PLACEHOLDER_TOKEN_IDã€_ROCM_SWA_REASON`ï¼Œç¡®è®¤ä»£ç åº“å…¶ä»–ä½ç½®æœªå†ä½¿ç”¨ã€‚  
2. **æµ‹è¯•è¦†ç›–**ï¼šè¿è¡Œå®Œæ•´å•å…ƒ/é›†æˆæµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠ OpenAI åè®®ã€é‡åŒ–è·¯å¾„ã€éŸ³é¢‘æ¨¡å‹å’Œ ROCm è¿è¡Œæ—¶çš„æµ‹è¯•ï¼Œä»¥é˜²è¯¯åˆ å¯¼è‡´è¿è¡Œæ—¶å¼‚å¸¸ã€‚  
3. **æ–‡æ¡£åŒæ­¥**ï¼šå¦‚æœä¸Šè¿°å˜é‡/å‡½æ•°æ›¾åœ¨æ–‡æ¡£æˆ–ç¤ºä¾‹ä¸­å‡ºç°ï¼Œéœ€è¦ç›¸åº”æ›´æ–°ï¼Œé¿å…ç”¨æˆ·æŸ¥æ‰¾å¤±æ•ˆçš„ APIã€‚  
4. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šå¯¹å¤–éƒ¨æ’ä»¶æˆ–ç”¨æˆ·è‡ªå®šä¹‰ä»£ç å¯èƒ½ä»ä¾èµ–è¿™äº›å·²åˆ é™¤ç¬¦å·ï¼Œå»ºè®®åœ¨å‘å¸ƒè¯´æ˜ä¸­æç¤ºè¿ç§»è·¯å¾„ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æäº¤ä»…æ˜¯ä»£ç æ¸…ç†ï¼Œé¢„è®¡ä¸ä¼šå½±å“åŠŸèƒ½ã€‚å®Œæˆä¸Šè¿°æ£€æŸ¥åå³å¯å®‰å…¨åˆå¹¶ã€‚

---

### [Refactor] Rename `gptq_marlin` to `marlin` to match MoE (#32952)
**SHA**: `4561f13` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4561f13985e86ea15c9c672fbcfa28f12530e3e5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆAPI é‡å‘½åï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­ï¼ˆå…¼å®¹æ€§å½±å“ï¼‰  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šå°†æ‰€æœ‰ä¸ GPTQâ€‘Marlin ç›¸å…³çš„å®ç°ã€CMake é…ç½®ã€Pythonâ€¯opsã€benchmark ä¸å•å…ƒæµ‹è¯•ç»Ÿä¸€æ”¹åä¸º **marlin**ï¼Œä»¥æ›´å¥½åœ°ä½“ç°è¯¥ GEMM å†…æ ¸å·²æ”¯æŒ GPTQã€AWQã€FP8 ç­‰å¤šç§é‡åŒ–æ–¹å¼ï¼Œå¹¶åŒ¹é… MoE æ¨¡å—çš„å‘½åã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- ç¼–è¯‘ç³»ç»Ÿï¼šCMake ä¸­è·¯å¾„ã€æ–‡ä»¶ glob å‡ä» `csrc/quantization/gptq_marlin/*` è¿è‡³ `csrc/quantization/marlin/*`ã€‚  
- C++ å®ç°ï¼š`torch::Tensor gptq_marlin_gemm` â†’ `marlin_gemm`ï¼Œæ³¨å†Œåç§°äº¦åŒæ­¥æ›´æ”¹ã€‚  
- Python æ¥å£ï¼š`ops.gptq_marlin_gemm` â†’ `ops.marlin_gemm`ï¼Œä»¥åŠæ‰€æœ‰å†…éƒ¨è°ƒç”¨ï¼ˆmarlin_utilsã€fp4/fp8 utilsã€æ¨¡å‹å±‚ï¼‰å‡å·²ç»æ›¿æ¢ã€‚  
- ç›¸å…³ benchmarkã€testã€MoE kernel ä»¥åŠæ–‡æ¡£æ³¨é‡ŠåŒæ­¥æ”¹åã€‚  
- ä»…åœ¨ ROCm ç¯å¢ƒçš„å…¼å®¹æ€§æ£€æµ‹ä¸­ä½¿ç”¨äº†æ–°åç§°ï¼›æ—§åç§°å·²å…¨éƒ¨åˆ é™¤ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å‘åå…¼å®¹**ï¼šå¯¹å·²æœ‰é¡¹ç›®ä»å¼•ç”¨ `gptq_marlin_gemm`ï¼Œå»ºè®®åœ¨ `torch_bindings.cpp` ä¸­ä¿ç•™ä¸€ä¸ª thin wrapperï¼ˆå¦‚ `torch::Tensor gptq_marlin_gemm = marlin_gemm;`ï¼‰å¹¶åœ¨æ–‡æ¡£ä¸­ç»™å‡ºé€€å½¹æç¤ºï¼Œä»¥å…å‡çº§åå‡ºç° `AttributeError`ã€‚  
2. **æ„å»ºéªŒè¯**ï¼šç¡®è®¤ `CMakeLists.txt` ä¸­çš„ `file(GLOB â€¦)` ä¸å®é™…æ–‡ä»¶è·¯å¾„ä¸€è‡´ï¼Œé¿å…é—æ¼æ–°åŠ å…¥çš„ `.cu` æˆ– `.cuh`ã€‚åœ¨å¤š GPU æ¶æ„ï¼ˆsm75ã€sm80ã€sm89ï¼‰ä¸Šè·‘ä¸€æ¬¡å®Œæ•´çš„ç¼–è¯‘â€‘æµ‹è¯•æµç¨‹ï¼Œç¡®ä¿ `set_gencode_flags_for_srcs` ä»é€‚é…ã€‚  
3. **æ–‡æ¡£ä¸ç¤ºä¾‹åŒæ­¥**ï¼šæ‰€æœ‰ READMEã€API æ–‡æ¡£ã€sample notebook ä¸­çš„ `ops.gptq_marlin_gemm` å¿…é¡»æ”¹ä¸º `ops.marlin_gemm`ï¼Œå¹¶åœ¨ release notes ä¸­è¯´æ˜æ­¤æ›´ååŠå…¶æ„ä¹‰ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šå½“å‰æµ‹è¯•å·²å…¨éƒ¨æ”¹åï¼Œå»ºè®®é¢å¤–å¢åŠ ä¸€ä¸ªå…¼å®¹æ€§æµ‹è¯•ï¼ˆè°ƒç”¨æ—§åå¹¶æœŸæœ›å¾—åˆ°åŒæ ·ç»“æœï¼‰ï¼Œç¡®ä¿ wrapper æ­£å¸¸å·¥ä½œã€‚  
5. **CI/CD æ£€æŸ¥**ï¼šåœ¨ CI ä¸­åŠ å…¥å¯¹ `torch.ops._C` å¯¼å‡ºåç§°çš„æ£€æŸ¥ï¼Œé˜²æ­¢æœªæ¥ä¸æ…å†æ¬¡æ›´åå¯¼è‡´ç ´åã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åç»Ÿä¸€äº†å‘½åç©ºé—´ï¼Œæå‡äº†ä»£ç å¯è¯»æ€§ä¸åŠŸèƒ½å½’å±ï¼Œä½†å› æ¶‰åŠå…¬å¼€ opsï¼ŒåŠ¡å¿…æä¾›é€€å½¹å…¼å®¹å±‚å¹¶åŠæ—¶æ›´æ–°æ–‡æ¡£ï¼Œä»¥é™ä½ç”¨æˆ·å‡çº§é£é™©ã€‚

---

### [cudagraphs] Refactor cudagraph capture loop (#32946)
**SHA**: `3a41459` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/3a4145950176d966558d06ed5b05b29c489555f0)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆåŠŸèƒ½å®ç°æ–¹å¼è°ƒæ•´ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. åœ¨ `CudagraphDispatcher` ä¸­æ–°å¢ `get_capture_descs`ï¼Œç»Ÿä¸€è¿”å›å·²åˆå§‹åŒ–çš„ CUDAâ€‘graph æ•è·æè¿°ç¬¦ï¼ŒæŒ‰ **PIECEWISE â†’ FULL** é¡ºåºå¹¶åœ¨æ¯ç»„å†…éƒ¨æŒ‰ `num_tokens` é™åºæ’åºã€‚  
2. `GPUModelRunner._capture_cudagraphs` å‚æ•°ä» â€œç¼–è¯‘ç”¨çš„ `(num_tokens, lora)` ç¬›å¡å°”ç§¯â€ æ”¹ä¸ºç›´æ¥æ¥å— `BatchDescriptor` åˆ—è¡¨ï¼›æ•è·å¾ªç¯æ”¹ä¸ºéå† `cudagraph_dispatcher.get_capture_descs()`ï¼Œå»æ‰äº† `itertools.product`ã€å¤æ‚çš„ `mixed_mode` / `decode_mode` åˆ¤æ–­ã€‚  
3. æ–°å¢å•å…ƒæµ‹è¯•éªŒè¯ `get_capture_descs` çš„åˆ†ç»„ã€æ’åºä»¥åŠæœªåˆå§‹åŒ–æ—¶è¿”å›ç©ºåˆ—è¡¨ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/cudagraph_dispatcher.py`ï¼ˆæ–°å¢å…¬å¼€ APIï¼‰  
- `vllm/v1/worker/gpu_model_runner.py`ï¼ˆæ•è·é€»è¾‘ã€è°ƒç”¨ç­¾åå˜æ›´ï¼‰  
- ç›¸å…³æµ‹è¯• `tests/v1/cudagraph/test_cudagraph_dispatch.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šå¦‚æœå¤–éƒ¨ä»£ç æˆ–æ—§çš„å†…éƒ¨è°ƒç”¨ä»ä½¿ç”¨åŸ `_capture_cudagraphs(compilation_cases, â€¦)` å‚æ•°ç­¾åï¼Œéœ€è¦æ·»åŠ é€‚é…å±‚æˆ–ä¿æŒæ—§å…¥å£çš„å…¼å®¹ã€‚  
2. **åˆå§‹åŒ–é¡ºåº**ï¼š`get_capture_descs` ä¾èµ– `initialize_cudagraph_keys`ï¼Œç¡®ä¿åœ¨æ¨¡å‹åŠ è½½é˜¶æ®µå·²æ­£ç¡®è°ƒç”¨ï¼Œå¦åˆ™ä¼šå¯¼è‡´æ•è·è¢«æ„å¤–è·³è¿‡ã€‚  
3. **æ’åºä¸å†…å­˜**ï¼šæ–°å®ç°æŒ‰ token æ•°é™åºæ•è·ï¼Œç†è®ºä¸Šæå‡æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œå»ºè®®åœ¨å¤§è§„æ¨¡æ‰¹æ¬¡ä¸‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç¡®è®¤å®é™…æå‡å¹…åº¦ã€‚  
4. **ç»Ÿä¸€ Decode æ ‡è®°**ï¼šå½“å‰é€šè¿‡ `batch_descs[0].uniform` æ¨æ–­ `uniform_decode`ï¼Œè‹¥æœªæ¥å‡ºç°æ··åˆ uniform æ ‡è®°çš„åˆ—è¡¨ï¼Œéœ€é˜²æ­¢è¯¯åˆ¤ã€‚  
5. **æ—¥å¿—ä¸è¿›åº¦æ¡**ï¼šæ•è·è¿›åº¦æ¡ä»ä»…åœ¨ rank 0 æ‰“å°ï¼Œä¿æŒåŸæœ‰è¡Œä¸ºï¼›è‹¥éœ€æ›´å¤šç»†ç²’åº¦æ—¥å¿—ï¼Œå¯åœ¨ `get_capture_descs` è¿”å›å‰åŠ å…¥è°ƒè¯•è¾“å‡ºã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡é‡æ„ç®€åŒ–äº† CUDAâ€‘graph æ•è·çš„ç”Ÿæˆé€»è¾‘ï¼Œæå‡äº†ä»£ç å¯è¯»æ€§ä¸å¯ç»´æŠ¤æ€§ã€‚è¯·åœ¨ CI ä¸­è·‘å…¨å¥—æ€§èƒ½åŸºå‡†ï¼ŒéªŒè¯åœ¨ä¸åŒ `cudagraph_mode`ï¼ˆFULLã€PIECEWISEã€FULL_DECODE_ONLYã€NONEï¼‰ä¸‹çš„è¿è¡Œæ—¶å’Œæ˜¾å­˜å ç”¨æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚

---

### [Model Runner V2] Add KV Connector support (#32742)
**SHA**: `8518b30` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8518b3044794d11b6a9881f2875cfa5d2823ac56)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šä¸º V1 GPUModelRunner å¼•å…¥ KVâ€¯Connectorï¼Œæ–°å¢ `kv_connector.py` å®ç° KVâ€¯Transfer çš„å‰åç½®é’©å­ï¼›`init_kv_cache` æ”¹ä¸ºè¿”å›ç¼“å­˜å­—å…¸ï¼›åœ¨æ¨¡å‹è·‘æ­¥ã€æ¨¡æ‹Ÿè¿è¡Œã€ç©ºè·‘ç­‰è·¯å¾„ä¸­ç»Ÿä¸€è°ƒç”¨ KVâ€¯Connectorï¼›åœ¨ `GPUWorker.shutdown` ä¸­æ·»åŠ  KVâ€¯Transfer çš„å®‰å…¨å…³é—­ã€‚  
**ğŸ¯ å½±å“èŒƒå›´**ï¼š`vllm/v1/worker/gpu/attn_utils.pyã€kv_connector.pyã€model_runner.pyã€gpu_worker.py` ä»¥åŠç›¸å…³çš„è°ƒåº¦/è¾“å‡ºç»“æ„ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **å…¼å®¹æ€§**ï¼š`init_kv_cache` è¿”å›å€¼æ”¹å˜åï¼Œé™¤ `model_runner.py` å¤–çš„å…¶ä»–è°ƒç”¨ç‚¹ï¼ˆè‹¥æœ‰ï¼‰éœ€è¦åŒæ­¥æ›´æ–°ï¼Œé˜²æ­¢å‡ºç° `None` æˆ–ç±»å‹é”™è¯¯ã€‚  
2. **å…³é—­æµç¨‹**ï¼š`ensure_kv_transfer_shutdown` ç°åœ¨ç›´æ¥åœ¨ `GPUWorker.shutdown` è°ƒç”¨ï¼Œå»ºè®®åœ¨å¼‚å¸¸è·¯å¾„ï¼ˆå¦‚æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼‰ä¹Ÿæ˜¾å¼è°ƒç”¨ï¼Œä»¥å…æ®‹ç•™è¿›ç¨‹æˆ–æ˜¾å­˜æ³„æ¼ã€‚  
3. **Dummyâ€¯run**ï¼šåœ¨ `_dummy_run` ä¸­ä¸´æ—¶ç¦ç”¨ KVâ€¯Connectorï¼Œç¡®ä¿ `set_disabled(True/False)` æˆå¯¹å‡ºç°ï¼›è‹¥åç»­åŠ å…¥å¹¶è¡Œ dummyâ€¯runï¼Œéœ€è€ƒè™‘çº¿ç¨‹å®‰å…¨ã€‚  
4. **å‰å‘ä¸Šä¸‹æ–‡**ï¼š`KVConnector.pre_forward` ä¸­å¯¹ `forward_context` çš„å¤„ç†æœ‰ä¸¤æ¡åˆ†æ”¯ï¼Œå»ºè®®æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›– `is_forward_context_available()` ä¸º `True/False` ä¸¤ç§æƒ…å½¢ï¼Œç¡®ä¿ `set_forward_context` æ­£å¸¸æ¢å¤ã€‚  
5. **æ—¥å¿—ä¸ç›‘æ§**ï¼š`ActiveKVConnector.post_forward` è¿”å›çš„ç»Ÿè®¡ä¿¡æ¯ç›®å‰ä»…åœ¨ `KVConnectorOutput` ä¸­ä¿å­˜ï¼Œå»ºè®®åœ¨ `sample_tokens` æˆ–ä¸Šå±‚è°ƒåº¦ä¸­è®°å½•å…³é”®æŒ‡æ ‡ï¼ˆå¦‚ `finished_sending`ã€`invalid_block_ids`ï¼‰ï¼Œä¾¿äºæ’æŸ¥ KVâ€¯Transfer å¼‚å¸¸ã€‚  
6. **æ€§èƒ½å›å½’**ï¼šKVâ€¯Connector ä»‹å…¥å‰åä¼šå¢åŠ é¢å¤–åŒæ­¥ï¼ˆ`wait_for_save`ï¼‰å’Œå…ƒæ•°æ®ç»‘å®šï¼Œå»ºè®®åœ¨æœ‰/æ—  KVâ€¯Transfer ä¸¤ç§é…ç½®ä¸‹è·‘ä¸€æ¬¡åŸºå‡†ï¼Œå¯¹æ¯”æ¨¡å‹ååç‡ä¸å»¶è¿Ÿï¼Œç¡®è®¤æ–°å¢å¼€é”€åœ¨å¯æ¥å—èŒƒå›´ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ”¹åŠ¨å®ç°äº† KVâ€¯Transfer ä¸æ¨¡å‹è·‘æ­¥çš„è§£è€¦ï¼Œä»£ç ç»“æ„æ›´æ¸…æ™°ï¼›ä½†éœ€æ³¨æ„ä¸Šè¿°å…¼å®¹æ€§ã€èµ„æºé‡Šæ”¾ä»¥åŠæµ‹è¯•è¦†ç›–ï¼Œç¡®ä¿åœ¨ä¸åŒéƒ¨ç½²ç¯å¢ƒï¼ˆæœ‰/æ—  KVâ€¯Transferï¼‰ä¸‹ä»ä¿æŒç¨³å®šã€‚

---

### [CI][torch nightlies] Use main Dockerfile with flags for nightly torch tests (#30443)
**SHA**: `68b0a6c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/68b0a6c1baf32221fa8ed98a941f611cbff2cdb1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / CI æ”¯æŒ  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- ä¸º Docker æ„å»ºå¼•å…¥ `PYTORCH_NIGHTLY` æ ‡å¿—ï¼Œä½¿åŒä¸€å¥— Dockerfile å¯åœ¨ **release** ä¸ **nightly** ä¸¤ç§ PyTorch ç‰ˆæœ¬ä¹‹é—´åˆ‡æ¢ï¼Œå–ä»£åŸæœ‰ `Dockerfile.nightly_torch`ã€‚  
- æ–°å¢ `use_existing_torch.py` è„šæœ¬ï¼Œç”¨äºåœ¨å®‰è£…å…¶å®ƒä¾èµ–å‰å‰”é™¤å·²é¢„è£…çš„ torch/torchvision/torchaudio è¡Œï¼Œé˜²æ­¢ `uv pip install` é‡æ–°æ‹‰å–ä¸ä¸€è‡´çš„ç‰ˆæœ¬ã€‚  
- åœ¨å¤šä¸ªé˜¶æ®µï¼ˆruntimeã€buildã€devã€installï¼‰ä¸­åŠ å…¥å¯¹ `PYTORCH_NIGHTLY` çš„æ¡ä»¶åˆ¤æ–­ï¼Œå¹¶å°† **torch ç‰ˆæœ¬ä¿¡æ¯**ï¼ˆ`torch_lib_versions.txt`ï¼‰åœ¨é•œåƒå±‚ä¹‹é—´ä¼ é€’ï¼Œä»¥ä¿è¯åç»­å±‚ä½¿ç”¨ç›¸åŒçš„äºŒè¿›åˆ¶ã€‚  
- æ—§çš„ `Dockerfile.nightly_torch` ä»…ä¿ç•™æç¤ºæ€§æ³¨é‡Šï¼Œè®¡åˆ’ç§»é™¤ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `docker/Dockerfile`ï¼ˆæ‰€æœ‰é˜¶æ®µï¼‰  
- `docker/Dockerfile.nightly_torch`ï¼ˆå·²åºŸå¼ƒï¼‰  
- æ–°å¢/æ”¹åŠ¨çš„è„šæœ¬ `use_existing_torch.py`  
- CI æµç¨‹ä¸­æ¶‰åŠ Docker æ„å»ºçš„ä½œä¸š  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **CI å‚æ•°**ï¼šç¡®ä¿åœ¨ nightly æµ‹è¯•çš„ workflow ä¸­æ˜¾å¼ä¼ å…¥ `--build-arg PYTORCH_NIGHTLY=1`ï¼Œå¦åˆ™ä»ä¼šä½¿ç”¨ release åŒ…ã€‚  
2. **ç‰ˆæœ¬é”å®š**ï¼š`torch_lib_versions.txt` é€šè¿‡ `uv pip freeze` æ•è·ï¼Œä»…åœ¨ nightly åœºæ™¯ä¸‹å¼•ç”¨ï¼›è‹¥æ‰‹åŠ¨ä¿®æ”¹ä¾èµ–æ–‡ä»¶ï¼Œéœ€é‡æ–°ç”Ÿæˆè¯¥æ–‡ä»¶é˜²æ­¢ç‰ˆæœ¬æ¼‚ç§»ã€‚  
3. **ç¼“å­˜è¡Œä¸º**ï¼š`--mount=type=cache` ä»ä¾èµ– `/root/.cache/uv`ï¼Œä½† torch åŒ…ä¼šå…ˆé€šè¿‡ `uv pip install --pre` æ‹‰å– nightlyï¼Œåç»­å±‚ä½¿ç”¨ç›¸åŒç¼“å­˜ï¼›å»ºè®®åœ¨ CI ä¸­æ¸…ç†è¯¥ç¼“å­˜ä»¥é¿å…æ—§ release åŒ…æ®‹ç•™ã€‚  
4. **è„šæœ¬å…¼å®¹æ€§**ï¼š`use_existing_torch.py` ç°åœ¨æ”¯æŒ `--prefix` æ–¹å¼ï¼Œä»…åˆ é™¤ä»¥ `torch=`ã€`torchvision=`ã€`torchaudio=` ä¸ºå‰ç¼€çš„è¡Œï¼Œé¿å…è¯¯åˆ ç±»ä¼¼ â€œtorchmetricsâ€ã€‚æ£€æŸ¥è‡ªå®šä¹‰ `requirements/*.in` æ˜¯å¦ä»ç¬¦åˆæ­¤å‰ç¼€è§„åˆ™ã€‚  
5. **æ–‡æ¡£ä¸é•œåƒæ ‡ç­¾**ï¼šæ›´æ–° README/CONTRIBUTING ä¸­çš„ Docker ä½¿ç”¨è¯´æ˜ï¼Œæ˜ç¡® `PYTORCH_NIGHTLY` å‚æ•°åŠé•œåƒæ ‡ç­¾çº¦å®šï¼ˆå¦‚ `vllm:nightly-cuda12.1`ï¼‰ã€‚  
6. **å›å½’æµ‹è¯•**ï¼šåœ¨ CI ä¸­åˆ†åˆ«è·‘ä¸€æ¬¡ **release** ä¸ **nightly** æ„å»ºï¼ŒéªŒè¯ `torch`ã€`torchvision`ã€`torchaudio` ç‰ˆæœ¬ä¸€è‡´æ€§ä»¥åŠ vLLM wheel èƒ½å¦æˆåŠŸå®‰è£…ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨å°† Docker ç»´æŠ¤æˆæœ¬æ˜¾è‘—é™ä½ï¼Œç»Ÿä¸€äº†æ„å»ºé€»è¾‘ï¼Œå…³é”®åœ¨äº CI å‚æ•°çš„æ­£ç¡®ä¼ é€’ä¸ `torch_lib_versions.txt` çš„åŒæ­¥ç®¡ç†ã€‚è‹¥åœ¨å®é™…éƒ¨ç½²ä¸­å‡ºç°ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œè¯·å…ˆæ£€æŸ¥ `use_existing_torch.py` æ˜¯å¦æˆåŠŸåˆ æ‰æ—§ä¾èµ–å¹¶ç¡®è®¤ç¼“å­˜å·²åˆ·æ–°ã€‚

---

### [Frontend] add logprob, compression_rate to 'verbose_json' features (#31059)
**SHA**: `9b77bb7` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9b77bb790dd6d833a9b31814ba00f48b5fa47afb)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆåœ¨ `verbose_json` å“åº”ä¸­åŠ å…¥ `avg_logprob` ä¸ `compression_ratio`ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. æ–‡æ¡£è¯´æ˜ä¸­åˆ é™¤äº† â€œavg_logprob, compression_ratioâ€ ä¸æ”¯æŒçš„æç¤ºã€‚  
2. `TranscriptionSegment` ä¸ `TranslationSegment` çš„ `avg_logprobã€compression_ratio` ä»å¯é€‰ (`float|None`) æ”¹ä¸ºå¿…å¡« `float`ï¼Œå¹¶åœ¨æ¨¡å‹å±‚é¢åŠ å…¥äº†å­—æ®µæ³¨é‡Šã€‚  
3. `speech_to_text` æµç¨‹åœ¨ç”Ÿæˆ `verbose_json` æ—¶ï¼š  
   - é€šè¿‡ `zlib.compress` è®¡ç®—æ–‡æœ¬çš„å‹ç¼©ç‡ï¼›  
   - é€šè¿‡ `FlatLogprobs` è®¡ç®—æ®µè½çš„å¹³å‡ logprobï¼›  
   - åœ¨é‡‡æ ·å‚æ•°é‡Œå¼ºåˆ¶æ‰“å¼€ `logprobs=1`ã€‚  
4. ç›¸å…³å•å…ƒæµ‹è¯•åŠ å…¥å¯¹æ–°å­—æ®µçš„æ–­è¨€ï¼Œç¡®ä¿è¿”å›å€¼éç©ºã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/entrypoints/openai/translations/protocol.py`ï¼ˆæ¨¡å‹ç»“æ„ï¼‰  
- `vllm/entrypoints/openai/translations/speech_to_text.py`ï¼ˆæ ¸å¿ƒå®ç°ï¼‰  
- æ–‡æ¡£ `docs/serving/openai_compatible_server.md`  
- æµ‹è¯• `tests/entrypoints/openai/test_transcription_validation_whisper.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šæŠŠå­—æ®µä»å¯é€‰æ”¹ä¸ºå¿…å¡«ä¼šå¯¼è‡´ä»æ—§ç‰ˆæœ¬ç”Ÿæˆçš„ `verbose_json`ï¼ˆæˆ–è‡ªè¡Œæ„é€ çš„ `TranscriptionSegment`ï¼‰åœ¨ååºåˆ—åŒ–æ—¶æŠ›é”™ã€‚å»ºè®®åœ¨æ¥å—å¤–éƒ¨è¾“å…¥ï¼ˆå¦‚ä»¿çœŸæˆ–ç¼“å­˜ï¼‰æ—¶æ·»åŠ å‘åå…¼å®¹çš„è½¬åŒ–å±‚ï¼Œæˆ–åœ¨ `OpenAIBaseModel` ä¸­ä¿ç•™ `validator` å¤„ç†ç¼ºå¤±å€¼ã€‚  
2. **æ€§èƒ½**ï¼šæ¯ä¸ªæ®µè½éƒ½ä¼šæ‰§è¡Œä¸€æ¬¡ `zlib.compress` ä¸ä¸€æ¬¡éå† `log_probs`ï¼Œå¯¹é•¿éŸ³é¢‘å¯èƒ½å¸¦æ¥è½»å¾® CPU å¼€é”€ã€‚è‹¥å¯¹ååé‡æ•æ„Ÿï¼Œå¯è€ƒè™‘åœ¨ `sampling_params.logprobs` ä¸º 0 æ—¶è·³è¿‡è®¡ç®—ï¼Œæˆ–ä½¿ç”¨æ›´è½»é‡çš„å‹ç¼©ç‡è¿‘ä¼¼ã€‚  
3. **é”™è¯¯å®¹å¿**ï¼š`avg_logprob` è®¡ç®—é»˜è®¤é™¤ä»¥ `idx-last_timestamp_start`ï¼Œä½†è‹¥æ®µè½ä»…åŒ…å«æ—¶é—´æˆ³ï¼ˆæ— æ–‡æœ¬ï¼‰ä¼šå‡ºç°é™¤é›¶ã€‚å»ºè®®åœ¨é™¤æ³•å‰æ£€æŸ¥åˆ†æ¯ï¼Œæˆ–åœ¨ `if token >= init_token ...` åˆ†æ”¯ä¸­ç¡®ä¿ `idx > last_timestamp_start + 1`ã€‚  
4. **æ—¥å¿—ä¸è°ƒè¯•**ï¼šæ–°å¢ `logprobs` é‡‡æ ·å‚æ•°åï¼Œæ—¥å¿—è¾“å‡ºé‡ä¼šå¢å¤§ã€‚å¯ä»¥åœ¨ `init_logger` ä¸­åŠ å…¥ `verbose_json` ä¸“ç”¨çš„è°ƒè¯•å¼€å…³ï¼Œé˜²æ­¢ç”Ÿäº§ç¯å¢ƒæ—¥å¿—çˆ†ç‚¸ã€‚  
5. **æ–‡æ¡£åŒæ­¥**ï¼šæ–‡æ¡£å·²æ›´æ–°ï¼Œä½†ä»éœ€åœ¨ API å‚è€ƒè¡¨ä¸­æ ‡æ³¨ `avg_logprob`ã€`compression_ratio` ä¸ºå¿…è¿”å­—æ®µï¼Œé¿å…ç”¨æˆ·è¯¯ä»¥ä¸ºå¯ä¸ºç©ºã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸º `verbose_json` æä¾›äº†æœ‰ä»·å€¼çš„è´¨é‡åº¦é‡ï¼Œä½†æ¶‰åŠæ¨¡å‹å­—æ®µå¿…é€‰åŒ–ã€é¢å¤–è®¡ç®—å’Œæ—¥å¿—é‡æå‡ï¼Œéœ€è¦åœ¨ç”Ÿäº§éƒ¨ç½²å‰åšå¥½å…¼å®¹æ€§ä¸æ€§èƒ½è¯„ä¼°ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (20)

### [Bugfix]: resolve torch.compile cache conflict between mm_encoder_tp_modes (#32842)
**SHA**: `1209b78` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/1209b784f2ba976eff2ea24bc33c61f35c6eb213)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `MultimodalConfig.compute_hash` ä¸­åŠ å…¥ `mm_encoder_tp_mode`ï¼Œå¹¶åœ¨ `VLLMConfig.compute_hash` ä¸­å½“ `compile_mm_encoder` å¼€å¯æ—¶ï¼Œå°†å¤šæ¨¡æ€é…ç½®çš„å“ˆå¸ŒåŠ å…¥æ•´ä½“å“ˆå¸Œï¼Œä»¥é¿å… `torch.compile` ç¼“å­˜å†²çªã€‚

---

### [EncoderCacheManager] Remove unnecessary copy (#32800)
**SHA**: `5fa0f6e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/5fa0f6efa9f8778b61a586fdf86b218e1d19395f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `EncoderCacheManager.free` ä¸­å»æ‰å¯¹ `get_cached_input_ids` ç»“æœçš„ `.copy()`ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åˆ—è¡¨ï¼Œæ¶ˆé™¤ä¸å¿…è¦çš„æ‹·è´ï¼Œæå‡è¿è¡Œæ—¶æ•ˆç‡ã€‚

---

### [Perf] Cache exc.errors() result in validation exception handler (#32984)
**SHA**: `0f19427` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0f19427db587ae425d62a1857ee3339015ee3084)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ OpenAI API æœåŠ¡å™¨çš„è¯·æ±‚éªŒè¯å¼‚å¸¸å¤„ç†å™¨ä¸­ç¼“å­˜ `exc.errors()` ç»“æœï¼Œé¿å…å¤šæ¬¡è°ƒç”¨ï¼Œæé«˜æ€§èƒ½å¹¶ç®€åŒ–ä»£ç é€»è¾‘ã€‚

---

### [Doc] Ignore typo check on doc (#32999)
**SHA**: `81c2a88` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/81c2a889ce87d1019a4aac7f0139defd389b3e71)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `pyproject.toml` çš„ `extend-exclude` åˆ—è¡¨ä¸­æ·»åŠ  `docs/governance/process.md`ï¼Œä½¿ typo æ£€æŸ¥å¿½ç•¥è¯¥æ–‡æ¡£ã€‚

---

### [docs] Update governance process links (#32995)
**SHA**: `5c86a89` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/5c86a89805ad1f078396b3b400918cc753b4f2a9)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå®Œå–„æ²»ç†æµç¨‹æ–‡æ¡£ï¼Œåˆ—å‡ºé¡¹ç›®æ ¸å¿ƒç»´æŠ¤è€…ï¼ˆProject Leadsï¼‰åŠå…¶èŒè´£ï¼Œç»†åŒ– Lead Maintainersã€Committers ä¸ Area Owners çš„èŒè´£ä¸é€‰æ‹”æ ‡å‡†ã€‚

---

### [Tests] Clarify pytest skip reasons with actionable context (#32981)
**SHA**: `0b9a735` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0b9a735e113ea97d3a75d5491b6514c03f6081e3)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `tests/samplers/test_beam_search.py` å’Œ `tests/v1/sample/test_topk_topp_sampler.py` ä¸­ï¼Œå°† `skip` æ ‡è®°çš„æ³¨é‡Šæ”¹ä¸ºæ›´å…·ä½“çš„è¯´æ˜ï¼Œæä¾›äº†å¯æ“ä½œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾¿äºåç»­å®šä½å’Œæ¢å¤è¢«è·³è¿‡çš„æµ‹è¯•ã€‚

---

### [Perf] Cache xpu_get_mem_info() result to avoid duplicate calls (#32983)
**SHA**: `14d03b8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/14d03b8ddb383a114e9085f7ece48629e11991b5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `determine_available_memory` ä¸­ç¼“å­˜ `xpu_get_mem_info()` çš„è¿”å›å€¼ï¼Œé¿å…é‡å¤è°ƒç”¨ï¼Œå®ç°ä¸€æ¬¡è·å–ç©ºé—²ä¸æ€»å†…å­˜åç›´æ¥è®¡ç®—å·²åˆ†é…å†…å­˜ï¼Œæå‡æ€§èƒ½ã€‚

---

### Auth_token added in documentation as it is required (#32988)
**SHA**: `c0d8204` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/c0d820457a55ec4054f6658b46b0745f6dc2807a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ Claude Code é›†æˆæ–‡æ¡£ä¸­æ–°å¢ `ANTHROPIC_AUTH_TOKEN` ç¯å¢ƒå˜é‡è¯´æ˜ï¼Œæ ‡æ³¨è¯¥å˜é‡ä¸ºå¿…éœ€ä¸”å¯éšæ„å¡«å…¥ã€‚

---

### [ROCm][ViT] Enable Flash Attention Triton backend on RDNA3/RDNA4 (#32944)
**SHA**: `97ef11d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/97ef11dd3406ab7a9080921c62f001d60399d928)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ–°å¢ `flash_attn_triton_available` æ£€æµ‹å¹¶åœ¨ RDNA3/RDMA4 (gfx11x/12x) ä¸Šå¯ç”¨ Flash Attention Triton åç«¯ï¼›åœ¨ ViT æ³¨æ„åŠ›è·¯å¾„åŠ å…¥å¯¹åº”æ¡ä»¶åˆ¤æ–­ã€‚

---

### [Bugfix] Fix FusedMoE LoRA kernel offs_token out of bound value (#32279)
**SHA**: `ecc3dd6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ecc3dd66cceebcd9fc48216ae3852a0e994fbdc1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®å¤ `fused_moe_lora_op.py` ä¸­ Triton kernel å¯¹ `sorted_token_ids_ptr` çš„è¶Šç•Œè®¿é—®ï¼Œæ–°å¢æ©ç å¹¶å°† `topk_weights_ptr` çš„é»˜è®¤å€¼ä» `0` æ”¹ä¸º `0.0`ï¼Œé˜²æ­¢ OOB é”™è¯¯ã€‚

---

### [Core][Bugfix] allow graceful worker termination (#32965)
**SHA**: `7e1f10d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/7e1f10d5622afe0049ba31765fa84a452dbfe9cb)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `multiproc_executor.py` ä¸­æ–°å¢å¯¹æ´»è·ƒå­è¿›ç¨‹çš„æ£€æŸ¥ä¸ç­‰å¾…ï¼Œå…ˆå°è¯•è®©è¿›ç¨‹è‡ªè¡Œé€€å‡ºï¼Œå†åˆ†åˆ«ä½¿ç”¨ `terminate`ï¼ˆSIGTERMï¼‰å’Œ `kill`ï¼ˆSIGKILLï¼‰å¼ºåˆ¶ç»“æŸï¼Œå¹¶åœ¨ä¿¡å·å¤„ç†å’Œçˆ¶è¿›ç¨‹ç›‘æ§ä¸­åŠ å…¥æ—¥å¿—ä¸å¯¹ `SystemExit` çš„æ˜¾å¼æ•è·ï¼Œç¡®ä¿å·¥ä½œè¿›ç¨‹èƒ½å¤Ÿå¹³æ»‘ã€å¯é åœ°ç»ˆæ­¢ã€‚

---

### [fix] add VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME to compile factors (#32912)
**SHA**: `0118cdc` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0118cdcc02ae16a137645e2289bf41f5e3da9d80)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `compile_factors` ä¸­åŠ å…¥ `VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME`ï¼Œå¹¶æ–°å¢å•å…ƒæµ‹è¯•éªŒè¯è¯¥å‡½æ•°çš„å“ˆå¸Œåœ¨æ¯æ¬¡å…¨æ–°è¿›ç¨‹åˆå§‹åŒ–æ—¶ä¿æŒä¸€è‡´ã€‚

---

### [CI] fix version comparsion and exclusion patterns in upload-release-wheels.sh (#32971)
**SHA**: `136c499` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/136c499f6ea8e34b18b9a7cda0518ed1df6ae8ad)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®æ­£ `upload-release-wheels.sh` çš„ç‰ˆæœ¬æ¯”è¾ƒï¼ˆå»æ‰å‰ç¼€ `v`ï¼‰ï¼Œæ–°å¢ `PURE_VERSION` å˜é‡ï¼Œå¹¶è°ƒæ•´ S3 ä¸‹è½½ä¸ PyPI ä¸Šä¼ çš„åŒ¹é…/æ’é™¤è§„åˆ™ï¼Œé¿å…è¯¯æ’é™¤ `aarch64` åŒ…å¹¶æ­£ç¡®è¿‡æ»¤ `dev` ä¸ `rc` è½®å­ã€‚

---

### [Bugfix] Fix missing is_layer_skipped check for FusedMoE in AWQConfig (#32935)
**SHA**: `ebd0a17` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ebd0a17e0e0b5dd326dc6533cf4d8f49806e69e1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `AWQConfig` ä¸­ä¸º FusedMoE æ·»åŠ ç¼ºå¤±çš„ `is_layer_skipped` æ£€æŸ¥ï¼Œä¿®æ­£å¯¼å…¥è·¯å¾„å¹¶æ”¹ä¸ºä½¿ç”¨ `AWQMarlinConfig.get_quant_method`ï¼ŒåŒæ—¶ä¼ é€’ `modules_to_not_convert` å‚æ•°ï¼Œé˜²æ­¢é”™è¯¯å±‚è¢«é‡åŒ–ã€‚

---

### [CI][AMD][BugFix] Update wvSplitK (and other skinny_gemm wrappers) to ensure tensors passed will be made contiguous for the kernel (#32831)
**SHA**: `6cc6d92` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6cc6d92be5e64f449e31e06d43c80955f646809e)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„ / BugFix  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ ROCm skinnyâ€‘gemm æ¥å£ä¸­å¼ºåˆ¶å°†è¾“å…¥å¼ é‡è®¾ä¸º contiguousï¼Œä»¥é˜²æ­¢éè¿ç»­å¼ é‡å¯¼è‡´çš„å†…æ ¸é”™è¯¯ï¼Œå¹¶åœ¨æºç ä¸­æ·»åŠ ç›¸åº”æ³¨é‡Šè¯´æ˜ã€‚

---

### [Bug] Fix benchmark script `moe_permute_unpermute` (#32949)
**SHA**: `dfab5f3` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/dfab5f37648d123480acb8a6246c6abbd80dca58)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®æ­£ benchmark è„šæœ¬çš„å¯¼å…¥è·¯å¾„å¹¶å°† `_moe_permute` è°ƒç”¨æ”¹ä¸ºå›ºå®š `block_m=16`ï¼ˆåŸä½¿ç”¨ `align_block_size`ï¼‰ï¼Œç¡®ä¿åŸºå‡†æµ‹è¯•æ­£å¸¸æ‰§è¡Œã€‚

---

### [Bugfix][CI] Fix pre-commit (#32956)
**SHA**: `2d6b537` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/2d6b537157af1ee2f65026b28a5258838e356f32)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `triton` ä¸ `triton.language` çš„ç›´æ¥å¯¼å…¥æ”¹ä¸ºç»Ÿä¸€é€šè¿‡ `vllm.triton_utils` å¼•å…¥ï¼Œå¹¶åˆ é™¤å†—ä½™çš„ import è¡Œã€‚

---

### [Model] Enable LoRA support for internvl2 (#32397)
**SHA**: `fec9da0` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/fec9da0af48da5fe9523914aeddf2c34f53166a9)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `internvl.py` ä¸­é‡æ„å›¾åƒ token è®¡ç®—é€»è¾‘ï¼Œæ–°å¢ `patch_tokens` å¹¶æ”¹è¿› `num_image_token` è®¡ç®—ï¼›å®ç° `get_num_mm_encoder_tokens` ä¸ `get_num_mm_connector_tokens` ä¸¤ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºè·å–å¤šæ¨¡æ€ç¼–ç å™¨å’Œè¿æ¥å™¨çš„ token æ•°é‡ã€‚

---

### [torch.compile][CI] Add back attn fusion on hopper/ada (#32940)
**SHA**: `bbbd696` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/bbbd696af9ebfadf38553e2361f3a31ed759e0ed)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `tests/compile/test_fusion_attn.py` ä¸­ç§»é™¤å¯¹ `cuda_device_count_stateless` çš„å¯¼å…¥ä¸æ£€æŸ¥ï¼Œæ›´æ–°æ¨¡å‹åç§°ä»¥åŒ¹é… RedHatAI ä¸ NVIDIA æœ€æ–°æ¨¡å‹ï¼Œå¹¶ç®€åŒ–å¯¹ FP4 åœºæ™¯çš„è·³è¿‡é€»è¾‘ã€‚

---

### [Hardware][AMD][CI][Bugfix] Fix Kernels Attention Cache test (#32904)
**SHA**: `305e53a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/305e53ade8f7cbdbece6fd211eb45a612800e657)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ AMD CI ç¯å¢ƒä¸‹ï¼Œå°† FP8 å¼ é‡çš„è§†å›¾ç±»å‹æ”¹ä¸ºä½¿ç”¨ `current_platform.fp8_dtype()` åŠ¨æ€è·å–ï¼Œä»¥ä¿®å¤æ³¨æ„åŠ›ç¼“å­˜æµ‹è¯•ã€‚

---

