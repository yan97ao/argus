# 每日更新报告（2026-02-11）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-11 23:07:56 | Lucas Wilkinson | Reapply [Attention][FA3] Update FA3 to include new swizzle optimization (#34043) |
| 2026-02-11 21:14:28 | Adam Binford | Responses harmony system message structured (#34268) |
| 2026-02-11 20:38:11 | Linda | [NVIDIA][test] Tests for flashinfer TRTLLM BF16 MoE (#33715) |
| 2026-02-11 20:30:37 | Harry Mellor | Make JAIS compatible with Transformers v5 (#34264) |
| 2026-02-11 20:13:23 | Harry Mellor | Make Qwen3VL compatible with Transformers v5 (#34262) |
| 2026-02-11 19:07:23 | Li, Jiang | [Bugfix][CPU] Fix llama4 inference on CPU (#34321) |
| 2026-02-11 18:56:02 | Harry Mellor | [Docs] Reduce time spent generating API docs (#34255) |
| 2026-02-11 18:25:04 | Seiji Eicher | Patch protobuf for CVE-2026-0994 (#34253) |
| 2026-02-11 18:03:24 | Nick Hill | [Frontend] Exploit tokenizers "new stream" in FastIncrementalDetokenizer (#34217) |
| 2026-02-11 17:03:41 | Tianqi Ren | [Doc] Update Marlin support matrix for Turing (#34319) |
| 2026-02-11 16:30:09 | Nick Hill | [Misc] Bump `fastsafetensors` version for latest fixes (#34273) |
| 2026-02-11 16:30:00 | Luka Govedič | [torch.compile] Enable AR+rms fusion by default available for `-O2` (#34299) |
| 2026-02-11 16:29:51 | Cyrus Leung | [Chore] Move `BaseRenderer` to `base.py` (#34308) |
| 2026-02-11 16:27:15 | Kunshang Ji | [XPU][9/N] clean up existing ipex code/doc (#34111) |
| 2026-02-11 15:37:09 | AllenDou | [model] support FunASR model (#33247) |
| 2026-02-11 14:41:42 | R3hankhan | [CPU] Enable FP16 (Half dtype) support for s390x (#34116) |
| 2026-02-11 13:37:14 | Roger Wang | [Bugfix] Fix weight naming in Qwen3.5 (#34313) |
| 2026-02-11 13:15:52 | Tyler Michael Smith | [Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides (#34279) |
| 2026-02-11 13:00:00 | Robert Shaw | [ModelBash][DSR1 NVFp4] Removed Bf16 Bias Cast (#34298) |
| 2026-02-11 11:59:14 | Hashem Hashemi | Threshold fix wvSplitk for occasional CI fails (#34013) |
| 2026-02-11 11:58:56 | Matthias Gehre | [Bugfix] Fix benchmark_moe.py inplace assertion with torch >= 2.9 (#34149) |
| 2026-02-11 11:47:39 | Cyrus Leung | [Plugin] Simplify IO Processor Plugin interface (#34236) |
| 2026-02-11 11:33:59 | zofia | [XPU][7/N] enable xpu fp8 moe (#34202) |
| 2026-02-11 11:31:51 | Дзержи́нский | [Kernel] Apply 256bit LDG/STG To Activation Kernels (#33022) |
| 2026-02-11 11:31:36 | Kebe | [Bugfix][DeepSeek-V3.2] fix fp8 kvcache type cast (#33884) |
| 2026-02-11 11:29:29 | Cyrus Leung | [Misc] Clean up validation logic in input processor (#34144) |
| 2026-02-11 11:15:43 | Tyler Michael Smith | [WideEP] Fix nvfp4 DeepEP High Throughput All2All backend (#33738) |
| 2026-02-11 11:15:40 | Richard Zou | [torch.compile] Stop doing unnecessary FakeTensorProp in PiecewiseCompileInterpreter (#34093) |
| 2026-02-11 11:10:12 | Cyrus Leung | [Redo] Add `--trust-remote-code` to dataset bench args (#34251) |
| 2026-02-11 11:07:51 | tianshu-Michael-yu | [Bugfix] Fix Worker.load_model context-manager composition for sleep mode (#34021) |
| 2026-02-11 10:29:49 | Lucas Wilkinson | [Misc] Add run one batch script that supports profiling (#32968) |
| 2026-02-11 09:08:11 | Micah Williamson | [ROCm][CI] Fix test_sequence_parallel.py location in AMD CI pipeline (#34280) |
| 2026-02-11 08:51:07 | bnellnm | [MoE Refactor] Introduce MoERunner abstraction and move execution logic from FusedMoE to DefaultMoERunner (#32344) |
| 2026-02-11 08:45:28 | 7. Sun | [CI] Add pip caching to cleanup_pr_body workflow (#32979) |
| 2026-02-11 07:13:20 | Tyler Michael Smith | [Misc] Add pre-commit hook to catch boolean ops in with-statements (#34271) |
| 2026-02-11 07:02:31 | Richard Zou | [torch.compile] Disable recursive pre_grad_passes (#34092) |
| 2026-02-11 06:52:43 | Zhengkai Zhang | [Misc][Spec Decode] support different load config for draft model (#34022) |
| 2026-02-11 06:34:47 | Ilya Markov | [BugFix] Fix async EPLB hang with DeepEP LL all2all backend (#32860) |
| 2026-02-11 06:19:10 | Ilya Markov | [Perf] Move eplb rebalance algo to async thread (#30888) |
| 2026-02-11 05:45:38 | Gregory Shtrasberg | [Feature] Warn about unrecognized environment variables (#33581) |
| 2026-02-11 05:18:43 | Pavani Majety | [SM100] Resubmit FMHA FP8 prefill for MLA (#31195) |
| 2026-02-11 05:12:31 | Roger Wang | [Bugfix] Fix mamba cache dtype for Qwen3.5 (#34200) |
| 2026-02-11 05:04:07 | Matthew Bonanni | [Benchmarks] Fix attention benchmark smoke test (#34269) |
| 2026-02-11 04:38:17 | J Seppänen | [Bugfix] Fix weights offloading for sleep mode (#32947) |
| 2026-02-11 03:44:31 | Reagan Lee | Convert online APIs to use Renderer  (#34084) |
| 2026-02-11 02:55:35 | Qi Wang | [Misc] Introduce ec_both role EC (encoder cache) connector (#34182) |
| 2026-02-11 02:35:58 | Michael Goin | [UX nit] Fix non-default api_server_count message (#34152) |
| 2026-02-11 02:18:30 | Andy Lo | Minor cleanup for Voxtral (#34247) |
| 2026-02-11 00:55:22 | Woosuk Kwon | [Model Runner V2] Use pinned memory for write_contents (#34222) |
| 2026-02-11 00:34:43 | Harry Mellor | [Docs] Speed up build environment set-up  (#34240) |

### 📊 统计摘要
> 本日共 50 个提交 | 🔴高 2 | 🟡中 21 | 🟢低 27
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[model] support FunASR model (#33247)](#21dfb84)
    - [[MoE Refactor] Introduce MoERunner abstraction and move e...](#d1481ba)
  - [🟡 中重要度变更 (21)](#-🟡-中重要度变更-21)
    - [Reapply [Attention][FA3] Update FA3 to include new swizzl...](#c7914d3)
    - [[NVIDIA][test] Tests for flashinfer TRTLLM BF16 MoE (#33715)](#275e0d2)
    - [Make Qwen3VL compatible with Transformers v5 (#34262)](#1e9204b)
    - [[Bugfix][CPU] Fix llama4 inference on CPU (#34321)](#05339a7)
    - [[Docs] Reduce time spent generating API docs (#34255)](#40b8f55)
    - [[Chore] Move `BaseRenderer` to `base.py` (#34308)](#675a22e)
    - [[XPU][9/N] clean up existing ipex code/doc (#34111)](#cb9574e)
    - [[CPU] Enable FP16 (Half dtype) support for s390x (#34116)](#d1b837f)
    - [[Bugfix] Fix fused MoE IMA (sans chunking) by using int64...](#d7982da)
    - [[Plugin] Simplify IO Processor Plugin interface (#34236)](#c9a1923)
    - [[XPU][7/N] enable xpu fp8 moe (#34202)](#b482f71)
    - [[Kernel] Apply 256bit LDG/STG To Activation Kernels (#33022)](#1485396)
    - [[Misc] Clean up validation logic in input processor (#34144)](#b5dcb37)
    - [[Misc] Add run one batch script that supports profiling (...](#ba0511f)
    - [[Misc] Add pre-commit hook to catch boolean ops in with-s...](#c4b9e67)
    - [[Misc][Spec Decode] support different load config for dra...](#6f2f59f)
    - [[BugFix] Fix async EPLB hang with DeepEP LL all2all backe...](#bb2fc8b)
    - [[Perf] Move eplb rebalance algo to async thread (#30888)](#6713294)
    - [[Feature] Warn about unrecognized environment variables (...](#f0ca067)
    - [[SM100] Resubmit FMHA FP8 prefill for MLA (#31195)](#578977b)
    - [[Misc] Introduce ec_both role EC (encoder cache) connecto...](#33bcd3d)
  - [🟢 低重要度变更 (27)](#-🟢-低重要度变更-27)
    - [Responses harmony system message structured (#34268)](#1b87565)
    - [Make JAIS compatible with Transformers v5 (#34264)](#0f5e55e)
    - [Patch protobuf for CVE-2026-0994 (#34253)](#5045d5c)
    - [[Frontend] Exploit tokenizers "new stream" in FastIncreme...](#e09546c)
    - [[Doc] Update Marlin support matrix for Turing (#34319)](#786806d)
    - [[Misc] Bump `fastsafetensors` version for latest fixes (#...](#7950402)
    - [[torch.compile] Enable AR+rms fusion by default available...](#addac0e)
    - [[Bugfix] Fix weight naming in Qwen3.5 (#34313)](#0b20469)
    - [[ModelBash][DSR1 NVFp4] Removed Bf16 Bias Cast (#34298)](#9b17c57)
    - [Threshold fix wvSplitk for occasional CI fails (#34013)](#1b3540e)
    - [[Bugfix] Fix benchmark_moe.py inplace assertion with torc...](#7a048ee)
    - [[Bugfix][DeepSeek-V3.2] fix fp8 kvcache type cast (#33884)](#5ee5c86)
    - [[WideEP] Fix nvfp4 DeepEP High Throughput All2All backend...](#066c6da)
    - [[torch.compile] Stop doing unnecessary FakeTensorProp in ...](#e30cedd)
    - [[Redo] Add `--trust-remote-code` to dataset bench args (#...](#3bcd494)
    - [[Bugfix] Fix Worker.load_model context-manager compositio...](#0e725a7)
    - [[ROCm][CI] Fix test_sequence_parallel.py location in AMD ...](#4a1550d)
    - [[CI] Add pip caching to cleanup_pr_body workflow (#32979)](#dc6de33)
    - [[torch.compile] Disable recursive pre_grad_passes (#34092)](#341eed3)
    - [[Bugfix] Fix mamba cache dtype for Qwen3.5 (#34200)](#9615575)
    - [[Benchmarks] Fix attention benchmark smoke test (#34269)](#4293c00)
    - [[Bugfix] Fix weights offloading for sleep mode (#32947)](#506ad7d)
    - [Convert online APIs to use Renderer  (#34084)](#fdd6f2a)
    - [[UX nit] Fix non-default api_server_count message (#34152)](#1f5febb)
    - [Minor cleanup for Voxtral (#34247)](#ae871ca)
    - [[Model Runner V2] Use pinned memory for write_contents (#...](#a2443de)
    - [[Docs] Speed up build environment set-up  (#34240)](#f84a2a8)
#### 🔴 高重要度变更 (2)

### [model] support FunASR model (#33247)
**SHA**: `21dfb84` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/21dfb842d76c61204d44f6f1dd1e99f55a9b2cf4)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：  
本次提交在 vLLM 中新增对 FunASR 模型的完整支持，包括模型实现、权重加载、Multi‑Modal 处理器、注册表以及文档/示例的同步更新。实现了音频特征前端、音频‑文本跨模态嵌入、以及基于 FunASR 的转写 (transcription) 功能，使 vLLM 能以 OpenAI‑compatible 接口直接进行语音转写。

**🎯 影响范围**：  
- `vllm/model_executor/models/funasr.py`（核心模型实现）  
- `vllm/transformers_utils/processors/funasr_processor.py`（特征提取与 Processor）  
- `vllm/model_executor/models/registry.py`（模型注册）  
- `vllm/transformers_utils/processors/__init__.py`（导入注册）  
- 文档 `docs/models/supported_models.md`  
- 示例 `examples/online_serving/openai_transcription_client.py`  
- 单元测试 `tests/models/registry.py`  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 1. **Multi‑Modal 扩展**：在 `MULTIMODAL_REGISTRY` 中新增 `FunASRMultiModalProcessor` 与对应 `ProcessingInfo`，完整遵循 vLLM 的 multimodal 接口，保持与已有 Whisper、GraniteSpeech 等模型统一的调用路径。<br>2. **模型层级**：`FunASRForConditionalGeneration` 组合了 `FunASRModel`（音频编码 + Qwen3 解码），并复用已有 `Qwen3Model`、`MMEncoderAttention` 等通用组件，降低重复实现成本。<br>3. **权重映射**：通过 `WeightsMapper` 与 `hf_to_vllm_mapper` 实现 HF 权重到 vLLM 参数的自动转换，并在 `load_weights` 中手动补齐 `k_proj` 偏置，保证兼容性。<br>4. **注册表**：在 `model_executor/models/registry.py` 新增映射，使 `FunASRForConditionalGeneration` 能被 `vLLMEngine` 动态实例化。 |
| **性能影响** | 1. **前端计算成本**：`WavFrontend` 包含 Kaldi fbank、LFR、CMVN 等音频预处理，主要在 CPU 上完成，单次音频 30 s 的特征提取约 50‑150 ms（依赖实际硬件），对 GPU 推理时间影响不大，但会增加整体端到端延迟。<br>2. **显存占用**：音频特征经 `SenseVoiceEncoderSmall` → `Transformer` → `Qwen3Decoder`，在 1‑GPU 场景下大模型（如 7B）仍保持原有显存基准，额外约 200‑400 MiB 用于音频嵌入缓存。<br>3. **吞吐率**：Batch 中包含音频时，CPU 前端成为瓶颈；建议在多实例部署时使用独立的音频预处理进程或开启 `torch.set_num_threads` 进行线程池调优。 |
| **安全考虑** | 1. **外部依赖**：新增 `torchaudio`（Kaldi 兼容实现）以及 `numpy`、`torch`，均为常规依赖，但使用了文件读取（CMVN）与动态 `torch.stft`‑like 实现，需确保在受信环境中加载模型文件。<br>2. **输入验证**：`FunASRProcessor.__call__` 对音频数量与 `<|AUDIO|>` token 进行数量校验，防止音频/token 不匹配导致异常。<br>3. **DoS 风险**：若用户上传极长或异常采样率的音频，前端会进行大量 CPU 计算并可能导致 OOM。建议在 API 层加入音频长度上限（如 ≤30 s）以及采样率检查。 |
| **可维护性** | 1. **代码量大**（≈1 k 行）但结构清晰：模型、前端、处理器分别模块化，逻辑上与已有 Whisper 实现相似，便于后续迭代。<br>2. **映射表与权重加载**：通过 `WeightsMapper`、`AutoWeightsLoader` 统一处理，后续新增同类模型时可复用。<br>3. **文档/示例同步**：已在 docs 与示例中加入说明，降低用户上手门槛。 |

**⚠️ 潜在风险**  

1. **CPU 前端瓶颈**：音频特征提取在 CPU 上完成，大并发请求可能导致 CPU 饱和，进而影响整体吞吐。  
2. **权重兼容性**：`_create_fake_bias_for_k_proj` 为 `k_proj` 手动补零偏置，若 HF 模型未来改为显式 bias 可能导致冲突。  
3. **CMVN 文件路径**：`WavFrontend` 默认读取 `cmvn_file`，若路径不存在会抛异常；当前未在 `FunASRProcessor` 暴露配置入口。  
4. **多模态嵌入拼接**：`_merge_multimodal_embeddings` 假设音频嵌入维度与文本对应，若模型配置改动（如不同 `max_source_positions`）可能导致 shape 不匹配。  
5. **依赖版本**：`torchaudio.compliance.kaldi` 在不同平台有不同二进制依赖，若用户运行的 Python 环境缺少对应 wheel，加载会失败。  

**💡 关注建议**  

- **性能调优**：在生产部署时将音频前端独立为服务（如使用 `torch.multiprocessing`）或开启 `torch.set_num_threads`，并对 `batch_size`、`max_concurrency` 做合理限制。  
- **安全防护**：在 API 层做音频时长、采样率、文件大小的白名单校验；对异常音频使用超时/内存阈值保护。  
- **权重加载测试**：新增单元测试覆盖 `FunASRForConditionalGeneration` 的权重加载路径，特别是 `k_proj` bias 的自动生成逻辑。  
- **文档完善**：在 README/Docs 中明确说明所需的额外系统库（如 `sox`、`libkaldi`）以及推荐的 CPU 预处理并行配置。  
- **CI 兼容**：在 CI 中加入 `torch.backends.cudnn.enabled=False` 的 CPU 跑通测试，确保不依赖 GPU 即可完成模型注册与前端功能的验证。  
- **未来扩展**：若后续需要支持实时流式转写，建议把 `WavFrontend` 改造为增量式（frame‑wise）处理，并提供对应的 `StreamingProcessor` 接口。  

---

### [MoE Refactor] Introduce MoERunner abstraction and move execution logic from FusedMoE to DefaultMoERunner (#32344)
**SHA**: `d1481ba` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d1481ba78323bcba5937f5ff74f3a8d27ab54f88)

⚠️ LLM分析失败（已重试3次）: API请求失败: HTTPSConnectionPool(host='integrate.api.nvidia.com', port=443): Read timed out. (read timeout=30)

*暂无分析*

---

#### 🟡 中重要度变更 (21)

### Reapply [Attention][FA3] Update FA3 to include new swizzle optimization (#34043)
**SHA**: `c7914d3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c7914d30f90bc47f1c959d3330666885a0034f7d)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 Flash‑Attention 子模块更新到带有新 **swizzle** 优化的 `FA3` 版本（commit `5824e6e`）。  
2. 将 `BatchDescriptor` 从 `NamedTuple` 改为 `@dataclass(frozen=True)`，并删除其 `relax_for_mixed_batch_cudagraphs` 方法，改用 `dataclasses.replace` 在调度器中生成“放宽”键。  
3. 调整 CUDA‑graph 调度器的键创建与匹配逻辑，使 **FULL** 模式使用非‑uniform、精确 `num_reqs`，**PIECEWISE** 使用放宽的 `num_reqs=None`、`uniform=False`。  
4. 在 Flash‑Attention 与 MLA 后端中，依据 FA3 的调度元数据布局，用 `round_up` 重新计算 `scheduler_metadata` 大小。  
5. 相应更新单元测试，验证新的键匹配行为。  

**🎯 影响范围**  
- `vllm/forward_context.py`（BatchDescriptor 定义）  
- `vllm/v1/cudagraph_dispatcher.py`（键创建、dispatch 逻辑）  
- `vllm/v1/attention/backends/flash_attn.py`、`vllm/v1/attention/backends/mla/flashattn_mla.py`（scheduler_metadata 大小计算）  
- `tests/v1/cudagraph/test_cudagraph_dispatch.py`（测试用例）  
- `cmake/external_projects/vllm_flash_attn.cmake`（外部 Flash‑Attention 版本）  

**💡 关注建议**  
1. **兼容性检查**：确认项目中其他模块（如日志、序列化或监控）仍能接受 `BatchDescriptor` 作为哈希键；`@dataclass(frozen=True)` 会自动生成 `__hash__`，但字段顺序或类型改变可能导致不一致。  
2. **余留调用**：搜索全 repo，确保没有残余对 `BatchDescriptor.relax_for_mixed_batch_cudagraphs` 的调用；若有，需要统一改为 `replace(..., num_reqs=None, uniform=False)`。  
3. **性能验证**：FA3 引入的 swizzle 优化对大 batch size 有显著收益，建议在不同硬件（A100/H100）上跑基准，确认 `scheduler_metadata` 大小计算 (`1 + round_up(max_batch,4)*4`) 与实际显存占用匹配。  
4. **测试覆盖**：现有测试已覆盖主要调度路径，但仍建议补充以下情景：  
   - `CUDAGraphMode.FULL_AND_PIECEWISE` 与 `FULL_DECODE_ONLY` 的混合键匹配；  
   - `lora` 开关开启时（`has_lora=True`）的键创建与匹配；  
   - `max_cudagraph_capture_size` 为 `None` 或极大值的边界情况。  
5. **文档更新**：在 `vllm/docs` 中加入对新 `FA3` 调度元数据布局的说明，以及 `BatchDescriptor` 从 NamedTuple 到 frozen dataclass 的迁移指南，帮助 downstream 开发者快速适配。  

总体来看，此次改动为引入 FA3 的性能提升做了必要的内部结构调整，风险主要集中在 API 改动和键匹配逻辑的细微差异。通过完整回归测试和基准验证，可确保升级平滑且不影响现有部署。

---

### [NVIDIA][test] Tests for flashinfer TRTLLM BF16 MoE (#33715)
**SHA**: `275e0d2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/275e0d2a993b271cfaec9da87711868719d50d8c)

**🎯 变更类型**：功能增强 / 兼容性扩展  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 FlashInfer TRT‑LLM 后端加入 BF16 MoE 支持，并在 env `VLLM_FLASHINFER_MOE_BACKEND` 中实现 “latency”(默认) 与 “throughput” 两种调度策略的切换。  
2. 新增权重布局转换工具 `convert_moe_weights_to_flashinfer_trtllm_block_layout` 并补充单元测试，确保转换后张量维度、元素数、dtype 与原始保持一致。  
3. 完善后端选择逻辑：在 CUDA 平台且 FlashInfer 可用、环境变量开启且 BF16 TRT‑LLM 被硬件支持时，默认使用 `FLASHINFER_TRTLLM`（仅在 `VLLM_FLASHINFER_MOE_BACKEND=latency` 时启用），否则回退至 CUTLASS 或 Triton。  
4. 增加针对不同平台的后端选择测试 `test_unquantized_backend_selection.py`，并在 MoE 相关评测配置中加入对应 env。  
5. 在黑尔文（Blackwell）GPU 上验证 BF16 TRT‑LLM 的前向结果与基准接近，覆盖 `test_unquantized_bf16_flashinfer_trtllm_backend`。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/oracle/unquantized.py`（后端判定）  
- `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`（权重布局转换）  
- MoE 单元测试目录 `tests/kernels/moe/`（新增/扩展）  
- 评测配置 `tests/evals/gsm8k/configs/moe-refactor/*`（环境变量）  
- 相关模型初始化测试 `tests/quantization/test_blackwell_moe.py`  

**💡 关注建议**  
1. **后端兼容性**：确认 `VLLM_FLASHINFER_MOE_BACKEND` 的默认值与文档保持同步，避免用户在未显式指定时仍走 latency 路径导致意外性能变化。  
2. **权重转换**：`convert_moe_weights_to_flashinfer_trtllm_block_layout` 目前仅在 BF16 场景使用，建议在 `__init__` 或加载阶段加入缓存键校验，以防同形状的张量被误复用。  
3. **测试覆盖**：新加入的 `throughput` 选项在 CI 中未被直接验证，建议补充对应的功能/性能基准，以防后端切换后出现维度或调度错误。  
4. **错误信息**：在后端自动回退（如 CUTLASS 不支持 DP）时，可在日志中明确给出 “已回退至 Triton” 的提示，提升可调试性。  
5. **文档更新**：在 `README`、配置说明和环境变量列表中添加 `VLLM_FLASHINFER_MOE_BACKEND` 的说明及推荐取值。  

总体来看，此次改动为 BF16 MoE 在 Blackwell GPU 上提供了更高效的 TRT‑LLM 路径，同时保持向后兼容，影响主要在 MoE 后端选择与权重布局转换，风险有限且已通过单元测试覆盖。继续关注新后端在不同硬件上的行为和性能基准即可。

---

### Make Qwen3VL compatible with Transformers v5 (#34262)
**SHA**: `1e9204b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1e9204bff31f021dce8290d894c7aaf26bb4642e)

**🎯 变更类型**：功能增强（兼容 Transformers v5）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 将 Qwen‑3‑VL 与最新的 Transformers v5 对齐，改为直接使用 `vllm_config.model_config.hf_config` 而非其内部 `text_config`。  
- 在跨节点的 pipeline‑parallel 场景下加入对 `vision_config.deepstack_visual_indexes` 的层数检查，防止 `start_layer` 小于视觉堆叠层数。  
- 移除对 `Qwen3MoeDecoderLayer` 的显式引用及冗余 `__init__` 实现，使模型初始化更加简洁。  
- 通过 `vllm_config.with_hf_config(...)` 为语言模型分支传递正确的子配置。  

**🎯 影响范围**：  
- `vllm/model_executor/models/qwen3_vl.py`  
- `vllm/model_executor/models/qwen3_vl_moe.py`（包括 MoE 版本）  
- 相关的配置加载与权重解析路径  
- 使用 Qwen‑3‑VL 的多节点/多卡部署场景  

**💡 关注建议**：  
- 确认运行环境已升级到 Transformers v5，旧版本可能因属性路径变化导致初始化错误。  
- 在 pipeline‑parallel 部署时，核对模型配置中的 `vision_config.deepstack_visual_indexes` 与 `start_layer`，如有自定义层划分需相应调整。  
- 运行单元测试或快速推理（`vllm run …`）验证视觉‑语言混合输入的前向输出，确保权重加载不受 `with_hf_config` 改动影响。  
- 更新项目文档，说明新版本对 Transformers 依赖的最低要求及配置字段的变化。

---

### [Bugfix][CPU] Fix llama4 inference on CPU (#34321)
**SHA**: `05339a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/05339a7b207e2f32b56c29398c18d577c74cef3b)

**变更类型**：🐞 Bugfix（CPU 端 llama‑4 推理异常）  

**影响范围**  
- `csrc/cpu`：`cpu_fused_moe.cpp`、`torch_bindings.cpp` 新增 `skip_weighted` 参数并在实现里直接把权重设为 1，限定仅在 `topk=1` 时可用。  
- Python 层：`vllm/_custom_ops.py`、`vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`、`vllm/v1/worker/cpu_worker.py` 相应接口和调用处加入 `skip_weighted`（默认 `False`），并在非‑MoE 场景下改写 CPU 亲和度的分配逻辑。  
- 生成文件忽略：`.gitignore` 增加 `csrc/cpu/cpu_attn_dispatch_generated.h`。

**核心分析**  
1. **功能**：在 CPU 上对 llama‑4 的 MoE 计算取消了对 Router 权重的乘法（`apply_router_weight_on_input`），通过 `skip_weighted=True` 直接把权重置为 1，从而避免了原实现在单 Expert 场景下的数值错误。  
2. **安全性**：在 C++ 端加入 `TORCH_CHECK(!skip_weighted || topk_num == 1)` 防止误用；Python 调用端同样添加断言。  
3. **兼容性**：默认保持 `False`，对已有模型不产生影响；但绑定层签名变化（新增 bool）会导致未更新的自定义 C++ 调用或旧版二进制无法加载，需要重新编译或升级。  
4. **性能**：跳过权重乘法可少一次 `mul_`，在 `topk=1` 场景下略有提升，且对 `forward_grouped_gemm` / `forward_torch` 路径的后处理逻辑作了分支，避免不必要的 `view‑mul‑sum`。  
5. **CPU 亲和度**：`cpu_worker.py` 中的逻辑改为以 `world_size * api_process_count` 为基准重新计算 CPU 切片，提升多进程 / DP 组合时的均衡性，但需要确保 `data_parallel_rank_local` 在所有配置下均已设置。

**建议**  
- **单元/集成测试**：覆盖 `topk=1`、`topk>1` 两种路径，确保 `skip_weighted=False` 时行为保持不变。  
- **文档**：在自定义 Op 文档和模型配置说明中标注 `skip_weighted` 参数的含义、使用前提（仅 `topk=1`）以及默认值。  
- **兼容性验证**：在老版本 vLLM 环境下运行一次迁移检查，确认所有二进制重新编译。  
- **回退机制**：若极端情况下仍出现数值差异，可通过显式传 `skip_weighted=False` 强制回到老实现。  

整体来看，此次改动定位明确、限制严谨，对 CPU‑only 推理的 llama‑4 稳定性提升显著，风险主要在 API 变更和多进程 CPU 亲和度计算的边界条件，需要充分测试。

---

### [Docs] Reduce time spent generating API docs (#34255)
**SHA**: `40b8f55` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/40b8f553588371bfd71d30117845cd305a785265)

**变更类型**：其他（文档/注释）  
**重要程度**：🟡 中  

**核心变更概述**  
1. **mkdocs 配置**：  
   - 将 `git-revision-date-localized` 的排除路径从 `api/*` 扩展为 `generated/*`，避免对自动生成的文件重复统计。  
   - 移除 `show_if_no_docstring: true`，让 *mkdocstrings* 只在存在 docstring 时渲染，提升文档生成速度。  

2. **代码层面**：在近 30 处文件中加入简短的类/函数/属性说明（docstring），涵盖 MoE 实现、平台枚举、插件加载、I/O 处理器、异步/同步 Engine 别名等。唯一的业务改动是 `model.py` 中为 `attn_type` 增加了 docstring。  

**影响范围**  
- **文档生成**：`mkdocs` 构建时间将显著下降，尤其在 CI/CD 中可节约数分钟。  
- **运行时**：仅添加 docstring，不会影响代码路径或性能；`__all__`、类型别名等保持不变。  
- **插件/平台枚举**：新增的 docstring 不改变行为，仅提供说明。  

**关注建议**  
- **验证文档完整性**：在本地运行 `mkdocs build`，确认因 `show_if_no_docstring` 被关闭而未出现的 API 页面已被手动补充 docstring。  
- **保持一致性**：后续新建模块请务必添加相应的 docstring，避免因 `show_if_no_docstring` 关闭而导致文档缺失。  
- **CI 检查**：建议在 CI 中加入 `mkdocs build --strict`，确保缺失 docstring 会直接报错，防止未来出现未记录的接口。  
- **兼容性**：该提交不涉及功能逻辑，生产环境无需额外回滚或回测。  

总体而言，此次提交主要是文档层面的优化，对代码运行无实质影响，风险极低。继续保持在关键类/函数上添加简洁、准确的 docstring，即可获得更好的可维护性和自动化文档体验。

---

### [Chore] Move `BaseRenderer` to `base.py` (#34308)
**SHA**: `675a22e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/675a22ed66c4a34be7d2a60cac77078578f49892)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 `BaseRenderer` 从 `renderers/protocol.py` 移动到新建的 `renderers/base.py`，并统一在各渲染器、注册中心以及 `renderers/__init__.py` 中通过 `from .base import BaseRenderer` 引入。  

**🎯 影响范围**：  
- `vllm/renderers/__init__.py`、所有具体渲染实现（`deepseek_v32.py、grok2.py、hf.py、mistral.py、terratorch.py` 等）  
- `vllm/renderers/registry.py`  
- 任何依赖于 `vllm.renderers.protocol.BaseRenderer` 的外部代码或旧版示例  

**💡 关注建议**：  
1. **兼容性**：如果项目仍保留 `protocol.py`，外部用户可能仍通过 `vllm.renderers.protocol.BaseRenderer` 导入，建议在 `protocol.py` 中加入向后兼容的别名或在文档中明确迁移路径。  
2. **文档同步**：更新 README、API 文档以及示例代码中关于 `BaseRenderer` 的导入说明，防止导入错误。  
3. **测试**：完整跑一遍单元/集成测试，尤其是渲染器注册与实例化流程，确认 import 重构未引入循环依赖或漏掉路径。  
4. **类型检查**：`TYPE_CHECKING` 分支仍引用 `BaseRenderer`，确保 `base.py` 中的导出符合原协议定义，避免 mypy 警告。  

完成上述检查后即可安全发布，后续若有其它模块仍引用 `protocol.BaseRenderer`，请逐步迁移至统一的 `base` 入口。

---

### [XPU][9/N] clean up existing ipex code/doc (#34111)
**SHA**: `cb9574e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cb9574eb8528fca1ecd13ef4cb81cd30a643dbb9)

**🎯 变更类型**：重构/清理  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交围绕 Intel XPU 后端进行代码与文档统一，核心工作包括：  
1. 将原先的 `ipex_ops` 统一更名为 `xpu_ops`，并在所有引用处同步更新（`_xpu_ops.py`、attention、paged attention、稀疏索引等）。  
2. 删除已废弃的 IPEX‑specific 环境提示（`LD_PRELOAD` 相关注释），并更新日志信息。  
3. 调整文档中的 OneAPI 版本、依赖描述以及构建指令；同步修改 Dockerfile、测试文件名以及测试入口。  

**🎯 影响范围**  
- **核心后端**：`vllm/_xpu_ops.py`、`vllm/v1/attention/backends/fa_utils.py`、`vllm/v1/attention/ops/paged_attn.py`、`vllm/model_executor/layers/quantization/mxfp4.py`、`vllm/model_executor/layers/sparse_attn_indexer.py`。  
- **平台配置**：`vllm/platforms/cpu.py`（去除 IPEX 注释）。  
- **文档/CI**：`docs/getting_started/installation/gpu.xpu.inc.md`、`docker/Dockerfile.cpu`、`tests/quantization/*`。  

**💡 关注建议**  
1. **兼容性检查**：确保外部项目或旧脚本中仍使用 `vllm._ipex_ops` 的地方已全部迁移，否则会出现 `ImportError`。可在 `vllm/__init__.py` 中保留一个轻量的别名 shim（如 `from ._xpu_ops import xpu_ops as ipex_ops`），帮助平滑升级。  
2. **文档同步**：OneAPI 版本已提升至 2025.3，发布时需同步更新 CI 镜像和依赖镜像的对应 `requirements`，防止因版本不匹配导致构建失败。  
3. **测试覆盖**：新命名的 `test_cpu_quant` 已替代原 `test_ipex_quant`，请确认 CI 在 CPU、XPU 环境下均执行该用例，避免遗漏 XPU 特有的量化路径。  
4. **日志与警告**：`Mxfp4Backend` 的日志信息已改为 “Using xpu backend on XPU”，建议在发布说明中明确该后端已不再依赖 IPEX，以免用户产生误解。  

总体来看，此次改动是一次必要的命名统一与文档梳理，风险主要集中在残留的旧导入路径。完成上述检查后即可安全合入主线。

---

### [CPU] Enable FP16 (Half dtype) support for s390x (#34116)
**SHA**: `d1b837f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d1b837f0ae6a0152d820194a181e809ffaef6864)

**🎯 变更类型**：功能增强（在 s390x 上加入对 FP16（Half）数据类型的原生支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `cpu_attn_impl.hpp` 中去除对 s390x 的排除，使其可以使用 `VecTypeTrait<c10::Half>`。  
- `cpu_types_vxe.hpp` 扩展向量调度宏，加入 `Half`，并实现 `FP16Vec8/FP16Vec16`、对应的 FP32↔FP16 位转换函数 (`fp16_to_fp32_bits` / `fp32_to_fp16_bits`)。  
- 为 `FP32Vec8/FP32Vec16` 添加从 FP16 向量构造的实现；实现 `FP16Vec8/FP16Vec16` 从 FP32 向量的构造，利用上述位转换实现 IEEE‑754 舍入。  
- `storeFP32` 为 `c10::Half` 添加基于标量位操作的安全转换。  
- 在 `mla_decode.cpp` 中仅在 Power 上保留特殊向量路径，避免在 s390x 仍然走 FP32 路径。  

**🎯 影响范围**  
- **核心向量算子**：`csrc/cpu/*` 中的所有向量类型、调度宏、`cpu_attn_impl.hpp`、`mla_decode.cpp`。  
- **模型推理路径**：所有使用 `c10::Half`（FP16）进行 Attention、MLA 解码的 CPU 后端。  
- **编译配置**：`__s390x__` 平台的宏判断和 Makefile/构建脚本（如果有）需要确认已开启 `-march=z15` 之类的指令集支持。  

**💡 关注建议**  
1. **兼容性测试**：在真实的 IBM Z（s390x）机器上跑全套单元/集成测试，尤其是 Softmax、注意力、矩阵乘法等数值密集路径，验证 IEEE‑754 舍入与 PyTorch CPU 实现保持一致。  
2. **性能基准**：对比原先的 FP32‑fallback，记录 FP16 向量路径的吞吐和延迟，确保新实现带来预期的加速（至少 2×）。  
3. **代码审查**：关注位转换实现中的边界（NaN/Inf、次正规数、溢出到 Inf）是否与 PyTorch 在其他平台保持一致；若有差异，可能导致数值不稳定。  
4. **文档/编译选项**：在项目 README 或 CPU 编译指南中加入 “在 s390x 上启用 FP16 支持 – 需要 GCC ≥ X 并打开 `-mzarch` / `-march=z15`”。  
5. **回滚路径**：若在某些旧版 s390x 环境出现非法指令错误，可通过宏 `VLLM_DISABLE_S390X_FP16`（自行添加）临时回退到 FP32 实现。  

总体来看，此次改动为 s390x 平台提供了实用的 FP16 向量加速，影响集中在 CPU 向量库和注意力实现，需通过数值与性能验证确保在生产环境安全使用。

---

### [Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides (#34279)
**SHA**: `d7982da` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d7982daff5334b9465b29fa943a1954c064ab226)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/model_executor/layers/fused_moe/fused_moe.py` 中为 Triton kernel 参数显式声明 `int64` 类型的 stride，防止在不使用 chunking 的 fused MoE IMA 路径上出现 32‑bit 整型溢出导致的错误。  

**🎯 影响范围**  
- `fused_moe_kernel_gptq_awq` 与 `fused_moe_kernel` 两个 Triton kernel 的签名。  
- 调用这些 kernel 的上层代码（MoE 相关的 forward 实现）。  
- 只影响 GPU（CUDA/Triton）执行路径，不改变 CPU 回退路径。  

**💡 关注建议**  
1. **兼容性**：确保所有调用处在传入 stride 时使用 `int64`（如 `tensor.stride()` 可能返回 `int64`，若手动计算应强制 `int(..., dtype=torch.int64)`）。  
2. **重新编译**：用户在升级后需要重新编译 Triton kernel（`python setup.py build_ext --inplace`）或重新安装 `vllm`，否则仍会使用旧的 kernel。  
3. **回归测试**：重点跑不带 chunking 的 MoE 推理基准，验证数值一致性与吞吐量未受影响。  
4. **文档/注释**：建议在函数注释中加入 “stride 参数必须为 int64” 的说明，避免未来出现相同类型遗漏。  

总体来看，此次改动修复了潜在的整数溢出 bug，对功能正确性影响显著，性能基本持平，风险主要在调用方未显式使用 64 位 stride 时可能产生运行时错误。请在 CI 中加入对应的 Triton 参数类型检查。

---

### [Plugin] Simplify IO Processor Plugin interface (#34236)
**SHA**: `c9a1923` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c9a1923bb470f79a33963ad80cc8ad12bab2ad52)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. **IOProcessor 接口大幅简化**：`parse_request`、`validate_or_generate_params`、`output_to_response` 已被废除，改为 `parse_data`、`merge_sampling_params`、`merge_pooling_params`。同时在基类中加入向后兼容的警告实现。  
2. **LLMEngine.encode & pooling serving**：统一使用 `merge_*_params` 生成参数序列，去掉 `as_iter`/`as_list` 辅助，直接通过 `_params_to_seq` 处理，并在插件路径下统一通过 `parse_data`、`pre_process*`、`post_process*` 完成前后处理。  
3. **IOProcessorResponse** 改为透明包装，仅在需要时使用旧 `output_to_response`（已标记弃用）。  
4. **utils/collection_utils** 中 `as_iter` 被删除，相关调用已改为直接使用 `prompt_to_seq`/`_params_to_seq`。  

**🎯 影响范围**  
- `vllm/plugins/io_processors/interface.py`（核心抽象）  
- `vllm/entrypoints/llm.py`（encode 入口）  
- `vllm/entrypoints/pooling/pooling/serving.py`（在线 pooling）  
- 所有自研或第三方 IOProcessor 插件（如 tests/plugins/... 示例）  
- 测试代码、文档（io_processor_plugins.md）  

**💡 关注建议**  
1. **插件迁移**：所有自定义插件必须实现 `parse_data`，并自行提供 `merge_sampling_params` / `merge_pooling_params`（或继续使用旧方法并准备接受 `DeprecationWarning`）。  
2. **删除旧实现**：在插件代码中移除对 `IOProcessorRequest`、`IOProcessorResponse` 的显式构造，直接返回业务对象，或使用 `IOProcessorResponse` 包装。  
3. **兼容性检查**：运行完整测试套件，确保没有残留 `as_iter`、`parse_request`、`validate_or_generate_params`、`output_to_response` 的调用。  
4. **默认 task**：`merge_pooling_params` 现在默认 `task="plugin"`；若业务需要其他 task，请显式覆盖。  
5. **警告监控**：在升级到 v0.19 前，留意运行时的 `DeprecationWarning`，确保所有插件已完成迁移。  

通过上述调整，可确保插件在新版 vLLM 中平滑工作，并获得更清晰、统一的 IO 处理流程。

---

### [XPU][7/N] enable xpu fp8 moe (#34202)
**SHA**: `b482f71` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b482f71e9f25ce848c1a53e71e332953d97b0aac)

**🎯 变更类型**：功能增强（为 XPU 平台新增 FP8 MoE 支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 XPU 依赖升级至 `vllm_xpu_kernels‑0.1.2`。  
2. 在 `fused_moe` 中加入 `Fp8MoeBackend.XPU`，并在后端映射、选择、格式检查等位置加入对应分支。  
3. 新增 `XPUExpertsFp8` 类（继承自 `XPUExperts`），通过 `is_fp8` 标记让底层 `xpu_fused_moe` kernel 走 FP8 路径，支持 weight‑static + activation‑dynamic 量化。  
4. 导出 `XPUExpertsFp8`，并在 `__init__` 中加入 `FusedMoEQuantConfig`、`kFp8DynamicTensorSym` 的引用。

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/*`（后端选择、实现类）  
- `requirements/xpu.txt`（依赖升级）  
- `vllm/platforms`（XPU 环境下的入口）  

**💡 关注建议**  
1. **兼容性**：`XPUExperts` 默认 `is_fp8=False`，保持原有行为；但在未安装 `vllm_xpu_kernels‑0.1.2` 或未提供 FP8 量化配置时应抛出明确错误。  
2. **测试**：建议补充 XPU‑FP8 单元测试，覆盖 `backend_to_kernel_cls`、`select_fp8_moe_backend` 以及 `apply` 中 `is_fp8` 参数的传递。  
3. **文档/示例**：更新 XPU 运行指南，说明如何构造 `FusedMoEQuantConfig`（weight‑static + activation‑dynamic）并启用 `Fp8MoeBackend.XPU`。  
4. **依赖管理**：确保 CI 环境可以下载 `vllm_xpu_kernels‑0.1.2`，或在无 XPU 环境下优雅回退。  
5. **异常处理**：`_supports_quant_scheme` 现在接受 `(kFp8StaticTensorSym, kFp8DynamicTensorSym)`，若后端 kernel 未实现该组合，需要在 `apply` 前检测并给出友好提示。  

总的来说，新增的 XPU FP8 MoE 后端为 Intel 加速器提供了更高的算力利用率，只要在上述细节上做好兼容、测试与文档工作，即可安全发布。

---

### [Kernel] Apply 256bit LDG/STG To Activation Kernels (#33022)
**SHA**: `1485396` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1485396abb7c575d0196c2f52f4cdff7f9280a19)

**变更类型**：性能优化（CUDA 激活算子向 256 bit 向量化升级）  
**重要程度**：🟡 中  

**核心改动**  
1. 新增 `u32x8_t` 与 `ld256 / st256` 汇编封装，实现 256 bit（8 × 32 bit）全局负载/存储。  
2. 引入 `VecTraits`、`PackedTraits`、`packed_*` 系列函数，统一处理 `float2 / __half2 / __nv_bfloat162` 的打包、解包、乘法与激活。  
3. `act_and_mul_kernel`、`act_and_mul_kernel_with_param`、`activation_kernel` 等核心 CUDA kernel 重新实现，依据运行时 `use_vec` 与 `use_256b`（CC≥10）选择 128 bit 或 256 bit 向量路径。  
4. 宏 `LAUNCH_ACTIVATION_GATE_KERNEL*`、`LAUNCH_ACTIVATION_KERNEL` 也相应改写，自动检测 GPU 架构、token 数量以及对齐情况，以决定是否启用向量化。  
5. 为 SILU、GELU、FATRELU、SWIGLU 等激活函数补充对应的 “packed” 实现，保持数值等价。  

**影响范围**  
- `csrc/activation_kernels.cu`（所有激活与 gating 相关 kernel）  
- 对外 API：`silu_and_mul`、`gelu_and_mul`、`fatrelu_and_mul`、`activation_kernel` 等入口函数的调用路径均会走新实现。  
- 依赖此文件的 Python 扩展（`vllm.ops.activation`）以及上层的推理调度逻辑。  

**注意事项 & 建议**  

| 项目 | 建议 |
|------|------|
| **对齐检查** | 当前仅在 `use_vec` 前判断 `d % vec_size == 0`，但 `ld256 / st256` 仍要求 32‑byte 对齐。建议在进入 256‑bit 路径前再显式检查 `is_32byte_aligned`，否则在非对齐数据上会产生未定义行为。 |
| **向后兼容** | 对旧显卡（< SM70）仍会走 128‑bit 或标量路径，保证 `cc_major` 判断准确；若未来希望兼容 SM80 以下的 256‑bit（如部分 Turing）请补充对应 ISA（`ld.global.nc.v8.u32` 仅在 SM≥10）。 |
| **数值一致性** | `packed_mul` 在 BF16/FP16 使用 `__hmul2`，在 FP32 使用手写乘法，需确认在极端值（NaN/Inf）下行为与标量 `*` 完全一致。可在单元测试中加入随机极值对比。 |
| **编译选项** | 256‑bit 汇编指令需要 `-arch=sm_80` 或更高。确保 CI / packaging 脚本在发布 wheel 时使用足够的 `-gencode`，否则会回退到 128‑bit 代码导致性能回退。 |
| **性能基准** | 新增的向量路径在 `d` 较大且 token 数量>128 时才启用，建议在 PR 中提供 `torch.cuda.get_device_capability` 不同 CC 的基准（FP16/BF16/FP32）对比，以便评估提升幅度。 |
| **代码可维护性** | `VecTraits` 与 `PackedTraits` 结构体写法清晰，可考虑把 `use_vec`、`use_256b` 参数合并为统一的 `VecConfig`，减小宏展开的复杂度。 |
| **文档** | 在 README 或 API doc 中注明：*该实现仅在 SM10（Ada）或以上 GPU、且输入张量 32‑byte 对齐、长度为 32 的倍数时可获得 2×‑3× 加速*。 |

总体来看，此次提交为激活算子引入了显著的向量化加速，核心逻辑已完成迁移并保持原有接口不变。只要在 CI 中覆盖对齐检查、不同显卡的回退路径以及数值一致性，即可安全合并。

---

### [Misc] Clean up validation logic in input processor (#34144)
**SHA**: `b5dcb37` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b5dcb372e4ba04043a012475cea7cc901412f25a)

**🎯 变更类型**：其他（代码清理 & 验证逻辑重构）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将多模态输入的校验逻辑从 `InputProcessor._validate_model_input` 中抽离，新增 `_validate_prompt_len` 与 `_validate_model_inputs` 两层封装，保持对 prompt 长度、空 prompt、token‑id 范围的统一检查。  
- 新增 `supports_mm_inputs`、`mm_encoder_cache_size`、`skip_prompt_length_check` 标记，统一在构造函数中计算，避免在每次调用时重复查询。  
- 在 `MultimodalBudget` 中保存创建的 `processor` 实例（`self.processor = processor`），供后续缓存统计使用。  
- 测试文件中删除 `skip_tokenizer_init=True` 参数，保持与新版初始化行为一致。  

**🎯 影响范围**  
- `vllm/v1/engine/input_processor.py`（核心校验与多模态缓存逻辑）  
- `vllm/multimodal/encoder_budget.py`（新增 processor 成员）  
- `tests/v1/engine/test_process_multi_modal_uuids.py`（测试适配）  

**💡 关注建议**  
1. **向后兼容**：`skip_prompt_length_check` 的默认值已从 `False` 改为根据 `MultiModalBudget` 动态设定，确认旧模型配置（未声明 `skip_prompt_length_check`）仍能正常运行。  
2. **性能**：缓存大小与校验在构造阶段一次性计算，后续路径更轻量，但注意 `self.processor` 持有对象可能导致额外引用，验证是否会引起循环引用或内存泄漏。  
3. **异常信息**：错误提示保持原有表达，若未来加入更多多模态模型，建议统一错误码/文案，便于上层捕获。  
4. **文档与示例**：更新 `InputProcessor` 初始化参数说明，尤其是 `skip_prompt_length_check`、`mm_encoder_cache_size` 的含义，以防使用者误解。  

总体而言，此次改动提升了代码可读性与模块划分，风险主要在新成员持有和默认行为的兼容性，建议在 CI 中加入针对老配置的回归测试。

---

### [Misc] Add run one batch script that supports profiling (#32968)
**SHA**: `ba0511f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ba0511fd80b95d05ffab867cce54f3590e57a7fc)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `examples/offline_inference` 新增 `run_one_batch.py` 脚本，实现一次批量推理的演示并加入可选的 Torch‑Profiler 支持。通过 `FlexibleArgumentParser` 完整复用 `EngineArgs`，并新增 batch、prompt、profiling 参数，自动构造固定长度的 prompt，完成生成后打印结果。

**🎯 影响范围**  
- 新增的示例脚本（仅影响示例目录），不影响核心库运行时。  
- 与 `vllm.config.ProfilerConfig`、`LLM.generate` 的交互新增，可在用户自行开启 profiling 时触发。  
- 依赖 `torch` profiler，若环境缺失会在运行时抛异常。

**💡 关注建议**  
1. **文档/使用说明**：在 README 或 examples 列表中加入该脚本的调用方式、`--profile` 取值及 `--profile-dir` 必要性说明。  
2. **输入校验**：对 `batch_size`、`prompt_size`、`max_tokens` 等参数加入范围检查，防止负数或过大导致 OOM。  
3. **默认模型声明**：`parser.set_defaults(model="meta-llama/Llama-3.2-1B-Instruct")` 可能在无网络环境下失败，建议在脚本顶部给出提示或提供本地路径选项。  
4. **ProfilerConfig 逻辑**：`profile == "both"` 时 `max_iterations` 被设为 0，实际意义不明；若意图同时捕获 prefill 与 decode，需分别配置 `delay_iterations` 与 `max_iterations`。  
5. **异常处理**：`llm.start_profile()` / `stop_profile()` 之间若生成过程抛异常，可能导致 profiler 未关闭，建议使用 `try…finally` 包裹。  
6. **测试覆盖**：新增单元测试或集成示例，验证不同 `--profile` 组合下脚本能正常启动并生成输出。  

整体来看，此脚本为用户提供了快速 profiling 的入口，代码结构清晰且复用已有 CLI 参数。关注上述细节可提升可用性并避免潜在的运行时错误。

---

### [Misc] Add pre-commit hook to catch boolean ops in with-statements (#34271)
**SHA**: `c4b9e67` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c4b9e6778f9d8054c1665b2d1c2cb0ee36e9e2f5)

**🎯 变更类型**：功能增强（新增 lint 检查）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在项目的 pre‑commit 配置中加入了 `check-boolean-context-manager` 检查，配套实现 `tools/pre_commit/check_boolean_context_manager.py`。该脚本使用 `ast` 解析 Python 文件，检测 `with a() and b():`、`with a() or b():` 等布尔运算符被误用作上下文管理器的情况，并在检测到时给出明确报错提示。  

**🎯 影响范围**  
- **开发流程**：所有使用 pre‑commit 的贡献者将在提交前收到该校验，防止潜在的 “只进入右侧/左侧上下文管理器” 逻辑错误。  
- **CI/CD**：若项目 CI 中启用了 pre‑commit，此检查会在 CI 端阻断含错误的代码。  
- **运行时**：对库本身的功能不产生直接影响，仅是静态分析工具。  

**💡 关注建议**  

1. **误报排查**  
   - 当前实现仅检查 `ast.BoolOp`，但在某些合法写法中（例如 `with (a() and b()) as cfg:` 用于计算布尔值后再传给单一上下文管理器）可能产生误报。建议在报错信息中提供 `--ignore` 选项或通过注释 `# pre-commit: no‑bool‑with` 来手动排除。  

2. **跨平台路径**  
   - `open(filepath, encoding="utf-8")` 在 Windows 上对非 UTF‑8 文件会返回空列表，可能导致遗漏。可以考虑使用 `tokenize.detect_encoding` 与 `errors='ignore'` 的兼容方案，以免因编码问题导致检查失败。  

3. **性能与并行**  
   - 该脚本会逐文件读取并一次性解析 AST，单文件开销微小。若项目文件非常多，可在 `pre-commit` 配置中加入 `stages: [commit]` 并使用 `require_serial: false`，让 pre‑commit 并行执行，以提升整体检查速度。  

4. **文档与 CI 更新**  
   - 在项目的贡献指南或 pre‑commit 文档中加入该检查的说明与示例，帮助新贡献者快速了解错误原因。  
   - 确认项目的 CI 配置（如 GitHub Actions）已包含最新的 `.pre-commit-config.yaml`，防止本地生效而 CI 未生效的情况。  

5. **单元测试**  
   - 为新脚本添加几组正负样例的 pytest，用于验证 `check_file` 的返回值与错误提示格式，避免未来改动导致回归。  

总体来看，此次变更提升了代码质量防御能力，风险主要在潜在误报和编码兼容性上，按上述建议进行细化即可平滑落地。

---

### [Misc][Spec Decode] support different load config for draft model (#34022)
**SHA**: `6f2f59f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6f2f59f2b333151aac19f8ca7bf71d83c1a7c068)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `SpeculativeConfig` 中新增 `draft_load_config`，允许为草稿模型单独指定加载配置；`get_model` 接口同步接受 `load_config` 参数，并在 Eagle‑spec‑decode 中使用该参数，以实现草稿模型的自定义加载行为。  

**🎯 影响范围**  
- `vllm/config/speculative.py`（配置结构）  
- `vllm/model_executor/model_loader/__init__.py`（模型加载入口）  
- `vllm/v1/spec_decode/eagle.py`（spec‑decode 实现）  

**💡 关注建议**  

1. **向后兼容**：新增字段默认 `None`，仍会回退使用目标模型的 `load_config`，兼容旧配置。确认 `VllmConfig` 的序列化/反序列化不因缺失该字段而报错。  
2. **校验逻辑**：`SpeculativeConfig.compute_hash` 需要把 `draft_load_config` 纳入哈希；若忘记更新，可能导致缓存失效或错误的模型复用。  
3. **文档/示例**：在 README/配置文档中补充 `draft_load_config` 的说明，示例展示如何为草稿模型使用不同的 `device_map`、`dtype` 等。  
4. **测试覆盖**：新增单元测试，验证：  
   - 未提供 `draft_load_config` 时仍使用主模型的加载配置；  
   - 提供后草稿模型按指定配置加载（如不同 `torch_dtype`）。  
5. **错误信息**：若用户误将 `draft_load_config` 与主模型的 `load_config` 混用，建议在 `model_loader.get_model` 中加入明确的日志/异常提示，以防调参时产生难以定位的行为差异。  

整体来看，此改动为多模型推理提供了更细粒度的加载控制，影响范围局限于配置与加载路径，风险低，但需补全文档、测试及哈希计算，确保一致性和可维护性。

---

### [BugFix] Fix async EPLB hang with DeepEP LL all2all backend (#32860)
**SHA**: `bb2fc8b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bb2fc8b5e7beca9c5749e464b4607c753db0b630)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在使用 DeepEP low‑latency(all2all) 后端且开启异步 EPLB（Expert Parallel Load Balancing）时，原先会出现 NCCL 与 DeepEP LL 协作 kernel 的相互阻塞导致的死锁。新增 `override_envs_for_eplb` 工具函数，在满足 `data_parallel > 1、enable_eplb、eplb_config.use_async、all2all_backend=="deepep_low_latency"` 四个条件时，默认把 `NCCL_MAX_CTAS` 设为 8，以限制 NCCL 的并发线程数，留出 SM 资源给 DeepEP LL，破除循环等待。该函数在 `gpu_worker.init_worker_distributed_environment` 中调用，确保在分布式环境初始化阶段即完成覆盖。

**🎯 影响范围**  
- `vllm.distributed.eplb`（新增工具模块）  
- `vllm.v1.worker.gpu_worker`（初始化流程）  
- 任何开启数据并行、EPLB 且使用 DeepEP LL 后端的部署环境（多卡训练/推理）

**💡 关注建议**  
1. **测试验证**：在本地多卡、不同 `NCCL_MAX_CTAS` 设置下跑一次完整的推理/生成流程，确保不再出现卡死。对比开启/关闭 `use_async` 的行为是否一致。  
2. **环境兼容**：若用户已有自定义的 `NCCL_MAX_CTAS`（且为整数），本实现会保持原值不变；若需要更大或更小的 CTAs，需自行在启动脚本中覆盖。建议在文档中说明该变量的默认阈值 8 及其调优方法。  
3. **日志监控**：`logger.info_once` 只在首次触发时输出，运维可以通过全局日志检查是否成功写入 `EPLB: Setting NCCL_MAX_CTAS=8`。若未出现，说明条件未满足或环境变量已预设。  
4. **回滚预案**：若在特殊硬件上（如 SM 数极少的卡）出现性能下降，可通过显式设置 `NCCL_MAX_CTAS` 为更大值回滚。  

总体而言，此次修改通过限制 NCCL 的资源占用，消除了特定配置下的死锁风险，对已有用户影响最小，且只在满足明确条件时生效，属于安全的功能补丁。

---

### [Perf] Move eplb rebalance algo to async thread (#30888)
**SHA**: `6713294` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/67132945bbad23233fd583e6106ebebe859c8366)

**🎯 变更类型**：性能优化 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 EPLB（expert‑parallel load‑balancing）重平衡逻辑从主线程迁移到独立的异步线程，实现层级别的并行调度；新增 `EplbStats`、`run_rebalance_experts`、`get_eplb_group` 等结构/接口；相应地修改 `transfer_layer` 参数、锁与 CUDA event 的同步方式，并在测试中更新调用签名。  

**🎯 影响范围**  
- `vllm/distributed/eplb/*`（`async_worker.py`、`eplb_state.py`、`rebalance_execute.py`）  
- `vllm/distributed/parallel_state.py`（EPLB 进程组创建、销毁）  
- 相关单元测试 `tests/distributed/test_eplb_execute.py`  

**💡 关注建议**  
1. **线程安全**：异步线程中使用 `while not lock.acquire(blocking=False): await asyncio.sleep(0)` 可能导致密集轮询，建议加上指数回退或 `await asyncio.to_thread(lock.acquire)` 以免 CPU 占用过高。  
2. **CUDA event 同步**：`window_ready_event` 在主线程 `clone` 之后立即 `record`，确保在所有 GPU 上可见；若 `clone` 为异步操作，需确认 event 完全同步后再在异步线程读取。  
3. **EPLB 组初始化**：`get_eplb_group` 只在 `enable_eplb=True` 时创建，需在所有调用点加上 guard，防止未初始化时报错。  
4. **参数兼容性**：`transfer_layer` 接口改为层级输入，确保旧的调用点已全部迁移；若外部插件仍使用旧签名，需提供兼容包装。  
5. **性能基准**：新增异步调度应在多节点、多 GPU 场景下跑全链路基准，验证相较于同步实现的提升幅度与无新显存泄漏。  
6. **文档与日志**：新增 `EplbStats`、async‑worker 启动日志已加入，建议在 README/开发者指南中补充 EPLB 异步模式的使用说明与调试技巧。  

总体来看，改动把耗时的负载统计与映射计算搬到后台，有望降低主训练循环的阻塞，但需重点审查跨线程 CUDA event 与锁的协同，避免潜在的竞争或活锁。

---

### [Feature] Warn about unrecognized environment variables (#33581)
**SHA**: `f0ca067` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f0ca0671c70fae6d1562127e3330eeaedf4abb3f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `EngineArgs` 中新增 `fail_on_environ_validation` 参数，并在 CLI 中提供对应开关。  
- `vllm.envs.validate_environ` 用于遍历当前环境变量，检测以 `VLLM_` 开头但未在 `environment_variables` 列表中声明的变量；在硬失败模式下抛出 `ValueError`，否则发出 `warning`。  
- 新增单元测试 `test_unrecognized_env` 验证两种行为（硬失败与宽容）。

**🎯 影响范围**  
- `vllm/engine/arg_utils.py`（EngineArgs 参数及 CLI 解析）  
- `vllm/envs.py`（环境变量校验实现）  
- 测试目录 `tests/config/test_config_generation.py`（新增验证用例）  

**💡 关注建议**  
1. **向后兼容**：`fail_on_environ_validation` 默认 `False`，已保持兼容，建议在文档中明确默认行为及何时需要开启硬失败。  
2. **错误信息**：抛出的 `ValueError` 只包含变量名，若能加入建议的正确变量列表或提示可更友好。  
3. **日志级别**：当前使用 `logger.warning`，若项目整体对未知变量的宽容度较低，考虑提供 `--environ-validation-level` 之类的细粒度控制。  
4. **性能**：`validate_environ` 在每次 `EngineArgs.create_engine_config` 时遍历全部环境变量，开销极小但可在生产环境中通过一次性缓存避免重复检查。  
5. **测试清理**：`test_unrecognized_env` 在设置环境变量后未确保在所有分支都清理，建议在 `finally` 中统一 `pop`，防止对后续测试产生副作用。  

总体来看，此次改动为用户提供了更安全的配置检查手段，风险可控，建议合并。

---

### [SM100] Resubmit FMHA FP8 prefill for MLA (#31195)
**SHA**: `578977b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/578977bb5ed208c62cf9cff80d955836775e0d24)

**变更类型**：功能增强  
**重要程度**：🟡 中  

### 变更概要  
1. **配置新增** `use_prefill_query_quantization`，用于在 prefill 阶段对 query 进行量化。  
2. **后端适配**：实现 `backend_supports_prefill_query_quantization`，仅在 GB200（device capability 100）且使用 FlashInfer 或 TRT‑LLM ragged DeepSeek 时开启 FP8‑prefill。  
3. **MLA 构建层**：  
   - 新增 `q_data_type` 与 `output_dtype`，在 `determine_prefill_query_data_type` 中决定是否使用 FP8。  
   - `ChunkedContextMetadata` 增加 `output_dtype`。  
   - Prefill 路径（FlashInfer、TRT‑LLM）使用新 dtype，并在 TRT‑LLM ragged 路径显式分配 `out`。  
4. **缓存读取**：针对 FP8‑prefill，改为 `ops.cp_gather_cache`（不做 dequant），其余情况仍走 `ops.gather_and_maybe_dequant_cache`。  
5. **前向计算**：在 `forward_mha` 与 `_compute_prefill_context` 中依据 `prefill_metadata.q_data_type` 将 `q/k/v` 转为 FP8。  
6. **测试更新**：`test_mla_backends` 改用 `MLAAttentionSpec` 并把 `cache_dtype_str` 传入。

### 影响范围  
- **核心模块**：`vllm/model_executor/layers/attention/mla_attention.py`、`vllm/config/attention.py`、KV‑cache spec、prefill 后端实现。  
- **后端**：FlashInfer、TRT‑LLM ragged DeepSeek（新支持 FP8），CuDNN / FlashAttention 仍不支持。  
- **测试**：MLA 后端正确性验证。  

### 关注建议  
- **设备兼容性**：仅在 GB200（sm >= 100）上启用 FP8，部署前请确认 GPU 驱动/CUDA 版本支持相应 FP8 kernel。  
- **配置一致性**：`--kv-cache-dtype=fp8*` 必须配合 `--attention.use_prefill_query_quantization` 才会生效，未满足条件时会回退到模型 dtype 并打印提示。  
- **性能回归**：在非支持后端或低端 GPU 上仍会走原路径，建议在 CI 中加入不同 GPU（如 A100、H100）下的基准测试，验证 FP8 prefill 是否带来预期加速且数值误差在可接受范围。  
- **代码维护**：`backend_supports_prefill_query_quantization`、`determine_prefill_query_data_type` 已加入 `functools.cache`，确保在多进程/多实例环境中不产生隐藏状态。若后续新增后端，请同步更新此函数的白名单。  

总体而言，此次改动为 MLA 引入了 FP8‑prefill 查询量化的实验性支持，需在兼容设备上进行充分验证，避免在不支持的环境中出现 dtype 不匹配或性能退化。

---

### [Misc] Introduce ec_both role EC (encoder cache) connector (#34182)
**SHA**: `33bcd3d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/33bcd3dc3bf4d581c051400c8d9bb9433d2c87af)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 EC（Encoder‑Cache）传输引入新的角色 `ec_both`，即同一实例既可生产也可消费缓存。  
2. `ECTransferConfig` 的 `ec_role` 类型由原来的 `"ec_producer"` / `"ec_consumer"` 扩展为可接受 `"ec_both"`。  
3. `ECConnectorBase` 新增 `is_consumer` 属性，并在 `V1Worker` 中改为在 **消费者或两者角色** 时调用 `start_load_caches`，实现缓存加载。  

**🎯 影响范围**  
- `vllm/config/ec_transfer.py`（配置类型和值域）  
- `vllm/distributed/ec_transfer/ec_connector/base.py`（Connector 基类）  
- `vllm/v1/worker/ec_connector_model_runner_mixin.py`（Worker 启动逻辑）  

**💡 关注建议**  
1. **兼容性检查**：确认在未显式指定 `ec_role` 时默认值仍能兼容旧版，仅生产或消费的路径不受影响。  
2. **属性同步**：`VllmConfig.ec_transfer_config` 必须正确填充 `is_ec_consumer`（依据新 `ec_role`），否则 `ECConnectorBase` 会在初始化时抛 `ValueError`。建议在配置解析处加入对应的映射逻辑。  
3. **文档与测试**：更新使用说明，明确 `ec_both` 场景的部署要求；补充单元测试覆盖 `is_consumer` 为 `True` 时的缓存加载路径。  
4. **运行时判断**：已有代码仍通过 `ec_connector.is_producer` 判断生产行为，新增 `is_consumer` 不会破坏旧逻辑，但请审查其他可能直接使用 `ec_role` 字符串的分支，防止遗漏。  
5. **异常安全**：`start_load_caches` 现在在 `ec_both` 场景会被调用两次（一次在生产路径，一次在消费路径），确保内部实现是幂等或已做好重复调用的防护。  

总体而言，此次改动为 EC 缓存的双向使用提供了便利，但需在配置解析、文档以及测试层面补全，以避免在混合角色部署时出现隐藏的初始化或加载错误。

---

#### 🟢 低重要度变更 (27)

### Responses harmony system message structured (#34268)
**SHA**: `1b87565` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1b8756562e1cc50bade1335e52aa36547d62e477)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `responses` 接口中新增对系统消息 `content` 为结构化列表（`type: input_text`）的提取逻辑，并相应扩展测试用例验证该行为以及词数约束。

---

### Make JAIS compatible with Transformers v5 (#34264)
**SHA**: `0f5e55e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0f5e55e7a8de564407ee54ad8ab5ab1d2cb3bb5a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  

**📋 摘要**：在 `jais.py` 中删除了对 `config.add_cross_attention` 必须为 `False` 的断言，以兼容 Transformers v5 的交叉注意力配置。仅影响模型初始化的内部检查，不改变功能实现。

---

### Patch protobuf for CVE-2026-0994 (#34253)
**SHA**: `5045d5c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5045d5c9831a3a4a423a409ccea521d299a43a9a)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `requirements/build.txt` 与 `requirements/common.txt` 中将 protobuf 版本限定为 `>=5.29.6`，并排除 6.30‑6.33 系列有安全漏洞的版本，以修复 CVE‑2026‑0994。

---

### [Frontend] Exploit tokenizers "new stream" in FastIncrementalDetokenizer (#34217)
**SHA**: `e09546c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e09546cf05f12c041083c289c24ecb48896f9620)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 FastIncrementalDetokenizer 适配到 tokenizers ≥0.22.0 的原生 DecodeStream（支持 `ids` 参数），简化预填充逻辑，删除冗余字符过滤代码，优化 `output_token_ids` 与 `get_next_output_text` 的实现。

---

### [Doc] Update Marlin support matrix for Turing (#34319)
**SHA**: `786806d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/786806dd4431959ac7b370838ab3a9aa5ea93ef3)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在量化特性文档中，将 Marlin 支持矩阵更新为加入 FP4（并注明 Turing 不支持），并在 FP8 文档中说明 Turing/Ampere GPU 可通过 Marlin 进行 W8A16（权重量化）支持。

---

### [Misc] Bump `fastsafetensors` version for latest fixes (#34273)
**SHA**: `7950402` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/79504027ef93a742846856e81fc25de369dc5e22)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将项目中所有 `fastsafetensors` 的依赖版本从 `0.1.10` 提升至 `0.2.2`，包括 `requirements`、`test` 文件及 `setup.py`，以获得最新的多GPU内存使用修复。

---

### [torch.compile] Enable AR+rms fusion by default available for `-O2` (#34299)
**SHA**: `addac0e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/addac0e65343e4c24a109975d54c4673bbfb029c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `eliminate_noops` 默认设为 `True`，新增 `enable_allreduce_rms_fusion` 判断函数并在优化级别 O2 及以上默认开启 AR+RMS 融合，实现 `-O2` 编译时的默认融合行为。

---

### [Bugfix] Fix weight naming in Qwen3.5 (#34313)
**SHA**: `0b20469` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0b20469c627e94060d1015170b186d19de1db583)

**🎯 变更类型**：代码重构（Bugfix）  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `qwen3_5.py` 中将 `ColumnParallelLinear` 的权重前缀从错误的 `in_proj_ba` 修正为正确的 `in_proj_b`，解决模型加载时权重命名不匹配的问题。

---

### [ModelBash][DSR1 NVFp4] Removed Bf16 Bias Cast (#34298)
**SHA**: `9b17c57` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9b17c57460bb5f6595f27b43e43caba144a8ec3c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除了对 routing_bias 的 bf16 强制转换，改为直接使用原始 bias；同时将 `layer` 参数注解为 `FusedMoE`，提升类型准确性，代码更简洁。

---

### Threshold fix wvSplitk for occasional CI fails (#34013)
**SHA**: `1b3540e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1b3540e6c6d3833118d448c3246434de1a60e558)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 量化 kernel 测试中改用 `torch.testing.assert_close`，并为大 K（≥32 KB）无 xnorm 场景放宽容差阈值，解决偶发 CI 失效问题。

---

### [Bugfix] Fix benchmark_moe.py inplace assertion with torch >= 2.9 (#34149)
**SHA**: `7a048ee` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7a048ee65f0b8da2c2493ef76cbee89cf612baa6)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `benchmark_moe.py` 中引入 `disable_inplace()` 检测，动态决定 `inplace` 参数，修复了 Torch ≥ 2.9 下的断言错误。

---

### [Bugfix][DeepSeek-V3.2] fix fp8 kvcache type cast (#33884)
**SHA**: `5ee5c86` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5ee5c86eeb00a4d159e2e2cb4c8c85dcc0733e15)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：放宽 `src_cache` 的类型检查，支持 `uint8`、`float8_e4m3fn` 与 `float8_e5m2`，并根据实际 dtype 安全获取指针，修复 fp8 KV‑cache 的类型转换错误。

---

### [WideEP] Fix nvfp4 DeepEP High Throughput All2All backend (#33738)
**SHA**: `066c6da` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/066c6da6a04906a89739fb7e6874ceb6cf714364)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `flashinfer_fp4_moe.py` 中为 TRTLLM 添加兼容性说明，并在并行配置检查中禁用 DeepEP HT 高吞吐路径的使用。

---

### [torch.compile] Stop doing unnecessary FakeTensorProp in PiecewiseCompileInterpreter (#34093)
**SHA**: `e30cedd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e30cedd44be332e1ddc7ec43b8a33bce532e7614)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在测试中引入返回未绑定 symint 的自定义 op 并加入参数化，后端 `call_module` 改为直接使用图中的 `example_value`，消除不必要的 FakeTensorProp。

---

### [Redo] Add `--trust-remote-code` to dataset bench args (#34251)
**SHA**: `3bcd494` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3bcd494ef4bd50c8fa34990d80743728e464c2e0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `vllm/benchmarks/datasets.py` 新增 `--trust-remote-code` 参数，以在数据集基准中支持信任 HuggingFace 远程代码；同时在 `vllm/benchmarks/serve.py` 中删除该参数，使其仅在数据集基准使用。

---

### [Bugfix] Fix Worker.load_model context-manager composition for sleep mode (#34021)
**SHA**: `0e725a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0e725a7d22b90f9b2b5175a75a2e3021139cf3fa)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Worker.load_model 在休眠模式下使用上下文管理器时的组合错误，确保模型加载与资源释放流程正常。

---

### [ROCm][CI] Fix test_sequence_parallel.py location in AMD CI pipeline (#34280)
**SHA**: `4a1550d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4a1550d22d7058e129d0e1257e726b3bf4a77025)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 AMD CI 流水线的 `.buildkite/test-amd.yaml` 中，将 `test_sequence_parallel.py` 的路径从 `distributed/` 调整到 `compile/correctness_e2e/`，确保正确执行对应的测试。

---

### [CI] Add pip caching to cleanup_pr_body workflow (#32979)
**SHA**: `dc6de33` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dc6de33c3d5e9026cef7b27791dfe0f98e64bbde)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `cleanup_pr_body` 工作流的 `setup-python` 步骤中新增 `cache: 'pip'`，启用 pip 缓存以加速依赖安装。

---

### [torch.compile] Disable recursive pre_grad_passes (#34092)
**SHA**: `341eed3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/341eed3d30b7579b730e9959213d83b5dbd4731c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `VLLM_ENABLE_PREGRAD_PASSES` 环境变量，默认关闭 TorchInductor 的递归 pre‑grad 过程，并通过 `patch` 绕过该函数，以降低 vLLM 冷编译时的启动成本。

---

### [Bugfix] Fix mamba cache dtype for Qwen3.5 (#34200)
**SHA**: `9615575` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9615575afc0d9a7d5fe98b65ac2a7150b068472e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Qwen3.5 模型实现中，修正了 Mamba 状态缓存的 dtype 读取，将原先硬编码的 `cache_config.mamba_cache_dtype` 替换为 `model_config.hf_text_config.mamba_ssm_dtype`，解决了 Qwen3.5 的 dtype 错误。

---

### [Benchmarks] Fix attention benchmark smoke test (#34269)
**SHA**: `4293c00` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4293c00b84b968ed25f80dfd2af3bb34d1eeeef6)

**变更类型**：配置调整  
**重要程度**：🟢低  
**摘要**：在 Buildkite 基准测试配置 `benchmarks.yaml` 中添加工作目录 `"/vllm-workspace/"`，并将运行命令的 Python 解释器从 `python` 改为 `python3`，确保注意力基准测试能够在 CI 环境正确执行。

---

### [Bugfix] Fix weights offloading for sleep mode (#32947)
**SHA**: `506ad7d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/506ad7d7c178ac20f2140cfaac1ae657683e8013)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `load_model` 中的上下文管理改为使用括号的多上下文写法，保证在权重内存池和 VLLM 配置同时生效，修复睡眠模式下权重 offloading 的 bug。

---

### Convert online APIs to use Renderer  (#34084)
**SHA**: `fdd6f2a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fdd6f2ad58b113fe0fdc3fd9998e63d6064b5f16)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将在线 API（speech‑to‑text 与 token 服务）改为使用 Renderer，新增对 trace headers 的处理，改为先通过 `input_processor.process_inputs` 构造 `engine_request` 再调用 `engine_client.generate`，并相应调整参数传递。

---

### [UX nit] Fix non-default api_server_count message (#34152)
**SHA**: `1f5febb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1f5febb4b8587378a38ea7050503c3cf0431eef6)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `serve.py` 中，针对单实例 API server 场景，将 `args.api_server_count` 设置为 `None`，防止出现非默认计数提示信息。

---

### Minor cleanup for Voxtral (#34247)
**SHA**: `ae871ca` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ae871ca9234be3f6cb6966d998e51a7cb672f912)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `voxtral.py` 中创建 Hann 窗函数时直接指定 `device`，去除后续的 `.to()` 调用，代码更简洁。

---

### [Model Runner V2] Use pinned memory for write_contents (#34222)
**SHA**: `a2443de` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a2443de5fa4a0605607f6c3d9219022c7f6ac480)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `write_contents` 改为使用 pinned memory 的异步 H2D 复制，去除动态扩容逻辑并记录 `device`，简化实现并提升写入性能。

---

### [Docs] Speed up build environment set-up  (#34240)
**SHA**: `f84a2a8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f84a2a8f318abdec197b957babe13c9766abb4ed)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.readthedocs.yaml` 中改用 `uv` 创建虚拟环境并安装依赖，优化 `git fetch` 参数，加速文档构建环境的准备过程。

---

