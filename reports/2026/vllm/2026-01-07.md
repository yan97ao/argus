# 每日更新报告（2026-01-07）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-07 23:53:54 | Kate Cheng | [Perf][Kernels] Enable FlashInfer DeepGEMM swapAB on SM90 (for W8A8 Linear Op) (#29213) |
| 2026-01-07 22:45:49 | R3hankhan | [OpenAI] Extend VLLMValidationError to additional validation parameters (#31870) |
| 2026-01-07 21:44:36 | Cyrus Leung | [Chore] Migrate V0 attention utils (#31891) |
| 2026-01-07 21:08:29 | Jared Wen | [Refactor] GLM-ASR Modeling (#31779) |
| 2026-01-07 19:25:03 | vllmellm | [ROCm][AITER] fix wrong argument passed to  AITER `flash_attn_varlen_func` (#31880) |
| 2026-01-07 17:18:52 | Andy Liu | [Bugfix][MTP] Fix GLM4 MoE fp8 loading with MTP on (#31757) |
| 2026-01-07 17:00:16 | BlankR | [Misc] Improve error messages for unsupported types and parameters (#30593) |
| 2026-01-07 16:18:28 | maang | [Model] Cleanup: Remove redundant manual definition of `make_empty_intermediate_tensors` in GLM-4-MoE (#31869) |
| 2026-01-07 16:10:29 | sihao_li | [XPU]fallback to TRITON_ATTN on xpu when use float32 dtype (#31762) |
| 2026-01-07 16:07:16 | weiyu | [Refactor][TPU] Remove torch_xla path and use tpu-inference (#30808) |
| 2026-01-07 15:36:13 | xuebwang-amd | [Bugfix][Kernel] fix bias adding in triton kernel implemented fused moe (#31676) |
| 2026-01-07 14:55:03 | Kevin McKay | [Bugfix][Hardware][AMD] Consolidate FP8 min/max values helper function (#31106) |
| 2026-01-07 14:49:39 | ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 | [BugFix] LoRA: Support loading base_layer of experts (#31104) |
| 2026-01-07 14:45:10 | tianshu-Michael-yu | [Bugfix] Fix race condition in async-scheduling for vlm model (#31841) |
| 2026-01-07 14:42:20 | tjp_zju | refactor: find_loaded_library (#31866) |
| 2026-01-07 13:31:34 | Lucas Wilkinson | [Attention][3/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31850) |
| 2026-01-07 13:04:53 | vllmellm | [ROCm][AITER] bugfix accuracy regression in ROCM_AITER_TRITON_MLA backend (#31816) |
| 2026-01-07 12:34:04 | Cyrus Leung | [Chore] Try remove `init_cached_hf_modules` (#31786) |
| 2026-01-07 12:08:47 | Jack Yang | fixed mypy warnings for files vllm/v1/attention with TEMPORARY workaround (#31465) |
| 2026-01-07 11:48:13 | Tyler Michael Smith | Change warning in get_current_vllm_config to report caller's line number (#31855) |
| 2026-01-07 11:27:40 | Cyrus Leung | [Doc] Update release docs (#31799) |
| 2026-01-07 10:09:32 | Ce Zhao | [Model] Enable LoRA support for PaliGemma (#31656) |
| 2026-01-07 10:02:42 | Yihua Cheng | [1/2][lmcache connector] clean up lmcache multi-process adapter  (#31838) |
| 2026-01-07 09:37:51 | Lucas Kabela | [Misc][BE] Type coverage for vllm/compilation [1/3] (#31554) |
| 2026-01-07 09:13:24 | vSeamar | [Frontend] Implement robust video frame recovery for corrupted videos (#29197) |
| 2026-01-07 09:12:23 | Andreas Karatzas | [ROCm][CI] Fix plugin tests (2 GPUs) failures on ROCm and removing `VLLM_FLOAT32_MATMUL_PRECISION` from all ROCm tests (#31829) |
| 2026-01-07 08:31:52 | Angela Yi | [CI] Add warmup run in test_fusion_attn (#31183) |
| 2026-01-07 07:46:56 | Cyrus Leung | [Bugfix] Handle mistral tokenizer in get_hf_processor (#31817) |
| 2026-01-07 07:23:11 | Andreas Karatzas | [ROCm][CI] Pinning timm lib version to fix ImportError in Multi-Modal Tests (Nemotron) (#31835) |
| 2026-01-07 07:21:15 | Andreas Karatzas | [ROCm][CI] Fix ModernBERT token classification test numerical accuracy on ROCm (#31820) |
| 2026-01-07 05:21:42 | Matthew Bonanni | [Spec Decode][UX] Add acceptance stats to `vllm bench serve` report (#31739) |
| 2026-01-07 04:24:19 | Elvir Crnčević | Report error log after vllm bench serve (#31808) |
| 2026-01-07 04:11:26 | Nikhil G | Fix RecursionError in MediaWithBytes unpickling (#31191) |
| 2026-01-07 03:10:18 | Li, Jiang | [Quantization][Refactor] Move CPU GPTQ kernel into MP linear (#31801) |
| 2026-01-07 02:50:43 | Charlie Fu | [ROCm][CI] Fix tests/compile unit tests (#28895) |
| 2026-01-07 02:50:37 | Benjamin Chislett | [Perf] Async Scheduling + Speculative Decoding + Structured Outputs (#29821) |
| 2026-01-07 01:57:56 | Yakine Tahtah | [Bugfix] Fix GLM-4 MoE router logits dtype for data parallel chunking (#31055) |
| 2026-01-07 01:36:24 | Masataro Asai | make 500: InternalServerError more informative (#20610) |
| 2026-01-07 01:32:55 | Ning Xie | [Log] add log about gpu worker init snapshot and requested memory (#29493) |
| 2026-01-07 01:32:46 | Vadim Gimpelson | [PERF] Speed-up of GDN attention decode part (Qwen3-Next) (#31722) |
| 2026-01-07 01:32:14 | Lucas Wilkinson | [Attention][2/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31774) |
| 2026-01-07 01:07:19 | Jinzhen Lin | [Quantization][MoE] remove unused ep logic from moe marlin (#31571) |
| 2026-01-07 00:00:40 | roikoren755 | [NemotronH] Use ReplicatedLinear for fc1_latent_proj (#31807) |

### 📊 统计摘要
> 本日共 43 个提交 | 🔴高 2 | 🟡中 20 | 🟢低 21
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[Refactor][TPU] Remove torch_xla path and use tpu-inferen...](#e759637)
    - [[Quantization][Refactor] Move CPU GPTQ kernel into MP lin...](#8becf14)
  - [🟡 中重要度变更 (20)](#-🟡-中重要度变更-20)
    - [[Perf][Kernels] Enable FlashInfer DeepGEMM swapAB on SM90...](#cc6dafa)
    - [[OpenAI] Extend VLLMValidationError to additional validat...](#1ab055e)
    - [[Chore] Migrate V0 attention utils (#31891)](#b665bbc)
    - [[Refactor] GLM-ASR Modeling (#31779)](#9741387)
    - [[Misc] Improve error messages for unsupported types and p...](#0790f07)
    - [[Bugfix][Hardware][AMD] Consolidate FP8 min/max values he...](#4614c5a)
    - [[BugFix] LoRA: Support loading base_layer of experts (#31...](#4829148)
    - [refactor: find_loaded_library (#31866)](#55caa60)
    - [[Chore] Try remove `init_cached_hf_modules` (#31786)](#aafd4d2)
    - [fixed mypy warnings for files vllm/v1/attention with TEMP...](#0a2c2dc)
    - [[Misc][BE] Type coverage for vllm/compilation [1/3] (#31554)](#873480d)
    - [[Frontend] Implement robust video frame recovery for corr...](#6f35154)
    - [[Spec Decode][UX] Add acceptance stats to `vllm bench ser...](#d498997)
    - [Fix RecursionError in MediaWithBytes unpickling (#31191)](#ada6f91)
    - [[ROCm][CI] Fix tests/compile unit tests (#28895)](#c071636)
    - [[Perf] Async Scheduling + Speculative Decoding + Structur...](#f7008ce)
    - [[Bugfix] Fix GLM-4 MoE router logits dtype for data paral...](#4e67a8f)
    - [[Log] add log about gpu worker init snapshot and requeste...](#6f5e653)
    - [[Attention][2/n] Remove usage of deprecated `seq_lens_cpu...](#4c73be1)
    - [[Quantization][MoE] remove unused ep logic from moe marli...](#2f4bdee)
  - [🟢 低重要度变更 (21)](#-🟢-低重要度变更-21)
    - [[ROCm][AITER] fix wrong argument passed to  AITER `flash_...](#41cfa50)
    - [[Bugfix][MTP] Fix GLM4 MoE fp8 loading with MTP on (#31757)](#d111bc5)
    - [[Model] Cleanup: Remove redundant manual definition of `m...](#1f33e38)
    - [[XPU]fallback to TRITON_ATTN on xpu when use float32 dtyp...](#59fe6f2)
    - [[Bugfix][Kernel] fix bias adding in triton kernel impleme...](#0dd5dee)
    - [[Bugfix] Fix race condition in async-scheduling for vlm m...](#efeaac9)
    - [[Attention][3/n] Remove usage of deprecated `seq_lens_cpu...](#c7a79d4)
    - [[ROCm][AITER] bugfix accuracy regression in ROCM_AITER_TR...](#6409004)
    - [Change warning in get_current_vllm_config to report calle...](#f09c5fe)
    - [[Doc] Update release docs (#31799)](#1b8af95)
    - [[Model] Enable LoRA support for PaliGemma (#31656)](#a051525)
    - [[1/2][lmcache connector] clean up lmcache multi-process a...](#5b833be)
    - [[ROCm][CI] Fix plugin tests (2 GPUs) failures on ROCm and...](#364a8bc)
    - [[CI] Add warmup run in test_fusion_attn (#31183)](#9a1d20a)
    - [[Bugfix] Handle mistral tokenizer in get_hf_processor (#3...](#309a8f6)
    - [[ROCm][CI] Pinning timm lib version to fix ImportError in...](#e5d427e)
    - [[ROCm][CI] Fix ModernBERT token classification test numer...](#2a42ae7)
    - [Report error log after vllm bench serve (#31808)](#dba9537)
    - [make 500: InternalServerError more informative (#20610)](#142c4d1)
    - [[PERF] Speed-up of GDN attention decode part (Qwen3-Next)...](#22dffca)
    - [[NemotronH] Use ReplicatedLinear for fc1_latent_proj (#31...](#28c9477)
#### 🔴 高重要度变更 (2)

### [Refactor][TPU] Remove torch_xla path and use tpu-inference (#30808)
**SHA**: `e759637` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e7596371a403903218ded3a9f446981fde5737f5)

**🎯 变更类型**：架构变更 / 重构  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交彻底移除 vLLM 项目内部对 `torch_xla` 和 **TPU** 的专有实现，统一改为仅使用外部插件 **`tpu_inference`**。为此删除了大量与 XLA 相关的代码（包括自定义 XLA kernel、Pallas MoE 实现、TPU LoRA XLA ops、TPU KV‑cache 更新逻辑、模型加载、分布式通讯等），并在平台层做了相应的切换：若未能导入 `tpu_inference`，则直接报错而不是回退到内部实现。与此同时，`AttentionBackendEnum` 中去掉了 `PALLAS` 后端，相关注意力实现、量化内核、模型加载器等也同步删减。

---

## 🎯 影响范围

| 受影响组件 | 具体内容 |
| ---------- | -------- |
| **平台层 (`vllm/platforms/tpu.py`)** | 完全依赖 `tpu_inference`，不再提供内部 `TpuPlatform` 实现；若未装插件直接报错。 |
| **注意力后端** | `AttentionBackendEnum.PALLAS` 被移除；所有对 Pallas（TPU 专属 MoE/Flash‑Attention） 的引用被删掉。 |
| **MoE 实现** | `vllm/model_executor/layers/fused_moe/moe_pallas.py` 与相关调用全部删除，默认使用 GPU/CPU MoE 实现。 |
| **量化路径** | `tpu_int8` 量化配置及对应 Linear 方法、XLA‑scaled‑MM 内核全部删除。 |
| **LoRA XLA Ops** | `vllm/lora/ops/xla_ops/*` 被删除，LoRA 在 TPU 上只能走 `tpu_inference` 提供的实现（若有）。 |
| **模型加载** | `vllm/model_executor/model_loader/tpu.py` 以及所有在加载阶段对 XLA 的同步/分片逻辑被删掉。 |
| **分布式通讯** | `tpu_communicator.py` 删除，通信统一交由插件实现。 |
| **Worker / Runner** | `vllm/v1/worker/tpu_worker.py`、`tpu_model_runner.py`、`tpu_worker.py` 大量代码被移除，仅保留插件包装入口。 |
| **使用统计** | `usage_lib.py` 中去除对 `torch_xla` 的检测，以免误报。 |
| **其他** | 多处内部工具函数（如 `shard_model`、`KVCache` 统计、`dummy_run` 等）被删除或注释。 |

---

## 🔍 技术洞察

### 架构影响
1. **职责分离**  
   - 原来 vLLM 自己承担了 TPU 端的全部编译、分片、KV‑cache、LoRA、Pallas MoE 等细节。现在这些职责全部交给 **`tpu_inference`** 插件，vLLM 只保留统一的调度/抽象层。  
   - 代码体积大幅缩减（单文件删除数千行），维护成本下降。

2. **插件化路径**  
   - `current_platform.is_tpu()` 仍然判定为 TPU，但实际实现仅在 `tpu_inference` 可用时才生效。缺失插件会导致启动失败，强制用户显式安装插件。  
   - 未来若 `tpu_inference` 更新，只需在插件内部升级，不必改动 vLLM 主仓库。

3. **后端统一**  
   - 以前的 **Pallas**（自研 XLA Kernel）被移除，用户只能使用 `FlashAttention`（CPU/XPU/CUDA）或者插件内部实现的 TPU 方案。  
   - 对 `AttentionBackendEnum` 的修改可能影响第三方代码通过枚举进行后端选择的兼容性。

### 性能影响
| 维度 | 正向影响 | 负向影响 | 说明 |
| ---- | -------- | -------- | ---- |
| **编译/启动时间** | 使用插件的预编译/缓存（`tpu_inference` 可能已经做了 XLA 编译缓存），避免 vLLM 自己的 `torch.compile`/`torch._dynamo` 再次编译 | 若插件未做足够的缓存或缺少关键优化（如 Pallas GMM），首次运行可能出现 **更慢** 的编译时间 | 原代码中大量 `torch.compile`、`torch._dynamo`、手动 `shard_model` 等导致巨额编译开销，现已省去 |
| **推理吞吐/延迟** | 插件可针对特定 TPU 型号（v5e、v6e 等）提供高度优化的 kernel，理论上可提升吞吐 | 移除 Pallas MoE、XLA‑scaled‑MM、int8 Quant 可能导致在使用这些特性的模型上 **性能下降**（或直接不可用） | 例如 `fused_moe` 在 Pallas 上有 2×‑3× 加速，删除后回退到通用实现 |
| **内存利用率** | 删除自研 KV‑cache 逻辑，改用插件实现，可能更贴合硬件的 **SMEM/VMEM** 管理 | vLLM 过去通过 `determine_available_memory` 手动计算页大小/块数，插件若不提供同等策略，可能导致 **OOM** 或 **过度保守** 的 block 分配 | 此前的 `PallasAttentionBackend.get_page_size` 等逻辑不复用 |
| **兼容性** | 统一插件后，**所有 TPU 代码路径** 只通过插件实现，降低多版本冲突 | 任何依赖 vLLM **内部 TPU API**（如自定义 LoRA XLA ops、使用 `torch_xla` 的用户代码）将失效 | 需要迁移到插件或放弃 TPU |

### 安全考虑
- **攻击面缩减**：移除数十个自研 XLA Custom Ops（`bgmv`, `kv_cache_update`, `gmm` 等）大幅降低潜在的安全/稳定性漏洞（如未正确检查输入导致内存越界）。  
- **依赖可信插件**：安全风险转移至 `tpu_inference` 包；若该插件本身存在漏洞或供应链攻击，vLLM 将受牵连。  
- **配置校验**：平台层在未检测到插件时直接抛异常，避免在运行时进入未定义路径导致未知行为。  

### 可维护性
- **代码量大幅下降**（约 6 k 行删除），维护成本显著降低。  
- **统一接口**（`TPUWorker` → `tpu_inference.worker.TPUWorker`）后，后续 TPU 功能迭代只需更新插件。  
- **文档/测试缺失**：大量原有单元测试（`tests/tpu/*`）被删除，意味着当前仓库不再提供 TPU 测试套件；若插件的 CI 不覆盖，整体质量保障减弱，需要在插件侧补齐。  

### 迁移风险
| 风险点 | 可能后果 | 缓解措施 |
| ------ | -------- | -------- |
| **插件缺失** | vLLM 启动时报错，用户无法在 TPU 上运行 | 在发行说明中明确 `pip install tpu_inference` 为必选依赖；提供友好错误提示。 |
| **功能回退**（MoE、int8、LoRA） | 依赖这些特性的模型在 TPU 上不可用或性能下降 | 在文档中标注 “TPU 目前不支持 Pallas MoE / int8 quant / XLA LoRA”。若需要，等待插件实现对应特性

---

### [Quantization][Refactor] Move CPU GPTQ kernel into MP linear (#31801)
**SHA**: `8becf14` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8becf146bdc42bb7ad3acb4af374d58de65c4432)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
1. 将原先分散在 `cpu_wna16.py` 中的 CPU‑GPTQ 线性实现抽离并统一到多卡（MP）Linear 框架中，新增 `CPUWNA16LinearKernel` 并删掉 `cpu_gptq` 配置类与实现。  
2. 相应地在量化配置、导入表、平台适配和 Marlin 兼容查询等位置同步更新，确保 CPU 侧也能走统一的 MP 代码路径，并加入对 AMX/向量指令的自动选择。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/quantization/**`（配置、kernel、utils）  
- `vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py`  
- `vllm/config/model.py`（量化后端列表）  
- `tests/quantization/test_cpu_wna16.py`（新增模型覆盖）  

**🔍 技术洞察**  

- **架构影响**  
  - **统一抽象**：引入 `MPLinearKernel` 的子类 `CPUWNA16LinearKernel`，把 CPU‑GPTQ 与其他 MP 量化实现（Cutlass、Exllama、XPU 等）放在同一层级，代码重复度大幅降低，后续维护和新平台适配更简洁。  
  - **配置简化**：`cpu_gptq` 配置类被删除，所有 CPU‑GPTQ 场景转而使用 `cpu_awq`（以及内部的 `has_g_idx` 标记）统一管理，降低用户侧的配置分支。  
  - **平台检测**：在 `gptq_marlin.py`、`marlin_utils.py` 中加入对 CPU 的兼容判断，使 Marlin‑style 量化在 CPU 上也能走查询路径（返回 `uint4`/`uint4b8`），保持行为一致。  

- **性能影响**  
  - **保持既有性能**：新 kernel 的实现仍然调用底层 `ops.cpu_gemm_wna16`，与原实现相同的算子路径未变，理论上性能不受影响。  
  - **潜在提升**：统一的 MP 框架在分布式张量切分、参数注册等环节使用统一的 `MPLinearLayerConfig`，可避免重复的张量转置与拷贝，极端场景下可能略微降低加载时的内存占用与 CPU 复制开销。  
  - **AMX/向量自适应**：在 `_get_isa_hint` 中自动检测 AMX 支持并在 BF16 场景下使用 `amx`，若硬件支持，将获得约 1.5‑2 倍的矩阵乘加加速。  

- **安全考虑**  
  - 变更仅涉及内部量化算子和配置的重构，无新增外部依赖或网络交互，安全风险极低。  
  - 唯一需关注的是向后兼容：删除 `cpu_gptq` 配置后，旧版配置文件或显式指定 `quant_method="gptq"` 的模型可能在加载时抛出 `KeyError`，若未做好兼容层或警告，可能导致意外的服务中断。  

**⚠️ 潜在风险**  

1. **向后兼容性**  
   - 老版用户仍然在配置文件或代码中使用 `"cpu_gptq"`，现在会被视为未知量化方法并在 `get_quantization_config` 中找不到对应类，导致启动失败。  
   - `vllm/config/model.py` 中的 `_verify_quantization` 已去除 `"cpu_gptq"`，同样会影响自动检测逻辑。  

2. **模型兼容性**  
   - 通过 `CPUWNA16LinearKernel.can_implement` 增加了对 **输入/输出维度必须是 32 的倍数** 的硬性检查，某些使用非 32 倍数 shape 的 GPTQ 量化模型（如特殊裁剪的层）将不再被接受。  

3. **平台检测差异**  
   - `marlin_utils._query_cpu_marlin_supported_quant_types` 现在返回 `uint4`（AWQ）和 `uint4b8`（GPTQ）两种类型，但如果后续扩展其他 CPU 量化实现，需同步更新该函数，否则可能出现误判。  

**💡 关注建议**  

- **对外文档 & 迁移指南**：在 Release Note 中明确标记 `"cpu_gptq"` 已废弃，建议用户改用 `"cpu_awq"` 并通过 `dynamic`/`modules_in_block_to_quantize` 完成相同的 GPTQ 行为。提供示例配置文件的迁移脚本。  
- **兼容层**：在 `get_quantization_config` 中保留对 `"cpu_gptq"` 的别名映射（抛出警告而非错误），以免突发业务中断。  
- **单元/集成测试**：新增针对 `CPUWNA16LinearKernel` 与 `MPLinearKernel` 组合的覆盖（包括非 32 倍数 shape 的负向测试），确保未来改动不破坏已有路径。  
- **性能回归基线**：在 CI 中加入 CPU‑GPTQ 的基准测试，对比旧实现的加载时延、推理吞吐量与新实现的差异，验证 AMX 推理路径的实际收益。  
- **监控 & 日志**：在 `CPUWNA16LinearKernel.can_implement` 失败的分支记录详细日志，帮助用户快速定位模型不兼容的原因（如维度、group‑size、平台不支持等）。  

通过上述措施，可在保持或提升性能的同时，平滑完成 API 重构，最小化对已有用户的影响。

---

#### 🟡 中重要度变更 (20)

### [Perf][Kernels] Enable FlashInfer DeepGEMM swapAB on SM90 (for W8A8 Linear Op) (#29213)
**SHA**: `cc6dafa` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cc6dafaef2bc6b0a3ea0d80efc93985fce6bc8b9)

**🛠️ 核心变更**  
1. **新增 FlashInfer FP8 Block‑Scale GEMM**：在 `vllm/utils/flashinfer.py` 中懒加载 `flashinfer.gemm.fp8_blockscale_gemm_sm90`，并提供 `has_flashinfer_fp8_blockscale_gemm` / `is_flashinfer_fp8_blockscale_gemm_supported` 判断函数。  
2. **环境开关**：`VLLM_BLOCKSCALE_FP8_GEMM_FLASHINFER` 新增到 `vllm/envs.py`，默认关闭。  
3. **量化流水线改造**：`fp8_utils.py` 中实现 `_flashinfer_fp8_blockscale_gemm_impl`，使用 `torch.cond` 在 **M < 32** 时走 FlashInfer‑DeepGEMM swapAB，≥ 32 时走原 DeepGEMM，确保 `torch.compile` 可捕获两条路径。  
4. **线性层调度**：`W8A8BlockFp8LinearOp.apply` 增加 `should_use_flashinfer_for_blockscale_fp8_gemm` 判断，结合 DeepGEMM 条件决定调用 `_run_flashinfer`。  
5. **测试覆盖**：在 `tests/kernels/quantization/test_block_fp8.py` 添加 FlashInfer‑FP8 端到端矩阵乘法验证（仅在 SM90+ 且 FlashInfer 可用时执行）。  

**📦 影响范围**  
- 线性层（W8A8 FP8）执行路径。  
- 量化工具链（`per_token_group_quant_fp8`、`per_block_cast_to_fp8`）。  
- 环境配置及依赖：需要 Hopper（SM90）GPU、FlashInfer ≥ 0.2（含 TensorRT‑LLM）以及对应的 CUDA Toolkit。  

**💡 建议**  
1. **兼容性检查**：在非 Hopper 环境或 FlashInfer 未安装时，确保回退到 DeepGEMM，不应因为 `torch.ops.vllm.flashinfer_fp8_blockscale_gemm` 未注册而抛异常。  
2. **文档与 CI**：在 README 与部署脚本中说明新环境变量的作用及开启条件；CI 中加入 Hopper GPU（或模拟）跑通的表单，防止后续回归。  
3. **性能基准**：提供两种 batch‑size 场景下的基准（M < 32 vs M ≥ 32），帮助用户判断是否开启该特性。  
4. **异常信息**：在 `should_use_flashinfer_for_blockscale_fp8_gemm` 中对不满足 N/K 对齐的情况给出明确日志，便于调试。  
5. **torch.compile**：验证 `torch.cond` 在真实模型编译后的图中确实保留两条路径，避免出现 “cond not supported” 的错误。  

总体来看，此次改动为 Hopper GPU 引入了更高效的 FP8 Block‑Scale GEMM，保持了向后兼容并通过单元测试验证正确性，建议在生产环境开启前完成上述兼容性与基准验证。

---

### [OpenAI] Extend VLLMValidationError to additional validation parameters (#31870)
**SHA**: `1ab055e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1ab055efe6b7f9abdd8774ed64ead1b83dd00253)

**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 将 `VLLMValidationError` 从 `vllm.entrypoints.openai.protocol` 移动到独立的 `vllm.exceptions` 模块。  
2. 在多个核心层（`sampling_params.py`、`engine/input_processor.py`、OpenAI 接口入口等）中改用该异常，并为温度、top‑p、max_tokens、logprobs、prompt_logprobs、truncate_prompt_tokens、bad_words、logit_bias 等参数加入更详细的校验信息（parameter、value）。  
3. 统一异常创建方式，异常处理器能够识别 `VLLMValidationError` 并返回 OpenAI 兼容的 BadRequest 错误。

**🎯 影响范围**：  
- `vllm.exceptions`（新增）  
- `vllm.entrypoints.openai.*`（api_server、protocol、serving_*）  
- `vllm.sampling_params`  
- `vllm.v1.engine.input_processor`  
- 相关的日志和错误响应路径

**💡 关注建议**：  
1. **导入检查**：确保所有内部和第三方代码已更新为 `from vllm.exceptions import VLLMValidationError`，避免残留的旧路径导致 ImportError。  
2. **兼容性**：若已有用户直接引用 `vllm.entrypoints.openai.protocol.VLLMValidationError`，考虑在 `protocol.py` 中保留向后兼容的别名或发出迁移提醒。  
3. **错误信息**：新异常的 `__str__` 会附加 `parameter` 与 `value`，验证日志和监控系统是否能正确解析；若有自定义错误处理逻辑，需要同步更新。  
4. **单元测试**：新增异常分支后，补充对应的测试用例，特别是对非法参数的请求返回 400 并携带错误详情。  
5. **文档更新**：在 API 文档和开发者指南中说明哪些参数会触发 `VLLMValidationError`，以及返回格式。  

总体来看，此次改动提升了错误可追溯性和代码组织，但需注意迁移期间的导入路径和异常捕获逻辑，以免引入隐藏的兼容性问题。

---

### [Chore] Migrate V0 attention utils (#31891)
**SHA**: `b665bbc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b665bbc2d4277feebf12a6fa38f24d367b5ed25e)

**🎯 变更类型**：重构（迁移 V0 attention utils 到 V1）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交把原 `vllm.attention.backends.utils` 中的常量 `PAD_SLOT_ID` 与函数 `get_mla_dims` 迁移至 `vllm.v1.attention.backends.utils`（并在 `vllm/v1/attention/backends/mla/common.py` 中重新实现），同步更新所有引用该工具的模块，包括 mamba 相关 Triton kernel、GDN/MLA 后端实现、GPU block‑table 以及对应单元测试。原文件 `vllm/attention/backends/utils.py` 被删除。

**🎯 影响范围**  
- **attention/backends**：所有 V0 后端实现（GDN、MLA）以及新 V1 实现的 import 路径更改。  
- **model_executor/layers/mamba**：`causal_conv1d.py`、`mamba_ssm.py` 中的 PAD_SLOT_ID 引用切换。  
- **worker/gpu/block_table.py**：同上。  
- **tests/kernels/mamba**：单元测试改为使用 V1 utils。  
- **v1/attention/backends/mla**：在 `common.py` 中重新定义 `MLADims` 与 `get_mla_dims`，并在其余 MLA 子模块统一使用。  

**💡 关注建议**  
1. **兼容性检查**：确认项目中未遗漏旧路径的引用（如 `vllm.attention.backends.utils`），防止运行时 ImportError。  
2. **单元测试**：执行全部测试，尤其是注意力相关的 `flashmla`、`gdn_attn` 与 mamba kernel，确保迁移未引入数值差异。  
3. **文档与示例**：更新开发者文档中关于 `PAD_SLOT_ID`、`get_mla_dims` 的说明，明确已弃用 V0 utils。  
4. **后向依赖**：若外部插件仍使用旧路径，考虑在 `vllm/attention/backends/__init__.py` 中提供向后兼容的别名或警告。  
5. **代码审计**：检查 `MLADims` dataclass 与 `ModelConfig` 兼容性，确保新增字段在不同模型配置下均能安全访问。  

整体来看，此次迁移提升了代码组织和版本统一性，但需做好兼容性验证与文档同步，以免影响现有使用者。

---

### [Refactor] GLM-ASR Modeling (#31779)
**SHA**: `9741387` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/974138751bdb8e41743d0beac0ac4c5d89308c2f)

**变更类型**：功能增强/代码重构  
**重要程度**：🟡 中  

**核心改动**  
1. **GLM‑ASR 编码器完全重写**：引入 `GlmAsrEncoderRotaryEmbedding`、`GlmAsrEncoderAttention`、`GlmAsrEncoderMLP`、`GlmAsrEncoderLayer`，使用 vLLM 的 `QKVParallelLinear、ColumnParallelLinear、RowParallelLinear、ApplyRotaryEmb、MMEncoderAttention` 实现张量并行、量化与 Flash‑Attention 加速。  
2. **权重加载**：新增 `packed_modules_mapping` 与自定义 `load_weights`，把原来的 `q/k/v` 权重堆叠到 `qkv_proj`，兼容 GPTQ 偏置等特殊情况。  
3. **多模态解析与处理**：`GlmAsrProcessingInfo`、`GlmAsrMultiModalDataParser`、`GlmAsrMultiModalProcessor` 从 `AudioFlamingo3*` 基类切换为基于 `BaseMultiModalProcessor` 的轻量实现；重新组织字段配置 (`_glmasr_field_config`) 以及 `get_dummy_*`，统一音频特征、掩码、chunk_counts 的处理逻辑。  
4. **音频投影**：`GlmAsrMultiModalProjector` 改为两层并行 MLP（`ColumnParallelLinear → activation → RowParallelLinear`），支持张量并行和量化。  
5. **音频前处理**：在 `GlmAsrModel` 中对 `audio_tower` 输出进行 dtype 对齐、序列裁剪（确保能被 `merge_factor` 整除）并按 `intermediate_size` 重塑；对 `chunk_counts` 的计算逻辑做了更稳健的实现。  
6. **工具函数**：`_get_audio_output_lengths_for_tower` 迁移到基于卷积层输出长度的显式计算，并在后续加入 `merge_factor` 处理。

**影响范围**  
- `vllm/model_executor/models/glmasr.py`（编码器、投影、模型主体）  
- `vllm/model_executor/models/glmasr_utils.py`（长度计算）  
- 多模态解析/处理相关模块 (`glmasr_processing`, `glmasr_multi_modal_*`)  
- 依赖 `AudioFlamingo3*` 的旧实现全部被替换，导致其他模型若仍引用这些类会出现导入错误。

**建议**  
1. **兼容性检查**：确认项目中未有其它地方仍然导入 `AudioFlamingo3*`，并更新相应 import。  
2. **分布式与量化测试**：在多卡、tensor‑parallel 环境下跑完整的 GLM‑ASR 推理/生成，用 `torch.distributed` 检验 `QKVParallelLinear`、`MMEncoderAttention` 的行为。  
3. **权重加载验证**：对比原始 HF 权重与新实现的 `load_weights`，确保 `q/k/v` 堆叠后数值一致，尤其是 GPTQ 或 LoRA 权重。  
4. **输入 dtype 与长度**：确保音频特征在送入 `audio_tower` 前已转为模型参数 dtype（如 bfloat16），并验证 `merge_factor` 对不同长度的音频均能整除，防止因截断导致信息丢失。  
5. **单元/集成测试**：新增/更新 GLM‑ASR 的测试用例，覆盖：`_glmasr_field_config`、`GlmAsrProcessingInfo.get_feature_extractor`、`_process_audio_input` 的 mask 与长度计算。  
6. **性能基准**：对比旧实现和新实现的推理吞吐、显存占用，确认并行/Flash‑Attention 改进带来的预期提升。  

总体而言，此次重构显著提升了 GLM‑ASR 在 vLLM 框架下的性能与可扩展性，但引入了大量新依赖和内部约定，需要通过上述验证确保功能和数值保持一致。

---

### [Misc] Improve error messages for unsupported types and parameters (#30593)
**SHA**: `0790f07` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0790f07695c72fe203e95f6d8c8ff15c8003e754)

**🎯 变更类型**：功能增强 / 其他  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：统一并细化了多处 `ValueError` 的提示信息，明确列出支持的取值范围或期望的输入类型，避免出现 “unsupported type” 之类的模糊报错。  

**🎯 影响范围**  
- `benchmarks/cutlass_benchmarks/sparse_benchmarks.py`（dtype 检查）  
- `vllm/attention/ops/chunked_prefill_paged_decode.py`（FP8 kv_cache dtype）  
- `vllm/config/lora.py`（lora 参数校验）  
- `vllm/distributed/device_communicators/pynccl_wrapper.py`（NCCL dtype）  
- `vllm/distributed/kv_transfer/.../vllm_v1_adapter.py`（new_block_ids 类型）  
- 量化相关模块：`auto_round.py、compressed_tensors_w8a8_fp8.py、mxfp4.py`（参数取值检查）  
- 多个模型实现：`ernie45_vl.py、granite_speech.py、minimax_text_01.py`（尺寸、上下文、attention 类型）  

**💡 关注建议**  
1. **兼容性**：报错文字的改变不影响功能，但如果外部脚本或日志监控依赖原始错误字符串进行匹配，需同步更新相应的正则或过滤规则。  
2. **文档同步**：确保 README、API 文档或配置说明中列出的合法取值与代码中提示的列表保持一致，防止出现 “文档说支持 X，实际报错不支持”。  
3. **单元测试**：为每个新错误信息添加对应的测试，用 `pytest.raises` 验证错误文本，以防后续改动误删或改写提示。  
4. **用户体验**：友好的错误信息有助于快速定位配置错误，建议在抛异常前先对输入进行前置校验（`assert`/`if`），并在日志中给出建议的默认值或示例。  

总体来看，此次改动提升了可调参数的可发现性和调试效率，对运行时行为无直接影响，风险较低。请在 CI 中加入对新错误信息的回归检查。

---

### [Bugfix][Hardware][AMD] Consolidate FP8 min/max values helper function (#31106)
**SHA**: `4614c5a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4614c5a539282e6a8055d3b370b0f58c6f094c25)

**🎯 变更类型**：Bugfix（硬件适配）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为了解决 ROCm MI300 FP8 fnuz（float8 e4m3fnuz）模式下默认 240.0 的量化上限导致的精度回退，新增 `get_fp8_min_max()` 辅助函数统一获取 FP8 的 `min/max`，并在所有涉及 FP8 量化的实现中改用该函数。  
**🎯 影响范围**：  
- `vllm.model_executor.layers.quantization.utils.quant_utils`（新增函数）  
- `input_quant_fp8.py、fp8_utils.py、quant_utils.py、deep_gemm.py` 中的 FP8 量化路径  
- 单元测试 `tests/kernels/quantization/test_fp8_min_max_helper.py`  
- 受影响的测试 `tests/kernels/quant_utils.py` 也相应改为使用新函数。  

**💡 关注建议**  
1. **平台检测**：`get_fp8_min_max()` 直接依赖 `current_platform.is_fp8_fnuz()`，确保在非 ROCm 环境仍返回 PyTorch 的 `torch.finfo`，否则可能出现意外的 224.0 取值。  
2. **向后兼容**：旧代码仍可能硬编码 224/240，建议在项目其余未涉及的模块中统一使用该函数，以防出现遗漏。  
3. **性能验证**：虽然改动仅是读取常量，但在大规模推理路径（deep_gemm、group‑quant）中多次调用，验证 `get_fp8_min_max()` 是否被 JIT/编译器内联，以避免额外函数调用开销。  
4. **测试覆盖**：目前仅覆盖 `get_fp8_min_max()` 本身，建议在 FP8 量化相关的端到端模型测试中加入 ROCm‑fnuz 环境的模拟，确保数值误差在可接受范围。  

整体来看，此次提交消除了 ROCm fnuz 场景的量化偏差，改动局限于 FP8 量化路径，风险较低，只需关注平台检测逻辑的可靠性和后续代码统一使用该工具函数即可。

---

### [BugFix] LoRA: Support loading base_layer of experts (#31104)
**SHA**: `4829148` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/482914849cf9ce61d3e0dffaa35096bb34de58f5)

**🎯 变更类型**：BugFix / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- `SharedFusedMoE.make_expert_params_mapping` 增加 `model` 参数，用于检测是否存在 `base_layer` 前缀，从而在 LoRA 适配模型时正确映射权重路径。  
- 所有使用该方法的 MoE 模型实现（如 `afmoe`, `mixtral`, `qwen3_moe` 等）统一改为传入 `self`。  
- 参数映射中的权重名拼接逻辑加入 `base_layer` 前缀，以兼容 LoRA‑adapter 的权重布局。  

**🎯 影响范围**  
- 核心层 `vllm/model_executor/layers/fused_moe/layer.py` 的接口签名和实现。  
- 近 40 处模型文件的 `make_expert_params_mapping` 调用。  
- 任何直接使用 `SharedFusedMoE.make_expert_params_mapping` 而未传入 `model` 的第三方插件或自定义 MoE 可能会因缺少必填参数而报错。  

**💡 关注建议**  
1. **兼容性**：若仍需支持旧调用方式，考虑为 `model` 参数提供默认值 `None` 并在内部跳过 `base_layer` 检测。当前改动会导致未更新的外部代码失效。  
2. **性能**：`model.named_parameters()` 在每次映射时遍历全部参数，可能在大模型加载阶段产生额外开销。建议将检测结果缓存（如在 `model` 上设置属性），或仅在 LoRA 场景下调用。  
3. **测试**：添加 LoRA‑adapter 的单元测试，验证 `base_layer` 前缀在权重加载、参数切分以及后向传播中的正确性。  
4. **文档**：更新 `make_expert_params_mapping` 的 docstring，说明 `model` 参数的用途、返回的权重名格式以及对 LoRA 的兼容策略。  

总体而言，此次改动解决了 LoRA‑adapted MoE 在权重加载时找不到参数的问题，但需要注意向后兼容和潜在的性能影响，建议通过上述措施确保迁移平滑。

---

### refactor: find_loaded_library (#31866)
**SHA**: `55caa60` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/55caa6051d675148aba009c85618c6d3adf85091)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原先散布在 `vllm/device_allocator/cumem.py` 与 `vllm/distributed/device_communicators/cuda_wrapper.py` 中的 `find_loaded_library` 实现抽取到 `vllm/utils/system_utils.py`，统一为公共工具函数，删除了两处重复代码。  
**🎯 影响范围**：  
- `vllm/device_allocator/cumem.py`（加载 CUDA‑MemAllocator 时会调用此函数）  
- `vllm/distributed/device_communicators/cuda_wrapper.py`（判断 CUDA Runtime 是否已加载）  
- 可能的第三方插件或自定义代码若直接引用了旧的私有实现，也会受到影响。  

**💡 关注建议**  

1. **兼容性检查**：虽然函数名和行为保持不变，但路径从相对实现迁移到 `utils.system_utils`，请确认所有内部与外部引用均已更新为 `from vllm.utils.system_utils import find_loaded_library`，避免出现 `ImportError`。  
2. **平台局限**：实现依赖 Linux `/proc/self/maps`，在 Windows/macOS 环境下将始终返回 `None`。若项目计划在非 Linux 平台运行，建议在函数入口添加平台判断并给出友好提示或回退实现。  
3. **异常安全**：当前实现若文件 `/proc/self/maps` 不可读（权限或容器限制），会抛出 `FileNotFoundError`/`PermissionError`。考虑捕获并返回 `None`，防止启动阶段因未捕获异常导致进程崩溃。  
4. **单元测试**：为 `find_loaded_library` 新增跨平台单元测试，模拟 `/proc/self/maps` 内容，验证库名匹配、路径解析以及异常路径的返回。  
5. **文档同步**：在项目的开发者手册或 API 文档中加入该函数的使用说明与平台限制，帮助用户了解何时可以依赖它。  

总体来看，此次重构提升了代码可维护性，减小了重复实现的风险，只要留意平台兼容性与异常处理，即可安全上线。

---

### [Chore] Try remove `init_cached_hf_modules` (#31786)
**SHA**: `aafd4d2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aafd4d23548ae54adeca1d4898cc15a4d2c390ac)

**🎯 变更类型**：重构 / 其他  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交尝试去除 `init_cached_hf_modules` 的使用，改为在 `WorkerWrapperBase` 初始化时不再接受 `vllm_config`，而在 `init_worker` 里统一注入并启用 `trace_function_call`。相应地，所有创建 `WorkerWrapperBase` 的地方（多进程、Ray、单进程、测试）均删掉了 `vllm_config` 参数。对 `worker_base.py` 进行了一系列简化：移除提前的 HF 模块初始化、精简 `init_worker` 参数提取逻辑、统一对 `parallel_config` 的访问方式。

**🎯 影响范围**  
- `vllm/v1/worker/worker_base.py`（核心包装类）  
- `vllm/v1/executor/*_executor.py`（多/单进程、Ray）  
- 相关测试文件 (`conftest.py`)  
- `vllm/utils/import_utils.py`（删除 `init_cached_hf_modules`）  

**💡 关注建议**  

1. **兼容性验证**：`WorkerWrapperBase` 构造函数签名变化后，外部插件或自定义代码如果仍自行实例化该类并传入 `vllm_config`，可能会报错。建议在文档或发布说明中明确该破坏性改动。  
2. **异常路径**：`init_worker` 现在使用 `kwargs.get("vllm_config")` 直接断言不为 `None`，若未来出现遗漏（如某些单元测试未补全）会触发 AssertionError。可考虑改为更友好的 `ValueError` 并给出提示。  
3. **`init_cached_hf_modules` 移除的安全性**：该函数原本用于在信任远程代码时提前初始化 Hugging‑Face 动态模块。现在的实现依赖 `trust_remote_code` 判断后不再调用，需确保在所有环境（尤其是使用 `trust_remote_code=True` 的模型）下仍能正常序列化/反序列化。建议在 CI 中加入对应模型的 smoke‑test。  
4. **代码清理**：`vllm/utils/import_utils.py` 只剩下 `import_pynvml`，若以后不再计划使用 `init_cached_hf_modules`，可以直接删除整个文件或将其合并到其他 utils，防止误导。  
5. **测试覆盖**：目前只修改了少量 `conftest.py`，但 `WorkerWrapperBase` 的行为变化（如 `self.worker` 的类型、`mm_receiver_cache` 初始化）应在单元测试中覆盖，防止回归。  

总体而言，此次重构简化了 Worker 包装层的初始化路径，去除了不必要的跨进程 HF 模块加载，代码更易维护。只要确认兼容性与远程代码信任路径的行为保持一致，即可安全合并。

---

### fixed mypy warnings for files vllm/v1/attention with TEMPORARY workaround (#31465)
**SHA**: `0a2c2dc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0a2c2dc3f14620de291e67d6d26fca72b09f5617)

**变更类型**：功能增强 / 重构（主要为类型安全与 MyPy 警告的临时规避）  
**重要程度**：🟡 中  

### 关键改动概览
1. **`tools/pre_commit/mypy.py`**  
   - 将 `vllm/v1/attention` 加入 `mypy` 检查范围，同时在 `v1` 目录的旧入口中去除重复条目，避免双重检查。  

2. **抽象层（`abstract.py`）**  
   - `AttentionType` 由普通类改为 `str, Enum`，兼容 `torch.compile` 并提供更严格的枚举检查。  
   - `block_size` 参数从 `int | None` 改为必填 `int`，消除潜在的 `None` 传播。  
   - 为 `AttentionImpl` 新增 `num_heads、head_size、scale` 的必备属性声明，帮助 MyPy 推断实现类的公共接口。  

3. **层基类（`attention_layer_base.py`）**  
   - 为 `AttentionLayerBase` 添加 `impl: AttentionImpl` 类型属性，统一实现与层之间的约束。  

4. **各后端实现（FlashAttention、FlashInfer、FlexAttention、GDN、Mamba、MLA 等）**  
   - 大量加入 **runtime assert**（如 `self.vllm_flash_attn_version is not None`），确保在缺失 FlashAttention 时快速报错。  
   - 对 `sliding_window`、`block_table` 等参数做 **显式 `list`/`None` 转换**，兼容后端期待的 `list[int] | None` 类型。  
   - 对 `max_cudagraph_capture_size` 等配置进行 **None‑安全判断**（先判断 `is not None` 再比较），避免 `None` 与整数比较导致异常。  
   - 在 `MLA`、`FlashMLASparse`、`ROCM_AITER_MLA_SPARSE` 等模块中把元数据类统一继承自 `AttentionMetadata`，完善协议继承链。  
   - 为 `KVSharingFastPrefillMetadata`、`create_fast_prefill_custom_backend` 等函数添加更精准的类型注解（如 `type[AttentionBackend]`）。  
   - 轻微的逻辑调整：如 `spec_token_tree` 解析前先判空、`topk_indices_buffer` 加了非空断言、`device_capability` 读取后判断 `None`，提升健壮性。  

5. **工具函数**  
   - `get_kv_cache_layout` 中引入局部 `cache_layout` 变量声明，消除未定义警告。  
   - `get_per_layer_parameters` 调用 `get_layers_from_vllm_config` 时加入 `# type: ignore`，让 MyPy 正确推断抽象基类。  

### 影响范围
- **核心层**：`vllm/v1/attention/*`（所有后端实现、元数据、抽象基类）  
- **模型执行**：涉及注意力前向的 `model_executor`、`attention_layer_base`、`kv_cache_interface` 等路径。  
- **工具链**：`tools/pre_commit/mypy.py`（CI 检查范围修改）。  

### 建议与注意事项
1. **运行时断言**：新增的 `assert self.vllm_flash_attn_version is not None` 会在未检测到 FlashAttention 的环境直接崩溃。若项目需要在不支持 FlashAttention 的平台上仍能跑（例如 CPU／ROCm），建议改为抛出自定义异常或回退到安全实现，而不是硬性断言。  
2. **block_size 必填**：抽象层改为 `int`，但部分实现（如 `flash_attn`）仍声明 `int | None`。请确认所有调用处已提供明确的块大小，或在上层配置中补全默认值，以免出现 `TypeError`。  
3. **兼容性**：部分后端在 `max_cudagraph_capture_size` 为 `None` 时会直接使用 `self.max_cudagraph_size`，这在旧配置中可能改变 cudagraph 行为（以前 `min(..., None)` 会抛错）。建议在文档或配置说明中明确该字段的可选性及默认策略。  
4. **测试覆盖**：  
   - 在 GPU、CPU、ROCm 三种平台上跑完整的单元/集成测试，确保断言不会误触。  
   - 对 `MLA`、`FlashInfer`、`Mamba` 等新加入的类型路径进行 **mypy** 全库检查，确认无残余 `Any`。  
   - 验证 `sliding_window` 为 `None` 时的代码路径（如 `cascade_attention`）仍能正常执行。  
5. **文档更新**：由于 `AttentionMetadata` 成为多个新元数据类的基类，建议在项目文档中补充该协议的字段说明，防止后续实现者遗漏必要属性。  

总体来看，此次提交通过细粒度的类型注解与断言显著提升了代码的可维护性与 `mypy` 检查通过率，对功能本身几乎无影响。但请在多平台环境下验证新增断言的容错行为，防止因环境差异导致意外崩溃。

---

### [Misc][BE] Type coverage for vllm/compilation [1/3] (#31554)
**SHA**: `873480d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/873480d133f3e32743a0e187b03da1e67635bdc2)

**🎯 变更类型**：类型覆盖（Type‑coverage）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交为 vllm 编译子系统新增/完善了大量 **PEP‑484** 类型注解，统一了 `Generator`、`Callable`、`ParamSpec` 等返回/参数签名的写法，并为多个内部函数补全 `-> None`、`-> bool`、`-> str` 等返回类型，同时将若干 `device: str` 参数宽松为 `str | None`，提升代码可检查性与 IDE 支持。  

**🎯 影响范围**  
- `vllm/compilation/*`：`backends.py、compiler_interface.py、counter.py、cuda_graph.py、fx_utils.py、inductor_pass.py、monitor.py、partition_rules.py、sequence_parallelism.py、torch25_custom_graph_pass.py、vllm_inductor_pass.py` 均加入或修改了类型声明。  
- 依赖 `torch._inductor`、`torch._dynamo` 的运行时路径未改变，但函数签名的细微调整（如 `example_inputs: list[Any]` → `Sequence[Any]`）可能影响外部调用者的类型推断。  

**💡 关注建议**  
1. **运行时兼容性**：类型标注仅是静态检查，确保未因 `device: str | None` 等宽松改动导致实际逻辑分支误判（如 `if device:`）。  
2. **调用方更新**：部分方法（如 `CompilerManager.compile_context`、`set_inductor_config`）返回 `Generator`，若有手动 `next()` 或 `as` 使用，请确认仍符合 `with` 语义。  
3. **测试与 CI**：在 CI 中加入 `mypy --strict`（或项目自定义的 `pyright`）检查，确保新注解不产生未捕获的类型错误。  
4. **文档同步**：更新 `README`/`dev` 文档中对 `CompilationConfig`、`VllmConfig` 等对象的使用示例，反映新的可选 `device` 参数。  
5. **性能无影响**：新增 `assert batch_descriptor is not None` 和少量 `Generator` 导入不会影响执行性能，可放心合入。  

总体来看，此次改动提升了代码可维护性和可检验性，建议在合并前完成全套类型检查并回归现有单元/集成测试。

---

### [Frontend] Implement robust video frame recovery for corrupted videos (#29197)
**SHA**: `6f35154` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6f351548b258d7ff618174817bfbdc0ee4758fb5)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在文档中新增 “Video Frame Recovery” 章节，说明通过 `--media-io-kwargs '{"video": {"frame_recovery": true}}'` 可以在读取受损或截断视频时使用前向扫描恢复帧。  
2. `vllm/multimodal/video.py` 中加入 `_read_frames_with_recovery`、`_can_use_for_recovery` 两个静态方法，实现基于动态窗口的前向恢复逻辑；为 `load_bytes` 接口增加 `frame_recovery: bool = False` 参数，分别在普通和 `opencv_dynamic` 两个后端中使用该逻辑。  
3. 新增 3 组单元测试，分别验证：① 人为 `grab` 失败的恢复；② 真实受损视频的恢复；③ 动态后端下的恢复。  
4. 小幅修正 `benchmarks/datasets.py` 中临时文件创建方式，以及在 `video.py` 中的 `TYPE_CHECKING` 导入。

**🔍 影响范围**  
- 视频读取后端 (`opencv`、`opencv_dynamic`) 的所有调用路径。  
- CLI `vllm serve` 参数解析——需要透传 `frame_recovery`。  
- 文档和测试套件。

**💡 关注建议**  
1. **性能**：恢复路径会遍历到下一个目标帧，最坏情况仍是完整遍历视频。建议在日志中提示扫描长度，或提供 `max_recovery_window` 限制，以免在超长视频上出现 O(N) 开销。  
2. **错误处理**：当 `failed_frames` 在视频结束前仍未得到恢复时仅记录警告，若业务对完整采样有硬要求，建议抛出异常或在元数据中返回 `unrecovered_frames`。  
3. **向后兼容**：新增 `frame_recovery` 参数默认 `False`，兼容旧代码；但 CLI‑side 需要确保 `--media-io-kwargs` 解析时不会因未知键导致错误。  
4. **代码可维护性**：`_read_frames_with_recovery` 中对 `cv2` 的局部导入与 `TYPE_CHECKING` 兼容性良好，建议在文件顶部加 `# noqa: F401` 注释以避免未使用警告。  
5. **测试**：当前测试依赖本地 `corrupted.mp4`，CI 环境需确保该资源存在并且 OpenCV 可读取；若未来更换后端，需同步更新恢复逻辑的对应实现。  

总体来看，功能实现完整、文档与测试同步，唯一需要关注的是恢复过程的性能与异常可见性，适当加入配置或显式警告即可提升稳健性。

---

### [Spec Decode][UX] Add acceptance stats to `vllm bench serve` report (#31739)
**SHA**: `d498997` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d49899732edd3e6c011ec9f922601d919250a4d2)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
在 `vllm/benchmarks/serve.py` 中新增对服务器 Prometheus `/metrics` 接口的抓取与解析，实现 **Speculative Decoding（投机解码）接受率**、接受长度、草稿数量、草稿/接受 token 数等统计，并在 benchmark 报告中以 JSON 字段和打印表格形式输出。  

**🎯 影响范围**：  
- `serve.py`（新增 `SpecDecodeMetrics`、`fetch_spec_decode_metrics`、相关计算与结果注入）  
- 依赖 `serve.py` 的 CLI `vllm bench serve`  
- 可能影响下游代码对 benchmark 结果字典的键假设（如仅期待原有字段）  

**💡 关注建议**：  
1. **鲁棒性**：当前解析仅在行前缀 `vllm:spec_decode` 时生效，若指标名称或标签顺序变化会导致全部返回 `None`，建议在解析前加上明确的正则匹配并容错。  
2. **除零安全**：`delta_drafts`、`delta_draft_tokens` 为 0 时仍会执行除法，虽已有 `if delta_drafts > 0`、`if delta_draft_tokens > 0` 保护，但后续 `acceptance_length = 1 + delta_accepted / delta_drafts` 仍在 `delta_drafts > 0` 条件外，需要移入分支。  
3. **性能开销**：每次 benchmark 前后各一次 HTTP GET，若指标服务响应慢会延长基准时间。建议使用较小的 timeout 并在超时时记录警告而非阻塞。  
4. **兼容性**：当服务器未开启投机解码或未暴露指标时，新增字段会缺失。请在文档中说明该行为，并在消费方使用 `result.get("spec_decode_acceptance_rate")` 而非直接索引。  
5. **测试覆盖**：添加单元测试模拟 `/metrics` 返回不同组合（仅 drafts、仅接受、无指标），验证 `fetch_spec_decode_metrics` 的返回值和后续统计的正确性。  

总体而言，此次改动为 benchmark 工具引入了对投机解码的重要可观测性，若按以上建议完善异常处理与文档说明，可安全上线。

---

### Fix RecursionError in MediaWithBytes unpickling (#31191)
**SHA**: `ada6f91` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ada6f91d561ab693fd85f028bc44b8c8058d3073)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 为 `MediaWithBytes` 在 `vllm/multimodal/base.py` 中的 `__getattr__` 增加了 “media 未初始化” 的防护，防止在 pickle 反序列化期间因属性递归而抛出 `RecursionError`。  
- 新增 `tests/multimodal/test_image.py` 中的 `test_media_with_bytes_pickle_roundtrip`，验证 pickle/unpickle 的完整性并作为回归测试。  
- 将该测试文件加入 `tools/pre_commit/check_pickle_imports.py`，确保在 CI 中检查 pickle 兼容性。  

**🎯 影响范围**：  
- `vllm.multimodal.base.MediaWithBytes`（核心实现）  
- 多模态相关的单元测试路径 `tests/multimodal/`  
- 预提交脚本 `tools/pre_commit/check_pickle_imports.py`  

**💡 关注建议**：  
1. **代码审查**：确认 `__getattr__` 中的 `raise AttributeError(name)` 不会影响已有的属性访问路径；如有特殊属性需要在对象未初始化时仍可访问，可在此处添加显式处理。  
2. **序列化兼容**：如果项目在未来对 `MediaWithBytes` 添加 `__setstate__`/`__reduce__`，请保持与当前防护逻辑一致，避免再次触发递归。  
3. **测试维护**：该回归测试已加入 CI，后续对 `MediaWithBytes` 的改动务必保持通过。若新增类似包装类，也考虑加入对应的 pickle 测试。  
4. **文档更新**：在 multimodal 章节补充说明 `MediaWithBytes` 现在支持安全的 pickle 序列化，方便用户在分布式环境下传递媒体对象。  

整体来看，此次改动消除了已报告的 `RecursionError`，对库的稳定性提升明显，风险有限，只需关注防护逻辑对异常路径的影响即可。

---

### [ROCm][CI] Fix tests/compile unit tests (#28895)
**SHA**: `c071636` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c07163663d0a5ab6db1e4833c44305545f847c85)

**🎯 变更类型**：其他（CI/测试适配）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交为 ROCm CI 环境修正多项单元测试，使其在 AMD GPU（ROCm）上能够通过。主要通过检测平台（`current_platform.is_rocm()`）动态切换注意力后端、跳过不支持的特性并修正 ROCm 下 Tensor 初始化行为。  

**🎯 影响范围**：  
- `tests/compile/fullgraph/*`（基本正确性、cudagraph 测试）  
- `tests/compile/test_noop_elimination.py`（模型内部张量初始化）  
- 相关平台检测代码：`vllm/platforms.py`、`vllm/attention/backends/registry.py`（新增 `AttentionBackendEnum` 引入保守检查）  

**💡 关注建议**：  
1. **平台检测统一**：建议在生产代码中也统一使用 `current_platform.is_rocm()`，避免硬编码后端名称。  
2. **张量初始化**：ROC​m 的 `torch.empty` 不保证内存已清零，已改为 `torch.randn`，若后续对数值无随机性要求，请在模型层显式初始化（如 `torch.zeros`）或添加注释说明。  
3. **跳过的测试**：`test_basic_correctness` 中对编码器自注意力的跳过说明了 ROCm 尚未实现对应后端，后续若实现 `ROCM_ATTN`，请移除 `skipif` 标记并更新 CI 配置。  
4. **注意力后端兼容**：`test_full_cudagraph` 新增对 `FLASHINFER` 的 ROCm 跳过，确保以后新增后端时同步更新 `AttentionBackendEnum` 与平台检查。  

总体来看，此次改动提升了 CI 的跨平台可靠性，对运行时行为无影响，只需在后续功能实现时保持平台检测的一致性即可。

---

### [Perf] Async Scheduling + Speculative Decoding + Structured Outputs (#29821)
**SHA**: `f7008ce` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f7008ce1c4c1cf92d8cc13ad7e966d4b82a32c65)

**变更类型**：功能增强（异步调度 + 采样推断 + 结构化输出）  
**重要程度**：🟡 中  

**变更摘要**  
- 为 AsyncScheduler 增加对结构化输出（JSON schema）在 Speculative Decoding 场景下的支持；  
- 在 `SchedulerOutput` 中新增 `has_structured_output_requests`、`num_invalid_spec_tokens` 等字段，用来记录是否存在结构化请求及因语法不匹配被剔除的草稿 token；  
- `SchedulerInterface` 拓展抽象方法为 `update_draft_token_ids_in_output`，并在 `Scheduler` 实现中完成草稿 token 的校验与补齐；  
- `EngineCore` 与 `GPUModelRunner` 增加异步复制草稿 token 到 CPU、同步取回以及在延迟调度时更新 scheduler_output 的逻辑；  
- 测试层加入结构化输出相关的采样参数并调整 `max_tokens`/`min_tokens` 的限制。

**影响范围**  
- `vllm/v1/core/sched/*`（调度器、输出对象、抽象接口）  
- `vllm/v1/engine/*`（核心执行流）  
- `vllm/v1/worker/gpu_model_runner.py`（GPU 端运行时）  
- 相关单元测试 `tests/v1/e2e/test_async_scheduling.py`

**关注建议**  
1. **抽象实现完整性**：所有现有调度器实现需实现新接口 `update_draft_token_ids_in_output`，否则运行时会触发 `NotImplementedError`。  
2. **异步拷贝正确性**：`GPUModelRunner._copy_draft_token_ids_to_cpu` 在 `use_async_scheduling` 且无结构化输出时会直接返回，需确认在混合场景（部分请求结构化、部分普通）下 `draft_token_ids_event` 与 `valid_sampled_token_count_event` 同步顺序不产生竞争。  
3. **性能开销**：复制草稿 token 至 CPU、额外的 `validate_tokens` 可能对吞吐量产生影响，建议在高并发条件下开启基准测试，评估是否需要在 `num_spec_tokens` 较大时开启分块拷贝或批量校验。  
4. **错误容忍**：`update_draft_token_ids_in_output` 中若 grammar 拒绝 token，现仅记录 warning 并用 `-1` 填充，后续统计 `num_invalid_spec_tokens` 会从 draft 计数中扣除。请确认 downstream（如 acceptance rate 统计）已正确使用该字段，避免负数或遗漏。  
5. **兼容性**：`_validate_supported_sampling_params` 移除了对 `structured_outputs` 的限制，但仍在 `async scheduling + spec decoding` 场景下禁止 penalties/bad_words。若未来要放宽这些限制，需要同步更新错误提示与文档。  
6. **测试覆盖**：当前仅在 E2E 测试中加入结构化输出，建议补充单元测试：① 语法不匹配导致 token 被剔除的路径；② 多请求混合（结构化 + 非结构化）期间的 `has_structured_output_requests` 标记正确性；③ `take_draft_token_ids` 在 `zeros_only` 分支的行为。  

总体来看，改动为 AsyncScheduler 引入结构化输出的关键特性，代码路径已基本闭环，但需留意异步拷贝的同步细节以及在高并发下的性能与正确性。

---

### [Bugfix] Fix GLM-4 MoE router logits dtype for data parallel chunking (#31055)
**SHA**: `4e67a8f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4e67a8f616f4f202fd7a549978914f398a738d49)

**变更类型**：Bug 修复  
**重要程度**：🟡 中  

**变更概述**  
1. 在 `FusedMoEConfig` 中新增可选字段 `router_logits_dtype`，默认取 `in_dtype`。  
2. `FusedMoE` 初始化与 `ensure_dp_chunking_init` 中使用该字段，确保路由 logits 缓冲区的 datatype 与配置保持一致。  
3. `GLM‑4 MoE` 现显式传入 `router_logits_dtype=torch.float32`，修正在数据并行切片 (DP chunking) 时出现的 dtype 不匹配错误。

**影响范围**  
- `vllm/model_executor/layers/fused_moe/config.py`  
- `vllm/model_executor/layers/fused_moe/layer.py`  
- `vllm/model_executor/models/glm4_moe.py`（以及所有基于 `FusedMoE` 的模型）

**关注建议**  
- **兼容性**：`router_logits_dtype` 默认回落到 `in_dtype`，所以现有模型在未显式指定时仍保持原行为。若项目中有自定义 `FusedMoE` 使用不同输入 dtype（如 `bfloat16`）但希望 logits 为 `float32`，请显式传参。  
- **性能与显存**：在 `fp16/bf16` 推理场景下，将 logits 存为 `float32` 会额外占用约 2×显存，请评估对吞吐量的影响。  
- **测试**：验证所有 MoE 相关模型在单卡、Tensor‑Parallel 与 Data‑Parallel 多卡组合下的前向/后向是否仍通过，特别是使用 `torch.float16`、`torch.bfloat16` 以及 `torch.float32` 的混合配置。  
- **文档**：在 `FusedMoE` 使用说明中补充 `router_logits_dtype` 参数的意义、默认行为以及推荐使用场景。  

总体而言，本次改动定位明确、影响范围受限，只要在自定义 MoE 实例中检查 dtype 对齐即可平滑迁移。

---

### [Log] add log about gpu worker init snapshot and requested memory (#29493)
**SHA**: `6f5e653` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6f5e65338346a0be4466bfb31423c2968b7363bd)

**🎯 变更类型**：功能增强 / 可观察性提升  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm/utils/mem_utils.py` 新增 `format_gib()`，统一把字节数转为保留两位小数的 GiB 并返回 `float`。  
2. 为 `MemorySnapshot` 实现 `__repr__`，使用 `format_gib` 输出可读的内存快照信息。  
3. 将 `request_memory` 的返回值从 `float` 改为 `int`（向上取整），并在错误信息中使用 `format_gib`。  
4. 大量改写 `gpu_worker.py`、`utils.py` 中的日志/调试信息，统一使用 `format_gib` 替代手写除法和 `GiB_bytes` 常量。  

**🎯 影响范围**  
- `vllm.utils.mem_utils`（新增函数、`MemorySnapshot.__repr__`、返回类型变化）  
- `vllm.v1.worker.gpu_worker`（日志、调试、错误信息）  
- `vllm.v1.worker.utils`（`request_memory` 实现）  
- 其它直接或间接引用 `MemorySnapshot`、`request_memory`、`GiB_bytes` 的模块。  

**💡 关注建议**  
1. **兼容性检查**：`request_memory` 现在返回 `int`，而旧代码可能期待 `float`（如做除法或比较）。请搜索项目中 `request_memory(` 的调用，确保没有依赖小数精度；若有，改为使用返回的整数或自行除以 `GiB_bytes`。  
2. **日志/监控**：`format_gib` 用 `round(..., 2)`，在极大数值上可能产生轻微误差。若对精度有严格要求，建议在关键位置保留原始字节数进行后续计算，仅在展示时使用 `format_gib`。  
3. **单元测试**：新增 `MemorySnapshot.__repr__`，应补充对应测试，验证输出格式及数值的两位小数。  
4. **文档更新**：`request_memory` 签名已改为返回 `int`，请在 API 文档和注释中同步更新说明。  
5. **依赖清理**：`GiB_bytes` 常量在本次改动中多数被删除，确认项目其余文件不再直接引用该常量，否则会出现未导入错误。  

总体来看，此次改动提升了内存信息的可读性和日志一致性，对功能没有实质性影响，只需注意返回类型的兼容性并补充相应测试即可。

---

### [Attention][2/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31774)
**SHA**: `4c73be1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4c73be14e0397e99162ca13a8b559670c5abd3b0)

**🎯 变更类型**：重构 / 代码清理  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：删除 `CommonAttentionMetadata` 中已废弃的属性 `seq_lens_cpu`、`num_computed_tokens_cpu`，统一改为调用新方法 `compute_num_computed_tokens()`，相应地在 GDN、Mamba、Mamba2 三个注意力后端更新了实现。  

**🎯 影响范围**：  
- `vllm/v1/attention/backends/gdn_attn.py`  
- `vllm/v1/attention/backends/mamba_attn.py`  
- `vllm/v1/attention/backends/mamba2_attn.py`  
- 可能波及到使用旧属性的其它内部模块或外部插件。  

**💡 关注建议**：  
1. **功能验证**：确认 `compute_num_computed_tokens()` 在 CPU 与 GPU 上返回的张量与旧属性完全等价，尤其在 prefix‑caching 与 cudagraph capture 场景下的形状与 dtype。  
2. **残留引用**：在全仓库搜索 `seq_lens_cpu`、`num_computed_tokens_cpu`，确保没有遗漏的直接访问或 `_num_computed_tokens_cpu` 赋值。  
3. **性能回归**：跑完整测试集和基准（prefill、decode、batch‑size 扩展）确认改动未引入额外的 copy 或同步。  
4. **文档与兼容**：若对外暴露了旧属性，需要在升级指南中注明已删除，或提供兼容层。  
5. **代码规范**：统一使用 `to(..., non_blocking=True)` 或 `cpu()` 的位置，避免不必要的同步。  

总体来看，改动简化了元数据访问路径，提升了代码可维护性，只要完成上述验证即可安全合并。

---

### [Quantization][MoE] remove unused ep logic from moe marlin (#31571)
**SHA**: `2f4bdee` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2f4bdee61ee0dd9358efaba720b7acc53b2ece00)

**🎯 变更类型**：重构 / 代码清理  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交删除了 MoE Marlin 实现中 `is_ep`（expert parallelism）相关的参数与分支逻辑。`kernel.h`、`marlin_template.h`、`ops.cu`、Python/torch 绑定均同步去掉 `is_ep`，并简化了无效块的跳过代码，使 kernel 只依据 `expert_ids_ptr` 直接取块 ID。  

**🎯 影响范围**  
- `csrc/moe/marlin_moe_wna16/*`（kernel、模板、CUDA ops）  
- `csrc/moe/torch_bindings.cpp`、`vllm/_custom_ops.py`、`vllm/model_executor/layers/fused_moe/fused_marlin_moe.py`（Python API）  
- 任何调用 `moe_wna16_marlin_gemm` 的上层模型代码。  

**💡 关注建议**  
1. **接口兼容**：确认未在外部项目中仍保留 `is_ep` 参数；如有，需同步更新或提供兼容层。  
2. **功能验证**：运行包含 `expert_ids` 为 `-1`（填充）情形的单元测试，确保去除分支后行为保持不变。  
3. **文档更新**：删除 `is_ep` 参数说明，补充 “该实现已不再区分 expert parallelism”。  
4. **性能回归**：对比前后 kernel 的执行时间，确保简化分支带来预期的轻微加速或至少不引入回退。  

总体而言，此次清理消除死代码，降低维护成本，风险主要在 API 变更和对极端 padding 场景的兼容性，建议在 CI 中加入相应回归测试后即可合并。

---

#### 🟢 低重要度变更 (21)

### [ROCm][AITER] fix wrong argument passed to  AITER `flash_attn_varlen_func` (#31880)
**SHA**: `41cfa50` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/41cfa50632c26c6064cabbbc43c9fc29c7792a2d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 与 AITER 后端的 `flash_attn_varlen_func` 调用处，将错误的参数 `return_softmax_lse` 修正为正确的 `return_lse`，避免参数传递异常。

---

### [Bugfix][MTP] Fix GLM4 MoE fp8 loading with MTP on (#31757)
**SHA**: `d111bc5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d111bc53ad2fbb5f28671019d21f5f753436e46d)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `glm4_moe_mtp.py` 中将对 `positions == 0` 的屏蔽改为 `torch.where`，避免原地修改引发错误；新增对 LM head `.weight_scale` 参数的检查，若模型未构建对应量化头则跳过加载，防止 KeyError。

---

### [Model] Cleanup: Remove redundant manual definition of `make_empty_intermediate_tensors` in GLM-4-MoE (#31869)
**SHA**: `1f33e38` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1f33e38e81a1b29bb4261a5e4e1fa430198b251a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `glm4_moe.py` 中删除了冗余的 `make_empty_intermediate_tensors` 手动实现，统一使用基类默认实现，代码行数减少 14 行。

---

### [XPU]fallback to TRITON_ATTN on xpu when use float32 dtype (#31762)
**SHA**: `59fe6f2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/59fe6f298e16ee8d2f54e2567b76516807a4733b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 XPU 注意力后端选择中加入 `dtype` 检查，若使用 float32 则输出警告并回退到 Triton Attention，实现对不支持 float32 的 Flash Attention 的兼容。

---

### [Bugfix][Kernel] fix bias adding in triton kernel implemented fused moe (#31676)
**SHA**: `0dd5dee` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0dd5dee9b9bc88453f5f3eacfde751e6b9ba4871)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fused_moe_kernel` 中将偏置加法从量化前移到反量化后执行，并保持路由权重乘法顺序不变，修复了 Triton kernel 中的偏置错误。

---

### [Bugfix] Fix race condition in async-scheduling for vlm model (#31841)
**SHA**: `efeaac9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/efeaac92f22f8a0a26c6bb9b9182f316210bb19c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：为多模态模型的 `is_mm_embed` 引入双缓冲并在每次调度切换缓冲区，防止上一轮异步拷贝仍在读取 CPU 数据导致的竞争条件。

---

### [Attention][3/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31850)
**SHA**: `c7a79d4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c7a79d41a03f925942e8fb8bc589df4f39bcb950)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：移除对已废弃的 `seq_lens_cpu`（以及相关 `num_computed_tokens_cpu`）属性的直接使用，改为通过 `common_attn_metadata.seq_lens.cpu()` 获取序列长度，确保兼容性并消除警告。

---

### [ROCm][AITER] bugfix accuracy regression in ROCM_AITER_TRITON_MLA backend (#31816)
**SHA**: `6409004` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6409004b2656baa147a3ecc6577e5b24fc225541)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `AiterTritonMLABackend` 的基类由 `MLACommonBackend` 改为 `AiterMLABackend`，并删除不再使用的 `MLACommonBackend` 与 `AiterMLAMetadataBuilder` 导入，保持实现类继承不变，修复 ROCm AITER 精度回退问题。

---

### Change warning in get_current_vllm_config to report caller's line number (#31855)
**SHA**: `f09c5fe` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f09c5feb7c66340e35c06aba37478cce74c20c10)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `get_current_vllm_config` 中将警告日志的 `stacklevel` 设置为 2，使日志指向调用者的代码行，便于定位未设置配置的来源。

---

### [Doc] Update release docs (#31799)
**SHA**: `1b8af95` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1b8af957f62b96173eb9f57c6609f489cb99fda6)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：更新了 `RELEASE.md`，优化了发布说明的链接、发布节奏与版本号策略描述，并重构了章节内容，使其更符合 SemVer 及项目的实际发布流程。

---

### [Model] Enable LoRA support for PaliGemma (#31656)
**SHA**: `a051525` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a051525e071c2387641b17b95533fb51ed9363e1)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `PaliGemmaForConditionalGeneration` 添加 `SupportsLoRA` 接口，实现 LoRA 映射与 token 计数方法；文档相应标记 LoRA 支持。

---

### [1/2][lmcache connector] clean up lmcache multi-process adapter  (#31838)
**SHA**: `5b833be` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5b833be49e02fec2542396a01c9575dfe6e1def2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `lmcache` 多进程适配器中加入已弃用警告、将私有 `_cleanup_lookup_result` 改为公开 `cleanup_lookup_result`，并在 `lmcache_mp_connector.py` 添加兼容性导入与调用修正。整体只涉及文件微调，功能未变。

---

### [ROCm][CI] Fix plugin tests (2 GPUs) failures on ROCm and removing `VLLM_FLOAT32_MATMUL_PRECISION` from all ROCm tests (#31829)
**SHA**: `364a8bc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/364a8bc6dc7d8fc344f07f01adc6a2336887e9bd)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm CI 流水线中移除 `VLLM_FLOAT32_MATMUL_PRECISION="tf32"` 环境变量，以避免与 terratorch 的精度冲突；同时在 ROCm 测试依赖中加入 `albumentations` 以支持插件测试。

---

### [CI] Add warmup run in test_fusion_attn (#31183)
**SHA**: `9a1d20a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9a1d20a89c3b1f2c2687dee585b22c93f05b2310)

**🎯 变更类型**：代码重构/测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/compile/test_fusion_attn.py` 中加入 warm‑up 运行，设环境变量禁用编译缓存，先执行一次前向以初始化量化尺度；重构编译调用为先获取 `torch.compile` 对象再调用，确保融合注意力的量化尺度在首次前向后正确加载。

---

### [Bugfix] Handle mistral tokenizer in get_hf_processor (#31817)
**SHA**: `309a8f6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/309a8f66ee0daec7dbee5030dac1bcfcfad7b3ec)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `get_hf_processor` 中新增对 `MistralTokenizer` 的检测，若使用该 tokenizer 则转为其内部的 `transformers_tokenizer` 传递，以修复兼容性问题。

---

### [ROCm][CI] Pinning timm lib version to fix ImportError in Multi-Modal Tests (Nemotron) (#31835)
**SHA**: `e5d427e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e5d427e93af5861e22c2b7b3ce88af0028fc41e3)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `requirements/rocm-test.txt` 中固定 `timm==1.0.17` 版本，以解决 ROCm CI 下多模态模型测试（Nemotron）出现的 ImportError。

---

### [ROCm][CI] Fix ModernBERT token classification test numerical accuracy on ROCm (#31820)
**SHA**: `2a42ae7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2a42ae790d3f42a381b782a504947255aa0db090)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `tests/models/language/pooling/conftest.py`，在 ROCm 环境下禁用 Flash 与 Memory‑Efficient SDP，启用 Math SDP 并将矩阵乘精度设为 high，以规避 HuggingFace Transformers 的数值准确性问题。

---

### Report error log after vllm bench serve (#31808)
**SHA**: `dba9537` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dba95378a66884c889b5dc9428ea68285a908658)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/benchmarks/serve.py` 中新增对基准运行期间失败请求的错误日志输出，最多显示前 10 条错误信息，以便快速定位问题。

---

### make 500: InternalServerError more informative (#20610)
**SHA**: `142c4d1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/142c4d173896b02e6b73e2dd05493c1f180c5977)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `async_llm.py` 中的异常处理日志改为输出具体异常类型及信息，并在打印异常时加入容错，以提供更清晰的 500 错误提示。

---

### [PERF] Speed-up of GDN attention decode part (Qwen3-Next) (#31722)
**SHA**: `22dffca` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/22dffca9822987f0e912bfd9635e94bbdd05def3)

**🎯 变更类型**：代码重构（性能优化）  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `fused_recurrent_gated_delta_rule_fwd` 中对 `BV` 的上限从 `8` 提升至 `32`（`min(triton.next_power_of_2(V), 32)`），以加快 Qwen3‑Next GDN 注意力解码的执行速度。

---

### [NemotronH] Use ReplicatedLinear for fc1_latent_proj (#31807)
**SHA**: `28c9477` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/28c94770adfcb9cfbc78a3221f915bfc830c6582)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 NemotronH 模型中，将 `fc1_latent_proj` 从 `ColumnParallelLinear` 替换为 `ReplicatedLinear`，去除不必要的 `gather_output` 参数，简化层实现。

---

