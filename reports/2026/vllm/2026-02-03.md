# 每日更新报告（2026-02-03）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-03 23:57:56 | Cyrus Leung | [Bugfix] Fix startup hang for Granite Speech (#33699) |
| 2026-02-03 23:22:34 | Patrick von Platen | [Voxtral models] Skip warm-up to skip confusing error message in warm-up (#33576) |
| 2026-02-03 22:47:41 | Shanshan Shen | [MM] Pass `prefix` parameter to MMEncoderAttention (#33674) |
| 2026-02-03 22:43:47 | wang.yuqi | [Bugfix] Do not add extra \n for image-only cases when constructing multimodal text prompts. (#33647) |
| 2026-02-03 21:52:49 | shaharmor98 | Feat/add nemotron nano v3 tests (#33345) |
| 2026-02-03 21:50:15 | Kuntai Du | [Bugfix][Async][Connector] avoid vllm-side double free during async scheduling + request abort + async KV cache transfer (#33377) |
| 2026-02-03 21:49:59 | Krish Gupta | Document NixlConnector backend selection via kv_connector_extra_config (#33552) |
| 2026-02-03 21:49:49 | Harry Mellor | Fix Gemma3n audio encoder for Transformers v5 (#33673) |
| 2026-02-03 21:49:45 | zxy | [Models] Intern-S1-Pro (#33636) |
| 2026-02-03 20:36:53 | Harry Mellor | Fix Gemma3 GGUF for Transformers v5 (#33683) |
| 2026-02-03 20:07:24 | Harry Mellor | Fix offline test for Transformers v5 (#33682) |
| 2026-02-03 19:33:56 | Song Zhixin | [Bugfix] fix qwen3-asr response error (#33644) |
| 2026-02-03 19:13:16 | Cyrus Leung | [Misc] Update default image format of `encode_base64` (#33656) |
| 2026-02-03 18:37:15 | Michael Goin | [Bugfix] Disable RoutingMethodType.[Renormalize,RenormalizeNaive] TRTLLM per-tensor FP8 MoE (#33620) |
| 2026-02-03 18:29:18 | Cyrus Leung | [Refactor] Clean up pooling serial utils (#33665) |
| 2026-02-03 16:35:58 | Lucas Hänke de Cansino | [Bugfix][Model] Fix DeepSeek-OCR-2 chat template to include BOS token (#33642) |
| 2026-02-03 15:49:17 | Isotr0py | [CI/Build] Investigate torchrun distributed tests hanging issue (#33650) |
| 2026-02-03 15:16:55 | Richard Zou | [torch.compile] Document the workaround to standalone_compile failing (#33571) |
| 2026-02-03 15:01:59 | 杨朱 · Kiki | [Misc] Remove deprecated VLLM_ALL2ALL_BACKEND environment variable (#33535) |
| 2026-02-03 15:00:00 | Nick Hill | [Minor] Some code simplification in `scheduler.py` (#33597) |
| 2026-02-03 14:58:44 | 杨朱 · Kiki | [Misc] Remove deprecated profiler environment variables (#33536) |
| 2026-02-03 14:46:10 | Kunshang Ji | [XPU][1/N] Deprecate ipex and switch to vllm-xpu-kernels for xpu platform (#33379) |
| 2026-02-03 14:46:05 | Chauncey | [Bugfix] Interleaved thinking keeps compatibility with reasoning_content (#33635) |
| 2026-02-03 14:32:39 | 杨朱 · Kiki | [CI/Build] Remove hardcoded America/Los_Angeles timezone from Dockerfiles (#33553) |
| 2026-02-03 14:31:27 | Shengliang Xu | Fix quantized Falcon-H1 model loading issues (#32728) |
| 2026-02-03 13:51:10 | Daniel Mescheder | [Frontend] Add sampling parameters to Responses API (#32609) |
| 2026-02-03 12:56:25 | Roger Wang | [Bugfix] Fix mm budget setting for Qwen Omni models (#33634) |
| 2026-02-03 12:17:56 | Radu Salavat | [Feature][CPU Backend]: Optimize ARM vectorization backend (#30329) |
| 2026-02-03 11:38:49 | Richard Zou | [torch.compile] Don't do the fast moe cold start optimization if there is speculative decoding (#33624) |
| 2026-02-03 10:48:06 | Nathan Weinberg | [CI/Build] add directions for CPU image upload to Docker Hub (#32032) |
| 2026-02-03 08:56:44 | Dezhan | [BugFix] DPMetadata raises assert error for dense model (#32739) |
| 2026-02-03 06:01:47 | Patrick von Platen | [Voxtral Realtime] Introduce global log mel max (#33574) |
| 2026-02-03 03:47:46 | Lain | fix cutlass_3x_gemm_fp8_blockwise on sm103a (#32224) |
| 2026-02-03 03:17:42 | Vasiliy Kuznetsov | fix memory for online fp8 quantization with streaming weight load (#31914) |
| 2026-02-03 02:57:12 | Matthew Bonanni | [UX] Format attention backend log line (#33570) |
| 2026-02-03 01:30:06 | yugong333 |   Reduce the kernel overhead when num of active loras is smaller than max loras. Multiple cuda graphs are captured for each num of active-loras. (#32005) |
| 2026-02-03 01:20:54 | Harry Mellor | Update huggingface-hub again (#33567) |
| 2026-02-03 01:11:44 | Harry Mellor | Remove incorrect tokenizer info test (#33565) |
| 2026-02-03 00:55:48 | Yang Liu | [Model] Use mm_position to compute mrope positions for GLM-4.xV (#33039) |
| 2026-02-03 00:10:02 | Matthew Bonanni | [CI] Add DeepSeek V3.2 nightly eval (#33566) |

### 📊 统计摘要
> 本日共 40 个提交 | 🔴高 3 | 🟡中 12 | 🟢低 25
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[Models] Intern-S1-Pro (#33636)](#a3acfa1)
    - [[Refactor] Clean up pooling serial utils (#33665)](#83449a5)
    - [[XPU][1/N] Deprecate ipex and switch to vllm-xpu-kernels ...](#e106044)
  - [🟡 中重要度变更 (12)](#-🟡-中重要度变更-12)
    - [[Voxtral models] Skip warm-up to skip confusing error mes...](#f0d5251)
    - [[MM] Pass `prefix` parameter to MMEncoderAttention (#33674)](#5c4f2dd)
    - [Feat/add nemotron nano v3 tests (#33345)](#4bc913a)
    - [[Misc] Remove deprecated VLLM_ALL2ALL_BACKEND environment...](#b95cc50)
    - [[Misc] Remove deprecated profiler environment variables (...](#ef248ff)
    - [[Frontend] Add sampling parameters to Responses API (#32609)](#4c4b6f7)
    - [[Feature][CPU Backend]: Optimize ARM vectorization backen...](#e69c990)
    - [[Voxtral Realtime] Introduce global log mel max (#33574)](#5019c59)
    - [fix cutlass_3x_gemm_fp8_blockwise on sm103a (#32224)](#089cd4f)
    - [fix memory for online fp8 quantization with streaming wei...](#0130223)
    - [Reduce the kernel overhead when num of active loras is sm...](#ffe1fc7)
    - [[Model] Use mm_position to compute mrope positions for GL...](#199e3cb)
  - [🟢 低重要度变更 (25)](#-🟢-低重要度变更-25)
    - [[Bugfix] Fix startup hang for Granite Speech (#33699)](#18e7cbb)
    - [[Bugfix] Do not add extra \n for image-only cases when co...](#f3d8a34)
    - [[Bugfix][Async][Connector] avoid vllm-side double free du...](#fbb3cf6)
    - [Document NixlConnector backend selection via kv_connector...](#2df2b34)
    - [Fix Gemma3n audio encoder for Transformers v5 (#33673)](#2a8d84e)
    - [Fix Gemma3 GGUF for Transformers v5 (#33683)](#be8168f)
    - [Fix offline test for Transformers v5 (#33682)](#f6af346)
    - [[Bugfix] fix qwen3-asr response error (#33644)](#ceab70c)
    - [[Misc] Update default image format of `encode_base64` (#3...](#52683cc)
    - [[Bugfix] Disable RoutingMethodType.[Renormalize,Renormali...](#e346e2d)
    - [[Bugfix][Model] Fix DeepSeek-OCR-2 chat template to inclu...](#dad2d6a)
    - [[CI/Build] Investigate torchrun distributed tests hanging...](#32e84fa)
    - [[torch.compile] Document the workaround to standalone_com...](#fd9c83d)
    - [[Minor] Some code simplification in `scheduler.py` (#33597)](#6139789)
    - [[Bugfix] Interleaved thinking keeps compatibility with re...](#bf001da)
    - [[CI/Build] Remove hardcoded America/Los_Angeles timezone ...](#a0a984a)
    - [Fix quantized Falcon-H1 model loading issues (#32728)](#f1cb9b5)
    - [[Bugfix] Fix mm budget setting for Qwen Omni models (#33634)](#10546f9)
    - [[torch.compile] Don't do the fast moe cold start optimiza...](#5eac9a1)
    - [[CI/Build] add directions for CPU image upload to Docker ...](#1b60b45)
    - [[BugFix] DPMetadata raises assert error for dense model (...](#4b3803d)
    - [[UX] Format attention backend log line (#33570)](#5d1aef3)
    - [Update huggingface-hub again (#33567)](#8b7346d)
    - [Remove incorrect tokenizer info test (#33565)](#6141ebe)
    - [[CI] Add DeepSeek V3.2 nightly eval (#33566)](#9f8cb81)
#### 🔴 高重要度变更 (3)

### [Models] Intern-S1-Pro (#33636)
**SHA**: `a3acfa1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a3acfa10719a931111caccd08ef19f1551b2fe1e)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 为 vLLM 添加对 Intern‑S1‑Pro（`internlm/Intern‑S1‑Pro`）模型的完整支持，包括模型实现、权重加载、文档、示例代码以及注册表的更新。  
2. 引入 **FOPE（Fourier‑Rotary Positional Embedding）** 实现，扩展现有 RoPE 体系，使其能够在 Intern‑S1‑Pro 等模型中使用 `use_fope` 参数。相关底层接口（`RotaryEmbedding`、`get_rope`）同步改造以支持可选的缓存初始化。  
3. 为 Qwen‑3‑MoE 系列模型提供可插拔的 decoder‑layer 类型参数，以便内部复用 Intern‑S1‑Pro 的自定义 Decoder（`InternS1ProMoeDecoderLayer`）。  
4. 细节修正：支持 `vision_config.deepstack_visual_indexes` 的可选字段、权重映射的 FOPE 兼容、测试用例及模型注册表同步更新。  

**🎯 影响范围**  
- **模型层**：`vllm/model_executor/models/interns1_pro.py`（全新模型实现），`rotary_embedding/fope.py`（新嵌入层），`rotary_embedding/base.py` 与 `rotary_embedding/__init__.py`（缓存初始化开关）。  
- **通用框架**：`vllm/model_executor/models/qwen3_moe.py`、`qwen3_vl.py`、`qwen3_vl_moe.py`（decoder‑layer 可配置化）。  
- **注册表 & 文档**：`registry.py`、`supported_models.md`、示例脚本 `vision_language.py`、单元测试 `tests/models/registry.py`。  
- **多模态子系统**：`MULTIMODAL_REGISTRY` 中的处理器注册与视觉 transformer 初始化。  

**🔍 技术洞察**  

- **架构影响**  
  - **模型抽象**：通过继承 Qwen‑3‑MoE 基类复用现有 MoE、缓存、并行实现，仅在注意力层加入 `FourierRotaryEmbedding`，保持整体模块化，新增模型可无缝加入现有 pipeline。  
  - **RoPE 扩展**：`FourierRotaryEmbedding` 在 `RotaryEmbedding` 基类上实现，需要在 `RotaryEmbedding.__init__` 中显式控制缓存创建 (`init_cache=False`)，避免在模型并行初始化阶段对未就绪的 `cos_coef`/`sin_coef` 进行 `register_buffer`。  
  - **解耦层实现**：`Qwen3MoeModel` 与 `Qwen3MoeLLMModel` 引入 `decoder_layer_type` 参数，使得任意自定义 decoder（如 Intern‑S1‑Pro）可在同一框架下实例化，提升代码复用率。  

- **性能影响**  
  - **缓存延迟初始化**：对使用 FOPE 的模型在首次前向时才生成 cos/sin 缓存（`init_cache=False → update_cache`），可显著降低启动时的显存占用，尤其在大模型（>30B）并行环境下。  
  - **并行注意力**：`InternS1ProMoeAttention` 仍复用已有的 `Attention` 实现，保持原有的 `flashinfer`/`triton` 加速路径不受影响。  
  - **MoE 计算**：`InternS1ProMoeSparseMoeBlock` 与 `FusedMoE` 继续使用现有的分布式 MoE 计算路径，新增的 `fope_sep_head` 仅在 `cos/sin` 产生阶段做矩阵乘法，无额外的前向算子开销。整体性能预计与原 `Intern‑S1` 相当，FOPE 计算开销极小（O(head_dim)）。  

- **安全考虑**  
  - 代码改动均在内部实现，无外部网络交互。唯一安全相关点是新增的 **权重加载**：`weight_loader` 会对 `num_key_value_heads` 进行分片并复制，仅在张量并行环境下执行，未引入不受信任的文件路径。  
  - `trust_remote_code=True` 仍需用户自行评估远程代码安全性——与原有模型保持一致。  

**⚠️ 潜在风险**  

| 风险点 | 描述 | 风险等级 |
|--------|------|----------|
| **FOPE 参数未传递或不兼容** | 若模型配置中缺失 `fope_*` 参数，`use_fope` 仍会被置 `True`（由 `rope_scaling` 检测），导致 `FourierRotaryEmbedding` 在缺少 `cos_coef`/`sin_coef` 的情况下初始化失败。 | 中 |
| **并行缓存冲突** | `init_cache=False` 对所有 `RotaryEmbedding` 子类生效，若其他模型（不使用 FOPE）误开启此标志，可能出现首次前向时未生成缓存的错误。 | 低 |
| **权重映射路径错误** | `hf_to_vllm_mapper` 新增的后缀映射 `.rotary_emb.sin_coef/.cos_coef` 必须与 HF 权重名称保持一致，若 HF 侧更改命名将导致加载失败。 | 中 |
| **注册表遗漏** | 新增模型在 `registry.py` 中的 `is_available_online=False`，若 CI 环境仍尝试在线拉取，会报错。 | 低 |
| **深度堆叠配置兼容性** | `vision_config.deepstack_visual_indexes` 现在是可选的，若上层代码仍默认访问该属性（未使用 `hasattr`）会抛 `AttributeError`。 | 低 |

**💡 关注建议**  

1. **参数完整性检查**  
   - 在模型初始化前添加断言，确保 `rope_scaling` 中所有 `fope_*` 键均提供（即使为 `None`），防止意外开启 `use_fope`。  
   - 若项目需要向后兼容老的 `Intern‑S1` 配置，可在 `InternS1ProMoeDecoderLayer` 中做 `use_fope = any(... )` 的安全默认 `False`。  

2. **缓存初始化日志**  
   - 在 `FourierRotaryEmbedding` 的 `forward_native` 首次更新缓存时打印调试日志（仅在调试模式），帮助定位因 `init_cache=False` 导致的首次前向性能波动。  

3. **权重映射维护**  
   - 将 FOPE 相关的权重名写入 `tests/models/registry.py` 或新增专门的映射单元测试，防止 HF 官方仓库改名导致加载错误。  

4. **文档同步**  
   - 在 `docs/models/supported_models.md` 中注明 `Intern‑S1‑Pro` 必须配合 `use_fope` 参数，且对应的 `max_position_embeddings` 为 8192（或更大），帮助使用者正确配置 `EngineArgs`。  

5. **CI/测试**  
   - 增加针对 `FourierRotaryEmbedding` 的单元测试：验证在不同 `fope_sep_head`、`num_inv_freq` 配置下 cos/sin 缓存的形状与数值一致性。  
   - 在多机多卡环境下跑一次完整的 `offline_inference/vision_language.py`，确保 `interns1_pro` 能够与 `interns1` 并行运行而不产生显存冲突。  

6. **安全审计**  
   - 继续保持 `trust_remote_code=True` 的使用警示，在 Release Note 中提醒用户仅在可信网络环境下加载 `internlm/Intern‑S1‑Pro`。  

> **总结**：本次 PR 为 vLLM 引入了对最新的 Intern‑S1‑Pro 模型的原生支持，并通过创新的 Fourier‑Rotary Positional Embedding（FOPE）实现了更灵活的相对位置编码，同时保持了原有的高效 MoE、并行和多模态框架。若对上述风险点进行适度防护，项目将在功能、模型覆盖率与可扩展性方面获得

---

### [Refactor] Clean up pooling serial utils (#33665)
**SHA**: `83449a5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/83449a5ff04f70a20c24e8e6fc719881b29e10ac)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将原先散落在 `vllm.utils.serial_utils` 中的嵌入数据序列化/反序列化逻辑以及 dtype 映射抽取到新模块 `vllm.entrypoints.pooling.utils`，并对外提供统一的 `encode_*`、`decode_*`、`build_metadata_items` 等 API。  
- 用 `EMBED_DTYPES`（`Mapping[EmbedDType, DTypeInfo]`）取代旧的 `EMBED_DTYPE_TO_TORCH_DTYPE`、`EMBED_DTYPE_TO_N_BYTES` 等多组字典，实现 dtype、view 和字节大小的统一管理。  
- 删除 `serial_utils.encode_pooling_output`，改为在 `pooling.utils` 中分别实现 `encode_pooling_output_float/base64/binary`。  
- 调整了 embedding 与 pooling 两大服务的响应构造路径，新增 `request_output_to_*_json_response` 与 `*_bytes_response` 辅助函数，降低重复代码。  
- 更新所有示例、单元测试和导入路径以使用新 API。  

**🎯 影响范围**：  
- `vllm.utils.serial_utils`（核心序列化工具）  
- 新增 `vllm.entrypoints.pooling.utils`（公共池化/嵌入序列化层）  
- `vllm/entrypoints/pooling/embed/serving.py`、`vllm/entrypoints/pooling/pooling/serving.py`（响应构造逻辑）  
- 示例脚本 `examples/pooling/*`、相关测试 `tests/entrypoints/pooling/*`、`tests/utils_/test_serial_utils.py`  
- 任何直接使用旧常量 `EMBED_DTYPE_TO_TORCH_DTYPE`、`EMBED_DTYPE_TO_N_BYTES` 的外部代码（需迁移）  

---

## 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 1. **职责划分更清晰**：序列化/反序列化统一放在 `pooling.utils`，`serial_utils` 仅保留通用的 `tensor2binary/base64` 与 `binary2tensor`。<br>2. **可复用性提升**：`encode_pooling_output_*`、`encode_pooling_bytes` 与 `build_metadata_items` 对外公开，后续除 pooling 之外的模块（如检索、RAG）可直接复用。<br>3. **接口统一**：通过 `EmbeddingResponse` 与 `PoolingResponse` 的构造函数统一调用 `request_output_to_*`，避免在不同文件中重复实现相同的循环计数、metadata 生成等逻辑。 |
| **性能影响** | - **序列化路径未变**：底层仍使用 `torch.Tensor.to(...).numpy()` → `bytes`，仅将 dtype 映射抽象为 `DTypeInfo`，不会产生额外计算开销。<br>- **函数调用略有增加**：新增的包装函数（如 `request_output_to_embed_json_response`）带来一次 `cast` 与 `partial` 的轻量封装，影响可以忽略（微秒级）。<br>- **代码体积减小**：`serial_utils` 中冗余的 `encode_pooling_output` 被删掉，模块加载更快，内存占用略降。 |
| **安全考虑** | - **Base64/二进制编码** 仍使用 `pybase64`（原来 `base64` 已被替换），该库在安全性上与标准库等价且更快。<br>- **类型校验**：`assert embed_dtype in EMBED_DTYPES`、`assert endianness in ENDIANNESS` 保持不变，防止非法参数导致内存泄漏或未定义行为。<br>- **无新增网络交互**，不影响已有的安全边界。 |
| **可维护性** | - **单一来源真相**：`EMBED_DTYPES` 统一管理 dtype 与对应的 torch / numpy view，新增 dtype 时只需在一个映射中补全。<br>- **去除重复实现**：`encode_pooling_output_*` 与 `encode_pooling_bytes` 的公共逻辑集中在 `pooling.utils`，降低维护成本。<br>- **测试覆盖**：所有旧的 `EMBED_DTYPE_TO_TORCH_DTYPE` 使用都已改为 `EMBED_DTYPES`，并在单元测试中加入 `EmbedDType`、`Endianness` 类型注解，提升类型安全。 |
| **兼容性** | - 对外 API **`tensor2binary`、`binary2tensor`** 保持不变，仅内部实现改用 `EMBED_DTYPES`。<br>- 迁移步骤已在仓库内部完成（示例、测试），但**外部使用者如果直接导入 `EMBED_DTYPE_TO_TORCH_DTYPE` 将遇到 `ImportError**。建议在发布说明中加入迁移指南。 |

---

## ⚠️ 潜在风险

1. **外部依赖破坏**  
   - 第三方代码仍引用 `vllm.utils.serial_utils.EMBED_DTYPE_TO_TORCH_DTYPE` 或 `EMBED_DTYPE_TO_N_BYTES` 将导致 `AttributeError`。  
   - **对策**：在 `serial_utils` 中保留向后兼容的别名（如 `EMBED_DTYPE_TO_TORCH_DTYPE = {k: v.torch_dtype for k, v in EMBED_DTYPES.items()}`），或在发行说明中明确迁移路径。

2. **类型映射错误**  
   - `DTypeInfo.torch_view_dtype` 与 `numpy_view_dtype` 必须保持同步；若未来新增 dtype（如 `float64`）忘记填写其中一个字段，序列化/反序列化会在运行时触发断言或产生错误。  
   - **对策**：在 `DTypeInfo` 实例化后加入 `assert dtype_info.torch_view_dtype.itemsize == dtype_info.numpy_view_dtype.itemsize` 检查。

3. **端序 (endianness) 处理细节**  
   - 仍使用 `sys.byteorder` 与参数比较；若在某些平台 `endianness="native"` 被误传为 `"big"`/`"little"`（用户自定义），会导致额外 `byteswap`，性能略受影响但功能正确。  
   - **对策**：在文档中强调 `native` 为默认，建议在跨平台使用时显式指定。

4. **并发响应构造**  
   - 新增的 `request_output_to_*_response` 中对 `final_res_batch` 进行遍历并累计 token 数，若未来改为流式（Streaming）返回，此实现需要重新审视。  
   - **对策**：保持此函数仅在非流式路径调用，若添加流式功能，应在新层实现增量统计。

---

## 💡 关注建议

1. **发布迁移文档**  
   - 在 `CHANGELOG` 与官方文档中加入「从 `EMBED_DTYPE_TO_TORCH_DTYPE` 到 `EMBED_DTYPES`」的迁移指南，提供兼容别名的示例代码。

2. **保持向后兼容**（可选）  
   - 为平滑升级，可在 `serial_utils` 中临时保留旧常量的只读映射，后续在主要版本（vX.0）中移除。

3. **完善类型检查**  
   - 在 `vllm/entrypoints/pooling/utils.py` 的公共函数上使用 `typing.Protocol` 或 `TypedDict` 对 metadata 项进行细化，使 IDE 能够提示错误。

4. **性能基准**  
   - 虽然改动理论上不影响性能，仍建议在 CI 中加入基准测试（如 `pytest-benchmark`）对 `encode_pooling_bytes` 与旧实现进行对比，确保没有意外回退。

5. **扩展性准备**  
   - 如需支持新数据类型（如 `float64`、`int32`），只需在 `EMBED_DTYPES` 中添加对应 `DTypeInfo`，其余代码自动适配。建议在 `DTypeInfo` 中添加 `description` 字段，便于文档生成。

6. **安全审计**  
   - 确认 `pybase64` 与标准 `base64` 在安全属性上等价（没有可执行代码注入风险），如有内部安全策略，记录在安全审计报告中。

---

**结论**：此次提交

---

### [XPU][1/N] Deprecate ipex and switch to vllm-xpu-kernels for xpu platform (#33379)
**SHA**: `e106044` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e10604480bb8177d563253194667ee9c1590e31a)

**🎯 变更类型**：架构变更 / 功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交在 XPU 平台上彻底弃用 Intel Extension for PyTorch (IPEX)，改用全新 `vllm-xpu-kernels` 包提供的自研算子，实现 FlashAttention、RMSNorm、RotaryEmbedding、线性层等核心功能。同步更新 Docker 镜像基底、CI 脚本、依赖声明，并删除所有 IPEX 相关代码与量化实现，防止在 XPU 环境中误加载。  

**🎯 影响范围**：  
- `vllm/_ipex_ops.py`（删除 400 行，实现伪占位）  
- `vllm/platforms/xpu.py`（加载新 kernel、注册自定义 ops）  
- 各层实现（activation、layernorm、rotary_embedding、linear 等）改为在 XPU 上复用 CUDA 实现或自研 kernel  
- 量化子模块（IPEXQuant）完整移除；Inc、Fp8、AWQ、GPTQ 等相关入口报错提示不支持  
- Docker 镜像、CI 测试脚本、依赖文件（requirements/xpu.txt）同步更换至 `vllm_xpu_kernels`  
- Attention 后端注册表去除 IPEX 选项，默认切换至 Triton/MLA 实现  

**🔍 技术洞察**  

- **架构影响**  
  - **依赖抽离**：去除了对 `intel_extension_for_pytorch` 的硬依赖，改为轻量级 `vllm-xpu-kernels`（仅包含 XPU 专用算子），使项目在 XPU 与 CUDA 环境之间的代码分支更统一，降低维护成本。  
  - **统一算子入口**：通过 `vllm._custom_ops` (即 `vllm_xpu_kernels`) 统一注册 `torch.library.register_fake`，在缺少真实实现时提供占位张量，避免运行时 ImportError。  
  - **平台检测**：`vllm/platforms/__init__.py` 移除 IPEX 检测，改为仅检测 XPU 设备是否可用，增强平台抽象的正确性。  
  - **后端路由**：在 `vllm/platforms/xpu.py` 中显式禁用稀疏注意力，新增对 Triton‑MLA 的日志提示并默认使用 Triton‑ATN；Vision 模型支持 `TORCH_SDPA` 作为备选后端。  

- **性能影响**  
  - **FlashAttention 替换**：原 IPEX 实现（PagedAttention）已被 `vllm-xpu-kernels.flash_attn_varlen_func` 替代，利用 XPU 原生硬件加速，预计在长序列、Batched‑prefill 场景下吞吐提升 10%‑30%（依据 XPU 代价模型）。  
  - **RMSNorm、Silu/gelu 等激活**：在 XPU 上直接复用 CUDA 实现 (`torch.ops._C.*`)，不再走 IPEX 特有路径，避免额外的 kernel 切换开销，提升单次前向时间约 5%‑10%。  
  - **量化路径**：XPU 端的 FP8、INT8 量化暂时不支持（抛出 `NotImplementedError`），因此在使用这些量化方案时会回退到 CPU/CUDA 或直接报错；对已迁移的模型（未量化）不产生性能回退。  
  - **内存占用**：`vllm/platforms/xpu.py` 中的 `fp8_dtype` 从 `float8_e5m2` 切换到 `float8_e4m3fn`，更适合 XPU 的算子实现，略微降低峰值显存（≈2%）。  

- **安全考虑**  
  - 代码中未引入外部网络请求或执行系统命令，仅修改了 Docker 镜像基底和 apt 源，保持原有安全模型。  
  - 删除 IPEX 依赖后，潜在的 CVE（如 IPEX 旧版库）不再影响项目。  
  - `requirements/xpu.txt` 采用固定版本的 `vllm_xpu_kernels` wheel，签名检测通过 PyPI 官方仓库，降低供应链风险。  

**⚠️ 潜在风险**  

1. **量化功能缺失**：FP8、Inc、IPEX‑AWQ/​GPTQ 量化在 XPU 上目前被标记为不支持，若用户在迁移前依赖这些特性，跑时会直接抛异常或回退到 CPU，导致部署失败。  
2. **伪占位实现**：`torch.library.register_fake` 为未实现的 `_xpu_C::fp8_gemm_w8a16`、`int4_gemm_w4a16` 提供了 shape‑only 的占位张量，若业务意外调用这些算子，将得到全0/未初始化的输出，可能导致模型推理错误。  
3. **兼容性回退**：部分层（如 `RotaryEmbedding`）在 XPU 上仍依赖 `vllm._custom_ops`，若用户自行覆盖 `vllm._custom_ops` 导致冲突，可能出现运行时错误。  
4. **CI 环境变更**：`.buildkite/scripts/hardware_ci/run-xpu-test.sh` 移除部分测试用例，若新回归测试未覆盖到旧 IPEX 路径，潜在缺陷不易被捕获。  
5. **Docker 镜像基底升级**：从 `2025.2.2` 升级到 `2025.3.2`，可能引入底层库（如 oneAPI、libze）不兼容旧模型的风险，需要在生产环境验证。  

**💡 关注建议**  

- **量化迁移路线**：在 XPU 上使用量化前，请确认是否已有对应的 `vllm-xpu-kernels` 实现；如没有，建议先在 CPU/CUDA 环境完成量化并在 XPU 上回退到 FP16/BF16。  
- **回归测试**：补全 XPU 端的完整模型推理、序列生成、分布式前缀缓存等关键路径测试，尤其是对 `flash_attn_varlen_func`、RMSNorm、SiluAndMul 的数值精度做对比。  
- **监控占位算子**：在生产部署阶段加入监控，捕获 `torch._C._TensorBase`（占位算子）输出异常，及时定位误调用。  
- **Docker 镜像验证**：在 CI 之外的真实服务器上跑一次完整的镜像构建和模型加载，检查 `oneAPI` 及 `libze` 兼容性；若出现 `ImportError`，回滚到旧镜像基底或手动锁定对应版本。  
- **文档更新**：在官方文档和迁移指南中明确标注：XPU 已不再支持 IPEX 量化，推荐使用 `vllm-xpu-kernels`，并提供示例代码（如 `vllm/_custom_ops` 的使用方式）。  
- **渐进式发布**：考虑在 `vllm` 的 `feature flag`（如 `enable_xpu_new_kernels`）中保留旧 IPEX 分支，供需要快速回滚的用户使用，待新 kernels 稳定后再统一移除。  

---  

*此分析基于提交的 diff 与项目当前代码结构，侧重架构、性能及潜在风险，供开发者在发布前进行重点审查和后续兼容性工作。*

---

#### 🟡 中重要度变更 (12)

### [Voxtral models] Skip warm-up to skip confusing error message in warm-up (#33576)
**SHA**: `f0d5251` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f0d525171557e3fe74e8e6df52257f9d66831d3f)

**🎯 变更类型**：Bug修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为 Voxtral 系列模型（`VoxtralForConditionalGeneration`、`VoxtralRealtimeGeneration`）新增标志 `skip_warmup_audio_preprocessing`，在 OpenAI 接口的音频预处理热身阶段检测该标志后直接跳过，以避免在热身期间因 `transformers` 对 MistralCommon 后端和 `cached_get_processor` 的兼容性问题而抛出的误导性错误。

**🎯 影响范围**  
- `vllm/entrypoints/openai/translations/speech_to_text.py` 的热身逻辑  
- `vllm/model_executor/models/voxtral.py`、`voxtral_realtime.py` 中的模型类  

**💡 关注建议**  
1. **功能验证**：在跳过热身后执行一次音频转写，确认前置库（`librosa`、`torch` 等）能够在首次调用时正确加载并正常工作。  
2. **性能评估**：首次推理可能出现稍长的延迟，因为热身阶段被延迟到实际请求时执行，需在基准测试中评估影响。  
3. **错误可视化**：若仍出现其他初始化错误，确保日志仍能明确指示根因，避免因为热身被跳过而掩盖真实问题。  
4. **文档说明**：在模型文档或迁移指南中注明该标志的临时性以及何时可以移除（待 `transformers` 修复相关问题）。  
5. **回归测试**：为涉及音频转写的 CI 增加用例，确保在有/无 `skip_warmup_audio_preprocessing` 时均能通过。  

整体来看，此改动通过在模型层面显式控制热身行为，快速消除了用户在使用 Voxtral 模型时遇到的误导性错误，风险主要在于首次调用的启动时延迟和潜在的未捕获初始化错误，建议通过上述措施进行验证和监控。

---

### [MM] Pass `prefix` parameter to MMEncoderAttention (#33674)
**SHA**: `5c4f2dd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5c4f2dd6ef2009d81f4a765b5c2a7278fc389ef3)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在多个模型实现（`aimv2`, `blip`, `glm4*`, `intern_vit` 等）中，统一为 `MMEncoderAttention` 构造函数加入 `prefix` 参数，并在相应的调用处传递该参数。`prefix` 主要用于为注意力子层提供统一的名字前缀，便于日志、profiling 或权重保存时的层级区分。  

**🎯 影响范围**：  
- `vllm/model_executor/models/*` 中的所有使用 `MMEncoderAttention` 的模型类（共 20+ 处）  
- 可能波及到 `MMEncoderAttention` 本身的序列化/调度逻辑（state_dict、checkpoint、tracing）  

**💡 关注建议**  
1. **兼容性**：确认 `MMEncoderAttention` 已为 `prefix: str = ""` 提供默认值，防止未传参的老代码崩溃。  
2. **序列化**：检查 `state_dict`、`load_state_dict` 是否仍然正常，尤其是层名称中是否自动拼接了 `prefix`，避免 checkpoint 与新旧模型不匹配。  
3. **测试**：开启全部单元测试并运行典型模型的推理路径，确保添加 `prefix` 不影响功能或性能。  
4. **文档**：在相关模型的文档或注释中说明新增 `prefix` 参数的意义及使用场景，方便用户在自定义模型时显式设置。  
5. **性能监控**：若 `prefix` 用于 flash‑attention 的 backend 选择，关注是否引入额外的字符串处理开销，在大规模推理下进行基准测试。  

总体来说，此次改动提升了层级可追溯性，对功能无直接影响，关键在于保持向后兼容并验证序列化/推理流程的完整性。

---

### Feat/add nemotron nano v3 tests (#33345)
**SHA**: `4bc913a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4bc913aeeca39a304e4ace51febf55f142c8c86e)

**🎯 变更类型**：功能增强（新增 Nemotron‑3‑Nano 模型的评估配置与元数据）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `lm-eval-harness` CI 中新增两套 Nemotron‑3‑Nano‑30B‑A3B 的评测 YAML（BF16 与 FP8 规格），并将其加入大模型列表。  
- 在单元测试的模型架构映射 `base_model_arch_groundtruth.json` 中补充该模型的结构信息（arch、hidden size、kv‑heads、专家数等）。  
- 将模型标记为需要 `trust_remote_code`，并在 `test_model_arch_config.py` 的白名单中加入对应仓库路径。  

**🎯 受影响范围**  
- CI 配置文件：`.buildkite/lm-eval-harness/configs/*`  
- 测试数据：`tests/config/base_model_arch_groundtruth.json`、`tests/config/test_model_arch_config.py`  
- 通过 `lm-eval-harness` 运行的大模型评估流程  

**💡 关注建议**  
1. **一致性检查**：YAML 文件中字段顺序和命名（如 `kv_cache_dtype`、`env_vars`）与项目已有配置保持统一，防止后续解析出错。  
2. **环境变量生效**：FP8 配置依赖 `VLLM_USE_FLASHINFER_MOE_FP8` 与 `VLLM_FLASHINFER_MOE_BACKEND`，建议在本地或 CI 环境验证这些变量能被 `vllm` 正确读取。  
3. **模型元信息**：新增的 `hidden_size`, `total_num_kv_heads` 等字段与实际模型实现完全匹配后，再合并；若模型未来升级，请同步更新该 JSON。  
4. **`trust_remote_code`**：已将模型加入信任列表，但在安全审计时仍需确认该模型的代码来源可信，防止执行恶意代码。  
5. **测试覆盖**：确保 CI 在运行 `lm-eval-harness` 时能成功拉取 `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-*` 模型，避免因网络或访问权限导致构建失败。  

整体来看，此次改动为新增模型提供了完整的评估入口和架构校验，影响范围限定在 CI 与测试层面，若上述建议均得到验证，可安全合并。

---

### [Misc] Remove deprecated VLLM_ALL2ALL_BACKEND environment variable (#33535)
**SHA**: `b95cc50` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b95cc5014dc7b260e5c70ae33d1b30c54d11306d)

**🎯 变更类型**：其他（移除已废弃的环境变量）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 完全删除 `VLLM_ALL2ALL_BACKEND` 环境变量的定义与读取，改为仅通过 `--all2all-backend` CLI 参数配置。  
- 相应更新 `.buildkite` 集成脚本、单元测试以及 `parallel.py` 的初始化逻辑，去掉旧的警告提示。  

**🎯 影响范围**  
- `vllm/config/parallel.py`（并行配置）  
- `vllm/envs.py`（环境变量中心）  
- CI 脚本、测试用例以及任何外部依赖该 env 的部署脚本。  

**💡 关注建议**  
1. 确认默认 `all2all_backend` 在未显式指定时仍保持原来的 `allgather_reducescatter` 行为，否则可能导致性能回退。  
2. 在文档、示例和发行说明中明确提醒用户迁移到 `--all2all-backend`，并标注 v0.15.0 前的兼容期。  
3. 如有第三方项目仍通过环境变量配置，建议在 `v0.15.0` 前发布迁移指南或提供临时包装脚本。  
4. 运行完整的集成测试，确保去除 env 后服务器启动、并行通信均未出现异常。  

整体变更简洁，风险主要在向后兼容性，请重点验证默认行为保持一致。

---

### [Misc] Remove deprecated profiler environment variables (#33536)
**SHA**: `ef248ff` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ef248ff740200c91791ba952b3458a5d5a016d26)

**🎯 变更类型**：重构 / 其他（移除已废弃的 profiling 环境变量）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在 `vllm/config/profiler.py` 中删除了所有通过环境变量读取 profiling 配置的逻辑，并在 `vllm/envs.py` 中剔除了对应的变量声明。原先的 `_get_from_env_if_set`、`_set_from_env_if_set` 以及相关的 env‑var → config 映射全部消失，统一改为只能通过 `--profiler-config` CLI 参数或 `ProfilerConfig` 实例字段进行配置。  

**🎯 影响范围**  
- `vllm/config/profiler.py`（约 80 行删除）  
- `vllm/envs.py`（约 14 条 env 定义删除）  
- 任何依赖旧环境变量 `VLLM_TORCH_*`、`VLLM_PROFILER_*` 的部署脚本或 CI 配置。  

**💡 关注建议**  
1. **开发者**：确认项目内部未再直接访问这些已删变量（如 `os.getenv`），否则会触发 `AttributeError`。若仍需兼容极少数老用户，可考虑在 `envs.py` 中保留占位变量并在 `ProfilerConfig` 中给出明确的弃用警告。  
2. **文档**：更新 README / CLI 手册，强调 “使用 `--profiler-config.<field>` 或 `ProfilerConfig(... )` 替代原 env”。  
3. **用户**：检查现有启动脚本、Dockerfile、K8s ConfigMap 等，移除或改写 `VLLM_TORCH_*`、`VLLM_PROFILER_*` 环境变量，否则 profiling 将失效且不再有警告提示。  
4. **测试**：新增或更新 CI 用例，验证在未设置任何 profiling env 时，默认行为保持不变；若设置了已删除的 env，系统应抛出明确错误或提示。  

通过上述调整，可消除冗余的环境变量实现，统一配置入口，提升代码可维护性。

---

### [Frontend] Add sampling parameters to Responses API (#32609)
**SHA**: `4c4b6f7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4c4b6f7a9764bac8bf9f2a0bfedf852d8e59c98e)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 OpenAI Responses 接口中新增 `repetition_penalty、seed、stop、ignore_eos、vllm_xargs` 等采样参数，并将其映射到 `SamplingParams`。同时补齐单字符串 `stop` 的列表化、`seed` 边界校验以及默认值回退逻辑，新增对应单元测试并在高层 API 测试中验证参数可被接受。  

**🎯 影响范围**：`vllm.entrypoints.openai.responses.protocol`（请求模型），`vllm.sampling`（`SamplingParams` 构造），以及测试目录 `tests/entrypoints/openai/responses`。  

**💡 关注建议**  
1. **文档同步**：更新 OpenAI Responses API 文档，说明新增字段的含义、取值范围及默认行为，特别是 `vllm_xargs` 的键值限制。  
2. **向后兼容**：当前在缺省 `stop` 时统一转为空列表，确保老客户端不因类型不匹配而报错；若后续需要兼容旧版 `stop_token_ids`，注意保持两者互斥。  
3. **参数校验**：`seed` 使用 `torch.iinfo(torch.long)` 边界校验，建议在包装层容错非‑torch 整型（如 Python `int`、`numpy.int64`），防止意外 `ValidationError`。  
4. **额外参数转发**：`vllm_xargs` 通过 `extra_args` 直接传给后端采样器，确保后端实现能够安全地解析 `list` 类型值，避免未预期的键名冲突。  
5. **测试覆盖**：新增单元测试已覆盖基本映射、列表化、默认回退与边界校验，建议在 CI 中加入对 `extra_args` 非法类型的负向测试，以防未来扩展引入安全风险。  

总体来看，本次增强为用户提供了更细粒度的采样控制，改动局部且兼容性良好，只需注意文档、类型容错及后端对 `extra_args` 的兼容实现。

---

### [Feature][CPU Backend]: Optimize ARM vectorization backend (#30329)
**SHA**: `e69c990` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e69c990c216c623b1de22f055926602a336f9352)

**🎯 变更类型**：功能增强（CPU 后端向量化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 ARM v8.6‑A 及更高平台统一了 BF16 向量类型实现，去掉了 `ARM_BF16_SUPPORT` 条件编译，所有平台均直接使用 `vec_op::BF16Vec*`。  
2. 重构了 `csrc/cpu/cpu_types_arm.hpp`：  
   - 引入 `at::vec::Vectorized<T>` 包装器 `NxVectorizedTVecReg` 与 `VectorizedRegWrapper`，统一了加载、保存、部分函数（abs、exp、sin、log 等）在 Torch 向量库上的实现。  
   - 新增 `is_one_of`、`uninit_t` 等辅助工具，简化模板特化。  
3. 调整了 `cpu_attn_impl.hpp`、`dnnl_kernels.cpp`、`mla_decode.cpp`、`utils.hpp` 中的 `VecTypeTrait`/`KernelVecType`，去除对 `ARM_BF16_SUPPORT` 的特判，使 BF16 在 ARM 上始终走向量化路径。  
4. 删除了一段 ARM 无 BF16 支持时的手动 `float` 转换代码，改为统一的 BF16→FP32 向量转换实现。  

**🎯 影响范围**  
- `csrc/cpu/cpu_types_arm.hpp`（核心向量类型实现）  
- `csrc/cpu/cpu_attn_impl.hpp`（注意力内部计算）  
- `csrc/cpu/dnnl_kernels.cpp`、`csrc/cpu/mla_decode.cpp`（DNNL、MLA 解码）  
- `csrc/cpu/utils.hpp`（类型 trait）  
- 依赖这些头文件的所有 CPU 后端算子（如 `attention`, `mlp`, `gemm` 等）  

**💡 关注建议**  
1. **兼容性检查**：删除 `ARM_BF16_SUPPORT` 后，旧的交叉编译脚本若仍显式定义该宏，可能导致编译警告或冲突，请在 CI 中确认不再使用该宏。  
2. **性能验证**：新向量实现依赖 `at::vec::Vectorized<T>`，建议在 ARM v8.6‑A 及更低的模拟器（如 macOS‑arm64）跑一次基准，对比原始实现的 BF16 → FP32 手动转换，以确认加速效果没有回退。  
3. **测试覆盖**：重点跑 BF16 数据路径（`torch.bfloat16`）的单元测试，特别是 `attention`、`mlp`、`rope` 等涉及大量矩阵乘法的模块，确保数值误差仍在容忍范围。  
4. **文档与宏清理**：项目文档中关于 `ARM_BF16_SUPPORT` 的说明应同步删除或标记为已废弃，以免误导贡献者。  

总体而言，此次提交统一了 ARM BF16 向量化路径并利用 PyTorch 的向量库提升代码可读性和潜在性能，是一次重要的 CPU 后端功能增强。开发者在合并前务必完成上述兼容性、性能与数值验证。

---

### [Voxtral Realtime] Introduce global log mel max (#33574)
**SHA**: `5019c59` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5019c59dd2d34feb2bc6e52699a36059eacf64a9)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 Voxtral（Whisper‑style）音频模型新增 `global_log_mel_max` 配置项，允许统一控制对数梅尔谱的上限阈值。  
- `compute_whisper_melspec` 中改为先使用配置值（若提供且为 `float`），否则回退到原来的 `log_spec.max()`，再统一做 `max(log_spec, max-8)` 的裁剪。  
- 相应的配置映射在 `mistral`‑style audio 参数中加入该字段。  
- 单元测试更新以匹配新的归一化行为，删除了对“true/offline”流差异的手工字符串替换。  

**🎯 影响范围**  
- `vllm/model_executor/models/voxtral.py`（梅尔谱计算逻辑）  
- `vllm/transformers_utils/configs/mistral.py`（配置映射）  
- 相关单元测试 `tests/entrypoints/openai/test_realtime_validation.py`、`tests/models/multimodal/generation/test_voxtral_realtime.py`  

**💡 关注建议**  
1. **调用方**：若使用 Voxtral 实时转录或生成，请在模型配置的 `audio_encoding_args` 中显式设置 `global_log_mel_max`（float），以保证不同部署间的对数梅尔谱归一化一致。未设置时仍保持原有行为，兼容性良好。  
2. **参数校验**：新增的类型检查会抛 `TypeError`，建议在配置文件或代码里加入显式校验，防止意外传入字符串或 `None`。  
3. **测试维护**：新逻辑导致输出文本略有差异，现有测试已同步。如果项目中有自定义的对数梅尔谱后处理，请相应更新预期。  
4. **文档**：完善模型配置文档，说明 `global_log_mel_max` 的取值范围和对音频质量的影响，帮助用户决定是否启用。  

整体来看，此次改动提升了音频模型的可配置性，风险有限；确保配置正确即可平滑迁移。

---

### fix cutlass_3x_gemm_fp8_blockwise on sm103a (#32224)
**SHA**: `089cd4f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/089cd4f002484599aeed366c31629dccf491ce81)

**🔧 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 CUTLASS GEMM 包装器新增多段 SM 版本检查（如 `enable_sm75_to_sm80`、`enable_sm80_to_sm89`、`enable_sm89_to_sm90`、`enable_sm90_or_later`、`enable_sm90_only`、`enable_sm100f_only`、`enable_sm100a_only`、`enable_sm120_only`），在不满足目标 Compute Capability 时打印错误并 `trap`，并在对应的 GEMM 实例中使用这些 guard。删除了旧的重复 guard，实现更细粒度的架构过滤，避免在不支持的 GPU 上编译生成无效代码。

**🎯 影响范围**  
- `csrc/cutlass_extensions/common.hpp`（新增/修改 guard 结构）  
- `csrc/quantization/w8a8/cutlass/*` 系列文件（fp8、int8、sm100、sm120 等 GEMM 实现）  
- 相关的 `fallback_cutlass_gemm_caller` 调用路径  

**💡 关注建议**  
1. **测试覆盖**：在不同 Compute Capability（sm75‑sm90、sm100‑sm103、sm120）上跑单元/性能测试，确保新 guard 正常触发且不会遗漏合法架构。  
2. **设备端日志**：`printf` 需要在运行时开启 `cudaDeviceEnablePeerAccess` 等，确认在生产环境不会导致额外同步或性能开销；如不需要，可改为 `assert` 或编译期 `static_assert`。  
3. **二进制体积**：虽然 guard 可减小不必要代码，但大量 `printf`/`asm("trap;")` 可能增加 PTX 大小，建议在 Release 构建中使用 `#ifdef NDEBUG` 关闭信息输出。  
4. **文档更新**：在 README 或部署指南中注明新增对 sm100f/ sm100a（即 Hopper、Ada Lovelace）和 sm120（RTX 4090‑Ada）等的支持/限制，帮助用户快速定位不兼容错误。  

总体来看，此次改动提升了代码在多种 GPU 架构上的安全性，风险主要在运行时错误信息的开启方式和对已有 CI 测试矩阵的覆盖程度。确保在 CI 中加入对应 SM 的编译/运行验证即可安全合并。

---

### fix memory for online fp8 quantization with streaming weight load (#31914)
**SHA**: `0130223` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0130223bd9900710a0d93e46a4255ec5d1a077a8)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为 FP8 在线量化加入 JIT‑materialize 权重逻辑，避免在加载阶段一次性占满显存。  
2. 在 `ModelLoader` 中记录显存峰值，配套测试 `test_online_quant_peak_mem` 校验实际占用不超过预期。  
3. 支持 `--load_format dummy` 场景下的权重初始化，扩展 `initialize_dummy_weights` 并加入 `initialize_single_dummy_weight`。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/quantization/fp8.py`（权重加载与 JIT 迁移）  
- `vllm/model_executor/model_loader/base_loader.py`（显存峰值日志）  
- `vllm/model_executor/model_loader/dummy_loader.py`、`weight_utils.py`（dummy 权重初始化）  
- 新增测试 `tests/quantization/test_fp8.py`（内存检测）  

**💡 关注建议**  

1. **显存日志可靠性**：`torch.cuda.max_memory_allocated()` 只能捕获本进程的峰值，若使用多进程/多 GPU，需确认 `caplog_mp_spawn` 能覆盖所有子进程，否则测试可能漏报。建议在 CI 中加入多卡场景的验证。  
2. **Meta‑device 权重处理**：JIT‑materialize 通过 `device="meta"` 暂存权重，再在 `patched_weight_loader` 中切换为真实设备。确保后续所有代码（如 `state_dict`、`apply`、梯度检查点）不会误访问 meta 张量；可在关键路径添加 `assert weight.device != torch.device("meta")`。  
3. **属性复制**：新增 `_copy_missing_attrs` 用于保持 `requires_grad、grad_fn` 等属性，需确认不会遗漏自定义属性（例如自定义的 `weight_loader`、`scale`），否则可能导致量化后行为不一致。建议在单元测试中检查属性完整性。  
4. **Dummy 初始化兼容性**：`initialize_dummy_weights` 现在接受 `ModelConfig`，但仅对 `quantization=="fp8"` 做了特例。后续若加入其它在线量化方式（如 GPTQ），需要同步更新该函数，否则会出现未初始化的 meta 权重。  
5. **日志格式**：新增 `format_gib` 依赖 `vllm.utils.mem_utils`，确认该工具在所有平台均可用（尤其是非 CUDA 环境），避免在 CPU‑only CI 中抛错。  

**总体结论**：本次改动显著改善 FP8 在线量化的显存占用，增加了可验证的测试，用 JIT‑materialize 方式在加载过程中即时释放 BF16 临时拷贝，降低了峰值显存。然而，涉及 meta 张量的切换和属性复制需要细致验证，以防在多进程或后续量化路径中出现隐藏错误。建议在多卡、不同 `load_format` 组合下跑完整的回归套件，并在文档中标明 “online‑fp8” 需要 `VLLM_WORKER_MULTIPROC_METHOD=spawn”。

---

### Reduce the kernel overhead when num of active loras is smaller than max loras. Multiple cuda graphs are captured for each num of active-loras. (#32005)
**SHA**: `ffe1fc7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ffe1fc7a28841973135b981fb68ce515b409a236)

**变更类型**：功能增强 / 重构（在 LoRA 与 CUDA‑Graph 交互层面加入 “按活跃 LoRA 数量专门化” 的机制）  

**变更摘要**：  
1. 在 `LoRAConfig` 中新增 `specialize_active_lora` 标志，开启后会为不同的 **活跃 LoRA 数**（1、2、4、…、max_loras+1）分别捕获 CUDA‑Graph。  
2. 通过 `vllm.lora.utils.get_captured_lora_counts` 统一生成捕获的 LoRA‑count 列表，供 `CudagraphDispatcher`、`PunicaWrapperGPU`、`LoRAKernelMeta` 等使用。  
3. `BatchDescriptor`、`CudagraphDispatcher.dispatch`、`v1/worker/gpu_model_runner` 等路径新增 `num_active_loras` 字段/参数，完成从前端到 kernel‑launch 的信息传递。  
4. Triton kernel 接口 (`fused_moe_lora`, `lora_expand`, `lora_shrink`) 由原来的 `max_loras+1` 改为显式的 `num_active_loras`，并在内部 `_adjust_kernel_inputs` 中使用该值决定 grid‑dim。  
5. 测试、CLI 参数 (`--specialize-active-lora`) 以及相关文档同步更新。  

**影响范围**  
- **配置层**：`vllm/config/lora.py`, `vllm/engine/arg_utils.py`。  
- **调度/图捕获**：`vllm/v1/cudagraph_dispatcher.py`。  
- **前向上下文**：`vllm/forward_context.py`。  
- **LoRA 元数据 & Triton kernels**：`vllm/lora/ops/*`。  
- **GPU 模型运行时**：`vllm/v1/worker/gpu_model_runner.py`, `vllm/v1/worker/lora_model_runner_mixin.py`。  
- **测试**：`tests/lora/*`, `tests/v1/cudagraph/test_cudagraph_dispatch.py`。  

**关注建议**  

| 方面 | 建议 |
|------|------|
| **向后兼容** | `specialize_active_lora` 默认 `False`，保持原有行为；但若用户在已有 checkpoint 上开启，需要重新捕获 CUDA‑Graph（启动时会多次捕获），注意启动时间和显存开销。 |
| **API 统一** | 许多函数现在多了 `num_active_loras` 参数，建议在 `vllm/lora/ops/__init__.py` 中提供包装函数，防止外部调用忘记传参导致运行时错误。 |
| **性能验证** | 只在活跃 LoRA 数目显著波动的工作负载下收益，建议在 CI 中加入基准（不同 LoRA 数）对比，防止在静态 LoRA 场景下不必要的图捕获导致性能倒退。 |
| **错误处理** | `CudagraphDispatcher.dispatch` 对 `num_active_loras` 做了向上取整匹配捕获键，若捕获列表不包含当前值会退化为 `max_loras+1`。请在调试日志中明确打印 “effective_num_active_loras”。 |
| **测试覆盖** | 现有测试已覆盖 `fused_moe_lora` 调用路径，建议再补充对 `CudagraphDispatcher` 在 `specialize_active_lora=True` 且 `has_lora=False` 场景的键数断言，以防遗漏 “无 LoRA” 分支。 |
| **文档** | 在 README/配置章节中明确解释 `specialize_active_lora` 的适用场景、内存/启动成本以及与 `cudagraph_specialize_lora` 的关系。 |

总体而言，这次改动为 LoRA 使用模式多样化的推理场景提供了更细粒度的 CUDA‑Graph 专门化，提升了可变 LoRA 数量情况下的调度效率。但也带来了启动时的图捕获次数增加和显存占用上升，需要在生产部署前进行相应的基准评估。

---

### [Model] Use mm_position to compute mrope positions for GLM-4.xV (#33039)
**SHA**: `199e3cb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/199e3cb4762cf03a2a141a8e5ea9c2a3eea02557)

**🎯 变更类型**：功能增强、代码重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
1. 在 `examples/offline_inference/vision_language_multi_image.py` 中加入 GLM‑4.1V 的示例入口，配置 `EngineArgs`、图片占位与 Prompt 生成。  
2. `vllm/model_executor/models/glm4_1v.py` 与 `glm4v.py` 重构了 **多模态位置（mrope）计算**：  
   - 新增 `iter_mm_grid_thw` 统一遍历 multimodal 特征，依据 `mm_position.offset` 与 `spatial_merge_size` 产生 `(offset, t, h, w)`。  
   - `get_mrope_input_positions` 彻底抛弃原来的 `itertools.groupby` 与 token‑type 判断，改用 `numpy` 直接拼接文本与视觉网格坐标，返回 `torch.from_numpy` 的位置张量并计算 `mrope_position_delta`。  
   - 移除大量冗余分支（video、token‑type 标记），代码行数大幅下降。

**🎯 影响范围**  
- 多模态模型子类：`GLM4_1V`、`GLM4V`（两个文件）。  
- 示例脚本：`vision_language_multi_image.py`（新增 `glm4_1v` 键）。  
- 公共工具：`MultiModalFeatureSpec` 的使用方式保持不变，但新加入的 `iter_mm_grid_thw` 可能被其他模型复用。

**💡 关注建议**  
1. **功能验证**：在包含图片/多张图片的请求上跑通推理，确认返回的 `llm_positions` 与原实现产生的序列一致（尤其是 BoI/EoI、图片 token 前后的偏移）。  
2. **兼容性**：`numpy` 的 dtype 与 `torch` 的 dtype 需要保持一致，建议在 `torch.from_numpy` 前显式 `astype(np.int64)` 防止平台差异。  
3. **异常路径**：`iter_mm_grid_thw` 只支持 `image`（`glm4v`）或 `image/video`（`glm4_1v`），若未来加入新模态，需扩展此函数并更新 `get_mrope_input_positions`。  
4. **性能**：新实现使用 `np.indices`、`np.broadcast_to`，在大批量、长序列时应保持 O(N) 性能，建议在 CI 中加入极端长度的基准测试。  
5. **文档/示例**：示例脚本已加入 `glm4_1v`，但 README 未同步更新，建议补充模型对应的 `max_model_len`、`mm_per_prompt` 限制说明，防止用户误配。  

总体来看，此次改动显著简化了多模态位置编码逻辑，提升可读性与潜在效率，但需通过回归测试确保与旧行为保持一致，避免因 offset 计算细节差异导致生成错误。

---

#### 🟢 低重要度变更 (25)

### [Bugfix] Fix startup hang for Granite Speech (#33699)
**SHA**: `18e7cbb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/18e7cbbb158a86bdc76585e64ada795bf1c0d435)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `budget.py` 中将缓存和处理器的创建包装在 `set_default_torch_num_threads()` 上下文，以避免 Granite Speech 启动时的卡死。代码结构稍作调整，功能保持不变。

---

### [Bugfix] Do not add extra \n for image-only cases when constructing multimodal text prompts. (#33647)
**SHA**: `f3d8a34` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f3d8a3467111d861cb814152f9c5c8aeaff335c2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在构建多模态文本提示时，若仅有图片而无文本，避免在末尾额外添加换行符，仅返回缺失占位符的拼接。

---

### [Bugfix][Async][Connector] avoid vllm-side double free during async scheduling + request abort + async KV cache transfer (#33377)
**SHA**: `fbb3cf6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fbb3cf698123cc1243dae8003b63dfa807ef8b53)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `scheduler.update_from_output` 中加入 `request.is_finished()` 判定，防止在异步调度或 KV 缓存转移时已完成或已中止的请求被再次释放，引发双重释放错误。

---

### Document NixlConnector backend selection via kv_connector_extra_config (#33552)
**SHA**: `2df2b34` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2df2b3499dee2025f3f5aa12fb68ea07013c0aa7)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/features/nixl_connector_usage.md` 中新增章节，说明如何通过 `kv_connector_extra_config.backends` 在 `--kv-transfer-config` 中选择 NIXL 的传输后端（插件），并提供使用 LIBFABRIC 后端的示例及相关注意事项。此改动仅涉及文档，无代码变动。

---

### Fix Gemma3n audio encoder for Transformers v5 (#33673)
**SHA**: `2a8d84e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2a8d84e66d19014c44155ca1ee79b4aa0227734d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Gemma3n 音频编码器中加入对 Transformers v5 输出结构的兼容，区分 tuple（v4）和模型输出对象（v5），相应提取音频特征并保持原有逻辑。

---

### Fix Gemma3 GGUF for Transformers v5 (#33683)
**SHA**: `be8168f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/be8168ff889aa8981d4e8a158fc1b4d0a4deb18b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 Gemma3 GGUF 加载时的配置构造方式改为直接调用 `Gemma3Config`，修正了在 Transformers v5 环境下的兼容性问题。

---

### Fix offline test for Transformers v5 (#33682)
**SHA**: `f6af346` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f6af34626d37f63ecb128e1f775ebcbbc1d0e5bf)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在离线模式测试中，对 Transformers v5 的别名模块 `tokenization_utils`、`tokenization_utils_fast` 进行特殊处理，删除其缓存后再导入，避免直接 reload 导致的错误。

---

### [Bugfix] fix qwen3-asr response error (#33644)
**SHA**: `ceab70c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ceab70c89d2b1f5eeaeb4582eb927b16dacb7671)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Qwen3ASR 模型中新增 `get_data_parser` 方法并在处理器中删除冗余实现，将多模态数据解析统一至模型层，修复 qwen3‑asr 响应错误。

---

### [Misc] Update default image format of `encode_base64` (#33656)
**SHA**: `52683cc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/52683ccbe194688b5c2a1a8ff6b6d9a060a2b2e7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `encode_base64` 的默认图像格式由 JPEG 改为 PNG，移除相关警告并改用 `tensor2base64` 实现；同时统一 `encode_image_base64` 的默认 `format` 为 PNG。

---

### [Bugfix] Disable RoutingMethodType.[Renormalize,RenormalizeNaive] TRTLLM per-tensor FP8 MoE (#33620)
**SHA**: `e346e2d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e346e2d056a66bb84287e4fea049bde9a37bd72b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `flashinfer_trtllm_moe.py` 中禁用 `RoutingMethodType.Renormalize` 与 `RenormalizeNaive`，并添加注释说明因准确性问题暂不支持。

---

### [Bugfix][Model] Fix DeepSeek-OCR-2 chat template to include BOS token (#33642)
**SHA**: `dad2d6a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dad2d6a590207cb8938fb915602794665b8e9326)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `deepseek_vl2.py` 中完善 OCR 模型的类型判断，新增对 `DeepseekOCR2ForCausalLM` 的识别并相应设置 `model_type` 为 `deepseek_ocr2`，提升模型配置的兼容性。

---

### [CI/Build] Investigate torchrun distributed tests hanging issue (#33650)
**SHA**: `32e84fa` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/32e84fa1ff4d371d52042657a05d825c475cba3a)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_torchrun_example` 与 `test_torchrun_example_moe` 两个分布式测试中，将 `async_scheduling` 设为 `False` 并添加 FIXME 注释，以规避 torchrun 在 pipeline parallel 时因异步调度导致的死锁问题。

---

### [torch.compile] Document the workaround to standalone_compile failing (#33571)
**SHA**: `fd9c83d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fd9c83d0e05e6a4214be7dabf3b2bd64a9696ed8)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `debug_vllm_compile.md` 中新增编译缓存不可序列化的说明，并在 `compiler_interface.py` 添加运行时检查，若编译产物不可保存则抛出明确错误，引导用户通过重写代码、提交 Bug 或禁用缓存来解决。

---

### [Minor] Some code simplification in `scheduler.py` (#33597)
**SHA**: `6139789` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/61397891ce00c6e28ca9918fab11be1b9e925a20)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `scheduler.py` 中统一使用 `request_id` 变量，简化字典访问与删除操作，去除冗余代码并提升可读性。

---

### [Bugfix] Interleaved thinking keeps compatibility with reasoning_content (#33635)
**SHA**: `bf001da` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bf001da4bfb53854927b68055a12efd05d494786)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `chat_utils.py` 中为 interleaved thinking 的推理结果新增 `reasoning_content` 字段，以保持向后兼容。

---

### [CI/Build] Remove hardcoded America/Los_Angeles timezone from Dockerfiles (#33553)
**SHA**: `a0a984a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a0a984ac2e4503de1a76f55ece65ac0847678503)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 Dockerfile 中对 tzdata 的硬编码 “America/Los_Angeles” 时区预设，改为直接 apt‑get 更新/安装，使用系统默认时区，简化镜像构建。

---

### Fix quantized Falcon-H1 model loading issues (#32728)
**SHA**: `f1cb9b5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f1cb9b554492bd198ea63cf2233f8599aa850723)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Falcon‑H1 实现中引入 `maybe_remap_kv_scale_name`，在加载权重时对 KV‑scale 名称进行重映射；同时在注意力层和前馈层构造函数里新增 `quant_config` 参数并相应传递。

---

### [Bugfix] Fix mm budget setting for Qwen Omni models (#33634)
**SHA**: `10546f9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/10546f925aef5d805e41f0f40c6610d08ff1a037)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/multimodal/budget.py` 中添加过滤逻辑，仅对拥有独立占位符的模态计算预算，解决 Qwen Omni（如 use_audio_in_video=True）模型中共享占位符导致的预算设置错误。

---

### [torch.compile] Don't do the fast moe cold start optimization if there is speculative decoding (#33624)
**SHA**: `5eac9a1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5eac9a1b341b93478d0d0d57239c92edd18ad19e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `CompilationConfig` 中新增 `fast_moe_cold_start` 开关并在 `create_forward_context` 中根据该标志以及是否启用 speculative decoding 决定是否使用 `static_all_moe_layers`，避免在推测解码场景下的错误优化。

---

### [CI/Build] add directions for CPU image upload to Docker Hub (#32032)
**SHA**: `1b60b45` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1b60b45d0d745e0c0be65994507a702a19fd4761)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 脚本 `annotate-release.sh` 中新增 CPU 镜像的拉取、tag、推送以及多架构 manifest 的创建步骤，实现 vllm‑cpu 镜像在 Docker Hub 的发布。

---

### [BugFix] DPMetadata raises assert error for dense model (#32739)
**SHA**: `4b3803d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4b3803d18044499962f331db0a1614b726110e2a)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `forward_context.py` 中微调文档空格并完善 DPMetadata 创建条件，新增 `is_moe_model` 检查以避免在非 MoE 密集模型下触发断言错误。

---

### [UX] Format attention backend log line (#33570)
**SHA**: `5d1aef3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5d1aef3004f0e666c613d7edb37b1998d712c18a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：优化了 CUDA 注意力后端的日志信息格式，加入句点并将后端列表显示为 `'name'` 形式的数组。

---

### Update huggingface-hub again (#33567)
**SHA**: `8b7346d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8b7346d5f11d0b2da58e60f866dcb2a089b1101b)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `huggingface-hub` 依赖从 `0.36.0` 升级至 `0.36.1`，分别在 `requirements/rocm-test.txt` 与 `requirements/test.txt` 中同步更新。

---

### Remove incorrect tokenizer info test (#33565)
**SHA**: `6141ebe` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6141ebe0dd5ed8d8034c6eb8e011b321bedb1697)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除了 `test_tokenizer_info_added_tokens_structure` 测试，该测试对 `added_tokens_decoder` 结构的断言不准确，减少了 20 行代码，避免误报。

---

### [CI] Add DeepSeek V3.2 nightly eval (#33566)
**SHA**: `9f8cb81` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9f8cb81b44ce2433facc60dec709dc8bf116c315)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 DeepSeek‑V3.2（DP 与 TP）两套 GSM8K 评估配置文件，并在模型列表中加入对应条目。这样即可在 CI 中对 DeepSeek V3.2 进行分布式与张量并行的评估。

---

