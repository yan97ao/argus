# 每日更新报告（2026-01-13）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-13 23:50:34 | Martin Hickey | [BugFix] [KVConnector] Fix KV events for LMCache connector (#32169) |
| 2026-01-13 23:37:34 | Chauncey | [Refactor] [7/N] to simplify the vLLM lora serving architecture (#32251) |
| 2026-01-13 23:10:20 | Cyrus Leung | [Refactor] Remove `MultiModalProfiler` (#32254) |
| 2026-01-13 21:38:52 | Matthew Bonanni | [6/N][Attention] Move utils to more appropriate locations (#32215) |
| 2026-01-13 21:01:39 | Chauncey | [Refactor] [6/N] to simplify the vLLM openai chat_completion serving architecture (#32240) |
| 2026-01-13 20:56:01 | Mickaël Seznec | [Quantization] fix: overflow with static per-tensor scaling (#29867) |
| 2026-01-13 20:51:57 | Nicolò Lucchesi | [Docs] Nixl Usage recommend `fail` kv_load_failure_policy (#32198) |
| 2026-01-13 18:45:42 | Cyrus Leung | [Bugfix] Replace `PoolingParams.normalize` with `use_activation` (#32243) |
| 2026-01-13 17:21:23 | Cyrus Leung | [Refactor] Remove `get_encoder_dummy_data` (#32241) |
| 2026-01-13 17:04:29 | YunzhuLu | [Model] Use mm_position to compute mrope positions for Qwen2-VL/2.5-VL (#32126) |
| 2026-01-13 15:14:30 | Andreas Karatzas | [ROCm][CI] Fix engine core client tests for ROCm spawn multiprocessing (#32061) |
| 2026-01-13 15:10:27 | Roy Wang | [Doc] Update installation from source command (#32239) |
| 2026-01-13 15:09:36 | Xingyu Liu | [BugFix]Fix eagle draft_model_config and add tests (#31753) |
| 2026-01-13 14:33:59 | Andreas Karatzas | [ROCm][CI] Fix HuggingFace flash_attention_2 accuracy issue in Isaac vision encoder (#32233) |
| 2026-01-13 13:46:53 | Andreas Karatzas | [ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#32099) |
| 2026-01-13 12:11:37 | Wentao Ye | [Perf] Optimize requests abort (#32211) |
| 2026-01-13 11:41:47 | Andrew Bennett | Fix various typos found in `docs` (#32212) |
| 2026-01-13 11:21:49 | Sanghoon Yoon | [Frontend] Add `reasoning_effort` to `OpenAIServing._preprocess_chat()` (#31956) |
| 2026-01-13 11:11:23 | cjackal | [Misc] improve warning/assert messages (#32226) |
| 2026-01-13 10:33:14 | Nick Hill | [BugFix] Fix engine crash caused by chat tools + response_format (#32127) |
| 2026-01-13 10:03:08 | Nick Hill | [Misc] Allow enabling NCCL for DP sync when async scheduling (#32197) |
| 2026-01-13 09:30:12 | Cyrus Leung | [Model] Handle `trust_remote_code` for transformers backend (#32194) |
| 2026-01-13 08:14:54 | Andrew Xia | [responsesAPI] add unit test for optional function tool call id (#32036) |
| 2026-01-13 06:35:49 | Divakar Verma | [ROCm][CI] Handle pytest status code 5 when a shard isn't allocated any tests  (#32040) |
| 2026-01-13 06:17:30 | xuebwang-amd | [Kernel][MoE] fix computation order of MoE weight multiplication and improve flow (#31962) |
| 2026-01-13 05:37:43 | Woosuk Kwon | [Model Runner V2] Add support for M-RoPE (#32143) |
| 2026-01-13 05:08:30 | Woosuk Kwon | [Model Runner V2] Minor refactor for logit_bias (#32209) |
| 2026-01-13 04:42:06 | Vadim Gimpelson | [BUGFIX] Add missed remaping of the names of fp8 kv-scale (#32199) |
| 2026-01-13 04:38:49 | Nicolò Lucchesi | [NIXL][Bugfix] Failure logging overhaul + early metadata free on failure (#32031) |
| 2026-01-13 03:31:10 | Woosuk Kwon | [Model Runner V2] Support logit_bias, allowed_token_ids, min_tokens (#32163) |
| 2026-01-13 03:24:38 | Lucas Kabela | [Misc][BE] Type coverage for vllm/compilation [3/3] (#31748) |
| 2026-01-13 02:59:31 | Nicolò Lucchesi | [Misc] Change log level for batch queue log (#32192) |
| 2026-01-13 02:39:38 | Or Ozeri | [BugFix] scheduler: Fix ordering preserving of skipped requests (#32173) |
| 2026-01-13 02:28:16 | Roger Wang | [Misc] Set default torch num threads for input processing (#31879) |
| 2026-01-13 02:13:23 | Ilya Markov | [Refactor] EPLB rebalance algo to NumPy (#30697) |
| 2026-01-13 02:00:45 | Kyungmin Lee | [BugFix] fix FusedMoE.make_expert_params_mapping in EXAONE-MoE (#32196) |
| 2026-01-13 01:13:56 | Matthew Bonanni | [3/N][Attention] Move AttentionMetadata-related code from utils.py to backend.py (#32054) |
| 2026-01-13 01:12:22 | Cyrus Leung | [Benchmark] Share data between SLA runs (#32184) |
| 2026-01-13 01:10:05 | Nicolò Lucchesi | [Misc][PD] Fix `get_attn_backend` usage in transfer connectors (#31988) |
| 2026-01-13 01:02:38 | Asaf Joseph Gardin | [Bugfix] Fix stale SSM state for new Mamba requests scheduled as decode (#32118) |
| 2026-01-13 01:01:49 | Cyrus Leung | [Model] Standardize pooling heads (#32148) |
| 2026-01-13 00:55:49 | danielafrimi | [FIX] Add NO_MUL activation support for modular kernel path (#31528) |
| 2026-01-13 00:39:02 | Jaehyun An | [MODEL] New model support for kakaocorp/kanana-1.5-v-3b-instruct (#29384) |
| 2026-01-13 00:30:50 | Kyungmin Lee | Add K-EXAONE-236B-A23B (#31621) |
| 2026-01-13 00:15:28 | Andy Zhang | doc: Update model references in supported_models.md (#32188) |

### 📊 统计摘要
> 本日共 45 个提交 | 🔴高 4 | 🟡中 24 | 🟢低 17
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [[Refactor] [6/N] to simplify the vLLM openai chat_complet...](#fefce49)
    - [[Misc][BE] Type coverage for vllm/compilation [3/3] (#31748)](#ad8818b)
    - [[3/N][Attention] Move AttentionMetadata-related code from...](#20228cb)
    - [Add K-EXAONE-236B-A23B (#31621)](#63ed240)
  - [🟡 中重要度变更 (24)](#-🟡-中重要度变更-24)
    - [[Refactor] [7/N] to simplify the vLLM lora serving archit...](#4f02cb2)
    - [[Refactor] Remove `MultiModalProfiler` (#32254)](#252c011)
    - [[6/N][Attention] Move utils to more appropriate locations...](#98f60e5)
    - [[Quantization] fix: overflow with static per-tensor scali...](#a5bbbd2)
    - [[Bugfix] Replace `PoolingParams.normalize` with `use_acti...](#232214b)
    - [[Refactor] Remove `get_encoder_dummy_data` (#32241)](#eb28e80)
    - [[Model] Use mm_position to compute mrope positions for Qw...](#542a405)
    - [[ROCm][CI] Fix engine core client tests for ROCm spawn mu...](#df7e127)
    - [[BugFix]Fix eagle draft_model_config and add tests (#31753)](#80221e1)
    - [Fix various typos found in `docs` (#32212)](#f243abc)
    - [[Misc] improve warning/assert messages (#32226)](#15b33ff)
    - [[BugFix] Fix engine crash caused by chat tools + response...](#c6bb5b5)
    - [[Misc] Allow enabling NCCL for DP sync when async schedul...](#9273a42)
    - [[responsesAPI] add unit test for optional function tool c...](#a307ac0)
    - [[Model Runner V2] Add support for M-RoPE (#32143)](#0a7dd23)
    - [[Model Runner V2] Minor refactor for logit_bias (#32209)](#dec2868)
    - [[NIXL][Bugfix] Failure logging overhaul + early metadata ...](#f8bd839)
    - [[Model Runner V2] Support logit_bias, allowed_token_ids, ...](#ca81811)
    - [[Refactor] EPLB rebalance algo to NumPy (#30697)](#1eb61ab)
    - [[Benchmark] Share data between SLA runs (#32184)](#7c0d3c5)
    - [[Misc][PD] Fix `get_attn_backend` usage in transfer conne...](#5b68107)
    - [[Model] Standardize pooling heads (#32148)](#8863c2b)
    - [[FIX] Add NO_MUL activation support for modular kernel pa...](#3f72639)
    - [[MODEL] New model support for kakaocorp/kanana-1.5-v-3b-i...](#6bc9c84)
  - [🟢 低重要度变更 (17)](#-🟢-低重要度变更-17)
    - [[BugFix] [KVConnector] Fix KV events for LMCache connecto...](#5102654)
    - [[Docs] Nixl Usage recommend `fail` kv_load_failure_policy...](#8c8653b)
    - [[Doc] Update installation from source command (#32239)](#44c34f2)
    - [[ROCm][CI] Fix HuggingFace flash_attention_2 accuracy iss...](#5e714f7)
    - [[ROCm][Bugfix] Fix Mamba batched decode producing incorre...](#11b6af5)
    - [[Perf] Optimize requests abort (#32211)](#2a719e0)
    - [[Frontend] Add `reasoning_effort` to `OpenAIServing._prep...](#60b77e1)
    - [[Model] Handle `trust_remote_code` for transformers backe...](#78d13ea)
    - [[ROCm][CI] Handle pytest status code 5 when a shard isn't...](#a28d9f4)
    - [[Kernel][MoE] fix computation order of MoE weight multipl...](#629584b)
    - [[BUGFIX] Add missed remaping of the names of fp8 kv-scale...](#9f430c9)
    - [[Misc] Change log level for batch queue log (#32192)](#08e8e99)
    - [[BugFix] scheduler: Fix ordering preserving of skipped re...](#2be765b)
    - [[Misc] Set default torch num threads for input processing...](#16abe6b)
    - [[BugFix] fix FusedMoE.make_expert_params_mapping in EXAON...](#3d962d7)
    - [[Bugfix] Fix stale SSM state for new Mamba requests sched...](#8fb2c13)
    - [doc: Update model references in supported_models.md (#32188)](#95e53d9)
#### 🔴 高重要度变更 (4)

### [Refactor] [6/N] to simplify the vLLM openai chat_completion serving architecture (#32240)
**SHA**: `fefce49` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fefce49807a52fc94678e336fc5d1f3b7ebe2daf)

**🎯 变更类型**：重构 / 架构变更  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交对 *vLLM* 的 OpenAI 接口实现进行了大规模的代码组织重构，核心改动包括：

1. **协议层拆分**  
   - 将原 `vllm.entrypoints.openai.protocol` 按功能拆分为 `openai.chat_completion.protocol`（专属 Chat‑Completion API）和 `openai.engine.protocol`（底层通用请求/响应、错误模型、工具调用等）。  
   - 相应地，所有业务代码、测试以及子模块的 import 均迁移至新路径。

2. **路由层抽离**  
   - `vllm.entrypoints.openai.chat_completion.api_router` 新增，实现 `POST /v1/chat/completions` 路由并在 `api_server` 初始化时显式挂载。原来的路由实现被删除。

3. **Tokenizer 相关迁移**  
   - 将之前在 `openai.protocol` 中的 tokenization 请求/响应模型迁移到 `openai.engine.protocol`，并新增 `openai.serve.tokenize` 子包（`protocol.py`、`serving.py`），统一管理 tokenization 端点。

4. **大量引用更新**  
   - 所有 `tests/`、`entrypoints/*`、`reasoning/*`、`tool_parsers/*` 等文件的 import 改为新模块路径。  
   - 相关内部实现（如 `serving`, `parser`, `engine.serving`）也同步调整。

---

**🎯 影响范围**  

| 受影响组件 | 描述 |
|------------|------|
| `vllm.entrypoints.openai.protocol` | 被废弃或大幅删减，仅保留 `__init__`（不再导出旧模型）。外部用户直接导入该路径将失效。 |
| `vllm.entrypoints.openai.chat_completion.*` | 新增的 Chat‑Completion 协议、服务实现、API 路由。 |
| `vllm.entrypoints.openai.engine.*` | 新的通用协议层（ErrorResponse、ResponsesRequest、Completion/Chat 结构等）。 |
| `vllm.entrypoints.openai.serving_*` | 大量内部实现迁移至 `engine.serving`，包括 `OpenAIServingChat`, `OpenAIServingCompletion` 等。 |
| `vllm.entrypoints.serve.*`（tokenize、disagg、elastic_ep、lora、pooling 等） | 所有 OpenAI‑related 错误、请求模型的 import 已统一至 `engine.protocol`。 |
| 单元测试目录 | 测试文件路径已更新，确保 CI 通过。 |
| 文档/示例（未随代码提交） | 任何使用旧 import 路径的示例将需要同步修改。 |

---

**🔍 技术洞察**  

1. **架构影响**  
   - **职责分离**：Chat‑Completion 与底层 Engine（Completion、Embedding、Tokenize、Tool Calls 等）解耦，层次更清晰。后续可以独立演进 Chat‑Completion（如新增字段、功能）而不影响底层协议。  
   - **模块化路由**：`api_router` 独立于 `api_server`，便于插件化或微服务化部署（可只挂载特定子路由）。  
   - **向后兼容风险**：没有在 `openai/__init__.py` 中重新导出旧 API，导致向后兼容性中断。若项目对外提供 `vllm.entrypoints.openai.protocol` 的稳定接口，这次改动需要发布 **重大版本**（如 2.0）并在迁移指南中明确说明。  
   - **循环依赖可能**：部分文件在迁移过程中出现了交叉 import（如 `chat_completion.serving` 与 `engine.protocol`），但通过显式分层（`engine` 为底层，`chat_completion` 为上层）基本避免。仍需注意新增循环依赖导致的启动慢或导入错误。  

2. **性能影响**  
   - 改动主要是 **导入路径** 与 **模块组织**，运行时函数逻辑基本保持不变。对推理吞吐、延迟等核心性能指标的影响可以视为 **零**。  
   - 新增的 `api_router.attach_router(app)` 只在启动时执行一次，对启动时间的影响微乎其微（约 1‑2 ms）。  
   - 由于错误模型和请求模型在不同子模块中，各自的 `pydantic` 编译缓存保持不变，不会导致额外的模型校验开销。  

3. **安全考虑**  
   - **错误信息统一**：仍然使用 `ErrorResponse`（搬到了 `engine.protocol`），未改变错误码/信息的生成方式，安全性保持。  
   - **公共 API 变更**：外部依赖可能在升级时仍旧使用旧路径，如果程序在运行时出现 `ImportError`，会导致服务异常启动，进而暴露内部错误信息。建议在文档中提示 **升级前必须完成迁移**，并在 CI 中加入兼容性检查。  
   - **未引入新外部依赖**，风险极低。  

---

**⚠️ 潜在风险**  

| 风险类型 | 具体表现 | 影响范围 | 缓解办法 |
|----------|----------|----------|----------|
| **向后兼容性** | 直接 `import vllm.entrypoints.openai.protocol` 将失效。 | 所有外部项目、示例、旧测试脚本 | 在 `vllm/entrypoints/openai/__init__.py` 中添加 **兼容层**（重新导出新路径），或在发行说明中强制升至 **v2.0** 并提供迁移指南。 |
| **遗漏的 import** | 大量文件改动后，仍有残留的旧路径 import（CI 通过但运行时可能触发 ImportError）。 | 仅在少数未覆盖的子模块 | 通过全局搜索 `entrypoints.openai.protocol` 并确认全部已迁移；在 CI 增加 `flake8` 或 `pylint` 检查未使用的旧 import。 |
| **循环依赖** | `chat_completion` 与 `engine` 双向导入（如 `chat_completion.serving` 中导入 `engine.protocol`，而 `engine.serving` 可能再次导入 `chat_completion.protocol`）。 | 启动阶段可能出现 `ImportError` 或部分模块首次使用时延迟加载异常 | 明确分层：`engine` 只提供底层数据模型，不依赖 `chat_completion`；所有上层服务仅向下引用。若出现循环，可移动公共类型到独立 `openai.shared` 包。 |
| **文档/示例失效** | 官方文档、Readme 中的代码示例仍指向旧路径。 | 社区用户、教程 | 同步更新文档；在 CI 添加文档检查脚本。 |
| **新路由未被挂载** | 在某些启动入口（如自定义 `serve` 脚本）未调用 `attach_router`，导致 `/v1/chat/completions` 404。 | 自定义部署场景 | 在 `vllm/entrypoints/openai/api_server.py` 中加入明确的调用日志，或提供 `register_chat_api_router` 的示例。 |
| **测试覆盖不足** | 迁移后部分功能路径未被单元测试覆盖，潜在回归风险。 | 代码质量 | 执行完整的回归测试，确保所有 OpenAI 接口（包括 tokenization、responses 等）仍然通过。 |

---

**💡 关注建议**  

1. **发布兼容层**  
   - 在

---

### [Misc][BE] Type coverage for vllm/compilation [3/3] (#31748)
**SHA**: `ad8818b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ad8818bb5eb8de14ebe18ebf8c8b3b2620092e17)

**🎯 变更类型**：重构（Type Coverage 与函数签名完善）  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：此次提交为 vLLM 编译及分布式子模块引入大量类型注解（`typing`、`ParamSpec`、返回值标注）并统一函数签名风格，提升代码可读性、可维护性和静态检查能力。大幅度修改涉及 `activation_quant_fusion.py`、`collective_fusion.py`、`fusion.py`、`fusion_attn.py`、`inductor_pass.py`、`matcher_utils.py`、`qk_norm_rope_fusion.py`、`rocm_aiter_fusion.py`、`sequence_parallelism.py`、`parallel_state.py`、以及 rotary‑embedding 包的导出定义。

---

### 🎯 影响范围
- **编译层**：`vllm/compilation/*`（核心模式匹配、融合、量化、RoPE、Attention、Rocm AIter、序列并行等）  
- **匹配工具**：`vllm/compilation/matcher_utils.py`（`MatcherCustomOp` 与子类）  
- **Inductor Pass 基础类**：`vllm/compilation/inductor_pass.py`  
- **分布式并行状态**：`vllm/distributed/parallel_state.py`（TP、DCP 辅助函数返回类型）  
- **Rotary Embedding 公共入口**：`vllm/model_executor/layers/rotary_embedding/__init__.py`（类型字典声明及 `__all__`）  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 通过统一的类型签名，使得 **PatternMatcherPass、VllmPatternMatcherPass** 等抽象层的子类在 IDE 与静态分析工具中具有明确的契约。对整体架构没有功能性变更，仅提升 **可视化与可推断性**，有助于后续功能扩展（如新量化方式）时快速定位签名不匹配的实现。 |
| **性能影响** | 新增的类型注解在运行时均为 **no‑op**（仅在 `typing` 模块内部），对执行路径、内存占用、GPU 调度无任何影响。<br>唯一的潜在微小开销是 **`ParamSpec`** 引入的闭包捕获，但在 JIT/Inductor 编译阶段已被优化掉，实际运行时间保持不变。 |
| **安全考虑** | 类型覆盖本身不牵涉安全逻辑；然而更严格的返回值标注有助于 **防止隐式类型错误**（如误把 `torch.Tensor` 当作 `list`），间接提升安全性。未引入新的外部依赖，兼容现有的 `torch` 与 `torch._inductor` 版本。 |
| **可维护性** | - 明确了 **函数入口/出口**（`-> None`、`-> torch.Tensor`、`-> tuple[...]`），降低了后续维护时的认知负担。<br>- 统一了 **`empty_*` 系列工具函数的签名**，避免散落的 `*args, **kwargs` 形式。<br>- 对 **`MatcherCustomOp`** 添加 `Any` 参数标注，统一了 `__call__`、`empty_*` 的接口，使得自定义 Op 的实现更易测试。 |
| **兼容性** | 仅在 **Python 3.9+** 环境下使用 `list[torch.Tensor]` 与 `dict[tuple[Any, ...], ...]` 的新语法，项目已声明最低 Python 3.9，故兼容性保持。<br>所有改动均为 **向后兼容**（未改变运行时行为），已通过现有单元测试与 CI。 |
| **工具链依赖** | 增加了对 **`typing.ParamSpec`** 的使用，建议 CI 中确保 `typing-extensions`（如在 3.9 环境）保持同步，以防止 `ParamSpec` 在旧版解释器报错。 |

---

### ⚠️ 潜在风险
1. **类型检查与实际实现不一致**  
   - 若某子类的 `register` 方法返回值与基类声明不匹配（如返回 `torch.Tensor` 实际返回 `tuple`），在开启 `mypy --strict` 时会报错，可能导致 CI 失败。  
2. **`ParamSpec` 与 `Callable` 组合的闭包**  
   - `wrap_trace_fn` 返回的 `wrapped` 使用 `P.args` 与 `P.kwargs`，若调用方未满足相同签名（如传入非 `fx.GraphModule`），运行时仍正常，但类型检查会提示不兼容。  
3. **分布式状态函数的返回类型**  
   - 新增 `-> int` 标注后，如果外部代码仍依赖旧的 `Any`（例如对返回值做 `isinstance(..., int)`），不会出现错误，但可能触发 **静态分析误报**。  
4. **`__all__` 与模块导出**  
   - 在 `rotary_embedding/__init__.py` 中仅保留 `RotaryEmbedding`，若有外部模块直接访问 `_ROPE_DICT`（原为私有但在某些脚本中使用），将变为未导出的符号，可能导致 `ImportError`。  

---

### 💡 关注建议
- **集成类型检查**：在 CI 中加入 `mypy --strict`（或 `pyright`）跑一遍完整项目，确保所有新签名与实现保持一致，防止未来代码回滚时出现隐蔽错误。  
- **回归测试**：虽然功能未改动，仍建议运行完整的 **vLLM 端到端基准**（包括 TP、DCP、RoPE、Quant、Attention）确认没有因为签名不匹配导致的 JIT 编译差异。  
- **文档更新**：在 `docs` 中补充 **“开发者指南 – 类型覆盖”** 小节，说明新加入的 `ParamSpec` 用法以及 `MatcherCustomOp` 的约定，以帮助贡献者遵守统一签名。  
- **兼容层**：若项目仍需支持 Python <3.9 的分支，可考虑在 `setup.cfg` 中加入 `typing-extensions` 并提供兼容别名 `list[torch.Tensor]` → `List[torch.Tensor]`，避免因语法差异导致构建失败。  
- **关注私有变量导出**：对 `_ROPE_DICT` 的使用做一次全局搜索，确认没有外部直接引用；若有，提供兼容的 getter 接口或在 `__all__` 中加入别名。  

> **总体结论**：本次提交通过系统化的类型注解显著提升了代码质量和可维护性，对运行时行为无影响。只要在 CI 中加入严格的类型检查并完成回归验证，即可安全合并，后续新功能（量化算子、融合模式等）将受益于更精确的接口约束。

---

### [3/N][Attention] Move AttentionMetadata-related code from utils.py to backend.py (#32054)
**SHA**: `20228cb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/20228cb8514ec2f8155b94fe0af204d839238280)

**🎯 变更类型**：重构 / 架构调整  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交将一套与注意力（Attention）相关的核心数据结构及构建器（`CommonAttentionMetadata`、`AttentionCGSupport`、`AttentionMetadataBuilder`）从 `vllm/v1/attention/backends/utils.py` 移动到 `vllm/v1/attention/backend.py`。随后对项目中几乎所有引用这些类的文件进行统一的 import 路径更新。文档与测试也同步修改，确保新位置可被正常使用。

**🎯 影响范围**  
- `vllm/v1/attention/backend.py`（新增大量类定义，体积显著增加）  
- `vllm/v1/attention/backends/utils.py`（删除上述类，保留工具函数）  
- 所有注意力后端实现文件（`cpu_attn.py`、`flash_attn.py`、`flashinfer.py`、`flex_attention.py`、`gdn_attn.py`、`linear_attn.py`、`mamba*_attn.py`、`mla/*`、`tree_attn.py`、`triton_attn.py`、等）  
- 多个模型层、调度器、测试、文档等位置的 import 路径调整。  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **职责划分更清晰**：`backend.py` 本身已经是注意力后端抽象（`AttentionBackend`、`AttentionImpl`）的核心位置，将元数据定义放置于此，使得“后端相关的所有概念”集中在同一模块，提升可读性与可维护性。<br>- **依赖关系**：`utils.py` 仍在类型检查阶段（`TYPE_CHECKING`）引用 `backend` 中的 `KVCacheLayoutType`，运行时不会产生循环导入。<br>- **向后兼容性**：原有外部代码（第三方插件、用户脚本）如果仍然从 `vllm.v1.attention.backends.utils import CommonAttentionMetadata` 导入，将在运行时抛出 `ImportError`。此变更属于 **破坏性 API 调整**，需要在文档或发行说明中提醒升级。 |
| **性能影响** | - **运行时**：仅是类的定义位置迁移，实际运行时逻辑未变更，性能基本不受影响。<br>- **导入成本**：`backend.py` 体积增大导致首次 import 时稍微变慢，但相对整个 vLLM 框架启动时间影响微乎其微（毫秒级）。 |
| **安全考虑** | - **无安全风险**：变更不涉及算子实现、内存管理或权限检查，仅是数据结构搬迁。<br>- **类型检查**：使用 `typing_extensions.deprecated` 标记的旧属性仍保留，保持向后兼容的同时提供迁移提示。 |
| **可维护性** | - **代码聚合**：所有注意力后端相关的抽象类现在位于同一文件，便于审阅和统一修改。<br>- **减少冗余**：原本在 `utils.py` 中重复出现的 `AttentionCGSupport`、`AttentionMetadataBuilder` 的定义被删除，降低了代码复制风险。 |
| **测试影响** | 所有受影响的测试文件已同步改为新的 import 路径，CI 通过。若外部项目未同步，可能出现导入错误。 |

**⚠️ 潜在风险**  

1. **破坏性 API 变更**  
   - 外部用户仍使用旧路径会导致 `ImportError`。需要在 `vLLM` 的发布说明中明确标记为 `v0.15.0`（或对应版本）前的迁移步骤。建议在 `utils.py` 中保留兼容层（如在文件底部添加 `from .backend import CommonAttentionMetadata  # noqa: F401`），但当前提交未这么做。  

2. **循环导入风险**  
   - `backend.py` 在 `TYPE_CHECKING` 环境下仍然导入 `utils`（如 `KVCacheLayoutType`），如果未来在运行时直接使用这些类型，可能触发循环导入。保持现有 `if TYPE_CHECKING:` 结构不变即可规避。  

3. **文档/示例同步**  
   - 项目文档、示例代码、外部教程需要更新，否则新用户会在阅读旧文档后遇到导入错误。  

4. **工具链兼容**  
   - 某些 IDE 的自动导入提示可能仍指向旧路径，导致开发者产生困惑。  

**💡 关注建议**  

- **兼容层（可选）**：在 `vllm/v1/attention/backends/utils.py` 底部添加向后兼容的 re‑export，以免立即破坏现有用户代码。  
  ```python
  # Backward compatibility for pre‑v0.15 releases
  from .backend import CommonAttentionMetadata, AttentionCGSupport, AttentionMetadataBuilder  # noqa: F401
  ```  
- **发布说明**：在本次版本的 Release Note 中明确声明 `CommonAttentionMetadata`、`AttentionCGSupport`、`AttentionMetadataBuilder` 已迁移至 `vllm.v1.attention.backend`，并提供迁移示例。  
- **CI 检查**：加入一项检测——确保项目内部所有 `import` 均已指向新路径，避免遗漏。  
- **文档更新**：检查所有用户手册、API 参考、示例 notebooks，确保使用新 import。  
- **监控**：在新版本发布后，观察用户提交的 Issue/PR，快速响应因旧路径导致的导入错误。  

---  

总体来看，此次重构提升了代码组织层次，风险主要集中在向后兼容性上。若在发布时提供合适的迁移指引或兼容层，影响可被有效控制。

---

### Add K-EXAONE-236B-A23B (#31621)
**SHA**: `63ed240` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/63ed2409e8cf55ed01df866a38c9d5dad34a393e)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：本次 PR 为 vLLM 引入全新的 K‑EXAONE‑236B‑A23B（代号 `ExaoneMoe`）模型支持，包括模型实现、MTP（多 Token 预测）推理路径、文档、注册表、测试以及配置层面的更新。新增的 MoE（Mixture‑of‑Experts）实现利用 FusedMoE、EP‑LB（专家负载均衡）以及 Tensor‑Parallel/Pipeline‑Parallel 机制，实现对 236‑B 规模模型的高效推理，并提供对应的 speculative（MTP）加速路径。

---

### 🎯 影响范围
- **模型执行层**：`vllm/model_executor/models/exaone_moe.py`、`exaone_moe_mtp.py`（全新 MoE 与 MTP 实现）  
- **模型注册与映射**：`registry.py` 中新增 `ExaoneMoEForCausalLM` 与 `ExaoneMoeMTP` 条目  
- **配置体系**：`config/speculative.py` 增加 `exaone_moe_mtp` 类型，关联 `n_predict` 参数  
- **文档**：`docs/models/supported_models.md` 增加模型描述  
- **测试**：`tests/models/registry.py` 新增模型可用性检查（包括最低 `transformers==5.0.0`）  
- **其它**：`exaone4.py` 参数 `reduce_results` 暴露，以兼容共享专家的结果合并逻辑  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 1. **全新 MoE 体系**：`ExaoneMoe` 通过 `FusedMoE` 实现路由、专家计算和结果归约，支持 EP‑LB（冗余专家）和 `reduce_results=False` 的显式控制。<br>2. **Pipeline‑Parallel 兼容**：在 `ExaoneMoeModel`、`ExaoneMoeDecoderLayer` 中加入 `PPMissingLayer` 处理，使模型在多层级 PP 场景下仍能正确切分。<br>3. **Speculative（MTP）路径**：`ExaoneMoeMTP` 包装 `ExaoneMoeMultiTokenPredictor`，在前向阶段融合输入嵌入与上一轮隐藏状态，实现多 Token 预测（MTP）加速。<br>4. **模型注册**：在 `registry.py` 中映射新模型，使其可以通过统一入口 `model="ExaoneMoEForCausalLM"` 加载。 |
| **性能影响** | - **正向**：利用 `FusedMoE` 的高效路由+分块计算，结合 EP‑LB 可提升负载均衡、减少热点专家卡顿。<br>- **正向**：MTP 通过一次前向生成多 token，降低解码层次数，显著提升吞吐。<br>- **潜在负担**：MoE 的专家切分会导致额外的 all‑reduce（`maybe_all_reduce_tensor_model_parallel`），在极大 TP/EP 规模下可能出现通信瓶颈。<br>- **内存**：`reduce_results=False` 会保留每个专家的原始输出，内存占用约为 `num_experts * hidden_size`，对大模型（236B）需确保足够显存。 |
| **安全考虑** | - 代码未引入外部网络调用或系统调用，不涉及安全风险。<br>- 唯一安全相关点是 **模型权重加载**：`AutoWeightsLoader` 仍然会跳过 `mtp.` 前缀的权重，防止意外覆盖；但若用户误将不兼容的权重（如旧版 `transformers` 导出的 checkpoint）加载，可能导致异常或信息泄露（异常报错信息）。建议在文档中强调 **需使用 transformers ≥ 5.0.0**。 |
| **兼容性** | - 需要 **transformers ≥ 5.0.0**（在测试中已加入版本校验）。<br>- 当 **TP size > num_experts** 时会抛出 `ValueError`，防止非法配置。<br>- `ExaoneMoe` 与原 `Exaone4` 共用 `ExaoneMoeAttention`、`ExaoneMoeGatedMLP`，保持代码复用。<br>- `reduce_results` 参数在 `exaone4.py` 中默认 `True`，对已有模型行为无影响。 |

---

### ⚠️ 潜在风险

1. **显存与通信压力**  
   - `reduce_results=False` 以及大量冗余专家会导致显存激增；在多节点部署时，全量 all‑reduce 可能成为性能瓶颈。  
2. **配置不匹配**  
   - `tp_size > num_experts` 会直接报错；用户若误配置 `tensor_parallel_size`（例如在 8‑GPU 环境下使用 2‑expert MoE）会导致启动失败。  
3. **权重加载路径**  
   - 访问 `mtp.` 前缀的权重被跳过；如果模型实际包含 MTP 权重（如自研 checkpoint），加载会不完整。  
4. **EP‑LB 参数默认值**  
   - `eplb_config.num_redundant_experts` 默认为 0，若用户期望启用冗余专家但未在配置文件中声明，可能错失负载均衡收益。  
5. **Speculative MTP 与 Prefix Caching 冲突**  
   - 当前实现在 `ExaoneMoeMTP` 中强制 `enable_prefix_caching=False`，若后续开启会导致未定义行为。  
6. **文档/测试同步**  
   - 文档已加入模型列表，但示例使用方式（如 `speculative_model` 参数）需要在用户手册中补充说明。  

---

### 💡 关注建议

| 受众 | 操作建议 |
|------|----------|
| **开发者** | 1. 在新增模型的 **CI** 中加入 **显存/吞吐基准**（不同 TP/EP 配置下）以监控 `reduce_results=False` 的显存占用。<br>2. 为 `ExaoneMoe` 与 `ExaoneMoeMTP` 增添 **单元测试**：包括 `tp_size > num_experts` 异常、EP‑LB 冗余专家启闭、MTP 前向输出一致性。<br>3. 将 `transformers` 版本校验抽象为公共函数，避免散落在多个测试文件。<br>4. 在 `config/speculative.py` 中加入注释，说明 `n_predict` 由模型配置 `num_nextn_predict_layers` 决定。 |
| **运维 / 使用者** | 1. 部署前确认 **Tensor‑Parallel** 大小 ≤ `num_experts`（模型默认 236‑B 给出 236/8 ≈ 30+ experts，可在 8‑GPU TP 下安全运行）。<br>2. 如需 **EP‑LB**，在 `vllm.yaml` 中显式设置 `parallel_config.eplb_config.num_redundant_experts`。<br>3. 若使用 **MTP 预测**，请关闭 Prefix‑Caching（已在代码中强制），并确保 `speculative_model` 指向同一模型仓库路径。<br>4. 关注 **Transformers 版本**，建议使用 `>=5.0.0` 并保持与 vLLM 同步升级，以避免权重命名不匹配。 |
| **安全审计** | - 目前无新增系统调用或网络请求，重点审计 **权重加载路径**，确保

---

#### 🟡 中重要度变更 (24)

### [Refactor] [7/N] to simplify the vLLM lora serving architecture (#32251)
**SHA**: `4f02cb2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4f02cb2eac20a1281784914acca80cd1fe2bfd15)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将 LoRA 适配器的请求模型（`LoadLoRAAdapterRequest`、`UnloadLoRAAdapterRequest`）从 `vllm.entrypoints.openai.engine.protocol` 中抽离，迁移到新模块 `vllm.entrypoints.serve.lora.protocol`。相应地，所有使用这些模型的入口（`openai.serving_models`、`serve.lora.api_router`、测试用例）更新了 import 路径。此次改动旨在把 LoRA 相关协议与 OpenAI 接口解耦，简化服务架构。

**🎯 影响范围**  
- `vllm/entrypoints/openai/engine/protocol.py`（删除模型定义）  
- `vllm/entrypoints/openai/serving_models.py`（改用新协议）  
- `vllm/entrypoints/serve/lora/api_router.py`（改用新协议）  
- 新增 `vllm/entrypoints/serve/lora/protocol.py`（定义 LoRA 请求）  
- 测试 `tests/entrypoints/openai/test_serving_models.py`（更新导入）  

**💡 关注建议**  
1. **兼容性**：若外部项目仍通过旧路径 (`engine.protocol`) 引入 LoRA 请求，将出现 ImportError。建议在 `engine.protocol` 中保留向后兼容的别名或在项目文档中明确迁移指引。  
2. **导出**：确保 `vllm.entrypoints.serve.lora` 包的 `__init__.py` 正确导出 `protocol`，防止在未显式导入时出现模块找不到的情况。  
3. **循环导入**：检查新 imports 是否引入循环依赖；目前看似仅是单向引用，但大规模重构后需运行完整的单元测试以验证。  
4. **文档 & 示例**：更新 API 文档、README 示例以及代码注释，指明 LoRA 请求模型的新位置。  
5. **回归测试**：运行全套测试（特别是 OpenAI、LoRA 相关路由）确保功能未受影响；如果有 CI，加入对旧路径的兼容性检查。  

总体而言，此次重构提升了代码组织性，对功能没有直接影响，只需注意迁移过程中的导入和兼容性。

---

### [Refactor] Remove `MultiModalProfiler` (#32254)
**SHA**: `252c011` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/252c011012658490c2e970c5bc739a84bf5759c0)

**🎯 变更类型**：重构 / 其他  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将原有的 `MultiModalProfiler` 类彻底移除，相关接口改为直接由各 `processor`（`BaseMultiModalProcessor`）的 `dummy_inputs` 对象提供。  
- 新增 `BaseMultiModalProcessor.dummy_inputs.get_decoder_dummy_data` 与 `get_dummy_mm_inputs` 等方法的直接调用路径。  
- `registry` 中获取最大 token、limit、decoder dummy data 的实现全部改为使用 `processor.allowed_mm_limits` 与 `processor.dummy_inputs`，不再实例化 profiler。  
- 测试代码同步更新，去除对 `MultiModalProfiler` 的导入和实例化。

**🎯 影响范围**  
- `vllm/multimodal/profiling.py`（类删除、相关逻辑移动）  
- `vllm/multimodal/registry.py`（所有 profiling 相关调用改写）  
- 单元测试 `tests/models/multimodal/processing/*`、`tests/multimodal/test_processing.py`（相应 import 与调用更新）  
- 可能影响使用 `MultiModalProfiler` 的外部工具或自定义插件。

**💡 关注建议**  
1. **兼容性检查**：确认项目内部或 downstream 项目未再直接引用 `vllm.multimodal.profiling.MultiModalProfiler`，否则会出现 ImportError。若有，需要在业务层改为使用 `processor.dummy_inputs`。  
2. **文档更新**：及时在开发者文档、API 参考中删除 `MultiModalProfiler`，补充说明 `processor.dummy_inputs` 提供的等价功能和使用示例。  
3. **性能验证**：虽然重构仅是调用路径变化，但建议跑一遍现有的 profiling 基准（内存/token 统计）以确保数值保持一致。  
4. **异常处理**：`processor.allowed_mm_limits` 和 `dummy_inputs` 的属性访问在异常场景（如未实现对应接口的自定义 processor）可能抛出 AttributeError，建议在高层捕获并给出友好提示。  

总体来说，此次重构使代码结构更简洁，去除了中间层对象，但需注意迁移旧调用并保证文档同步。

---

### [6/N][Attention] Move utils to more appropriate locations (#32215)
**SHA**: `98f60e5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/98f60e5acbcbbc9b6e7d6370c4e19128fc3b0b32)

**变更概览**  
本次提交把原来散落在 *vllm/v1/attention/backends/utils.py* 中的 “ubatch / metadata 切分” 相关实现迁移到 *vllm/v1/worker/ubatch_utils.py*，并将 `subclass_attention_backend*` 的实现从 utils 移到 *vllm/v1/attention/backend.py*。随后统一所有引用位置，涉及 attention 层、模型 runner、cudagraph 辅助等 20+ 文件。

**核心影响**  
1. **API 重定位**：`split_attn_metadata、slice_query_start_locs、_make_metadata_with_slice` 等函数现在在 `vllm.v1.worker.ubatch_utils`，所有原来直接 `from vllm.v1.attention.backends.utils import …` 的地方必须改为新路径。若外部项目仍依赖旧路径，会导致 ImportError。  
2. **循环依赖的消除**：`AttentionMetadataBuilder` 由 `vllm.v1.attention.backends.utils` 抽离到 `vllm.v1.attention.backend`，可避免 worker 与 attention 之间的循环导入。  
3. **实现位置更合理**：ubatch 逻辑本身属于调度/worker 层，迁移后代码组织更清晰，后期维护和功能扩展（如新增 ubatch 切分策略）更自然。  
4. **测试调整**：仅有 `tests/v1/attention/test_attention_splitting.py` 小幅改动，说明大多数内部单元测试仍通过。  

**潜在风险**  
- **向后兼容性**：公共 API（如 `vllm.v1.attention.backends.utils.split_attn_metadata`）被删除，若用户在自己的代码中直接引用，升级后会报错。建议在发布说明中标记为 **破坏性改动**，并提供临时 shim（在旧模块中转发）以降低升级阻力。  
- **CUDAGraph 兼容性**：迁移过程未改动实现本身，仍然在切分时创建新张量，这会破坏 cudagraph 捕获。若项目依赖 cudagraph 进行高效推理，需要再次确认文档或在函数 docstring 中提醒。  
- **导入顺序**：部分文件（如 `vllm/v1/worker/gpu/cudagraph_utils.py`）从 `backend` 导入 `AttentionMetadataBuilder`，而 `backend` 又在内部使用 `AttentionBackend`。确保没有新的循环依赖。  

**建议**  
1. **提供兼容层**：在 `vllm/v1/attention/backends/utils.py` 添加轻量的转发函数（例如 `def split_attn_metadata(...): return ubatch_utils.split_attn_metadata(...)`），直至下一个 major 版本移除。  
2. **更新文档**：在 **API 迁移指南** 中列出所有被移动的符号及新导入路径，并在 `attention` 模块的 README 中提醒。  
3. **CI 检查**：加入对外部依赖的导入检测（如 `importlib.util.find_spec`），确保旧路径不再被意外使用。  
4. **性能回归**：跑一次完整的 cudagraph 基准，确认切分函数仍保持原有的执行时间与显存占用。  

总体而言，此次重构提升了代码结构的语义清晰度，风险主要集中在向后兼容和导入循环上，按以上建议处理即可平稳发布。

---

### [Quantization] fix: overflow with static per-tensor scaling (#29867)
**SHA**: `a5bbbd2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a5bbbd2f24983f309f915a1de911b9301e1699e2)

**变更类型**：功能增强 / Bug 修复  
**重要程度**：🟡 中  

**核心变更**  
1. **quant_utils.py**  
   - 新增 `get_attribute_fallback` 与 `get_and_maybe_dequant_weights`，统一获取层权重并在需要时完成反量化（支持 Unquantized、Fp8、以及通用的 “eye‑matrix” 方式）。  
   - `scaled_dequantize` 入口改为仅返回张量，去掉了原来的二元组返回，防止在静态 per‑tensor 缩放下出现溢出。  
   - 引入 `TYPE_CHECKING` 防止循环导入。  

2. **mla/common.py**（Attention 后端）  
   - 删除重复的权重获取/反量化实现，改用上述工具函数。  
   - 通过显式 `out_dtype=act_dtype` 调用，确保在加载后使用的权重类型一致。  

**影响范围**  
- `vllm.model_executor.layers.quantization.*`（尤其是 Fp8、static‑per‑tensor 量化路径）  
- `vllm.v1.attention.backends.mla` 中的多路注意力实现以及相关的 LoRA、KV 投影权重加载。  

**关注建议**  
- **开发者**：对自定义 `LinearBase` 子类确保提供 `weight`、`qweight` 或 `weight_packed` 任意属性；若实现新的量化方法，需在 `get_and_maybe_dequant_weights` 中加入对应的反量化逻辑。  
- **测试**：重点覆盖 static per‑tensor scaling 场景，验证 `scaled_dequantize` 不再产生 overflow，并检查 `kv_b_proj` 权重在不同 dtype（fp16、bf16、fp32）下的数值一致性。  
- **部署**：此改动主要在离线加载阶段产生额外的 O(N³) 计算（eye‑matrix 方式），仅在未实现专用量化 BMM 时使用，建议在生产环境使用已实现的 Fp8 或 Unquantized 路径，以免影响启动时延。  

整体来看，此次提交修复了量化路径的数值溢出并抽象出通用权重处理逻辑，提升代码可维护性和安全性。请在后续新量化实现中遵循新增工具函数的接口。

---

### [Bugfix] Replace `PoolingParams.normalize` with `use_activation` (#32243)
**SHA**: `232214b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/232214b2aece00201a2355ed7368452f761b76db)

**🎯 变更类型**：功能增强（参数统一）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原先用于归一化的 `PoolingParams.normalize`、`PoolerConfig.normalize`、`softmax`、`activation` 全面替换为统一的 `use_activation` 参数，并在代码、示例、测试以及文档中同步更新。旧字段标记为 **DEPRECATED**，在 v0.15 将被移除，相关 `get_use_activation` 辅助函数会在读取旧字段时抛出一次性警告并转为 `use_activation`。  

**🎯 影响范围**  
- `vllm/config/pooler.py`、`vllm/pooling_params.py`、各模型层的 pooler 实现。  
- 所有示例脚本、CLI 参数 (`--pooler-config`) 以及 JSON 配置。  
- 单元测试、集成测试均已改为使用 `use_activation`。  
- 兼容层 `get_use_activation` 仍支持旧字段，但会在日志中提醒。  

**💡 关注建议**  
- **开发者**：检查自定义的 `PoolerConfig`、`PoolingParams` 初始化代码，确保改为 `use_activation`；若仍使用 `normalize`、`softmax`、`activation`，请尽快迁移，以避免 v0.15 升级后抛出 `AttributeError`。  
- **用户**：在使用 `vllm serve` 或 `vllm embed` 时，更新 `--pooler-config` JSON 为 `"use_activation": true/false`，不要继续依赖 `"normalize"`。旧字段仍可临时使用，但会产生一次性警告。  
- **测试/CI**：确认所有自定义测试或第三方插件已同步参数名称，避免因 `PoolingParams.verify` 参数校验变化导致 false‑positive。  
- **文档**：完善迁移指南，明确默认行为（`use_activation` 默认 `True`），并在发布说明中突出即将移除的字段。  

整体来看，此次改动统一了激活相关的配置，提升了 API 一致性，影响主要在配置层面，功能逻辑保持不变，只需同步参数名称即可。  

---

### [Refactor] Remove `get_encoder_dummy_data` (#32241)
**SHA**: `eb28e80` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/eb28e8068dd870283f1a4fe8251ed9254d6f477e)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交将原先用于 Encoder‑Side 伪数据生成的 `get_encoder_dummy_data` 接口及相关属性 `pad_dummy_encoder_prompt` 彻底移除，改为在 `BaseProcessingInfo` 中引入 `skip_prompt_length_check` 标记，只有在需要跳过 Encoder 长度校验的模型（如 Whisper、Nemotron‑Parse）里返回 `True`。相应地，`v1/engine/input_processor` 在校验提示长度时改为检查该标记，而不再依赖已删除的 `EncDecMultiModalProcessor`。

**🎯 影响范围**  
- `vllm/model_executor/models/nemotron_parse.py`、`vllm/model_executor/models/whisper.py`（新增 `skip_prompt_length_check` 实现）  
- `vllm/multimodal/processing.py`（移除 `pad_dummy_encoder_prompt`，新增默认 `skip_prompt_length_check=False`）  
- `vllm/multimodal/profiling.py`、`vllm/multimodal/registry.py`（删除 Encoder dummy‑data 生成逻辑及对应导入）  
- `vllm/v1/engine/input_processor.py`（改用 `skip_prompt_length_check` 判断是否跳过长度检查）  

**💡 关注建议**  

1. **兼容性**：`get_encoder_dummy_data` 已被移除，任何外部或内部代码仍然调用该函数都会导致 `AttributeError`。建议在文档中标明已弃用，并在下一 major 版本前提供兼容包装或迁移指南。  
2. **内存 profiling**：删除 Encoder dummy 数据后，只有 Decoder 侧的 profiling 能力保留。如果用户依赖 Encoder 端的内存占用估算，需要在新版中另行实现或通过手动构造 dummy 数据。  
3. **属性命名**：`skip_prompt_length_check` 语义更明确，但其默认值 `False` 与旧的 `pad_dummy_encoder_prompt=False` 不完全等价。请确保所有 Encoder‑Decoder 模型（尤其是以后新增的）在实现 `BaseProcessingInfo` 时显式覆盖该属性，以免误触长度校验。  
4. **单元测试**：确认所有涉及多模态模型的单元测试已覆盖新路径，尤其是 Whisper、Nemotron‑Parse 的提示长度验证和 `MultiModalProfiler.get_decoder_dummy_data` 的行为。  
5. **代码清理**：因为 `MultiModalEncDecInputs`、`DummyEncoderData` 等类型已不再使用，建议在对应模块中删除未使用的导入并运行 lint/typing 检查，以避免残留的 `import` 警告。  

总体来看，本次重构简化了 Encoder‑Side 的伪数据逻辑，降低了多模态处理的复杂度，但需要注意向后兼容性和 profiling 功能的潜在缺失。建议在下一个发布周期提供迁移说明或保留一个轻量的兼容层。

---

### [Model] Use mm_position to compute mrope positions for Qwen2-VL/2.5-VL (#32126)
**SHA**: `542a405` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/542a4059b2bb0f790e82822c8b9cbcf8cde91adb)

**🎯 变更类型**：功能增强 / 代码重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Qwen2‑VL 系列模型新增 `iter_mm_grid_thw` 统一遍历多模态特征的实现，并在 `get_mrope_input_positions` 中改用该迭代器重新计算 MROPE 位置。实现细节从逐 Token 查找切换为基于 `mm_position.offset` 的一次遍历，并改用 NumPy 进行网格索引生成，去除了旧的 `image_token_id / video_token_id` 判断逻辑。

**🎯 影响范围**  
- `vllm/model_executor/models/qwen2_vl.py`  
- `vllm/model_executor/models/qwen2_5_vl.py`（两者保持同步）  
- 依赖 `MultiModalFeatureSpec` 的位置计算路径（模型推理阶段的多模态嵌入定位）。

**💡 关注建议**  
1. **功能验证**：使用包含多张图片、单帧/多帧视频的混合 Prompt 进行回归测试，确认 `mm_position.offset` 的排序与原先通过 `vision_start_token_id`、`image_token_id`、`video_token_id` 的顺序保持一致。  
2. **数值一致性**：因为位置索引改为 NumPy `int64`，确保在后续的 Torch 运算中不会出现 dtype 隐式转换导致精度或溢出问题。  
3. **性能评估**：对比旧实现的 `torch.arange` + `torch.stack` 与新实现的 `np.indices` + `np.broadcast_to` 的时间和显存占用，特别是大规模视频（t·h·w 较大）时的表现。  
4. **向后兼容**：若外部代码仍依赖 `vision_start_token_id` 等 token‑id 查找方式，需保证这些 token 仍在模型词表中但不再参与位置计算，以免引入意外错误。  
5. **文档与注释**：建议在 `MultiModalFeatureSpec` 文档中补充 `mm_position.offset` 的定义及其在位置计算中的作用，帮助后续维护者快速理解新逻辑。  

总体来说，此次改动简化了位置计算路径、提升了可读性并可能带来轻微性能收益，但需重点检查多模态特征顺序与旧版兼容性。

---

### [ROCm][CI] Fix engine core client tests for ROCm spawn multiprocessing (#32061)
**SHA**: `df7e127` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/df7e12715fa5cac36069d3a58d869d9b939027a9)

**🎯 变更类型**：功能增强 / CI 兼容性  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 ROCm 环境下必须使用 `spawn` 启动方式的多进程测试新增四个 `sitecustomize.py` 生成 fixture，实现子进程中对 `EngineCore` 的 monkey‑patch。  
- 将原本在测试文件中定义的 `MyDataclass`、`echo_*` 等实用函数迁移为 `TestMessage` dataclass 与对应 `echo_dc*` 实现，并通过 `inspect.getsource` 动态写入子进程的 `sitecustomize.py`。  
- 相应地更新所有调用点的断言类型，从 `MyDataclass` 改为 `TestMessage`。  

**🎯 影响范围**  
- `tests/v1/engine/test_engine_core_client.py`（核心）  
- 受影响的模块：`vllm.v1.engine.core.EngineCore`（仅在测试进程中被临时替换），以及 `msgspec` 对自定义类型的序列化路径。  

**💡 关注建议**  
1. **环境变量清理**：`subprocess_*_patch` 通过 `monkeypatch.setenv("PYTHONPATH", …)` 注入临时路径，建议在 fixture 结束时恢复原 `PYTHONPATH`，防止对后续测试产生泄漏。  
2. **模块路径一致性**：`TestMessage.__module__` 被强制设置为 `tests.v1.engine.test_engine_core_client`，确保跨进程反序列化时能够定位该类型；若后续移动或重命名测试文件，需要同步更新此字符串。  
3. **跨平台兼容**：虽然改动仅为 ROCM CI 章节，但 `sitecustomize.py` 会在所有平台的子进程中生效，建议在非 ROCm 环境下通过条件 (`sys.platform` 或 env flag) 跳过这些 fixture，避免不必要的文件写入。  
4. **代码可维护性**：`inspect.getsource` 依赖函数定义的原始缩进，若未来对这些 util 函数做改动（如添加装饰器），请同步检查生成的 `sitecustomize.py` 是否仍可正确执行。  

总体而言，此次改动提升了 ROCm CI 对多进程的可靠性，对生产代码无直接影响，保持现有测试通过即可。若后续引入新的自定义返回类型，请同样通过上述 fixture 方式进行跨进程挂载。

---

### [BugFix]Fix eagle draft_model_config and add tests (#31753)
**SHA**: `80221e1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/80221e1884bd36751e8ae0308acf71f42946a05e)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：修正了 Eagle 系列模型在 Speculative 推理时的 draft 配置读取错误；新增针对 Eagle Draft‑Model Config 的单元测试并同步测试数据中的 `text_model_type` 为 `eagle`，同时在 `speculative.py` 中补全对 `hf_text_config` 的生成及架构缓存的更新，扩展 `is_deepseek_mla` 判定以兼容 `deepseek_mtp`。  

**🎯 影响范围**：`vllm.config.speculative`, `vllm.transformers_utils.model_arch_config_convertor`, 测试目录 `tests/config` 与 `tests/test_config.py`，以及涉及 Eagle 模型的并行/推理路径。  

**💡 关注建议**：  
1. 运行全套测试（包括新加入的 Eagle 相关用例），确认 `SpeculativeConfig.draft_model_config` 在不同 `model_type` 下均返回正确的 `architectures` 与 `model_type`。  
2. 检查其他非 Eagle 的 Speculative 场景，确保在 `__post_init__` 中对 `hf_text_config` 的新赋值不会覆盖或冲突已有配置。  
3. 留意 `model_arch_config_convertor.is_deepseek_mla` 的扩展，验证 `deepseek_mtp` 在已有 DeepSeek 流程中仍能正确识别并走 LORA‑kv‑rank 分支。  
4. 若项目对外提供配置文件模板，更新文档中 `text_model_type` 的取值说明，避免用户仍使用旧的 `deepseek_mtp` 造成兼容性错误。  

整体来看，此次改动解决了 Eagle Draft‑Model 配置缺失导致的运行时异常，风险较低，只需通过回归测试确认对其他模型的无副作用。

---

### Fix various typos found in `docs` (#32212)
**SHA**: `f243abc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f243abc92d805b366ca00911d6ae5572faa39bff)

**🎯 变更类型**：其他（文档排版/拼写修正）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在 vLLM 官方文档中批量纠正了约 30 处拼写、标点、格式及链接错误，提升了可读性与专业度。涉及章节包括贡献指南、模型接入、部署、设计细节、特性说明、安装快速上手以及治理合作等。  

**🎯 影响范围**：  
- `docs/` 目录下的所有子模块（贡献、部署、设计、特性、模型、使用指南等）。  
- 对代码本身无任何影响，唯一受影响的是文档渲染与搜索索引。  

**💡 关注建议**：  
1. **CI 检查**：确保 CI 中仍保留 Markdown/Lint 检查，防止类似错别字再次进入主库。  
2. **链接验证**：部分相对路径已修正（如 `fused_moe/modular_kernel.py`），建议在 CI 加入链接完整性校验。  
3. **文档发布**：更新后记得触发文档站点重新构建，确认页面渲染正常，尤其是代码块和图片引用。  
4. **后续维护**：建议在 CONTRIBUTING 中加入“文档拼写/格式审查”步骤，引导 PR 作者自行使用拼写检查工具（如 `cspell`）减轻维护负担。  

整体而言，此次修改仅提升文档质量，对用户使用和社区贡献体验有积极作用，风险极低。

---

### [Misc] improve warning/assert messages (#32226)
**SHA**: `15b33ff` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/15b33ff06473d3efddefde4990363f1aecff1d48)

**变更类型**：其他（改进 warning / assert 文本）  
**重要程度**：🟡 中  

**变更概述**  
本次提交统一并修正了多个模块中提示信息的文字表达，主要包括：  
- `compiler_interface.py` 中的断言信息添加了缺失的空格，避免拼接后出现阅读障碍。  
- `config/*`（`compilation.py、model.py、vllm.py`）以及 `lora/ops/triton_ops/utils.py`、`attention/backends/fa_utils.py` 中的 `logger.warning_once` 文本均改为更完整、语义清晰的句式，并统一了大小写、标点与空格。  
- 同时消除了“founded”拼写错误，改为 “found”。  

**影响范围**  
- **编译子系统**：`vllm/compilation/compiler_interface.py`、`vllm/config/compilation.py`、`vllm/config/vllm.py`。  
- **模型配置**：`vllm/config/model.py`。  
- **LoRA 插件**：`vllm/lora/ops/triton_ops/utils.py`。  
- **Flash‑Attn 检测**：`vllm/v1/attention/backends/fa_utils.py`。  

**潜在风险**  
- 这些改动仅涉及字符串常量，不会影响运行时逻辑或性能。唯一需要留意的是，部分 warning 文本现在包含了更具体的指示（如建议手动删除缓存目录），如果未来实现了更自动化的清理机制，提示内容可能需要再次同步更新。  
- `logger.warning_once` 的实现会在首次出现时记录一次，若已有旧日志中出现不完整信息，用户仍可能看到旧日志。测试或文档需要更新对应的示例。  

**建议**  
1. **单元测试**：为关键路径（如 `load` 中的 assert、`set_splitting_ops_for_v1` 的警告）添加断言，确保在异常情况下能够捕获并验证新提示文本。  
2. **文档同步**：更新 README/运维手册中涉及 “Inductor cache lookup failed” 或 “piecewise cudagraph” 的说明，使用新的完整句子。  
3. **审查输出**：在 CI 中加入对 `warning_once` 内容的 snapshot 测试，防止未来误删空格或标点导致信息退化。  
4. **安全考虑**：当前提示不泄露路径或内部状态，保持此风格；若以后加入更详细的调试信息，请评估是否会暴露用户文件系统。  

总体而言，这是一轮细致的可读性提升，风险极低，只需确保相应的测试与文档同步即可。

---

### [BugFix] Fix engine crash caused by chat tools + response_format (#32127)
**SHA**: `c6bb5b5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c6bb5b56030df628697bcccad551276f81d2019e)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：解决在 `response_format=json_object` 与 `tool_choice="required"` 同时使用时导致的引擎崩溃。核心做法是：在工具强制调用时清除 `response_format`，并在 `SamplingParams` 初始化后执行 `__post_init__` 进行结构化输出的二次校验。新增回归测试验证该组合不再崩溃并正常返回 tool‑calls。  

**🎯 影响范围**：  
- `vllm/tool_parsers/abstract_tool_parser.py`（结构化输出与工具调用的冲突处理）  
- `vllm/v1/engine/input_processor.py`（结构化输出参数的后置校验）  
- 测试套件 `tests/tool_use/test_chat_completions.py`（新增回归用例）  

**💡 关注建议**：  
1. **兼容性**：清除 `response_format` 可能影响已有用户依赖该字段的场景，请在发布说明中明确 “当 tool_choice 为 required/forced 时，response_format 将被忽略”。  
2. **文档更新**：在 API 文档中添加 “tool_choice 与 response_format 的冲突处理规则”。  
3. **回归测试**：继续在 CI 中运行新增的回归测试，防止未来改动再次引入同类冲突。  
4. **审计其他组合**：检查 `tool_choice="auto"`、`function` 等其他组合是否仍会触发类似的结构化输出冲突。  

总体而言，此次修复定位准确，改动集中在结构化输出与工具解析的交叉路径，影响范围可控，建议在下个 minor 版本发布并同步文档。

---

### [Misc] Allow enabling NCCL for DP sync when async scheduling (#32197)
**SHA**: `9273a42` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9273a427b5ec1491f7e8420c8fdab8d203843b75)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `disable_nccl_for_dp_synchronization` 引入 `None` 默认值，使其在配置初始化阶段可以延迟决定。  
- 在 `VLLMConfig.__post_init__` 中根据 `async_scheduling` 自动填充该标志：开启异步调度时默认禁用 NCCL，关闭时默认使用 NCCL。  
- 添加统一的 `field_validator("_skip_none_validation")` 实现，对 `None` 时跳过 Pydantic 校验，保持向后兼容。  

**🎯 影响范围**  
- `vllm/config/parallel.py`、`vllm/config/scheduler.py`、`vllm/config/vllm.py`（配置加载与默认值决定）。  
- `vllm/engine/arg_utils.py`（CLI 参数映射）。  
- 依赖 `dp_utils.py` 中 NCCL/Gloo 同步逻辑的所有分布式运行路径。  

**💡 关注建议**  

1. **默认行为一致性**  
   - 确认在旧版配置（未显式指定 `disable_nccl_for_dp_synchronization`）下，异步调度开启时默认使用 Gloo，同步调度时使用 NCCL，保持旧版文档描述。  
   - 若用户手动设置 `disable_nccl_for_dp_synchronization=False`，即使异步调度打开，也应保持 NCCL，不被后置逻辑覆盖。  

2. **验证与错误信息**  
   - `field_validator` 已改为 `mode="wrap"`，但仍需确保在非 `None` 情况下仍触发原有类型检查（bool）。建议在单元测试里加入 `disable_nccl_for_dp_synchronization="invalid"` 的负例，确认报错。  

3. **文档与示例**  
   - 更新配置章节，说明该字段默认 `None`，并说明何时会被自动设为 `True/False`。  
   - 在 README 或示例脚本中展示显式关闭/开启 NCCL 的方式，以防用户误以为自动决策不可覆盖。  

4. **兼容性测试**  
   - 添加 CI 场景：  
     - async scheduling + 未指定字段 → 应记录日志 “Disabling NCCL …”。  
     - async scheduling + 手动 `disable_nccl_for_dp_synchronization=False` → 不应自动改写。  
     - sync scheduling + 未指定字段 → `disable_nccl_for_dp_synchronization=False`。  
   - 验证在多机器、不同 `process_group`（NCCL/Gloo）环境下，`dp_utils` 调用的 `all_reduce` 与期望的 backend 匹配。  

5. **性能回归**  
   - NCCL 与 Gloo 在 DP 同步上的性能差异显著，建议在关键基准测试里记录两种路径的吞吐和延迟，确保默认开启 async 时的 Gloo 不会产生不可接受的回退。  

6. **代码可读性**  
   - `field_validator` 的实现可以抽取为公共工具函数（如 `skip_none`)，在 `scheduler.py` 与 `parallel.py` 重复使用，降低维护成本。  

总体来看，此改动为异步调度提供了更安全的默认配置，避免在不支持 NCCL 的环境下因自动使用 NCCL 而崩溃。只要做好文档、测试以及显式覆盖的保证，风险较低，建议合并。

---

### [responsesAPI] add unit test for optional function tool call id (#32036)
**SHA**: `a307ac0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a307ac073432d3f224c58a54d363fa93f6a659f4)

**🎯 变更类型**：功能增强（为已有 bug‑fix 添加单元测试）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在 `tests/entrypoints/test_responses_utils.py` 中新增了 10 余个测试，用于验证 `_maybe_combine_reasoning_and_tool_call` 在以下场景下的安全性：`id` 为 `None`、为空字符串、未以 `MCP_PREFIX` 开头、最后一条消息非 assistant、assistant 消息缺少 `reasoning`、以及非 `ResponseFunctionToolCall` 类型。通过这些测试确保函数在边界条件下不会抛出 `TypeError`，并且只在符合 MCP 约定的情况下合并推理与工具调用。

**🎯 影响范围**：  
- `vllm.entrypoints.responses_utils._maybe_combine_reasoning_and_tool_call`（核心逻辑）  
- `vllm.entrypoints.constants.MCP_PREFIX`（用于判断是否为 MCP 工具调用）  
- 测试套件 `tests/entrypoints/test_responses_utils.py`  

**💡 关注建议**  
1. **代码健壮性**：确认函数内部已经加入 `if not isinstance(item, ResponseFunctionToolCall) or not item.id:` 等防御式检查，以兼容本次测试的全部情形。  
2. **类型提示**：函数签名目前接受 `ResponseFunctionToolCall`，但测试中会传入普通 `dict`，建议使用 `typing.Protocol` 或 `hasattr` 检查，以避免在生产环境出现同类错误。  
3. **常量可见性**：`MCP_PREFIX` 已在测试中直接导入，确保该常量在 `vllm.entrypoints.constants` 中保持公开且不随内部重构而改名。  
4. **依赖版本**：测试依赖 `openai.types` 包，请在 CI 环境中锁定合适的 `openai` 版本，防止类型结构变化导致导入错误。  
5. **文档更新**：建议在 `responses_utils` 的函数 docstring 中补充 “当 `item.id` 为 `None` 或不匹配 MCP 前缀时返回 `None`” 的说明，提升可维护性。  

总体而言，此次改动仅是增加覆盖率的单元测试，不会改变运行时行为。但它揭示了之前在 `id` 为 `None` 时的潜在 `TypeError`，请确保对应的实现已修正，否则 CI 通过后仍可能在实际调用中复现。保持现有测试通过后，可考虑在后续版本中进一步抽象边界检查逻辑，以减少类似错误的重复出现。

---

### [Model Runner V2] Add support for M-RoPE (#32143)
**SHA**: `0a7dd23` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0a7dd23754ed5e01303e0eb4e64ace5e70251f46)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
为 Model Runner V2 添加 M‑RoPE（多模态 RoPE）支持。新增 `MRopeState` 用于预计算和缓存 M‑RoPE 位置，扩展输入缓冲、CUDA 图管理、模型执行路径以在有‑M‑RoPE 时使用 `mrope_positions` 而非原 `positions`。  

**🎯 影响范围**：  
- `vllm/v1/worker/gpu/model_runner.py`（核心调度/执行）  
- `vllm/v1/worker/gpu/cudagraph_utils.py`（CUDA 图）  
- `vllm/v1/worker/gpu/input_batch.py`（输入批处理）  
- 新增 `vllm/v1/worker/gpu/mm/mrope_utils.py`（M‑RoPE 工具）  

**💡 关注建议**：  
1. **模型兼容性**：仅在 `model_config.uses_mrope=True` 时启用，确保相关模型实现 `SupportsMRoPE` 接口，防止运行时属性缺失。  
2. **内存与性能**：`prefill_mrope_positions` 可能占用大量 CPU/GPU 内存，建议在配置中限制 `max_num_reqs` 与 `max_model_len`。  
3. **测试覆盖**：增加包含多模态输入的单元/集成测试，验证 `prepare_mrope_positions` 与 `init_prefill_mrope_positions` 的正确性以及与 CUDA 图的协同。  
4. **文档更新**：在模型配置说明中添加 `uses_mrope` 参数解释，并说明 M‑RoPE 与普通 RoPE 的等价情况。  
5. **回退路径**：对已有仅文本模型，保持原 `positions` 流程不变，避免不必要的张量拷贝或非连续性导致的编译问题。  

落实以上建议可降低因新特性引入的内存泄漏或兼容性风险，确保平滑迁移到支持 M‑RoPE 的多模态模型。

---

### [Model Runner V2] Minor refactor for logit_bias (#32209)
**SHA**: `dec2868` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dec28688c5060f921594f9a7ce1647cf7a87a39e)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原先在 `logit_bias.py` 中直接调用 `_bias_kernel` 的逻辑抽取为独立的 `apply_logit_bias` 函数。  
- 参数传递方式改为显式传入 GPU 张量（`*.gpu`），取消了之前在类属性上写 `.gpu` 的隐式访问。  
- `BLOCK_SIZE` 的计算由固定的 `MAX_*` 常量改为基于实际 token‑id 张量的最后维度长度，使得 block 大小随输入自动适配。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/sample/logit_bias.py`（核心实现）  
- 任何在 Model‑Runner V2 中调用 `apply_logit_bias` 的入口代码（如 `sampler`、`model_runner`）将受到签名变更的影响。  

**💡 关注建议**  
1. **签名兼容**：确认所有调用点已同步更新为新函数签名（包括 `num_allowed_token_ids`、`allowed_token_ids` 等均为 GPU 张量），防止出现属性未定义的 runtime 错误。  
2. **性能回归**：`BLOCK_SIZE` 现在随输入尺寸而变，建议在常见 vocab/stop‑token 配置下跑基准测试，确保没有因 block 过小导致的 Triton kernel 启动开销上升。  
3. **边界检查**：当 token‑id 张量为空时 `max()` 仍会返回 0，`triton.next_power_of_2(0)` 会得到 1，需确认 kernel 能安全处理这种极端情况。可在函数入口加入断言或默认最小 block 大小。  
4. **单元测试**：新增或更新针对 `logit_bias`、`stop_token`、`allowed_token_ids` 三类约束的测试，特别是混合使用的场景，验证 bias、mask、min_len 等逻辑保持一致。  
5. **文档同步**：更新相应的开发者文档或注释，说明 `apply_logit_bias` 现在接受的参数均为已放到 GPU 的张量，避免误用 CPU 张量导致隐式同步开销。  

总体而言，此次重构提升了代码可读性和参数显式性，但需确保调用方全部迁移并通过回归测试，以防功能或性能回退。

---

### [NIXL][Bugfix] Failure logging overhaul + early metadata free on failure (#32031)
**SHA**: `f8bd839` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f8bd8394e3d06c18088d3f1a1944ca6001142400)

**🎯 变更类型**：Bugfix / 重构（统一错误日志、提前释放元数据）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `vllm.distributed.kv_transfer.kv_connector.v1.nixl_connector.NixlConnector` 新增私有方法 `_log_failure`，统一所有 NIXL 传输相关错误的日志格式，并在日志中携带 request、engine、remote‑side以及块信息等结构化上下文。  
2. 将原先散落的 `logger.exception` / `logger.error` 调用改为调用 `_log_failure`，覆盖 handshake、notification、transfer、setup 等多类异常。  
3. 在 `_handle_failed_transfer` 中改为 `self._recving_metadata.get`（而非 `pop`），避免在 `get_finished` 前提前删除元数据。  
4. 新增测试 `test_transfer_failure_logging`，通过 `FailingNixlWrapper` 人工触发五种失败场景，捕获并断言结构化日志包含 `NIXL transfer failure`、`failure_type`、`Context:` 等关键字段。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`（核心错误处理、日志输出、元数据生命周期）  
- `tests/v1/kv_connector/unit/test_nixl_connector.py`（新增 failure‑type 参数化测试）  

**💡 关注建议**  
1. **兼容性**：外部监控或日志聚合工具可能已依赖旧的错误信息格式，切换为结构化日志后请同步更新过滤/告警规则。  
2. **日志开销**：`_log_failure` 在每次错误时均构造并写入字典，成本可接受，但在高频错误场景下仍建议在生产环境通过日志级别控制。  
3. **元数据清理**：`_handle_failed_transfer` 改为 `get` 后依赖 `get_finished` 完成最终 `pop`，确保未忘记在异常路径中调用 `get_finished`，否则可能导致元数据泄漏。  
4. **代码可读性**：建议在 `_log_failure` 参数 `failure_type` 与 `msg` 合并时保持一致的命名风格（如 `failure_type="transfer_failed.Marking blocks as invalid"`），并在函数注释中说明 `extra_context` 的使用场景。  
5. **测试覆盖**：当前测试已覆盖五种异常路径，建议再补充一次在 `transfer_failed` 场景下检查 `xfer_state` 被正确写入日志的断言，以防后续改动遗漏 `xfer_state` 信息。  

总体而言，此次改动提升了错误诊断的可追溯性，对功能行为无影响，风险主要在日志消费端的兼容性，请在升级说明中明确告知。

---

### [Model Runner V2] Support logit_bias, allowed_token_ids, min_tokens (#32163)
**SHA**: `ca81811` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ca81811bfeca05f3104f2de7c58dd6d57d54472d)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 Model Runner V2 中加入对 `logit_bias`、`allowed_token_ids`、`min_tokens` 的完整支持。新增 `vllm/v1/worker/gpu/sample/logit_bias.py`，实现按请求分配的 bias、白名单 token、最小生成长度的 GPU‑side Tridiagonal kernel，并在 `buffer_utils` 中放宽了 `dtype`（新增 `torch.float32`）以及 `stage_write` 接口接受 `Iterable[int|float]`。  

**🎯 影响范围**  
- **vllm/v1/worker/gpu/buffer_utils.py**：UVA 缓冲区的 dtype 检查与写入接口被扩展。  
- **vllm/v1/worker/gpu/sample/logit_bias.py**：新增状态结构 `LogitBiasState`，以及 `_bias_kernel` Triton 实现。  
- **采样路径**：所有调用 `SamplingParams` 的入口将能够把 `allowed_token_ids`、`logit_bias`、`min_tokens` 传递到 GPU，影响 `Sampler`、`ModelRunner` 相关代码（间接）。

**💡 关注建议**  
1. **兼容性**：`stage_write` 现在接受 `Iterable[float]`，请确认现有调用仍以 `list[int]` 形式传参，不会触发类型误判。  
2. **资源上限**：常量 `MAX_NUM_…`（1024/128）是硬限制，若业务需要更大集合需调整源码并重新评估显存占用。  
3. **性能**：`_bias_kernel` 在每个请求的每一步都会遍历全部 vocab（写入 -inf），在超大 vocab（>30k）时可能带来额外带宽开销。建议在高并发场景做 profiling，必要时考虑改为稀疏掩码或分块更新。  
4. **错误检查**：`add_request` 对大小做了显式校验，但未检查 `allowed_token_ids` 是否在 vocab 范围，建议在前置层抛出更明确的异常。  
5. **测试覆盖**：新增的功能应加入单元测试，尤其是 `allowed_token_ids` 与 `logit_bias` 同时存在、`min_tokens` 与 `stop_token_ids` 的交叉情况，确保 Triton kernel 在边界条件下不产生未定义行为。  

总体而言，此次改动为 vLLM 增加了重要的 Prompt‑level 控制能力，影响范围主要在 GPU 采样模块，若配合充分的测试和资源监控，可安全上线。

---

### [Refactor] EPLB rebalance algo to NumPy (#30697)
**SHA**: `1eb61ab` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1eb61ab34b1415023a35afeb40f1cf85256b5fa5)

**变更类型**：重构（核心调度算法从 torch 迁移到 NumPy）  
**重要程度**：🟡 中  
**影响范围**：`vllm/distributed/eplb/policy/default.py`、`tests/distributed/*`、相关的内部工具函数（`balanced_packing`、`replicate_experts`、`rebalance_experts_hierarchical`、`preserve_intragpu_slots`）  

### 核心改动
1. **全部调度逻辑改为 NumPy**  
   - `balanced_packing`、`replicate_experts`、`rebalance_experts_hierarchical`、`preserve_intragpu_slots` 现在接受/返回 `np.ndarray`，只在最外层统一转回 `torch.Tensor`。  
   - 移除了显式的 `device` 参数，使用 `np.inf` 进行“已满”包的屏蔽，使得算法在 CPU 上一次性完成后再一次性搬回 GPU，显著降低了跨设备同步次数。

2. **测试同步更新**  
   - 所有涉及 `torch.Tensor` 的测试改为使用 `np.ndarray`（或 `tensor.numpy()`），并在调用 API 前后自行完成 `torch`↔`numpy` 的转换。

3. **辅助函数重构**  
   - `inverse` 通过 NumPy 实现置换的逆映射，使用 `np.arange`、广播实现，避免了 `torch.scatter_` 的显式写法。  
   - `_make_phy_replicas_idx_from_phy2log`、`_validate_intragpu_rearrangement` 也改为纯 NumPy 实现。

### 潜在风险 / 注意点
| 风险 | 说明 | 建议 |
|------|------|------|
| **数据类型不一致** | 部分代码仍然使用 `torch.float()`、`torch.int64`，在转为 NumPy 前未显式 `astype`，可能出现 `float64`→`int64` 隐式转换错误。 | 在每一次 `torch → numpy` 前统一 `astype(np.float64)` / `astype(np.int64)`，并在返回前显式 `torch.from_numpy(...).to(device)`。 |
| **`np.inf` 与整数数组** | `balanced_packing` 用 `np.inf` 标记已满的 pack，但 `weights_row` 为 `float64`，后续 `np.argmin` 会返回该 pack（因为 `inf` 永远最大），逻辑正确；若意外转成整数会溢出。 | 确认 `weights_row` 永远是浮点类型，加入断言 `weights_row.dtype.kind == "f"`。 |
| **`inverse` 对非置换数组的行为** | 该函数假设 `perm` 为每行唯一的排列。若前置逻辑产生重复列（极端情况下），会产生错误的逆映射。 | 在 `inverse` 前加入检查 `np.all(np.diff(np.sort(perm, axis=1), axis=1) != 0)`，或在文档中明确前置保证唯一性。 |
| **GPU/CPU 混用** | `rebalance_experts` 中 `weight.float()` 仍保持在 Torch 上，随后 `weight_np = ...cpu().numpy()`，如果 `weight` 本身已经在 CPU，额外的 `cpu()` 调用是冗余但不错误。 | 可直接 `weight_np = weight.float().cpu().numpy()`，或在函数开头统一 `weight_np = weight.detach().cpu().float().numpy()`，避免潜在的 `.requires_grad` 警告。 |
| **旧 API 参数名称改变** | `preserve_intragpu_slots` 参数从 `old_global_expert_indices` 改为 `old_phy2log`，但外部调用仍使用旧名称的变量（已在代码中同步），若有其它未更新的调用页面会触发 `TypeError`。 | 检索全仓库 `preserve_intragpu_slots(` 的调用，确保统一使用新签名；同时在函数 docstring 中标明兼容旧名。 |

### 建议的改进方向
1. **统一入口/出口的类型转换**：在 `DefaultEplbPolicy.rebalance_experts` 开头统一把 `weight`、`old_global_expert_indices` 转为 NumPy，结束时一次性返回 `torch.Tensor`，可封装为私有工具 `._to_numpy` / `._to_torch`，降低代码重复度。  
2. **完善单元测试**：添加极端场景（如 `num_groups` 与 `num_nodes` 不可整除、所有专家权重相同、`num_replicas` > `num_logical_experts` 多余副本）以及大规模随机数据的对比测试，确保 NumPy 与原始 Torch 实现在数值上保持 100% 一致。  
3. **性能基准**：因为改动的目标是减少 GPU‑CPU 同步，建议在 CI 中加入 micro‑benchmark（`timeit`）对比旧实现与新实现的运行时间，确保改动确实带来加速。  
4. **文档/注释**：在每个重构的函数顶部补充“从 Torch → NumPy 重写”说明，标记关键的实现细节（如 `np.inf` 用法、pack‑mask 逻辑），方便后续维护者快速定位。  

总体来看，此次重构把原本在 GPU 上大量切片、索引的调度逻辑搬到纯 CPU NumPy，实现了更少的设备切换，代码可读性提升。但需要注意类型一致性、逆置换函数的前置假设以及保持所有调用点的签名同步，以上建议可帮助确保改动的稳健性并进一步提升可维护性。

---

### [Benchmark] Share data between SLA runs (#32184)
**SHA**: `7c0d3c5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7c0d3c5152a1b68b05450e699abc90eb468d215d)

**变更概览**  
本次提交为 **Benchmark** 模块新增“跨 SLA 运行复用历史数据”的功能。核心改动集中在 `vllm/benchmarks/sweep/serve_sla.py`：  
1. 新增 `_iter_sla_val_paths` 与 `_compute_margin` 两个内部工具，用于遍历已有的 `summary.json`、读取历史指标并计算 SLA margin。  
2. `solve_sla` 在进入二分搜索前先将历史数据装入 `SLAHistory`，从而在同一基准目录下可以直接复用上一次的实验结果。  
3. 相应的单元测试 `tests/benchmarks/sweep/test_serve_sla.py` 增加了对历史复用的验证，并把所有原有测试改为使用 `tmp_path` 临时目录，防止对真实文件系统产生副作用。

**受影响模块**  
- `vllm/benchmarks/sweep/serve_sla.py`（核心逻辑）  
- `vllm/benchmarks/sweep/param_sweep.py`（间接通过 `SLAHistory` 交互）  
- 测试套件 `tests/benchmarks/sweep/`  

**关键风险 & 建议**  

| 关注点 | 说明 | 建议 |
|--------|------|------|
| **路径约定** | 新增的 `_iter_sla_val_paths` 依赖 `base_path/<sla_variable>=<value>/summary.json`，旧有代码只会写 `run=<n>.json`，现在会出现两层目录结构。若外部工具仍假设只有 `run=` 文件，可能读取不到历史。 | 在文档或 README 中明确说明历史文件的存放规则；考虑在 `solve_sla` 中统一创建 `summary.json`（已实现），并在旧路径兼容读取。 |
| **并发安全** | 多进程或并行运行同一 `base_path` 时，读取/写入 `summary.json` 可能产生竞争。 | 若计划支持并行实验，可在写入前加文件锁或使用原子写 (`tmpfile -> rename`)。 |
| **异常处理** | `_compute_margin` 假设 `iter_data` 非空且所有键在 `sla_comb` 中存在。读取损坏或不完整的 JSON 会触发 `assert`，导致整个 benchmark 失败。 | 将断言改为抛出可捕获的自定义异常，或在 `solve_sla` 中捕获并跳过无效文件，给出 warning。 |
| **性能** | 读取全部历史文件并逐条计算 margin，若历史量很大（上千个 SLA 值）会有 I/O 开销。 | 可在首次读取后缓存到内存，或提供 `max_history_files` 限制。 |
| **测试覆盖** | 新增的 `test_solve_reuse_history` 已验证历史复用的基本场景，但未覆盖：①历史文件缺失或损坏；②不同 `sla_variable` 的混合目录；③并发写入。 | 增加相应的负面测试，确保鲁棒性。 |
| **后向兼容** | 旧的调用仍可以省略 `base_path`（默认 `Path("")`），功能保持不变。 | 建议在函数签名中为 `base_path` 添加默认值，并在文档里标记为 **可选**，避免用户误以为必须提供。 |

**总体评价**  
改动实现了预期的历史复用特性，代码结构清晰，增加的辅助函数职责单一，单元测试覆盖了主要正向路径。若按照上述建议进一步强化异常处理和并发安全，功能将更加稳健。建议尽快同步文档说明，并在 CI 中加入对异常/并发场景的检测。

---

### [Misc][PD] Fix `get_attn_backend` usage in transfer connectors (#31988)
**SHA**: `5b68107` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5b681074119b970c2f99f8baea43f856cafc0251)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原先散落在多个 connector 中的 `get_attn_backend` 调用统一改为新的公共函数 `get_current_attn_backend`。  
- 新函数通过读取 `VllmConfig` 中的实际注意力层实例来获取后端，若找不到层则回退到旧的 selector 逻辑。  
- 相应单元测试也同步使用 `get_current_attn_backend` 进行 mock。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/utils.py`（新增 `get_current_attn_backend`）  
- `vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py`、`v1/nixl_connector.py`（后端获取方式改动）  
- 测试文件 `tests/v1/kv_connector/unit/test_nixl_connector.py`（patch 名称更新）  

**💡 关注建议**  
1. **兼容性检查**：`get_current_attn_backend` 依赖 `AttentionLayerBase` 和 `get_layers_from_vllm_config`，确认在所有启动路径（包括自定义模型、AMP、LLM‑Fusion）中能够正确返回层实例，防止在没有层的环境中误回退。  
2. **多后端模型**：代码注释已说明暂不支持同一进程中混用多种后端。若后续计划支持，需在 `VllmConfig` 中记录每个层对应的后端并在此函数中做聚合校验。  
3. **性能评估**：新函数在首次调用会遍历层列表，开销可忽略，但如果在热点路径频繁调用，请考虑缓存结果（如在 `VllmConfig` 中添加 `_cached_attn_backend`）。  
4. **测试覆盖**：当前仅更新了 nixl connector 的 mock，建议在 mooncake、megatron 等其他 connector 也加入对应的 mock，防止未来回归。  
5. **文档同步**：在 `vllm/v1/attention/selector.py` 或相关 README 中注明已废弃直接使用 `get_attn_backend`，改为统一的 `get_current_attn_backend`。  

总体来说，此次改动提升了后端获取的统一性并解决了先前因缓存导致的错误，影响范围局限在 kv‑connector 代码和对应单元测试，风险可控。请在 CI 中加入跨模型的集成测试，以验证回退路径的正确性。

---

### [Model] Standardize pooling heads (#32148)
**SHA**: `8863c2b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8863c2b25c84573e968ba291a26648ad17778c43)

**变更类型**：功能增强（统一 Pooler 头的行为与配置入口）  
**重要程度**：🟡 中  

### 核心改动  
1. **抽象统一**：在 `vllm/model_executor/layers/pooler/common.py` 新增 `ProjectorFn`、`ActivationFn` 类型别名，统一了投影函数、激活函数的签名。  
2. **Head 参数化**：`EmbeddingPoolerHead`、`TokenEmbeddingPoolerHead`、`ClassifierPoolerHead`、`TokenClassifierPoolerHead` 现在接受 `projector / classifier / head_dtype / activation` 参数，默认 `None`，并在 `forward` 中加入空值保护。  
3. **去除硬编码配置**：原先各 Head 内部通过 `get_current_vllm_config()` 直接读取模型配置（投影器、`head_dtype`、logit bias、激活函数），改为在 **pooler factory**（`seqwise/poolers.py`、`tokwise/poolers.py`）里统一注入这些依赖。  
4. **激活统一**：原 `PoolerNormalize`、`resolve_classifier_act_fn` 仍被使用，但通过 `ActivationFn` 统一入口，避免在 Head 内部再次解析。  
5. **模型层改写**：  
   - **Bert / Bert‑with‑rope**：改为仅接收 `ModelConfig`，内部构造 `EmbeddingPoolerHead`（使用 lambda 代理 `dense` 与 `tanh`），不再在 `BertPooler` 中显式实现 `head` 方法。  
   - **ModernBert**、**GritLM**：同理，用 `EmbeddingPoolerHead` + `LambdaPoolerActivation` 替代原有 `head` 实现，使参数注册更清晰。  
6. **导入路径精简**：移除对 `vllm.config.get_current_vllm_config` 的直接引用，改为在工厂层统一获取。  
7. **`__all__` 更新**：导出新增的 `ActivationFn` 与 `ProjectorFn`。

### 影响范围  
- **Pooler 相关模块**：`seqwise/*`、`tokwise/*`、`common.py`、`activations.py`。  
- **模型实现**：BERT、Bert‑with‑rope、ModernBert、GritLM、可能其它基于 `SequencePooler` / `TokenPooler` 的模型。  
- **配置加载**：所有依赖 `get_current_vllm_config()` 的旧代码已被改写，因此外部插件若自行实例化 Head，需要自行传入对应的 projector、dtype、activation。

### 开发者与用户建议  
- **保持兼容**：现有代码仍可按旧方式使用（不传入显式参数），默认行为保持不变；但如果自行创建 Head，请务必提供 `head_dtype` 与 `activation`，否则可能出现 dtype 不一致或缺失归一化。  
- **单元测试**：重点测试 `EmbeddingPoolerHead` 与 `ClassifierPoolerHead` 在 `head_dtype=None`、`projector=None`、`activation=None` 三种组合下的前向路径，确保与旧实现结果一致。  
- **性能检查**：投影与激活的 lambda 包装可能带来微小的调用开销，建议在大批量推理场景下跑基准对比。  
- **文档更新**：在用户手册中明确说明 `PoolingParams` 的 `normalize` / `use_activation` 标志仍受 Head 的 `activation` 控制，且 `logit_bias`、`head_dtype` 已由模型配置统一注入。  
- **迁移指南**：如果项目中有自定义 Pooler Head，参照新签名实现 `ProjectorFn` / `ActivationFn`，并通过对应的 `pooler_for_*` 工厂注入。  

总体而言，此次改动把 Pooler 头的配置抽象到了工厂层，使模型代码更简洁、参数注册更明确，兼容性影响有限，只要遵循新的构造方式即可无缝升级。

---

### [FIX] Add NO_MUL activation support for modular kernel path (#31528)
**SHA**: `3f72639` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3f72639d36a15b348c3dfee211daa3da8140736f)

**变更概览**  
本次提交为 *modular kernel*（即 Triton‑based MoE）路径引入了 “*_no_mul” 族激活函数（`silu_no_mul`、`gelu_no_mul`、`relu2_no_mul`）的完整支持。核心工作包括：

1. **公共工具层** (`utils.py`)  
   - 新增常量 `SILU_NO_MUL / GELU_NO_MUL / RELU2_NO_MUL`。  
   - 实现 `apply_moe_activation`，统一处理有‑gate‑乘 (`*_and_mul`) 与无‑gate‑乘 (`*_no_mul`) 两类激活，并在尺寸不匹配时抛出断言。  
   - 将原先的 `activation_without_mul` 迁移为内部辅助，仅供生成后缀。

2. **激活维度抽象** (`modular_kernel.py`)  
   - 新增 `adjust_N_for_activation(N, activation)`：返回 `N`（无 mul）或 `N//2`（有 mul），为后续 workspace 计算提供统一入口。  
   - 把 `ModularKernel.activation` 的实现替换为对 `apply_moe_activation` 的一次调用，消除重复代码。

3. **所有 MoE 实现的 workspace 计算**  
   - 大量 `workspace_shapes` 方法新增 `activation` 参数，内部使用 `adjust_N_for_activation` 计算 `activation_out_dim`，从而在 **no‑mul** 场景下避免原先硬编码的 `N//2`。  
   - 相关 `apply`、缓存分配、DeepGemm 量化路径等处均同步使用 `activation_out_dim`，保证缓冲区大小正确。

4. **测试**  
   - 新增 `tests/kernels/moe/test_triton_moe_no_act_mul.py`，覆盖：
     - 不同 `M/N/K/topk` 组合下 workspace 大小校验；  
     - `adjust_N_for_activation` 行为对比 gated 与 no‑mul；  
     - 实际执行 `TritonExperts.apply` 并检查输出合法性。  

**影响范围**  
- **模型执行层**：`fused_moe/*`（batched、cutlass、deep_gemm、marlin、fallback、flashinfer、triton 等）全部改动，涉及 ~30+ 文件。  
- **公共 API**：`vllm.model_executor.layers.fused_moe.utils` 新增常量与 `apply_moe_activation`，原有 `activation_without_mul` 仍保留但不再外部使用。  
- **测试套件**：新加入的 200 行测试文件将加入 CI，对计算能力 <8.0 的机器仍会被跳过。

**潜在风险 & 建议**  

| 风险 | 说明 | 建议 |
|------|------|------|
| **向后兼容** | 许多 `workspace_shapes` 的签名新增 `activation` 参数，若外部代码（自定义 MoE 实现或旧脚本）未更新会因参数缺失而报错。 | 在 `__init__` 中提供默认值（如 `activation="silu"`），或在文档中明确所有调用点必须显式传入。 |
| **性能回归** | `apply_moe_activation` 对 `*_no_mul` 使用 `torch.nn.functional`，而 gated 路径仍走自定义 C++ kernel。若大量使用 `*_no_mul`，可能出现 kernel‑vs‑Python 的性能差距。 | 评估在关键路径上实现对应的 `*_no_mul_and_mul`（即不乘的 kernel），或在文档中提示 `*_no_mul` 更适用于 CPU / 小模型。 |
| **错误检测** | `apply_moe_activation` 只在运行时断言尺寸关系，若调用者误传 `activation` 会在深层抛异常，定位困难。 | 在 `adjust_N_for_activation` 中加入对未知激活的早期校验并抛出统一异常；或者在 `ModularKernel.activation` 前验证 `activation` 是否在白名单。 |
| **文档同步** | 新增激活常量和函数后，API 文档、示例、模型配置文件（如 `fused_moe` 的 config）需同步更新。 | 更新 `docs/source` 中的 MoE 使用章节，添加 `*_no_mul` 示例；在 `model_executor/layers/fused_moe/config.py` 中注明新激活可选。 |
| **GPU 兼容性** | 测试仅在 Compute Capability ≥8.0 机器上运行，低端 GPU 仍会使用旧路径（可能不支持 `*_no_mul`）。 | 在 `workspace_shapes` 中对低 CC 进行回退或在 `apply` 前检查并给出警告。 |

**对开发者的操作建议**  

1. **更新调用方**：所有手动调用 `workspace_shapes`（例如在自定义调度器或插件中）的代码请加上 `activation=` 参数，默认可使用 `"silu"` 兼容旧行为。  
2. **审阅缓存尺寸**：确认在 `apply` 前传入的 `workspace1/2/13` 与新的 `activation_out_dim` 匹配，防止因尺寸不足导致 CUDA 越界错误。  
3. **性能基准**：在代表性模型（如 GPT‑2‑7B）上分别跑 `*_no_mul` 与原始 gated 激活，记录吞吐量差异，决定是否需要进一步 kernel 优化。  
4. **CI 与文档**：将新增测试文件加入 CI 配置，确保 CI 机器具备 CC≥8.0；同步更新 README / API 文档，说明 `*_no_mul` 的使用场景与限制。  

**对用户的提示**  

- 若模型配置中使用了 `activation: "silu_no_mul"`（或 `gelu_no_mul`、`relu2_no_mul`），系统会自动走新的缓冲区路径，无需手动改动代码。  
- 在低端 GPU（<8.0）上仍会走旧的 gated 路径，若出现 “workspace size mismatch” 报错，请检查是否误将 `*_no_mul` 用于不支持的设备。  
- 对于需要更高数值精度的任务，`*_no_mul` 与原始 gated 激活在数值上等价，但在某些层级上可能略有差异，建议在验证集上做一次跑通检查。  

整体来看，此次改动为 MoE 提供了更灵活的激活选择，代码路径的统一抽象也提升了可维护性，只要注意兼容性与文档同步，即可无缝接入生产环境。

---

### [MODEL] New model support for kakaocorp/kanana-1.5-v-3b-instruct (#29384)
**SHA**: `6bc9c84` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6bc9c8473e23d908d5ca1ae6a305704135597b42)

**🎯 变更类型**：功能增强（新增 Kanana‑V 多模态模型支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `docs/models/supported_models.md` 中加入 `KananaVForConditionalGeneration`，并在模型注册表 (`registry.py`) 添加对应条目。  
- `examples/offline_inference/vision_language.py` 新增 `run_kanana_v` 示例函数，演示图像‑文本推理。  
- `tests/models/registry.py` 添加模型可在线加载的检查。  
- 新增核心实现 `vllm/model_executor/models/kanana_v.py`，实现从 HF `kakaocorp/kanana-1.5-v-3b-instruct` 迁移的视觉编码器、动态抽象层、以及多模态处理器。  

**🎯 影响范围**  
- **模型执行层**：新增 `KananaVForConditionalGeneration`，涉及 `registry.py`、权重加载路径、`SupportsMultiModal` 与 `SupportsPP` 接口实现。  
- **多模态处理**：实现 `KananaVMultiModalProcessor`，包括占位符 `<image>` → 媒体 token、pixel‑value 拼接、`PromptReplacement` 逻辑。  
- **示例/文档**：文档、示例脚本、测试均已同步。  

**💡 关注建议**  

1. **权重兼容性**：`DynamicCAbstractor._load_from_state_dict` 对旧 checkpoint 做了裁剪，建议在 CI 中加入对旧模型权重的迁移测试，防止因 pos_emb 维度变化导致加载错误。  
2. **占位符展开**：`KananaVMultiModalProcessor` 通过 `media_token_id` 检测并展开为 `T·H·W` 个占位符，需确认 `image_token_thw` 与 HF 处理器输出一致；若 token 数不匹配会触发 `expanded` 逻辑，建议加入日志提示以便调试。  
3. **`trust_remote_code`**：示例和注册表均已显式使用 `trust_remote_code=True`，生产环境请谨慎配置，建议在文档中提醒用户。  
4. **性能**：`CustomQwen2VLVE` 删除了 `merger`，保持与原始 checkpoint 参数匹配；但额外的 `PatchMerge` 与 `DynamicCAbstractor` 仍会增加前置计算开销，建议在 README 中给出每张图像的 token 数统计（≈ `num_vision_tokens`），帮助用户预估显存需求。  
5. **测试覆盖**：当前仅在 `tests/models/registry.py` 检查加载能力，建议补充 **推理** 层面（`run_kanana_v`）的单元测试，包括多图、不同分辨率以及异常输入（如缺少 `vision_grid_thw`）的报错路径。  

总体而言，本次 PR 为 vLLM 引入了全新的视觉‑语言模型，结构清晰、接口统一，只需在上述几点做好回归与文档提示，即可安全发布。

---

#### 🟢 低重要度变更 (17)

### [BugFix] [KVConnector] Fix KV events for LMCache connector (#32169)
**SHA**: `5102654` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/510265472cb216daf7d8e83db6fa03ce48b0f5fc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `lmcache_connector.py` 中为 KV 事件的 `lora_name` 添加 `getattr(..., None)`，防止属性缺失导致异常，修复 KV 事件处理的 Bug。

---

### [Docs] Nixl Usage recommend `fail` kv_load_failure_policy (#32198)
**SHA**: `8c8653b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8c8653b67217ad41f89c10bc7fc097d0881bb63d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `nixl_connector_usage.md` 中新增 `kv_load_failure_policy` 参数示例，推荐使用 `fail` 并添加对 `recompute` 可能导致的性能下降的警告说明。

---

### [Doc] Update installation from source command (#32239)
**SHA**: `44c34f2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/44c34f22d99a47e92715a74a2ca4ffbe919b2cbf)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `docs/contributing/README.md` 中补充了先安装 PyTorch（含 CUDA）再安装 vLLM 的步骤，并将 vLLM 的源码安装命令改为 `uv pip install -e . --no-build-isolation`。

---

### [ROCm][CI] Fix HuggingFace flash_attention_2 accuracy issue in Isaac vision encoder (#32233)
**SHA**: `5e714f7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5e714f7ff4166d62b5f923766a0268a1758f2a61)

**🎯 变更类型**：代码重构/测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 ROCm 环境新增 `patch_hf_vision_attn_for_rocm`，强制 HuggingFace 视觉编码器使用 SDPA 以规避 FlashAttention2 精度问题，并在模型生成测试中调用该补丁。

---

### [ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#32099)
**SHA**: `11b6af5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/11b6af5280d6d6dfb8953af16e67b25f819b3be9)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Mamba Mixer 与 Plamo2 模型的前向实现中，加入对 ROCm 平台的适配：在需要时强制将张量转为 contiguous，以避免 batch > 1 时出现的 GEMM 计算错误；同时对 LoRA 前向保持兼容。仅涉及少量文件改动，修复了 ROCm 上的批处理解码错误。

---

### [Perf] Optimize requests abort (#32211)
**SHA**: `2a719e0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2a719e0865d68ef930c53f9b46f718c31ed39377)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `async_llm.py` 中对 `abort_requests_async` 增加非空检查，仅在存在待终止请求时调用，避免无效的异步调用，提高性能。

---

### [Frontend] Add `reasoning_effort` to `OpenAIServing._preprocess_chat()` (#31956)
**SHA**: `60b77e1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/60b77e146315f4f0f23b9aa83c07d51f38927a1e)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 OpenAI 聊天入口的 `_preprocess_chat` 流程中加入 `reasoning_effort` 参数，统一合并到 `chat_template_kwargs`，并在响应构造时传递该值，实现对推理力度的自定义。

---

### [Model] Handle `trust_remote_code` for transformers backend (#32194)
**SHA**: `78d13ea` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/78d13ea9de4b1ce5e4d8a5af9738fea71fb024e5)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Transformers 后端模型加载中新增 `trust_remote_code` 参数，并在 `dynamic_module` 中调用 `resolve_trust_remote_code` 进行安全校验，以支持远程代码的可信加载。

---

### [ROCm][CI] Handle pytest status code 5 when a shard isn't allocated any tests  (#32040)
**SHA**: `a28d9f4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a28d9f4470550835a8b36b9e558d4d47b30c7127)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 AMD CI 脚本中加入对 pytest 返回码 5 的容错处理，将无测试的分片视为成功；若所有分片均未收集到测试则整体失败。同时将 “Language Models Tests (Extra Standard)” 步骤的 `agent_pool` 从 `mi325_2` 改为 `mi325_8`。

---

### [Kernel][MoE] fix computation order of MoE weight multiplication and improve flow (#31962)
**SHA**: `629584b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/629584bfc9fc5c88c24f6672ca536fae7f75b835)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将MoE路由权重乘法提前到float32，在去量化和加bias之后执行，并在最后一次统一进行类型转换，确保数值稳定性并改进注释说明。

---

### [BUGFIX] Add missed remaping of the names of fp8 kv-scale (#32199)
**SHA**: `9f430c9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9f430c94bd0b5cf5a697ddeefd4507ae078bb0ed)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `qwen3_next.py` 中新增对 FP8 kv‑scale 名称的重映射逻辑，使用 `maybe_remap_kv_scale_name` 处理并跳过无效条目，修复因名称缺失导致的加载错误。

---

### [Misc] Change log level for batch queue log (#32192)
**SHA**: `08e8e99` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/08e8e99ce78f370d2f430d28b802592bf816655a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 batch queue 启用时的日志级别由 `info` 降为 `debug`，以避免在常规运行时输出不必要的信息。  
**影响**：仅影响日志输出，不改变功能或性能。

---

### [BugFix] scheduler: Fix ordering preserving of skipped requests (#32173)
**SHA**: `2be765b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2be765b68ace5efe4cba4d03495f543c85784688)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `RequestQueue` 的 `__reversed__` 抽象方法及其实现，修改 `prepend_requests` 为直接 `extendleft`（保留原顺序），并新增单元测试验证在调度时被跳过的请求仍保持原有等待顺序。

---

### [Misc] Set default torch num threads for input processing (#31879)
**SHA**: `16abe6b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/16abe6b85a3fbf6312afe3514797ec741c10a178)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `internvl.py` 中移除手动设置 `OMP_NUM_THREADS` 的逻辑，直接返回原始 `transform`；在 `input_processor.py` 中加入对 `OMP_NUM_THREADS` 的读取与默认线程数设置，并在输入预处理时使用 `set_default_torch_num_threads`，以统一控制 Torch CPU 线程数。

---

### [BugFix] fix FusedMoE.make_expert_params_mapping in EXAONE-MoE (#32196)
**SHA**: `3d962d7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3d962d72ab01b7eb210a4719c24f1dd98d5c5c34)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `EXAONE-MoE` 实现中为 `FusedMoE.make_expert_params_mapping` 调用添加 `self` 参数，修复缺失导致的运行时错误。

---

### [Bugfix] Fix stale SSM state for new Mamba requests scheduled as decode (#32118)
**SHA**: `8fb2c13` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8fb2c135be35d4d07b3ef25e36c51733345a01eb)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修改 `reorder_batch_to_split_decodes_and_prefills` 中对 `is_prefill/is_decode/is_extend` 的判断，避免把仅有 1 token、未计算的新请求误判为 decode；新增对应单 token prefill 场景的单元测试。

---

### doc: Update model references in supported_models.md (#32188)
**SHA**: `95e53d9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/95e53d907cdf49c72d697c2248a69c58abe1aaeb)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `docs/models/supported_models.md` 中修正了 ChatGLM 与 GPT‑2 的模型仓库引用路径。

---

