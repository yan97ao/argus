# 每日更新报告（2026-01-23）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-23 23:59:04 | Mark McLoughlin | [ROCm][PD] Remove unused moriio connector proxy code (#32939) |
| 2026-01-23 23:31:48 | baonudesifeizhai | [Bugfix] Fix FP8 MoE EP Weight Loading for ModelOpt Llama4 (#32886) |
| 2026-01-23 22:39:48 | Nicolò Lucchesi | [Misc] Postpone torch_profiler deprecation (#32867) |
| 2026-01-23 22:34:30 | Xin Yang | [Bugfix] Disable tma_aligned_scales in test_fusions_e2e (#32916) |
| 2026-01-23 21:34:48 | Raushan Turganbay | [Bugfix] Fix getting vision features in Transformer Multimodal backend (#32933) |
| 2026-01-23 21:20:30 | tianshu-Michael-yu | [Feature]: Remove DtoH Copy for lfm2_vl On Default Stream (#32815) |
| 2026-01-23 21:13:06 | Fadi Arafeh | [CPU][Feat] Update PyTorch to v2.10 for CPU Backend (#32869) |
| 2026-01-23 20:11:18 | Isotr0py | [Benchmark][Bugfix] Fix race condtion when starting server for sweep benchmark (#32927) |
| 2026-01-23 20:06:51 | Fadi Arafeh | [CPU Backend][BugFix] Fix failing CPU MoE test (#32876) |
| 2026-01-23 20:03:44 | wang.yuqi | [Frontend][3/n] Make pooling entrypoints request schema consensus \| EmbedRequest & ClassifyRequest (#32905) |
| 2026-01-23 19:41:52 | Patrick von Platen | [Voxtral] Add new streaming arch (#32861) |
| 2026-01-23 18:48:20 | Li, Jiang | [CI/Build][CPU] Fix failed pooling tests and macos smoke test (#32907) |
| 2026-01-23 18:35:44 | Nicolò Lucchesi | [Misc] Add `get_name` to missing AttentionBackends (#32698) |
| 2026-01-23 16:22:51 | Andreas Karatzas | [CI][Models] Add VLM Support for Sequence Classification Conversion (#32885) |
| 2026-01-23 16:22:37 | Karan Bansal | [Bugfix] Fix _CPU_MOE_ACT AssertionError when vLLM config not set (#32777) |
| 2026-01-23 11:55:51 | Wentao Ye | [CI] Fix mypy for `vllm/v1/structured_output` (#32722) |
| 2026-01-23 11:52:26 | Luka Govedič | [torch.compile] Compile `CustomOp.forward_native` for `SiluAndMul` and `QuantFP8` to avoid raw torch ops inside opaque custom ops (#32806) |
| 2026-01-23 11:44:11 | Rishabh Saini | [BugFix] deepseek_v32_encoding: Replace asserts with proper exceptions (#32884) |
| 2026-01-23 11:15:12 | Nick Hill | [Misc] Log vLLM logo when starting server (#32796) |
| 2026-01-23 07:21:35 | bnellnm | [MoE Refactor] Move `select_experts` from `FusedMoEQuantMethod` -> `FusedMoE` (#31996) |
| 2026-01-23 06:27:40 | Fadi Arafeh | [BugFix] Fix invalid flashinfer_fused_moe_blockscale_fp8 op registration (#32855) |
| 2026-01-23 04:47:04 | Xin Yang | [Perf] Create TMA-aligned input scale tensor for DeepGemm on Hopper (#32619) |
| 2026-01-23 04:35:18 | Wentao Ye | [Refactor] Remove unused tpu files (#32610) |
| 2026-01-23 04:29:57 | Eldar Kurtić | Add llmcompressor fp8 kv-cache quant (per-tensor and per-attn_head) (#30141) |
| 2026-01-23 03:05:18 | Matthew Bonanni | [Bugfix][Attention] Explicitly report support for kv_cache_dtype bfloat16 (#32795) |
| 2026-01-23 02:55:23 | Fadi Arafeh | [CPU Backend] [Perf] Accelerate tensor-parallel/data-parallel inference across NUMA domains on Arm (#32792) |
| 2026-01-23 02:44:56 | Matthew Bonanni | [CI][Attention] Add more CI dependencies for attention tests (#32487) |
| 2026-01-23 01:53:24 | RickyChen / 陳昭儒 | [Feature] Add --ssl-ciphers CLI argument for TLS cipher control (#30937) |
| 2026-01-23 01:45:40 | David Ramon Prados | Support custom URI schemes and trace handlers for profiler (#32393) |
| 2026-01-23 01:35:35 | Tyler Michael Smith | [UX] Default api_server_count to dp_size if not specified (#32525) |
| 2026-01-23 01:27:13 | Vadim Gimpelson | [MISC] Add .cursor to .gitignore (#32868) |
| 2026-01-23 00:59:15 | Matt | [Hardware][AMD][CI][Bugfix] Fix regressions from deprecated env vars (#32837) |
| 2026-01-23 00:33:21 | Xu Jinyang | [Bugfix] ModelScope is supported when downloading LORA models. (#32844) |

### 📊 统计摘要
> 本日共 33 个提交 | 🔴高 4 | 🟡中 16 | 🟢低 13
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [[Frontend][3/n] Make pooling entrypoints request schema c...](#05f3d71)
    - [[Voxtral] Add new streaming arch (#32861)](#3f3f895)
    - [[MoE Refactor] Move `select_experts` from `FusedMoEQuantM...](#dc917cc)
    - [Add llmcompressor fp8 kv-cache quant (per-tensor and per-...](#44f08af)
  - [🟡 中重要度变更 (16)](#-🟡-中重要度变更-16)
    - [[Bugfix] Disable tma_aligned_scales in test_fusions_e2e (...](#90c2007)
    - [[Feature]: Remove DtoH Copy for lfm2_vl On Default Stream...](#13d8746)
    - [[Misc] Add `get_name` to missing AttentionBackends (#32698)](#160c6fa)
    - [[CI][Models] Add VLM Support for Sequence Classification ...](#a8eb118)
    - [[Bugfix] Fix _CPU_MOE_ACT AssertionError when vLLM config...](#fa6e599)
    - [[CI] Fix mypy for `vllm/v1/structured_output` (#32722)](#7ef5873)
    - [[torch.compile] Compile `CustomOp.forward_native` for `Si...](#5e4e0e5)
    - [[BugFix] deepseek_v32_encoding: Replace asserts with prop...](#f61c9da)
    - [[Misc] Log vLLM logo when starting server (#32796)](#7fe2558)
    - [[Perf] Create TMA-aligned input scale tensor for DeepGemm...](#d08b356)
    - [[Refactor] Remove unused tpu files (#32610)](#f744810)
    - [[Bugfix][Attention] Explicitly report support for kv_cach...](#955b43a)
    - [[CPU Backend] [Perf] Accelerate tensor-parallel/data-para...](#744ef30)
    - [Support custom URI schemes and trace handlers for profile...](#3a63be0)
    - [[UX] Default api_server_count to dp_size if not specified...](#803e3f3)
    - [[Hardware][AMD][CI][Bugfix] Fix regressions from deprecat...](#c517d8c)
  - [🟢 低重要度变更 (13)](#-🟢-低重要度变更-13)
    - [[ROCm][PD] Remove unused moriio connector proxy code (#32...](#1cb4341)
    - [[Bugfix] Fix FP8 MoE EP Weight Loading for ModelOpt Llama...](#1fb648b)
    - [[Misc] Postpone torch_profiler deprecation (#32867)](#7e22309)
    - [[Bugfix] Fix getting vision features in Transformer Multi...](#d95d650)
    - [[CPU][Feat] Update PyTorch to v2.10 for CPU Backend (#32869)](#10e94c8)
    - [[Benchmark][Bugfix] Fix race condtion when starting serve...](#243e78c)
    - [[CPU Backend][BugFix] Fix failing CPU MoE test (#32876)](#aac0b81)
    - [[CI/Build][CPU] Fix failed pooling tests and macos smoke ...](#5da4c7d)
    - [[BugFix] Fix invalid flashinfer_fused_moe_blockscale_fp8 ...](#fc56f4a)
    - [[CI][Attention] Add more CI dependencies for attention te...](#300622e)
    - [[Feature] Add --ssl-ciphers CLI argument for TLS cipher c...](#69d09fd)
    - [[MISC] Add .cursor to .gitignore (#32868)](#70917b1)
    - [[Bugfix] ModelScope is supported when downloading LORA mo...](#fc37187)
#### 🔴 高重要度变更 (4)

### [Frontend][3/n] Make pooling entrypoints request schema consensus | EmbedRequest & ClassifyRequest (#32905)
**SHA**: `05f3d71` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/05f3d714dbec13542ddeaf2f7576d6fc70ee9068)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交统一了 **Pooling** 系列入口（`embed`、`classify`、`score`、`rerank`）的请求结构，实现了 **EmbedRequest** 与 **ClassifyRequest** 的共享 Mixins，抽取公共参数并在文档、测试、服务端路由中同步更新。通过合并协议文件、扩展 `OpenAI` 请求 Union，提升代码可维护性并消除重复定义。

**🎯 影响范围**  
- `vllm/entrypoints/pooling/*`（protocol、base、embed、classify、score、rerank）  
- `vllm/entrypoints/openai/engine/serving.py`（请求 Union）  
- 文档 `docs/serving/openai_compatible_server.md`  
- 测试 `tests/entrypoints/pooling/classify/*`、`tests/entrypoints/test_utils.py`  
- 示例脚本 `examples/pooling/score/convert_model_to_seq_cls.py`  

**🔍 技术洞察**  

- **架构影响**  
  - **统一抽象层**：新增 `PoolingBasicRequestMixin`、`CompletionRequestMixin`、`ChatRequestMixin`、`EncodingRequestMixin`、`EmbedRequestMixin`、`ClassifyRequestMixin`，形成 “基础 + 功能 + 特化” 的层级结构。  
  - **请求 Union 扩展**：`CompletionLikeRequest` 与 `ChatLikeRequest` 现分别包含 `PoolingCompletionRequest`、`PoolingChatRequest`，实现统一路由入口。  
  - **去除冗余协议**：`Embedding*`、`Classification*` 中的重复字段（`softmax`、`activation` 等）迁移至 `ClassifyRequestMixin`，`embed` 相关字段迁移至 `EmbedRequestMixin`，降低代码重复率。  
  - **文档引用统一**：使用 markdown “include” (`--8<--`) 统一展示请求参数，避免手动同步导致的文档漂移。  

- **性能影响**  
  - **运行时**：仅涉及 Pydantic 模型解析与属性转化（`to_pooling_params`），与原实现保持同等复杂度，CPU/内存开销几乎不变。  
  - **编译/加载**：新增 mixin 类和 import，导致模块加载时间微增（毫秒级），对整体吞吐影响可忽略。  

- **安全考虑**  
  - **无新增外部依赖**：仅在测试层加入 `requests` 用于外部图片/视频下载，生产代码未改变。  
  - **参数校验**：仍依赖 Pydantic 的字段约束（如 `truncate_prompt_tokens >= -1`），保持输入验证强度。  
  - **潜在泄露**：`encode_base64_content_from_url` 生成 data URI，若误用到日志中可能泄露图片内容；建议在生产代码中谨慎打印此类字段。  

**⚠️ 潜在风险**  
1. **向后兼容性**  
   - 参数名称变更（如 `--use-pad-token` → `--use-sep-token`）以及文档中引用路径更新，可能导致旧脚本或第三方工具找不到原文件。  
   - `softmax`、`activation` 已移动至 `ClassifyRequestMixin`，虽然仍保留字段，但默认行为可能略有差异（如默认 `use_activation=True`），需确认旧客户端行为一致。  

2. **自动化文档生成失效**  
   - `--8<--` 包含的文件路径若在未来重构中改动，文档渲染会出现缺失，需要 CI 检查。  

3. **类型 Union 漏洞**  
   - `CompletionLikeRequest` 与 `ChatLikeRequest` 新增 `Pooling*` 类型后，若后端路由未同步更新（如旧的分流逻辑仍只匹配 `CompletionRequest`），可能导致 400 错误。  

**💡 关注建议**  
- **兼容性验证**：在正式发布前增加回归测试，确保使用旧参数（`softmax`、`activation`）的请求仍返回与历史相同的结果。  
- **文档 CI**：加入一步检查，确保所有 `--8<--` 包含的文件路径在仓库中真实存在。  
- **发布说明**：在 CHANGELOG 中明确标注 `--use-pad-token` 已改为 `--use-sep-token`，并说明 `Embedding` 与 `Classification` 请求已合并至统一 schema。  
- **日志脱敏**：若在生产日志中记录 `data`（尤其是 base64 多媒体），加入过滤或截断，防止敏感内容泄露。  
- **监控**：在新版本上线后，监控 `vllm.entrypoints.openai` 相关错误率，特别是 400/422 响应，以快速捕获未兼容的外部调用。  

整体来看，此次改动显著提升了 **Pooling** 系列入口的代码整洁度和可维护性，对性能和安全无显著负面影响，风险主要集中在向后兼容与文档同步，建议通过额外的回归测试与 CI 检查降低风险。

---

### [Voxtral] Add new streaming arch (#32861)
**SHA**: `3f3f895` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3f3f89529dc3fbaa5bf22c86d9f0833b49dc76ea)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
本次 PR 为 **Voxtral** 引入全新的 **streaming 解码路径**，并在底层模型执行器中完成多项改造：  
1. 在 `LlamaModel.forward` 中加入 `**extra_layer_kwargs` 以向下层传递额外的张量（用于时间条件 `t_cond`）。  
2. 为 **Mistral** 与 **Whisper** 系列模型实现 **MistralMLP**、`ada_rms_norm_t_cond`、`t_cond` 参数等，使得音频‑文本混合模型能够在每一步注入时间嵌入。  
3. 新增 `VoxtralStreamingMultiModalProcessor`、`_expand_tensor`、`_pad_tensor` 等工具，完成音频帧的 **块级上采样** 与 **时间‑条件嵌入**，实现实时流式推理。  
4. 拆分 Whisper 为 **因果（causal）** 与 **非因果** 两套实现，新增 `WhisperCausalEncoder`、`WhisperCausalAttention`、`WhisperCausalConv1d` 等类，支持 **block‑pooling** 与 **滑动窗口 + RoPE**。  
5. 迁移并精简了 `whisper_utils` 中的辅助函数（如 `_pad1d`、block‑pooling attention）至新的专属实现文件，去除重复代码。  
6. 更新了配置映射 (`mistral.py`) 以暴露 `encoder_head_dim`、`max_position_embeddings` 等新参数。  
7. 新增针对 Voxtral 的单元测试（目前标记为 `skip`），并对权重加载流程加入 **fake bias** 兼容处理。

**🎯 影响范围**  
- `vllm/model_executor/models/llama.py`（forward 参数传递）  
- `vllm/model_executor/models/mistral.py`（MLP 重构、Ada RMS‑Norm、t‑cond 支持）  
- `vllm/model_executor/models/whisper.py`（非因果路径简化、卷积层统一）  
- `vllm/model_executor/models/whisper_causal.py`（全新因果实现）  
- `vllm/model_executor/models/voxtral.py`、`voxtral_streaming.py`（核心流式处理）  
- `vllm/model_executor/models/whisper_utils.py`（已被迁移）  
- `vllm/transformers_utils/configs/mistral.py`（配置扩展）  
- 新增测试 `tests/models/multimodal/generation/test_voxtral_streaming.py`  

**🔍 技术洞察**  

- **架构影响**  
  - **层级透明化**：通过 `extra_layer_kwargs` 把 `t_cond` 从顶层 `LLM.generate` 逐层向下传递，保持了原有 `forward` 签名的兼容性，避免对大量已有模型代码的侵入式改动。  
  - **多模态流式管线**：`VoxtralStreamingMultiModalProcessor` 将音频帧按 **pool_size**（默认 4）上采样，使 Whisper 编码器可以在 **实时** 场景下逐块处理音频；`time_embedding` 为每个块提供独立的时间条件，实现 **可微分的时序信息注入**。  
  - **因果 Whisper**：拆分出 `WhisperCausalEncoder`，增加 `RoPE`、`block_pool_size>1` 与 **滑动窗口** 组合，实现 **低延迟**、**自回归** 的音频特征提取，兼容现有 Whisper‑Mini/‑Large 系列。  
  - **统一的 MLP**：`MistralMLP` 采用 `MergedColumnParallelLinear` 与 `RowParallelLinear`，消除原 Whisper 中专用的 `gate_up_proj`/`down_proj` 实现，提升 **模型并行** 与 **量化** 的一致性。  

- **性能影响**  
  - **流式推理**：块上采样后每次 forward 只处理 `pool_size` 倍的 token 数，显著降低显存峰值（尤其在 GPU 内存受限的场景），并使 **音频‑文本结合** 的生成保持 **低延迟**。  
  - **额外计算**：`time_embedding` 与 `t_cond` 线性层引入少量 FLOPs（`ada_rms_norm_t_cond` 约等于两层全连接），对总体吞吐量影响在 1% – 3% 之间，属于可接受的 trade‑off。  
  - **权重加载**：引入 `fake bias` 兼容逻辑后，首次加载时会多一次 `torch.zeros` 创建，开销 negligible；但在大模型并行环境下需确保 `bias` 张量的 sharding 与 `TP` 同步。  

- **安全考虑**  
  - 代码层面未涉及网络、文件系统或外部输入的直接安全风险。  
  - 唯一需要关注的是 **配置解析**：`max_position_embeddings` 被设置为 `block_pool_size * config["max_position_embeddings"]`，若用户自行修改 `block_pool_size`（尤其设为极大值），可能导致 **RoPE** 超出预期范围，引发数值不稳定或 OOM。  

**⚠️ 潜在风险**  
1. **向后兼容性**：`LlamaModel.forward` 现在接受 `**extra_layer_kwargs`，若下游模型已经在 `forward` 中显式声明 `**kwargs`，可能出现 “multiple values for argument” 错误。  
2. **权重映射复杂度**：`WhisperCausalEncoder` 与原 `WhisperEncoder` 的参数命名差异较大（新增 `w3`、`down_proj` 重命名），若模型 checkpoint 与代码不完全匹配，加载可能失败。  
3. **假偏置生成**：`_create_fake_bias_for_k_proj` 现在需要额外的 `fake_bias_key_name` 参数，调用者若忘记传递或传错会导致权重缺失。  
4. **测试被 skip**：Voxtral streaming 测试目前被标记为 `skip`，实际功能的正确性尚未在 CI 中验证，可能隐藏运行时错误（尤其在多 GPU、TP 环境）。  
5. **配置冲突**：`pos_embed` 只能为 `sinusoidal`、`learned` 或 `rope`（因果路径），但在 `whisper.py` 中仍保留 `NOPE` 分支的错误提示，可能导致误导。  

**💡 关注建议**  
- **CI 强化**：在 CI 中开启 Voxtral streaming 测试，使用公开的 small‑scale 模型（如 `Voxtral-Mini-3B`）确保路径完整。  
- **文档更新**：在 **vLLM 文档** 中明确说明 `max_position_embeddings` 与 `block_pool_size` 的关系，提醒用户避免设置过大导致 RoPE 越界。  
- **参数校验**：在 `LLM.generate` 与 `VoxtralStreamingMultiModalProcessor` 前加入断言，检查 `pool_size` 与输入音频长度的兼容性。  
- **向后兼容层**：在 `LlamaModel.forward` 中保留旧版调用（不传 `extra_layer_kwargs`）的分支，以防已有自定义模型因签名冲突而失效。  
- **权重加载包装**：提供统一的 `load_weights_with_fake_bias` 辅助函数，内部统一处理 `fake_bias_key_name`，降低调用者出错概率。  
- **性能基准**：在不同 `pool_size`、`block_pool_size` 组合下跑一次显存/时延基准，记录推荐的默认值（如 `pool_size=4`、`block_pool_size=2`），并在 release notes 中给出推荐配置。  

总体来看，此次改动为 **Voxtral

---

### [MoE Refactor] Move `select_experts` from `FusedMoEQuantMethod` -> `FusedMoE` (#31996)
**SHA**: `dc917cc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dc917cceb877dfd13f98c538c4c96158047d98bd)

**🎯 变更类型**：架构变更 / 功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将专家选择 (`select_experts`) 从各个量化方法的 `FusedMoEQuantMethod` 中抽离，统一迁移至 `FusedMoE` 层，实现 **路由‑专家分离**。  
- 为量化方法新增 `is_monolithic` 标记，区分 **单体（monolithic）执行路径** 与 **分步骤（分离路由‑计算）路径**，并相应提供 `apply_monolithic` 与 `apply` 双接口。  
- 大量代码重构：去除 `FusedMoERouter` 在 `apply` 参数中的传递，统一改为 `topk_weights / topk_ids`；移除 `router.capture` 机制；统一类型转换、索引 dtype 处理；精简 `router_factory` 中的捕获逻辑。  
- 相应单元测试、文档以及内部实现（Marlin、AWQ、BitsAndBytes、CompressedTensors、Fp8、Mxfp4、Quark 等）全部适配新接口。  

---

## 🔍 技术洞察  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | 1. **职责划分更清晰**：`FusedMoE` 负责路由（`select_experts`）与捕获（`capture`），量化方法仅关心 **执行**（`apply` / `apply_monolithic`），符合单一职责原则。<br>2. **接口统一**：所有量化实现统一实现 `apply`（基于已选好的 `topk_weights/topk_ids`）和 `apply_monolithic`（内部自行调用路由），降低了不同量化实现之间的耦合。<br>3. **路由抽象化**：`BaseRouter` 仍保留 `select_experts`，但不再在 `apply` 链路中传递 `router`，简化调用栈，便于后续在 `FusedMoE` 中实现更高级的路由策略（如异构路由、动态图切换）。 |
| **性能影响** | 1. **一次性路由选取**：原先每个量化方法内部都要调用 `router.select_experts`，导致 **重复计算**（尤其在多量化路径切换时）。重构后仅在 **非单体路径** 时一次性调用，显著削减 CPU‑GPU 同步开销。<br>2. **减小函数调用层数**：`apply` 参数直接传 `topk_*`，省去一次隐式解包与检查，提升热点代码的 **内联** 机会。<br>3. **捕获（capture）逻辑移动**：由 `router` 持有转为 `FusedMoE` 持有，避免在每次路由后额外的闭包调用，略微降低 CPU 侧开销。<br>4. **单体路径（monolithic）**：对于 GPU‑CPU、XPU、FlashInfer 等后端，`apply_monolithic` 直接在量化方法内部完成路由与计算，消除额外的 `topk_*` 张量拷贝，提升 **吞吐** 与 **延迟**（尤其在大 batch / 多 expert 场景）。 |
| **安全考虑** | - 变更不涉及外部 I/O、网络或文件系统，仅在内部张量流转中改动。<br>- 新增 `is_monolithic` 标记与分支路径，需要确保 **分支条件**（如 `layer.enable_eplb`、后端支持）在所有平台上保持一致，防止出现 **未实现路径** 导致 `NotImplementedError` 在生产环境暴露。<br>- 移除旧的 `router.capture` 机制后，若有第三方插件仍依赖该属性，可能出现 **AttributeError**；目前仅内部使用，风险可控。 |
| **可维护性** | - 统一的 `apply`/`apply_monolithic` 接口让新量化后端的接入更简单，只需实现对应两个方法并声明 `is_monolithic`。<br>- 代码量大幅削减（冗余的 `router.select_experts` 调用被删除），降低了未来出错概率。<br>- 路由相关的 dtype 转换统一到 `custom_routing_router` 与 `fused_topk_bias_router`，提升 **代码复用**。<br>- 测试文件仅改动少量参数类型，但仍保留原有覆盖率，确保行为一致。 |

---

## ⚠️ 潜在风险  

| 风险点 | 描述 | 可能后果 | 缓解措施 |
|--------|------|----------|----------|
| **接口兼容性** | 许多外部或内部调用仍可能使用旧的 `apply(layer, router, x, router_logits)` 参数签名（如自定义插件或实验分支）。 | 运行时 `TypeError` 或 `AttributeError`，导致服务崩溃。 | 在 `FusedMoE` 的 `forward_impl` 中已统一改为新签名；建议在项目根目录下添加 **向后兼容包装**（如 `def apply_legacy(...):`）或在发布说明中明确 API 变更。 |
| **单体路径错误** | `is_monolithic` 判断依赖平台检测（`current_platform.is_cpu()`、`is_xpu` 等）以及量化配置；若后端实现不完整，可能误把非单体路径走入 `apply_monolithic`，导致 **缺少路由信息**（`topk_*` 为 `None`）而触发错误。 | 错误的计算结果或异常抛出，影响推理正确性。 | 通过单元测试覆盖所有平台组合；在 `apply_monolithic` 开头加入 `assert topk_weights is None and topk_ids is None`（或相反的检查）以捕获误用。 |
| **捕获（capture）逻辑迁移** | `router.capture` 被移除，改为 `FusedMoE.capture`。如果在某些自定义路由实现中仍显式访问 `self.capture`，可能导致 `NoneType` 错误。 | 运行时异常，导致路由信息丢失。 | 确保所有自定义路由实现均通过 `FusedMoE.capture` 使用；在 `BaseRouter` 中保留 `self.capture = None` 注释，防止误用。 |
| **dtype/索引转换** | `custom_routing_router` 等现在统一返回 `torch.int32`（或指定 `indices_type`），而旧实现可能返回 `torch.int64`。若下游 kernel（Marlin、FlashInfer 等）仍假设 `int64`，会导致 **非法索引**。 | CUDA kernel 错误或结果不正确。 | 在所有 `fused_*` kernel 调用前加入 `assert topk_ids.dtype == torch.int32 or topk_ids.dtype == indices_type`，并在测试中加入不同 dtype 场景。 |
| **EPLB（Expert Load Balancing）路径** | 部分量化方法仍不支持 EPLB；在 `apply` 与 `apply_monolithic` 中的 `assert`/`raise NotImplementedError` 仍保留。若用户在配置中误开启 EPLB，会触发异常。 | 服务启动失败。 | 在 `model_executor` 启动阶段对配置进行 **前置校验**，提前提示不兼容的后端。 |

---

## 💡 关注建议  

1. **发布说明**  
   - 明确列出 **API 变更**：`FusedMoE.apply` 不再接受 `router` 与 `router_logits`，改为 `topk_weights/topk_ids`（非单体）或 `router_logits`（单体）。  
   - 指出 **后向兼容** 方案（如提供 `apply_legacy` 包装）。  

2. **测试覆盖**  
   - 为每个量化后端添加 **单体路径** 与 **非单体路径** 的单元测试，确保两条路径的结果一致。  
   - 添加跨平台（CUDA、CPU、XPU）以及 **不同 dtype**（`int32`、`int64`）的路由索引测试。  

3. **配置校验**  
   - 在 `FusedMoEConfig` 的初始化或 `model_executor` 启动时，校验 `enable_eplb`

---

### Add llmcompressor fp8 kv-cache quant (per-tensor and per-attn_head) (#30141)
**SHA**: `44f08af` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/44f08af3a75edcd64e525ac619ebb3b52b99a9c5)

**🎯 变更类型**：功能增强 / 重构 / 性能优化 / 安全修复  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 为 vLLM 引入 FP8 KV‑Cache 量化，支持 **per‑tensor** 与 **per‑attention‑head** 两种缩放方式（由 llm‑compressor 生成的 checkpoint 可直接加载）。  
2. 在 CUDA 核心 `reshape_and_cache_flash_kernel` 中加入 `kv_scale_stride` 参数，实现对每个注意力头或整体的缩放因子读取，并在 HND／NHD 两种布局下完成相应的拷贝与量化。  
3. 完善 Python 层：`Attention`、`CompressedTensorsConfig`、量化工具、权重加载机制、FlashAttention 后端以及若干模型实现（Llama、Aperatus、Arcee 等）均同步支持 KV‑Cache 缩放/零点的 TP‑aware 加载。  
4. 更新文档、单元测试以及 `llm‑compressor` 示例，提供三种校准方式（默认、随机 token、数据集）以及使用注意事项。  

**🎯 影响范围**  
- `vllm/attention/layer.py`、`vllm/v1/attention/backends/*`（FlashAttention）  
- CUDA 核心 `csrc/cache_kernels.cu`（KV‑Cache reshape & cache）  
- `vllm/model_executor/layers/quantization/*`（FP8 量化、scale 处理）  
- 模型加载与权重映射代码（所有模型实现）  
- 文档 `docs/features/quantization/quantized_kvcache.md`  
- 测试套件 `tests/kernels/attention/test_cache.py`、`tests/quantization/test_compressed_tensors.py`  

---

## 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **模块化扩展**：在 `Attention` 中通过 `_init_kv_cache_quant` 把 KV‑Cache 量化属性抽离为统一接口，避免各模型实现散落逻辑。<br>- **后端兼容**：FlashAttention 后端检测 `vllm_flash_attn_version >= 3` 才启用 per‑head 缩放，保持向后兼容；其它后端仍走 per‑tensor。<br>- **TP‑aware 加载**：通过 `get_tensor_model_parallel_rank/size` 对 attn_head 规模的 scale 进行切分或复制，解决多卡部署时的参数分配不一致问题。 |
| **性能影响** | - **内存**：FP8 KV‑Cache 将原本的 FP16/BF16 降至 8bit，理论可把 KV‑Cache 大小缩减约 2×，显著提升长上下文或高并发请求的吞吐。<br>- **计算**：当前实现仍在解码阶段将 KV‑Cache 反量化回 FP16，故 **计算时延未变**。后续若引入 fused dequant+attention（FlashAttention 3+）将进一步降低显存读取和算子调度开销。<br>- **核改动**：在 `reshape_and_cache_flash_kernel` 中仅添加一次条件分支与 scale 读取，对热点路径的指令数影响极小（≈ +2‑3%）。 |
| **安全考虑** | - **零点处理**：新增 `*_zero_point` 参数在加载时被创建但在实际计算中被忽略（只支持对称量化），因此不会泄露未校准的偏置信息。<br>- **输入校验**：`validate_kv_cache_scheme` 对 `num_bits`、`type`、`strategy`、`symmetric` 做了显式检查，防止不受支持的量化配置导致未定义行为。<br>- **分布式安全**：TP‑aware加载确保每个卡仅读取本地对应的 scale，避免跨卡读取敏感模型参数。 |
| **可维护性** | - **统一 Scale 处理**：`quant_utils.prep_scale_for_group_broadcast` 将标量/向量的广播统一化，降低后续手动 reshape 的错误率。<br>- **文档同步**：MD 文档已补充 per‑head 说明、校准流程和兼容性提示，降低使用门槛。<br>- **测试覆盖**：新增 `kv_scale_type` 参数的组合测试，覆盖 tensor/attn_head、NHD/HND、CUDA/Triton 两种实现，提升回归安全性。 |
| **兼容性** | - **旧模型**：默认 `kv_cache_dtype="auto"` 不受影响，已有模型仍使用原始 FP16/BF16 KV‑Cache。<br>- **FlashAttention 版本**：仅在 `fa_version >= 3` 时开启 per‑head 支持，低版本仍回退到 per‑tensor，保证旧部署的可运行性。<br>- **compressed‑tensors**：在 `CompressedTensorsConfig.from_config` 中剔除仅针对 Attention 本身的 quant 配置，避免与 vLLM KV‑Cache 量化冲突。 |

---

## ⚠️ 潜在风险  

1. **Per‑head scale 与模型并行不匹配**  
   - 当 `total_num_kv_heads` 与 `tensor_parallel_size` 的比例不为整数时，会进入 “复制” 分支。若量化 checkpoint 中的 scale 按 *global* 头数排列，但模型实际在 TP 中将每卡的 KV‑head 数设为 1，可能导致 **scale 复制错误**（仅复制第一个值）。需要在 checkpoint 端确保 `total_num_kv_heads` 能被 TP 拆分或在加载前进行显式检查。  

2. **零点参数未参与计算**  
   - 代码创建 `*_zero_point` 但随后在 `scaled_dequantize` 中直接忽略。若将来引入非对称量化，现有代码将导致错误或精度下降。建议在注释中注明当前仅支持 symmetric 量化，防止误用。  

3. **FlashAttention 3 兼容性**  
   - per‑head 量化依赖 FlashAttention ≥ 3。若用户在 CUDA 环境中使用旧的 FlashAttention 2.x，`supports_per_head_quant_scales` 为 False，后端仍会使用 per‑tensor scale；此时 **scale tensor shape 不匹配**（长度为 `num_kv_heads`），可能触发 RuntimeError。确保在模型配置里检测 `fa_version` 并在不兼容时降级为 per‑tensor。  

4. **GPU 端 Kernel 边界**  
   - `kv_scale_stride` 为 0 时走 per‑tensor路径，若意外传入负数或大于 1 的值会导致越界访问。尽管 Python 层已有 `kv_scale_stride` 只在两种情形（0/1）设置，仍建议在 kernel 入口添加断言以捕获异常。  

5. **测试覆盖不足的组合**  
   - Triton 实现仍不支持 HND 布局以及 attn_head scaling；如果将来在 Triton 后端开启 HND 支持，需要同步更新 `reshape_and_cache` 的逻辑，否则将产生隐藏错误。  

---

## 💡 关注建议  

| 对象 | 建议 |
|------|------|
| **开发者** | - 在模型配置里显式声明 `kv_cache_dtype="fp8"`（或 `fp8_e4m3` / `fp8_e5m2`）并配合 `calculate_kv_scales=False`，除非使用 llm‑compressor 校准。<br>- 若开启 per‑head 量化，确保使用 FlashAttention ≥ 3 并在启动脚本中 `attention_config={"backend":"FLASH_ATTN"}`。<br>- 在多卡部署前，验证 `total_num_kv_heads % tensor_parallel_size == 0`，或把 `kv_cache_scheme.strategy="tensor"` 作为回退。 |
| **模型作者** | - 在导出 checkpoint（via llm‑compressor）时，明确在 `kv_cache_scheme` 中写入 `strategy:"attn_head"` 并提供 `total_num_heads/total_num_kv_heads`，这样加载时会自动进行 TP‑aware 切分。<br>- 若计划在未来支持非对称量化，请在 `CompressedTensorsConfig` 中保留 `zero_point` 并在 `scaled_dequantize` 中加入相应乘加逻辑。 |
| **运维/用户** | - 通过 `vllm --kv-cache-dtype fp8 --log-level INFO` 观察启动日志

---

#### 🟡 中重要度变更 (16)

### [Bugfix] Disable tma_aligned_scales in test_fusions_e2e (#32916)
**SHA**: `90c2007` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/90c200793237e61ea4c2b149d5f0394f7888b796)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增环境变量 `VLLM_USE_DEEP_GEMM_TMA_ALIGNED_SCALES`（默认 `True`），用于控制 DeepGEMM 计算时是否创建 TMA 对齐的 scale 张量。  
2. 在 `fp8_utils.py` 中将硬编码的 `tma_aligned_scales=True` 替换为上述环境变量的取值。  
3. 为了暂时规避 fusion 相关的错误，在 `tests/compile/distributed/test_fusions_e2e.py` 中通过 `monkeypatch.setenv` 将该变量设为 `0`（即关闭），并在注释中标记为待解决的 TODO。  

**🎯 影响范围**  
- `vllm/envs.py`（环境变量声明与默认值）  
- `vllm/model_executor/layers/quantization/utils/fp8_utils.py`（FP8 量化路径的行为）  
- `tests/compile/distributed/test_fusions_e2e.py`（E2E fusion 测试的执行环境）  

**💡 关注建议**  
1. **文档同步**：将新环境变量写入 README/Config 文档，说明其默认行为、性能影响以及何时需要手动关闭。  
2. **兼容性验证**：虽然默认保持原有行为（`True`），但在已有部署中开启/关闭可能导致数值差异或性能回退，建议在 CI 中加入对比测试。  
3. **测试隔离**：`monkeypatch.setenv` 只在当前测试函数生效，确保不会泄漏到其他测试；若有并行测试，最好在用例结束后显式恢复环境变量或使用 `with monkeypatch.context()`。  
4. **后续清理**：TODO 标记表明 fusion 仍存在缺陷，待修复后需移除测试中的强制关闭逻辑，防止误将该变量永久置为 `0`。  
5. **性能基准**：建议添加微基准（benchmark）来衡量开启 `VLLM_USE_DEEP_GEMM_TMA_ALIGNED_SCALES` 与关闭时的吞吐、显存占用差异，帮助用户决定是否保留默认。  

总体来说，此次改动通过可配置化解决了 fusion 引发的失败，影响范围局限于 FP8 量化路径和相应测试，风险可控。后续需完善文档、测试隔离及性能评估，以确保用户平滑迁移。

---

### [Feature]: Remove DtoH Copy for lfm2_vl On Default Stream (#32815)
**SHA**: `13d8746` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/13d8746c545576bcb6e0771ee4dc0b6fae694fa1)

**🎯 变更类型**：性能优化 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. **视觉模型**：在 `lfm2_siglip2` 中实现了 *packed*（无填充）前向路径，去掉了在 CPU‑GPU 之间的 `DtoH` 拷贝。  
   - `forward`、`resize_positional_embeddings_packed` 直接在 CPU 上完成变形与插值，只在必要时回到 GPU。  
   - `Siglip2Model`、`Siglip2Attention`、`Siglip2Model` 的调用签名改为 `pixel_values_packed`、`spatial_shapes`，省去 `packed_mask` 与 `unpack` 步骤。  
2. **多模态投影**：`LFM2VL` 的 `forward` 改为在 packed 视觉特征上完成像素‑unshuffle 与投影，避免在中间生成大尺度 padded 张量。  
3. **注意力后端**：在 GDN、Mamba、utils 中引入 `*_cpu` 版本的 `query_start_loc`（以及 `spec_sequence_masks_cpu`），在 CPU 上完成长度、切片等元数据计算，避免了显式的 DtoH 同步。  
   - `compute_causal_conv1d_metadata` 现在接受 CPU tensor，内部不再在 GPU 上执行 `diff().to("cpu")`。  

**🎯 影响范围**  
- `vllm/model_executor/models/lfm2_siglip2.py`、`lfm2_vl.py`（视觉塔及多模态投影）  
- `vllm/v1/attention/backends/gdn_attn.py`、`mamba_attn.py`、`utils.py`（注意力元数据计算）  
- 相关注册表、处理器调用路径均需使用新的 packed 接口。  

**💡 关注建议**  
1. **兼容性**：确保外部调用（HF 处理器、vllm 推理入口）都已更新为传递 `pixel_values_packed` 与 `spatial_shapes`，否则会触发断言或维度错误。  
2. **正确性**：  
   - `spatial_shapes` 必须始终在 CPU，且每个维度可被 `downsample_factor` 整除；建议在公开 API 中加入更明确的校验与错误信息。  
   - `resize_positional_embeddings_packed` 采用 `torch.nn.functional.interpolate`，在 CPU 上强制转 float32，后再转回原 dtype；确认在 bfloat16 场景下不会出现精度回退。  
3. **性能回归**：  
   - 新的 packed 路径在大 batch / 多 tile 场景下应显著降低显存占用与 DtoH 开销，建议在 CI 中加入针对 `image_pixels_to_features` 的基准测试，比较前后 GPU‑CPU 同步次数与运行时。  
   - 对 `compute_causal_conv1d_metadata` 的 CPU‑GPU 迁移要确保 `device` 参数正确传递，避免隐式的跨设备拷贝。  
4. **测试覆盖**：  
   - 添加单元测试验证 `pixel_values_packed` 与原始 padded 输入在数值上保持一致（尤其是 positional embedding resize 后的结果）。  
   - 对 `LFM2VL` 投影的 `pixel_unshuffle` 部分，检查在不同 `factor` 与不规则 `spatial_shapes` 下是否能正确抛异常。  
5. **文档更新**：在模型说明、使用示例以及 API 文档中明确 “packed” 接口的要求与优势，帮助使用者进行迁移。  

总体而言，此次改动通过在 CPU 完成可变长张量的元数据与位置编码处理，显著降低了 DtoH 同步次数和显存碎片，属于核心性能提升。但需注意兼容性校验、异常处理以及相应的测试/文档同步，以避免在生产环境出现错误或性能回退。

---

### [Misc] Add `get_name` to missing AttentionBackends (#32698)
**SHA**: `160c6fa` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/160c6fa3872aaed5e9d425deb9ad8573da359f2b)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为所有缺失 `get_name` 实现的自定义 Attention 后端统一新增 `get_name()` 静态方法，返回对应后端在注册表中的标识字符串。顺便对 `mamba2_attn.py` 的导入格式做了轻微整理。

**🎯 影响范围**  
- `vllm/v1/attention/backends/*`：`gdn_attn.py、linear_attn.py、mamba1_attn.py、mamba2_attn.py、mla/indexer.py、short_conv_attn.py`  
- 依赖 `AttentionBackend` 注册/查找机制的代码路径（如配置解析、模型构建时的后端选择）。

**💡 关注建议**  
1. **注册一致性**：确认 `AttentionBackend` 抽象基类已声明 `@abstractmethod get_name`，否则新增方法可能只在子类出现，导致基类检查失效。  
2. **兼容老版本**：若已有配置文件使用后端名称硬编码，需保证新返回值与旧名称完全匹配，否则会出现 “backend not found” 错误。  
3. **单元测试**：新增或更新对应的后端单元测试，验证 `AttentionBackend.registry.get(name)` 能正确返回实例。  
4. **文档同步**：在文档/README 中补充这些后端的名称列表，避免用户在 `--attention-backend` 参数中手动拼写错误。  
5. **性能影响**：该改动仅是元信息返回，不影响运行时计算，确认在生产环境的启动时间没有显著变化。  

整体来看，改动提升了后端的可发现性和配置统一性，对功能没有实质性破坏，只需在上述几个点做好回归测试即可安全合并。

---

### [CI][Models] Add VLM Support for Sequence Classification Conversion (#32885)
**SHA**: `a8eb118` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a8eb1182f172c616af99c1bfc5ba70b792793114)

**🎯 变更类型**：功能增强（为 VLM 添加序列分类转换支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. **layernorm.py**：将 `forward_static` 拆分为无‑残差与有‑残差两套实现，并在 CUDA 路径上对两者分别使用 `torch.compile`，提升编译兼容性与可读性。  
2. **adapters.py**：新增 `_get_language_model_for_seq_cls` 与 `_disable_seq_cls_loading_on_inner_model`，使序列分类权重加载在多模态模型（VLM）内部的语言模型上执行，防止递归加载。相应的权重加载逻辑 (`load_weights_using_from_2_way_softmax`、`load_weights_no_post_processing`) 也随之改为使用这些工具，并对 `score`、`lm_head` 名称做了 VLM‑aware处理。  
3. **triton_attn.py**：将 `nested_tensor` 的布局改为 `torch.jagged`，兼容新版 PyTorch 对 ragged 张量的实现。

**🎯 影响范围**：  
- `vllm/model_executor/layers/layernorm.py`（层归一化前向路径）  
- `vllm/model_executor/models/adapters.py`（模型权重加载、序列分类适配）  
- `vllm/v1/attention/backends/triton_attn.py`（Triton 注意力实现）  
- 相关的 VLM 与普通 LLM 代码路径均会受此改动影响。

**💡 关注建议**  
1. **兼容性验证**：在未编译的环境下确认 `torch.compile` 的 fallback 是否仍能正常工作；尤其注意 `torch.compile` 对 `torch.jagged` 的支持情况。  
2. **单元测试**：为 `load_weights_*` 增加 VLM（多模态）模型的测试，验证内部 `language_model` 的 `score`、`lm_head` 权重在序列分类转换后是否正确映射并被释放。  
3. **性能基准**：比较拆分前后的 `LayerNorm` 在 CUDA 编译与非编译路径上的吞吐，确保拆分不会引入额外的显存搬迁或数据类型转换开销。  
4. **文档更新**：在 README / API 文档中说明新增的 VLM 序列分类支持以及相关配置（如 `classifier_from_token`、`method`）的暂时禁用机制。  

总体而言，改动为 VLM 引入序列分类提供了必要的适配层，逻辑更加模块化，但需通过针对 VLM 场景的测试和基准确认其正确性与性能保持。

---

### [Bugfix] Fix _CPU_MOE_ACT AssertionError when vLLM config not set (#32777)
**SHA**: `fa6e599` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fa6e599a618bba1e7edd3f6bcd708123fd625436)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 解决在 vLLM 配置尚未初始化时，`_CPU_MOE_ACT`（懒加载的 `dict`）在模块导入阶段触发 `CustomOp.__init__()`，导致 `AssertionError`。  
2. 将原来的懒加载字典替换为静态映射 `_CPU_MOE_ACT_FN`，直接指向激活函数的 **native** 实现。  
3. 为 `SwigluOAIAndMul` 提供了纯 PyTorch 实现 `_swigluoai_forward_native`，避免实例化 `CustomOp`。  
4. 相应地更新了测试代码以及内部调用路径。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py`（激活函数映射与 `cpu_fused_moe` 核心路径）  
- `tests/kernels/moe/test_cpu_fused_moe.py`（单元测试）  
- 其他任何在运行时通过 `_CPU_MOE_ACT` 访问激活实现的代码（如果还有残余引用，需要同步改为 `_CPU_MOE_ACT_FN`）。  

**💡 关注建议**  

1. **兼容性检查**：确认项目中没有遗漏的旧引用（如 `_CPU_MOE_ACT[...]`），否则在运行时会抛出 `KeyError`。可以在迁移期保留一个薄包装层，抛出明确的弃用警告。  
2. **行为一致性**：`_swigluoai_forward_native` 与原 `SwigluOAIAndMul.forward_native` 必须保持数值一致（尤其是 `clamp` 限幅和 `alpha`、`limit` 参数），建议在 CI 中加入对比测试（e.g., `torch.allclose`）防止回归。  
3. **文档更新**：在激活函数章节说明 “silu” 与 “swigluoai” 现在均通过纯 PyTorch 实现，不再依赖自定义 CUDA/CPU Op。  
4. **性能评估**：虽然改为 Python 实现避免了初始化错误，但可能略慢于 C++ CustomOp。建议在关键路径做基准对比，若发现显著回退，可考虑在配置已就绪后再切回 CustomOp（通过延迟加载）。  
5. **异常信息**：当前 `assert activation in _CPU_MOE_ACT_FN` 抛出 AssertionError，建议改为 `raise ValueError`，提供更友好的错误信息。  

整体来看，此改动消除了启动阶段的配置依赖，提升了库的鲁棒性。只要完成上述兼容性和数值验证，便可安全合并。

---

### [CI] Fix mypy for `vllm/v1/structured_output` (#32722)
**SHA**: `7ef5873` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7ef587375280b475719c37463ca37f2ceb38e54e)

**🎯 变更类型**：Bug 修复 / 类型安全提升  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交主要为 `vllm/v1/structured_output` 及推理解析相关模块补全、纠正 MyPy 类型注解。将大量 `list[int]` 参数改为更通用的 `Sequence[int]`，在平台特定实现中加入 `type: ignore`，并在结构化输出入口加入空列表兜底及 `structured_outputs` 为 `None` 的提前返回，避免运行时异常。  

**🎯 影响范围**  
- `vllm/v1/structured_output`（核心结构化输出流程）  
- `vllm/reasoning/*`（所有 ReasoningParser 实现）  
- `vllm/v1/attention/backends/*`（Flash‑Attn 相关的 ROCm/IPEX 适配）  
- `tools/pre_commit/mypy.py`（CI 检查配置）  

**💡 关注建议**  
1. **类型一致性**：将 `Sequence[int]` 替换后，务必确认所有调用方仍使用可迭代的序列（如 `list`、`tuple`、`numpy.ndarray`），如果有自行实现的自定义序列，应实现 `__getitem__` 与 `__len__`。  
2. **空列表兜底**：`request.prompt_token_ids or []` 防止 `None` 传入，但后续逻辑仍假设 `prompt_token_ids` 已经是完整序列，建议在文档或 `Request` 类中明确该字段的默认值。  
3. **结构化输出开关**：在 `backend_guidance.validate_guidance_grammar` 加入 `structured_outputs is None` 检查是必要的，避免不必要的解析错误。请在 `SamplingParams` 的默认值和文档中说明该字段的可选性。  
4. **平台特定的 `type: ignore`**：这些注释抑制了 MyPy 的错误，但最好在后续迭代中提供更精确的类型（如为 `flash_attn` 包装一层 `Protocol`），以逐步消除 `ignore`。  
5. **CI 配置同步**：`tools/pre_commit/mypy.py` 已恢复对 `vllm/v1/structured_output` 的检查，请在本地运行 `pre-commit run -a` 验证所有文件均通过 MyPy。  

总体而言，此次修改提升了项目的类型安全和 CI 可靠性，对运行时行为无功能性影响。后续可以考虑在核心接口统一使用 `Sequence[int]`，并在文档中标注对应的兼容要求。

---

### [torch.compile] Compile `CustomOp.forward_native` for `SiluAndMul` and `QuantFP8` to avoid raw torch ops inside opaque custom ops (#32806)
**SHA**: `5e4e0e5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5e4e0e51f4fbb6d6fbf7a94feef07bd3617bfa5f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 `CustomOp` 引入 `compile_native` 参数，允许在自定义算子内部对 `forward_native` 进行 `torch.compile` 编译，从而避免在不透明的自定义算子（如 fused‑moe、unified_attention）中出现裸的 eager Torch ops。默认打开编译，Eager 后端下仍保持不编译。相应层（`SiluAndMul`、`QuantFP8`、RotaryEmbedding 等）及测试用例已适配新参数。  

**🎯 影响范围**  
- **核心**：`vllm/model_executor/custom_op.py`（新增编译调度与 `maybe_compile` 实现）  
- **层实现**：`activation.py`、`input_quant_fp8.py`、`rotary_embedding/common.py`（构造函数传递 `compile_native`）  
- **编译配置**：`matcher_utils.py`（默认 `compile_native=False`）  
- **测试**：`tests/compile/test_silu_mul_quant_fusion.py`、`tests/kernels/core/test_activation.py`（显式关闭或开启编译）  

**💡 关注建议**  
1. **使用场景**：在开启 `vLLM` 的 `torch.compile`（非 eager）时，保持 `compile_native=True` 以获得跨算子融合的性能提升；若遇到不兼容的后端或调试需求，可在实例化层时设 `compile_native=False`。  
2. **后端兼容**：编译只在非 eager (`backend != "eager"` ) 且 `CompilationMode != NONE` 时生效，确保部署环境的 `torch` 版本支持 `torch.compile` 与对应 backend。  
3. **性能监控**：`maybe_compile` 使用 `dynamic=True`，避免重复编译，但仍会产生一次编译开销；在大批量推理前先进行一次热身，以免首次调用出现延迟。  
4. **潜在风险**：编译后的函数不再参与跨算子图融合，若业务强依赖融合（例如自定义 fused kernel），仍应在上层将其“解包”。  
5. **回归测试**：新加入的测试覆盖了打开/关闭 `compile_native` 的路径，建议在 CI 中保留这些用例，并在新增自定义算子时同步添加对应的 `compile_native` 参数。  

总体而言，此次改动为 vLLM 在使用 `torch.compile` 时提供了更细粒度的控制，提升了在复杂自定义算子场景下的执行效率，使用时注意后端与编译模式的匹配即可。

---

### [BugFix] deepseek_v32_encoding: Replace asserts with proper exceptions (#32884)
**SHA**: `f61c9da` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f61c9da711d846220cb59738d10c0a937ffb80e2)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 `deepseek_v32_encoding.py` 中大量 `assert` 替换为显式的异常（`ValueError` / `RuntimeError`），提升在生产环境下的错误可捕获性和调试信息。  

**🎯 影响范围**  
- **核心模块**：`vllm/tokenizers/deepseek_v32_encoding.py`（消息渲染、工具调用解析、思考模式解析等全部逻辑）  
- **下游**：所有使用 DeepSeek‑V3.2 编码器的推理路径、对话格式化与工具调用解析均会受此修改影响。  

**💡 关注建议**  
1. **异常兼容**：原先 `assert` 在 `-O`（优化）模式下会被剔除，导致错误悄然通过。改用显式异常后，调用方应准备捕获 `ValueError`/`RuntimeError`，防止进程异常终止。  
2. **错误信息**：新抛出的异常已加入具体上下文（索引、长度、无效 token 等），建议在日志层面统一格式化，以便监控系统快速定位。  
3. **单元测试**：补充针对异常路径的测试，验证如 `index` 越界、`thinking_mode` 非法、工具调用格式错误等场景是否抛出预期异常。  
4. **向后兼容**：若已有代码依赖于 `assert` 抛 `AssertionError`，需检查并更新异常捕获逻辑。  

总体来看，此次改动提升了代码在生产环境的鲁棒性，但需要确保调用链对新异常类型做好处理并补足相应的测试用例。

---

### [Misc] Log vLLM logo when starting server (#32796)
**SHA**: `7fe2558` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7fe255889e583102ec4da454071686f0ad5880b9)

**🎯 变更类型**：其他（日志展示功能增强）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm.entrypoints.utils` 新增 `log_version_and_model`，在服务启动时打印彩色/单色的 vLLM 徽标并展示版本号与模型名。  
2. 两个入口脚本（gRPC 与 OpenAI API）改为使用该函数，而不再仅记录版本号。  
3. `vllm.envs` 增添 `VLLM_DISABLE_LOG_LOGO` 环境变量，用于关闭徽标打印。  
4. `vllm.logger` 新增 `current_formatter_type`，用于判断根 logger 使用的 formatter（彩色或换行），为徽标渲染提供依据。  

**🎯 影响范围**  
- **入口模块**：`vllm/entrypoints/grpc_server.py`、`vllm/entrypoints/openai/api_server.py`。  
- **工具函数**：`vllm/entrypoints/utils.py`（新增函数及依赖 `envs`、`logger`）。  
- **环境变量**：`vllm/envs.py`（新增 `VLLM_DISABLE_LOG_LOGO`）。  
- **日志系统**：`vllm/logger.py`（新增 `current_formatter_type`，引入 `ColoredFormatter`、`NewLineFormatter`）。  

**💡 关注建议**  
1. **兼容性**：`log_version_and_model` 依赖根 logger 名为 “vllm” 并仅有单个 handler。若项目自行更改 logger 配置，可能返回 `None` 并导致仅输出普通文字信息。建议在文档中说明此约束或在函数里加入更宽容的检测逻辑。  
2. **环境变量生效**：确保在启动脚本之前已读取 `VLLM_DISABLE_LOG_LOGO`，否则徽标仍会打印。可以在 CI 中加入针对该变量的快速验证。  
3. **性能影响**：仅在启动阶段执行一次字符串模板渲染，几乎可以忽略不计。  
4. **测试覆盖**：新增单元测试，分别验证彩色、单色以及禁用徽标的三种场景；并测试 `current_formatter_type` 在不同 logger 配置下返回期望值。  
5. **文档更新**：在 README/CHANGELOG 中注明新增 `VLLM_DISABLE_LOG_LOGO`，并给出如何在 CI/CD 环境下静默启动的示例。  

总体而言，此次改动提升了启动日志的可读性和品牌辨识度，对核心功能没有影响，唯一需要关注的是 logger 配置的前置假设和新增环境变量的使用。

---

### [Perf] Create TMA-aligned input scale tensor for DeepGemm on Hopper (#32619)
**SHA**: `d08b356` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d08b356ee08dc8e0b0dd052ed7bef5245e21d7af)

**🎯 变更类型**：性能优化（针对 Hopper GPU 的 DeepGemm，新增 TMA‑aligned scales）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `per_token_group_quant_fp8` 中加入 `tma_aligned_scales` 参数，实现列‑major、TMA 对齐的比例张量布局；提供辅助函数 `get_tma_aligned_size`。相应的测试、benchmark 与模型层调用均已更新，以使用该布局并在 DeepGemm 中提升数据搬运效率。  
**🎯 影响范围**  
- `vllm.utils.deep_gemm`（新增对齐工具）  
- 量化工具 `per_token_group_quant_fp8`、`fp8_utils`、`input_quant_fp8`  
- 单元测试 `test_block_fp8.py、test_per_token_group_quant.py`  
- 基准 `benchmark_fp8_block_dense_gemm.py`  

**💡 关注建议**  
1. **兼容性**：默认 `tma_aligned_scales=False`，因此老代码不受影响；若外部显式传 `True`，需确认 GPU 为 Hopper（支持 TMA），否则可能出现不匹配的 stride 错误。可以在入口加 `assert` 或 runtime warning。  
2. **stride 计算**：`torch.empty_strided` 的 stride 与 `get_tma_aligned_size` 紧耦合，建议添加单元测试覆盖 2‑D 与 3‑D 以上情况，防止高维张量产生错误布局。  
3. **文档**：在 `fp8_utils.per_token_group_quant_fp8` 与 `InputQuantFP8` 的 docstring 中补充 `tma_aligned_scales` 说明，以及何时开启（DeepGemm‑e8m0 + Hopper）。  
4. **性能评估**：当前仅在 benchmark 中验证，建议在真实模型推理路径（如 LLM forward）加入对比实验，确保 TMA‑aligned scales 能显著降低 kernel launch 延迟或提升吞吐。  
5. **代码可读性**：`per_token_group_quant_fp8` 中的分支略显冗长，可抽取 “create column‑major TMA‑aligned tensor” 为独立函数，提升可维护性。  

总体而言，此次改动为 DeepGemm 在 Hopper 上的关键性能提升提供了必要的布局支持，同时保持向后兼容。但请在发布前确保对齐逻辑在所有支持的张量维度上都经过充分验证，并在文档中明确使用前提。

---

### [Refactor] Remove unused tpu files (#32610)
**SHA**: `f744810` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f74481018412e4bc63d0fc396ec675ca4bf9bf18)

**🎯 变更类型**：重构（删除未使用的 TPU 相关实现）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：此次提交在 `vllm/v1/sample/tpu` 与 `vllm/v1/worker` 两个目录下，彻底删除了 `metadata.py、sampler.py、__init__.py` 以及 `tpu_worker.py` 共计 353 行代码。这些文件原本实现了面向 XLA/TPU 的采样逻辑和工作线程，但在主仓库中已不再被引用或已迁移至外部插件。删除后，代码基座体积更小，构建时间和依赖也会相应减少。  

**🎯 影响范围**  
- **采样子模块**：`vllm/v1/sample/tpu.*` 被直接移除，任何仍然 `import vllm.v1.sample.tpu.*` 或在配置中启用 TPU 采样的代码将出现 `ImportError`。  
- **工作进程**：`vllm/v1/worker/tpu_worker.py` 被删除，若外部仍通过 `USE_TPU_INFERENCE` 动态导入该类，需确认插件路径已更新。  
- **文档/示例**：项目文档、示例脚本或 CI 配置中出现的 TPU 示例需要同步清理或指向新的插件实现。  

**💡 关注建议**  
1. **搜索残余引用**：在全仓库执行 `grep -R "tpu_worker\|sample.tpu" -n`，确保没有遗漏的导入或类型注解。  
2. **保持向后兼容**：若仍希望保留旧接口，可在 `vllm/v1/sample/tpu/__init__.py` 中加入轻量级的占位模块并抛出明确的错误提示（例如 “TPU support moved to `vllm.plugins.tpu`”），避免用户在升级后遇到模糊的 `ImportError`。  
3. **更新 CI/测试**：删除相关测试文件或将其标记为 `skip`，防止因缺失模块导致 CI 失败。  
4. **文档同步**：在 README、docs/usage.md 以及任何关于硬件加速的章节中，说明 TPU 支持已转移至外部插件，并提供相应的安装与使用指南。  
5. **发布说明**：在本次发布的 changelog 中明确注明 “移除未使用的 TPU 实现，建议使用官方 TPU 插件”，帮助用户快速定位迁移路径。  

总体而言，此次删除是一次清理性重构，对核心 CPU/GPU 路径没有功能性影响，只需确保剩余代码不再依赖这些已删除的模块，并在文档和 CI 中同步更新。

---

### [Bugfix][Attention] Explicitly report support for kv_cache_dtype bfloat16 (#32795)
**SHA**: `955b43a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/955b43a5a5142046bd8a1bd22fd219bf944a6265)

**🎯 变更类型**：Bugfix / Attention  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：显式声明 `kv_cache_dtype` 支持 `bfloat16`，并统一使用 `is_quantized_kv_cache` 判断是否为量化 KV‑cache（仅限 fp8 系列）。为此在 `AttentionBackend`、各后端实现、CPU 平台检查以及 KV‑cache 量化层中加入 `bfloat16` 列表并调整相关逻辑。  

**🎯 影响范围**  
- `vllm/v1/attention/backend.py` 与 `is_quantized_kv_cache` 实现  
- 所有注意力后端（FlashAttn、FlashInfer、FlexAttention、MLA 系列、Triton 等）对 `supported_kv_cache_dtypes` 的更新  
- CPU 平台的配置检查 (`vllm/platforms/cpu.py`)  
- KV‑cache 量化层 `vllm/model_executor/layers/quantization/kv_cache.py` 的后处理逻辑  

**💡 关注建议**  
1. **功能验证**：在 GPU 与 CPU 两种后端下分别跑一次 `kv_cache_dtype=bfloat16` 的完整推理，确认不再误报 “FP8 KV cache 不兼容”。  
2. **回退路径**：`is_quantized_kv_cache` 现在仅返回 `True` 对 fp8 前缀的 dtype，确保原先对非‑auto（如 `bfloat16`）的老代码不再误触量化路径。  
3. **文档与配置**：更新 README/Config 示例，说明 `bfloat16` 已被正式支持且不需要 `calculate_kv_scales`。  
4. **兼容性检查**：CPU 后端仍不支持任何 KV‑cache 量化，代码已在 `cache_dtype.startswith("fp8")` 时抛异常，请在 CI 中加入对应测试。  
5. **日志与警告**：注意 `CPU backend doesn't support KV cache quantization fallback to auto.` 的警告在 `bfloat16` 场景下不应出现，确保日志输出符合新逻辑。  

总体而言，此次修改对 KV‑cache 数据类型的兼容性提升明显，影响范围集中在注意力后端注册与平台检查，开发者应重点验证不同后端的 `bfloat16` 配置能正常运行且不触发量化相关的错误或警告。

---

### [CPU Backend] [Perf] Accelerate tensor-parallel/data-parallel inference across NUMA domains on Arm (#32792)
**SHA**: `744ef30` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/744ef30484169694ef4aaba9420a03ed5b861b66)

**🎯 变更类型**：功能增强/性能优化（新增 ARM ASIMD 后端并在 NUMA 域上加速 Tensor‑Parallel / Data‑Parallel 推理）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. CMake 中检测到 ASIMD（Arm SVE/NEON）特性后，编译 `csrc/cpu/shm.cpp` 以提供共享内存通信实现。  
2. 在 `cpu_types_arm.hpp` 中补齐 ARM 向量类（FP16/BF16/FP32/INT8）并实现非时间局部加载的兼容包装，同时新增 64‑元 INT8 向量实现与掩码存储。  
3. `shm.cpp` 使用 `std::atomic<char>`（AArch64）替代 volatile，实现跨核/跨 NUMA 的严格内存顺序（acquire/release），并在自旋等待时使用 `yield` 指令。  
4. Python 层 `cpu_communicator.py` 与 `cpu_worker.py` 扩展对 ARM 架构的检测，使 SHM CCL 与 OMP 亲和策略在 ARM 上可用。  

**🎯 影响范围**  
- 编译脚本 `cmake/cpu_extension.cmake`  
- 低层向量实现 `csrc/cpu/cpu_types_arm.hpp`、`csrc/cpu/shm.cpp`  
- Torch C++ 扩展绑定 `csrc/cpu/torch_bindings.cpp`  
- 分布式通信层 `vllm/distributed/device_communicators/cpu_communicator.py`  
- CPU worker 启动逻辑 `vllm/v1/worker/cpu_worker.py`  

**💡 关注建议**  
1. **功能验证**：在真实的 ARM 服务器（如 AWS Graviton、Alibaba 2代）上跑完整的 TP/PP 测例，确认共享内存创建、数据发送/接收以及 `nt_save` 行为均正确。  
2. **性能基准**：对比 X86 AVX512 与 ARM ASIMD（尤其 64‑byte INT8 向量）在相同 batch/seq 长度下的吞吐量，关注 NUMA 亲和绑定是否真的降低跨节点访问延迟。  
3. **线程安全**：`std::atomic<char>` 只在 AArch64 使用，确保编译选项 `-march=armv8-a+simd` 生效；在混合编译（x86+arm）环境下确认宏定义不会误导。  
4. **回退机制**：若目标平台不支持 ASIMD，CMake 会自动排除 `shm.cpp`，请保持 X86 路径不受影响。  
5. **文档与 CI**：在 README/CHANGES 中补充 “ARM CPU 支持”，并在 CI 中加入至少一套 ARM‑Docker 测试，以防止未来修改破坏原子顺序或向量实现。  

总体而言，此提交为 ARM CPU 引入了与 X86 类似的共享内存通信与高效向量算子，是面向多 NUMA 域的关键性能提升，需重点在实际 ARM 芯片上做完整回归与基准测评。

---

### Support custom URI schemes and trace handlers for profiler (#32393)
**SHA**: `3a63be0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3a63be0faa8e45401b9eddaafaca3deecac0a882)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `_is_uri_path` 辅助函数，实现对任意 URI（`gs://`、`s3://`、`hdfs://` 等）与本地路径的区分，防止对 URI 进行 `os.path.abspath`。  
2. `ProfilerConfig` 验证逻辑改为使用 `_is_uri_path`，统一处理所有非本地 URI。  
3. `WorkerProfiler` 构造器新增 `on_trace_ready` 参数，可自定义 trace 处理函数；默认仍使用 `torch.profiler.tensorboard_trace_handler`。  
4. 在 `_stop` 中写入 `profiler_out_{rank}.txt` 前加入 URI 检测，避免对远程路径执行普通文件写入。  
5. 为 `_is_uri_path` 编写了覆盖多种场景的单元测试。  

**🎯 影响范围**  
- `vllm/config/profiler.py`（路径处理、配置校验）  
- `vllm/profiler/wrapper.py`（trace handler 可配置、文件写入条件）  
- 相关测试 `tests/v1/worker/test_gpu_profiler.py`  

**💡 关注建议**  
1. **兼容性**：新增的 `on_trace_ready` 参数默认值为 `None`，保持现有调用不受影响；请在文档中说明该可选参数的用途及示例。  
2. **路径校验**：`_is_uri_path` 仅检查 `"://"` 并判断 scheme 长度 > 1，已覆盖大多数情况，但对 `file://`、`ftp://` 等合法 URI 也会返回 `True`（符合预期）。建议在函数注释中明确“仅用于检测是否应跳过本地文件操作”。  
3. **异常处理**：当 `profiler_dir` 为 URI 且用户仍希望生成本地统计文件时，当前会静默跳过写入。可以考虑在日志中提示 `"profiler_dir is a remote URI, skipping local file write"`，提升可调试性。  
4. **测试覆盖**：已添加路径判定测试，建议再补充一次 `WorkerProfiler` 使用自定义 `on_trace_ready` 的集成测试，验证自定义回调能够被正确调用。  
5. **跨平台**：Windows 驱动字母 (`C://`) 被正确识别为本地路径（返回 `False`），但在实际运行中 `os.path.abspath` 仍会将其转为合法路径，建议在文档中提醒 Windows 环境下的使用注意。  

总体来看，此次改动提升了 profiler 对远程存储的兼容性，并为用户自定义 trace 处理提供了灵活入口，影响范围局限在配置与 profiler 包，风险较低。后续可通过文档和日志提示进一步降低使用误解的概率。

---

### [UX] Default api_server_count to dp_size if not specified (#32525)
**SHA**: `803e3f3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/803e3f3f688677701a616d692125e181116d6b69)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `--api-server-count` 的默认值由固定的 **1** 改为 **None**，在运行时根据数据并行（DP）模式自动推导：  
  - **外部 LB**（`--data-parallel-external-lb` / `--data-parallel-rank`） → 保持 1（外部负责分发）。  
  - **混合 LB**（`--data-parallel-hybrid-lb` / `--data-parallel-start-rank`） → 使用本地 DP 大小 `data_parallel_size_local`。  
  - **内部 LB**（默认） → 使用全局 DP 大小 `data_parallel_size`。  
- 在 `headless` 模式下强制 `api_server_count = 0` 并阻止手动指定正数。  
- 新增对外部/混合 LB 同时开启的冲突检测。  
- 相应地在 `serve.py` 调整了启动逻辑：`api_server_count < 1` → `run_headless`，`>1` → `run_multi_api_server`，`=1` → 单进程服务器。

**🎯 影响范围**  
- `vllm/entrypoints/cli/serve.py`（启动流程）  
- `vllm/entrypoints/openai/cli_args.py`（CLI 参数默认值）  
- 依赖 `api_server_count` 与 DP 参数交叉使用的使用场景（如自定义启动脚本、Kubernetes 部署模板）。

**💡 关注建议**  
1. **回归测试**：在不同 DP 配置（内部、外部、混合）以及 `--headless` 下执行完整的启动‑服务‑请求链路，确保默认值被正确推导且没有遗漏的进程启动。  
2. **文档更新**：CLI 手册需说明 `--api-server-count` 现在可省略，并阐明默认行为及冲突错误信息。  
3. **兼容性检查**：老版脚本若显式依赖默认 `1`（如假设 `api_server_count` 永不为 `None`）可能触发 `None`‑相关异常，请在升级说明中提醒用户。  
4. **异常路径**：`is_external_lb` 与 `is_hybrid_lb` 同时为真时抛出 `ValueError`，建议加入单元测试覆盖此分支，防止未来参数组合导致未捕获错误。  

总体来看，此改动提升了多机/多卡部署的可用性，但需确保相关文档、测试和旧脚本的兼容性。

---

### [Hardware][AMD][CI][Bugfix] Fix regressions from deprecated env vars (#32837)
**SHA**: `c517d8c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c517d8c93404738effe29b5beea606111aba9258)

**🎯 变更类型**：其他 / Bugfix  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交修复了因旧版环境变量 `ROCM_ATTN`（已被弃用）在 CI 与代码中使用不当导致的回归。CI 脚本改为显式设置 `ROCM_ATTN=1`，测试脚本在检测到该变量时加入对应的 `--attention-backend ROCM_ATTN` 参数；平台实现则改用安全的 `get_current_vllm_config_or_none()`，在配置可能为空时避免抛异常，并在 `use_prefill_decode_attention` 为 true 时启用 ROCm Attention 后端。

**🎯 影响范围**  
- `vllm/platforms/rocm.py`（注意力后端选择逻辑）  
- CI 配置文件 `.buildkite/test-amd.yaml`（AMD 硬件测试步骤）  
- 测试脚本 `tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh`（环境变量处理）

**💡 关注建议**  
1. **文档同步**：在项目 README/AMD 部署指南中注明 `ROCM_ATTN` 仅用于 CI 或显式开启的场景，避免用户在生产环境误用。  
2. **向后兼容**：若历史用户仍在代码中直接调用 `get_current_vllm_config()`，建议保留该函数或提供别名，以免破坏已有脚本。  
3. **测试覆盖**：在本地或 CI 中跑一遍不设 `ROCM_ATTN` 的路径，确保回退到默认后端不受影响；同时验证 `ROCM_ATTN=0` 时不意外激活 ROCm 后端。  
4. **错误日志**：在 `rocm.py` 中加入更明确的日志（如 “ROCM_ATTN env var ignored: config missing or prefill‑decode disabled”），帮助排查未来类似问题。  
5. **环境变量管理**：考虑将 `ROCM_ATTN` 纳入统一的环境变量注册表（如 `vllm/config.py`），统一检查、打印和清理，防止散落的硬编码。  

以上修改主要是提升 AMD/ROCm 环境的稳定性，风险较小，但建议在下一个 CI 循环后监控测试通过率及运行时日志，以确认没有遗漏的依赖。

---

#### 🟢 低重要度变更 (13)

### [ROCm][PD] Remove unused moriio connector proxy code (#32939)
**SHA**: `1cb4341` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1cb4341fbcca4e58fa1ac5c1d55691eff16bf52b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `moriio_toy_proxy_server.py` 中未使用的 `send_request_to_decode` 函数及相关变量，精简示例代码。

---

### [Bugfix] Fix FP8 MoE EP Weight Loading for ModelOpt Llama4 (#32886)
**SHA**: `1fb648b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1fb648bf107e9c24e7f99671c77406d126506d7a)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Llama4 模型的 MoE 权重加载中，新增对老旧 PyTorch（<2.11）在 CPU 上不支持 FP8 索引的兼容处理，通过先转为 float16 再索引后恢复 dtype，避免崩溃。

---

### [Misc] Postpone torch_profiler deprecation (#32867)
**SHA**: `7e22309` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7e22309755aca3ce02f45b39f074008dc5e89b89)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 torch_profiler 环境变量的废弃提示版本从 “v0.14.0” 调整为 “v0.15.0”，以推迟其废弃时间。

---

### [Bugfix] Fix getting vision features in Transformer Multimodal backend (#32933)
**SHA**: `d95d650` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d95d65076292df5cfb4d06ed0bdba7496825c499)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `multimodal.py` 中处理 `self.get_image_features` 返回的 tuple 或 dict，统一提取视觉特征张量，解决 Transformer v5 及以后版本获取特征异常的问题。

---

### [CPU][Feat] Update PyTorch to v2.10 for CPU Backend (#32869)
**SHA**: `10e94c8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/10e94c84f6ce4e5f1dff8a2fefc242e9ee687d0c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 CPU 后端的 PyTorch 版本从 2.9.1 升级至 2.10.0，更新 Dockerfile 与相关 requirements 文件的依赖声明。

---

### [Benchmark][Bugfix] Fix race condtion when starting server for sweep benchmark (#32927)
**SHA**: `243e78c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/243e78c20fd74a68f86b6523c1f607eb3cc14ab2)

**🎯 变更类型**：代码重构 / Bugfix  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：为 sweep 基准的服务器启动引入 `server_ready_timeout` 参数，并在 `ServerProcess` 中实现 `is_server_ready` 与 `wait_until_ready` 检查，防止因服务器未就绪导致的竞态错误。CLI 亦新增对应选项。

---

### [CPU Backend][BugFix] Fix failing CPU MoE test (#32876)
**SHA**: `aac0b81` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aac0b817fad9947608157529e68aa867ff76c804)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `tests/kernels/moe/test_moe.py` 的 `test_moe_sum` 用例上新增 `@pytest.mark.usefixtures("default_vllm_config")`，确保 CPU MoE 测试在默认 vLLM 配置下运行，修复了之前的失败。

---

### [CI/Build][CPU] Fix failed pooling tests and macos smoke test (#32907)
**SHA**: `5da4c7d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5da4c7d789fe0c4ca2c49913441f99df24715a97)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 CPU 侧共享内存初始化宏条件，避免在 macOS 上误编译；在 CPU unquantized GEMM 分发中新增对 meta 权重的检查，防止缺失层导致崩溃。

---

### [BugFix] Fix invalid flashinfer_fused_moe_blockscale_fp8 op registration (#32855)
**SHA**: `fc56f4a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fc56f4a0714bd1727ea100a9d75d18ce1a138684)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `flashinfer_fused_moe_blockscale_fp8` 中 `routing_method_type` 的默认值从枚举改为 `int(RoutingMethodType.DeepSeekV3)`，修复注册时的类型错误。

---

### [CI][Attention] Add more CI dependencies for attention tests (#32487)
**SHA**: `300622e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/300622e609a52b76526182bdfca615d5f9e448a2)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Buildkite CI 配置文件中为注意力相关的测试步骤新增 `vllm/config/attention.py` 与 `vllm/model_executor/layers/attention` 目录的文件依赖，使得这些文件变动时 CI 能自动重新运行相应的注意力测试。

---

### [Feature] Add --ssl-ciphers CLI argument for TLS cipher control (#30937)
**SHA**: `69d09fd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/69d09fdd6cb66fdfc92f2f2388c4899dfd93d0ad)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `--ssl-ciphers` CLI 参数，用于在启动 OpenAI API 服务器时指定 TLS 加密套件，并在 `run_server_worker` 中传递该配置。

---

### [MISC] Add .cursor to .gitignore (#32868)
**SHA**: `70917b1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/70917b1c55d3ba392ed86e77dbc0508f405851cd)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.gitignore` 中新增对 `.cursor/` 目录的忽略，防止光标相关的临时文件被提交。

---

### [Bugfix] ModelScope is supported when downloading LORA models. (#32844)
**SHA**: `fc37187` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fc37187a51474b42c0b80cd672d3bdaacb0c026d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/lora/utils.py` 中加入 ModelScope 下载支持；依据 `VLLM_USE_MODELSCOPE` 环境变量切换至 ModelScope 的 `snapshot_download`，统一异常捕获并保持对 HuggingFace Hub 的兼容。

---

