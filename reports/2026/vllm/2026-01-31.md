# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-01-31ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-01-31 23:22:25 | jma99_2333 | Support clear mm and encoder cache (#33452) |
| 2026-01-31 23:12:17 | â„ğ• ğ•ğ•ğ• ğ•¨ ğ•„ğ•’ğ•Ÿ | [BugFix][Router Replay] Capture Logical Experts with EPLB (#33013) |
| 2026-01-31 22:48:34 | Luka GovediÄ | [fix][torch.compile] Fix cold-start compilation time increase by adding kv cache update to splitting ops (#33441) |
| 2026-01-31 22:48:28 | Cyrus Leung | [Doc] Update plugin deprecation notices (#33476) |
| 2026-01-31 22:04:20 | cmunley1 | support return prompt token ids in responses  (#33378) |
| 2026-01-31 21:38:39 | Roy Wang | [Misc] Replace deprecated interface seed_everything (#33474) |
| 2026-01-31 21:21:32 | Cyrus Leung | [Bugfix] Fix incompatibility between #33372 and #32863 (#33475) |
| 2026-01-31 21:00:54 | Angela Yi | [ez] Add structured torch.compile logs (#33213) |
| 2026-01-31 20:51:15 | Cyrus Leung | [Frontend] Use new Renderer for Completions and Tokenize API (#32863) |
| 2026-01-31 17:51:26 | caozuoba | [perf] v1/spec_decode: skip softmax for all-greedy rejection sampling (#32852) |
| 2026-01-31 17:03:57 | jennyyyyzhen | [ROCM] Enable aiter attn backend for qwen3-next model (#32492) |
| 2026-01-31 16:14:54 | Jinwu | [BugFix] Add synchronize in CutlassW4A8LinearKernel to ensure data is ready for use. (#33078) |
| 2026-01-31 15:38:46 | Yanan Cao | [Kernel] [Helion] [3/N] Helion kernel registry (#33203) |
| 2026-01-31 15:16:22 | Fadi Arafeh | [CPU][Feat] Enable KleidiAI accelerated int4 dynamic quant with BF16 activations on Arm CPUs (#33122) |
| 2026-01-31 14:53:08 | AutumnAurelium | Add EAGLE3 support for AFMoE (#33111) |
| 2026-01-31 14:48:27 | Dimitrios Bariamis | Add support for Mistral Large 3 inference with Flashinfer MoE (#33174) |
| 2026-01-31 14:21:51 | Matthias Gehre | [Bugfix] Handle Asym W4A16 (ConchLinearKernel) for CT (#33200) |
| 2026-01-31 13:24:49 | NicolÃ² Lucchesi | [Bugfix] Fix `Qwen3ASR` language asr tag in output  (#33410) |
| 2026-01-31 12:53:01 | Yanan Cao | [Kernel] [Helion] [2/N] Helion kernel wrapper (#32964) |
| 2026-01-31 12:50:30 | Francesco Fusco | [Attention] Clarify comment explaining attn_logits +1 dimension (#33427) |
| 2026-01-31 12:49:00 | Patrick von Platen | [Streaming -> Realtime] Rename all voxtral related classes, fn, files (#33415) |
| 2026-01-31 12:28:29 | Micah Williamson | [ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_state_loader to reduce flakiness (#33277) |
| 2026-01-31 12:19:22 | Russell Bryant | [Misc] offest -> offset in comments and variable names (#33444) |
| 2026-01-31 12:15:06 | Lucas Wilkinson | [BugFix] Fix whisper FA2 + full cudagraphs (#33360) |
| 2026-01-31 12:14:54 | Michael Goin | [UX] Use gguf `repo_id:quant_type` syntax for examples and docs (#33371) |
| 2026-01-31 11:36:56 | Isotr0py | [Misc] Algin Qwen3-VL-embedding image example outputs with HF repo example (#33419) |
| 2026-01-31 11:33:26 | Nick Hill | [ModelRunner V2] Fix spec decoding + logprobs (#33391) |
| 2026-01-31 11:30:00 | Matthew Bonanni | [Attention] Move MLA `forward` from backend to layer (#33284) |
| 2026-01-31 10:54:16 | Wentao Ye | [Deprecation] Deprecate `seed_everything` and `scatter_mm_placeholders` in v0.15 (#33362) |
| 2026-01-31 09:26:15 | Alberto Ferrer | [Bugfix] Fix typo in read_offset variable name (#33426) |
| 2026-01-31 09:05:23 | Gregory Shtrasberg | [Bugfix][ROCm] Fixing the skinny gemm dispatch logic from #32831 (#33366) |
| 2026-01-31 08:37:42 | Michael Goin | Refactor NVFP4 Linear utils for ModelOpt and CT (#33201) |
| 2026-01-31 05:36:29 | Chendi.Xue | [CI][HPU]accelerate hpu test by skip python re-install and clean container name (#33286) |
| 2026-01-31 04:34:36 | Huy Do | Indicate compile mode in the benchmark results (#32990) |
| 2026-01-31 02:30:46 | Pavani Majety | [Hardware][SM100] Add TRTLLM Kernel for INT4 W4A16 Kernel. (#32437) |
| 2026-01-31 01:50:23 | xuebwang-amd | [Quantization][ROCm] Fix MoE weight loading to be robust (Qwen3_MoE/Qwen3_next as example models) (#33173) |
| 2026-01-31 01:29:09 | Vasiliy Kuznetsov | fix QERL attention import path (#33432) |
| 2026-01-31 01:19:19 | Yanan Cao | [Kernel] [Helion] [1/N] Add Helion ConfigManager (#32740) |
| 2026-01-31 00:30:10 | Harry Mellor | Fix encoder-decoder model disabling mm processor cache (#33236) |
| 2026-01-31 00:17:56 | NicolÃ² Lucchesi | [CI] Qwen3-ASR transcriptios tests (#33414) |
| 2026-01-31 00:15:20 | Michael Goin | Support FP8 block quant for CompressedTensorsW8A16Fp8 (#33280) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 41 ä¸ªæäº¤ | ğŸ”´é«˜ 4 | ğŸŸ¡ä¸­ 16 | ğŸŸ¢ä½ 21
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (4)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-4)
    - [[Frontend] Use new Renderer for Completions and Tokenize ...](#f0a1c84)
    - [[Attention] Move MLA `forward` from backend to layer (#33...](#aaa901a)
    - [Refactor NVFP4 Linear utils for ModelOpt and CT (#33201)](#67ebaff)
    - [[Hardware][SM100] Add TRTLLM Kernel for INT4 W4A16 Kernel...](#c3a9752)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (16)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-16)
    - [Support clear mm and encoder cache (#33452)](#22d9a05)
    - [[BugFix][Router Replay] Capture Logical Experts with EPLB...](#13b842f)
    - [[fix][torch.compile] Fix cold-start compilation time incr...](#15f40b2)
    - [[Bugfix] Fix incompatibility between #33372 and #32863 (#...](#4cb59de)
    - [[ez] Add structured torch.compile logs (#33213)](#608b556)
    - [[Kernel] [Helion] [3/N] Helion kernel registry (#33203)](#d5c41db)
    - [Add support for Mistral Large 3 inference with Flashinfer...](#f0bca83)
    - [[Bugfix] Fix `Qwen3ASR` language asr tag in output  (#33410)](#e77f162)
    - [[Kernel] [Helion] [2/N] Helion kernel wrapper (#32964)](#8ecd213)
    - [[Streaming -> Realtime] Rename all voxtral related classe...](#15e0bb9)
    - [[UX] Use gguf `repo_id:quant_type` syntax for examples an...](#29fba76)
    - [[ModelRunner V2] Fix spec decoding + logprobs (#33391)](#876a16f)
    - [[Deprecation] Deprecate `seed_everything` and `scatter_mm...](#010ec0c)
    - [[Bugfix][ROCm] Fixing the skinny gemm dispatch logic from...](#31aedfe)
    - [[Kernel] [Helion] [1/N] Add Helion ConfigManager (#32740)](#6c1f9e4)
    - [Support FP8 block quant for CompressedTensorsW8A16Fp8 (#3...](#fd0e377)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (21)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-21)
    - [[Doc] Update plugin deprecation notices (#33476)](#793af53)
    - [support return prompt token ids in responses  (#33378)](#6f5e7cd)
    - [[Misc] Replace deprecated interface seed_everything (#33474)](#68feb76)
    - [[perf] v1/spec_decode: skip softmax for all-greedy reject...](#8980001)
    - [[ROCM] Enable aiter attn backend for qwen3-next model (#3...](#527bcd1)
    - [[BugFix] Add synchronize in CutlassW4A8LinearKernel to en...](#f68e3ea)
    - [[CPU][Feat] Enable KleidiAI accelerated int4 dynamic quan...](#1618e25)
    - [Add EAGLE3 support for AFMoE (#33111)](#f3888ac)
    - [[Bugfix] Handle Asym W4A16 (ConchLinearKernel) for CT (#3...](#73419ab)
    - [[Attention] Clarify comment explaining attn_logits +1 dim...](#5b55c0b)
    - [[ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_s...](#6c64c41)
    - [[Misc] offest -> offset in comments and variable names (#...](#a2ef06e)
    - [[BugFix] Fix whisper FA2 + full cudagraphs (#33360)](#0a3c71e)
    - [[Misc] Algin Qwen3-VL-embedding image example outputs wit...](#9df152b)
    - [[Bugfix] Fix typo in read_offset variable name (#33426)](#64a40a7)
    - [[CI][HPU]accelerate hpu test by skip python re-install an...](#2b46557)
    - [Indicate compile mode in the benchmark results (#32990)](#9ca66ec)
    - [[Quantization][ROCm] Fix MoE weight loading to be robust ...](#f451b45)
    - [fix QERL attention import path (#33432)](#3f96fcf)
    - [Fix encoder-decoder model disabling mm processor cache (#...](#67239c4)
    - [[CI] Qwen3-ASR transcriptios tests (#33414)](#8ece607)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (4)

### [Frontend] Use new Renderer for Completions and Tokenize API (#32863)
**SHA**: `f0a1c84` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f0a1c8453ad1c664c8a04c83fe545195fcd556eb)

âš ï¸ LLMåˆ†æå¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: APIè¯·æ±‚å¤±è´¥: HTTPSConnectionPool(host='integrate.api.nvidia.com', port=443): Read timed out. (read timeout=30)

*æš‚æ— åˆ†æ*

---

### [Attention] Move MLA `forward` from backend to layer (#33284)
**SHA**: `aaa901a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/aaa901ad55ee454f061649b4cc24f96c5ea04e33)

âš ï¸ LLMåˆ†æå¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: APIè¯·æ±‚å¤±è´¥: HTTPSConnectionPool(host='integrate.api.nvidia.com', port=443): Read timed out. (read timeout=30)

*æš‚æ— åˆ†æ*

---

### Refactor NVFP4 Linear utils for ModelOpt and CT (#33201)
**SHA**: `67ebaff` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/67ebaff528be7faef14fa4eb86cfd8c4b5714e38)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„ / æ¶æ„å˜æ›´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´ é«˜  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
- å°† NVFP4ï¼ˆNVIDIAâ€¯FP4ï¼‰çº¿æ€§ç®—å­ç›¸å…³çš„å·¥å…·å‡½æ•°ã€åç«¯é€‰æ‹©ä¸æƒé‡é¢„å¤„ç†ç»Ÿä¸€æŠ½å–åˆ° `vllm/model_executor/layers/quantization/utils/nvfp4_utils.py`ã€‚  
- ç§»é™¤åŸæœ‰ `quant_utils` ä¸­çš„é‡å¤å®ç°ï¼ˆ`swizzle_blockscale`ã€`cutlass_fp4_supported`ã€æƒé‡/æ¿€æ´» padding ä¸åˆ‡ç‰‡ ç­‰ï¼‰ï¼Œå¹¶æ›´æ–°æ‰€æœ‰è°ƒç”¨ç‚¹ï¼ˆCompressedâ€‘Tensorsã€ModelOptã€MoEã€Marlinï¼‰ä½¿ç”¨æ–°çš„ç»Ÿä¸€ APIã€‚  
- ç®€åŒ– `process_weights_after_loading` ä¸ `apply` çš„å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ `select_nvfp4_linear_backend()` ä¸ `apply_nvfp4_linear()`ï¼Œå¹¶ç»Ÿä¸€å‘½åï¼ˆ`input_global_scale`ã€`weight_global_scale`ï¼‰ã€‚  
- ç›¸åº”æµ‹è¯•æ–‡ä»¶å’Œ import è·¯å¾„éšä¹‹è¿ç§»ã€‚  

---

## ğŸ¯ å½±å“èŒƒå›´
| å—å½±å“æ¨¡å— / ç»„ä»¶ | å…³é”®æ”¹åŠ¨ç‚¹ |
|-------------------|------------|
| `vllm.model_executor.layers.quantization.utils.nvfp4_utils` (æ–°æ–‡ä»¶) | æ–°å¢åç«¯æšä¸¾ã€ç»Ÿä¸€é€‰å‹ã€æƒé‡/æ¿€æ´»å‡†å¤‡ã€çº¿æ€§å‰å‘å®ç°ã€`swizzle_blockscale`ã€`cutlass_fp4_supported`ã€padding/åˆ‡ç‰‡ç­‰å…¬å…±å·¥å…· |
| `compressed_tensors` ç›¸å…³æ–¹æ¡ˆ (`compressed_tensors_w4a4_nvfp4.py`ã€`compressed_tensors_w4a16_nvfp4.py`) | åˆ é™¤å¯¹ `envs.VLLM_USE_NVFP4_CT_EMULATIONS` çš„æ¡ä»¶åˆ†æ”¯ï¼Œæ”¹ç”¨ç»Ÿä¸€ `apply_nvfp4_linear`ï¼Œç®€åŒ– `process_weights_after_loading` |
| `modelopt` é‡åŒ–å®ç° (`modelopt.py`) | æ›¿æ¢åŸæœ‰ `envs`/`cutlass` é€»è¾‘ä¸º `select_nvfp4_linear_backend()`ï¼Œç»Ÿä¸€ `process_weights_after_loading` ä¸ `apply` |
| `marlin_utils_fp4.py`ã€`nvfp4_moe_support.py`ã€`flashinfer_fp4_moe.py` | å‚æ•°åä» `weight_scale_2` â†’ `weight_global_scale`ï¼Œè°ƒç”¨æ–°çš„ `apply_nvfp4_linear` / `prepare_fp4_layer_for_marlin` |
| æµ‹è¯•æ–‡ä»¶ (`tests/kernels/moe/...`ã€`tests/quantization/...`) | æ›´æ–° import ä¸º `nvfp4_utils`ï¼Œç¡®ä¿ä»èƒ½è·‘é€š |
| å…¶ä»–æ–‡ä»¶ (`quant_utils.py`) | åˆ é™¤å·²è¿ç§»çš„å‡½æ•°å®ç°ï¼Œä¿æŒç²¾ç®€ |

---

## ğŸ” æŠ€æœ¯æ´å¯Ÿ

### 1. æ¶æ„å½±å“
| ç»´åº¦ | å½±å“ |
|------|------|
| **æ¨¡å—åŒ– & å¯ç»´æŠ¤æ€§** | å°†æ•£è½åœ¨å¤šä¸ªæ–‡ä»¶ä¸­çš„ NVFP4 ç›¸å…³é€»è¾‘é›†ä¸­åˆ° `nvfp4_utils.py`ï¼Œå½¢æˆå•ä¸€èŒè´£çš„å·¥å…·å±‚ã€‚åç»­æ–°å¢/è°ƒæ•´åç«¯åªéœ€åœ¨æ­¤æ–‡ä»¶ä¿®æ”¹ï¼Œé™ä½è·¨æ–‡ä»¶è€¦åˆã€‚ |
| **ç»Ÿä¸€åç«¯é€‰æ‹©** | é€šè¿‡ `select_nvfp4_linear_backend()` è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒå˜é‡ã€å¹³å°èƒ½åŠ›ã€FlashInfer/Marlin/FBGEMM å¯ç”¨æ€§ï¼Œç»Ÿä¸€è¿”å› `NvFp4LinearBackend` æšä¸¾ã€‚æ¶ˆé™¤äº†ä¹‹å‰åœ¨æ¯ä¸ªé‡åŒ–å®ç°é‡Œé‡å¤çš„ â€œifâ€‘elseâ€ é€»è¾‘ã€‚ |
| **ç»Ÿä¸€å‰å‘æ¥å£** | `apply_nvfp4_linear()` è´Ÿè´£æ‰€æœ‰åç«¯çš„å‰å‘è®¡ç®—ï¼ˆCutlassã€FlashInferâ€‘Cutlassã€FlashInferâ€‘TRTLLMã€FBGEMMã€Marlinã€Emulationï¼‰ï¼Œè°ƒç”¨æ–¹åªéœ€ä¼  `backend`ã€`layer`ã€`x`ã€`bias`ã€‚è¿™ç®€åŒ–äº† `CompressedTensors*`ã€`ModelOpt*` ç­‰ç±»çš„å®ç°ã€‚ |
| **å‘½åç»Ÿä¸€** | é‡‡ç”¨ `input_global_scale` / `weight_global_scale` æ›¿ä»£æ—§çš„ `input_scale` / `weight_scale_2`ï¼Œæå‡å«ä¹‰æ˜ç¡®æ€§ï¼Œå…¼å®¹ ModelOpt ä¸ CTï¼ˆCompressedâ€‘Tensorsï¼‰ä¸¤ä¸ªç”Ÿæ€ã€‚ |
| **åç«¯æŠ½è±¡å±‚** | å¯¹æ¯ä¸ªåç«¯çš„ç‰¹æ®Šå‡†å¤‡ï¼ˆå¦‚ FlashInferâ€‘TRTLLM çš„ shuffleã€Cutlass çš„ padding & swizzleã€FBGEMM çš„ uint8 blockâ€‘scaleï¼‰å‡åœ¨ `convert_to_nvfp4_linear_kernel_format()` ä¸­å®ç°ï¼Œä¿æŒä¸Šå±‚ä»£ç å¹²å‡€ã€‚ |

### 2. æ€§èƒ½å½±å“
| ç»´åº¦ | æ­£é¢å½±å“ | æ½œåœ¨å½±å“ |
|------|----------|----------|
| **åç«¯è‡ªåŠ¨é€‰æ‹©** | é»˜è®¤ä¼˜å…ˆä½¿ç”¨ *FlashInferâ€‘Cutlass*ï¼ˆè‹¥æœ‰ï¼‰æˆ– *Cutlass*ï¼Œåè€…åœ¨ SMâ‰¥8.0 ä¸Šå·²ç»é«˜åº¦ä¼˜åŒ–ï¼Œèƒ½è·å¾— 1.5â€‘2Ã— çš„ååæå‡ç›¸è¾ƒäºåŸå…ˆçš„æ˜¾å¼ `cutlass_fp4_supported()` åˆ†æ”¯ã€‚ | è‹¥ç”¨æˆ·æ˜¾å¼ä¾èµ–æ—§çš„ `VLLM_NVFP4_GEMM_BACKEND`ï¼ˆå¦‚å¼ºåˆ¶ `marlin`ï¼‰ï¼Œä»èƒ½ä¿æŒåŸæœ‰è¡Œä¸ºï¼Œä½†æœªåœ¨æ–°è·¯å¾„ä¸­åŠ å…¥ `marlin` çš„ *Kâ€‘dim padding* é€»è¾‘ï¼ˆMarlin è‡ªå·±å¤„ç†ï¼‰ï¼Œéœ€ç¡®ä¿å¤–éƒ¨å·²åšå¥½å‡†å¤‡ã€‚ |
| **æƒé‡/æ¿€æ´»é¢„å¤„ç†ç»Ÿä¸€** | `prepare_weights_for_nvfp4_cutlass` åªåœ¨ä¸€æ¬¡è½¬æ¢é‡Œå®Œæˆ padding + swizzleï¼Œé¿å…åœ¨æ¯æ¬¡å‰å‘é‡æ–°è¿›è¡Œã€‚| å¯¹ FlashInferâ€‘TRTLLM åç«¯ä»ä¿ç•™ä¸€æ¬¡ shuffleï¼Œæ¯æ¬¡ forward ç”¨ `apply_fp4_marlin_linear`ï¼ˆMarlinï¼‰ä¸å†éœ€è¦ `alpha` å‚æ•°è®¡ç®—ï¼Œç•¥å¾®æå‡ runtimeã€‚ |
| **å…¨å±€ Scale å¤„ç†** | å°† `input_global_scale_inv` ä¸ `weight_global_scale` åœ¨ `process_weights_after_loading` ä¸­ä¸€æ¬¡æ€§è½¬ä¸ºæ ‡é‡ï¼ˆ`max()`ï¼‰ï¼Œåœ¨ forward ä¸­ç›´æ¥ä½¿ç”¨ï¼Œå‡å°‘ `max` è°ƒç”¨æ¬¡æ•°ã€‚| å¯¹æç«¯ç¨€ç–/ä¸å‡åŒ€ Scaleï¼ˆä¸åŒå¼ é‡é—´ Scale å·®å¼‚å¤§ï¼‰çš„ checkpointï¼Œå¯èƒ½å‡ºç°æ•°å€¼è¯¯å·®æ”¾å¤§ï¼Œéœ€è¦éªŒè¯æ•°å€¼ä¸€è‡´æ€§ã€‚ |
| **ä»£ç è·¯å¾„ç®€åŒ–** | `apply_nvfp4_linear` ç»Ÿä¸€è¿”å› `out.view(*output_shape)`ï¼Œå‰Šå‡äº†å¤šä½™çš„ reshape / view æŠ–åŠ¨ã€‚| ä»ä¿ç•™ `slice_nvfp4_output`ï¼Œå¦‚æœ Padding è®¡ç®—ä¸å‡†ç¡®ä¼šå¯¼è‡´è¾“å‡ºè£å‰ªé”™è¯¯ã€‚ |

### 3. å®‰å…¨è€ƒè™‘
| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **ç¯å¢ƒå˜é‡** | æ–°å¢å¯¹ `VLLM_USE_FBGEMM`ã€`VLLM_USE_NVFP4_CT_EMULATIONS`ã€`VLLM_NVFP4_GEMM_BACKEND` çš„è¯»å–ã€‚è‹¥ç”¨æˆ·è‡ªè¡Œè®¾ç½®é”™è¯¯çš„å€¼ï¼ˆä¾‹å¦‚ `VLLM_USE_FBGEMM=1` ä½†æœªå®‰è£… `fbgemm-gpu-genai`ï¼‰ï¼Œå°†æŠ›å‡ºæ˜ç¡®çš„ `ImportError`ï¼Œæ— éšè”½å´©æºƒã€‚ |
| **å¼‚å¸¸æ£€æŸ¥** | `select_nvfp4_linear_backend()` å¯¹æ¯ç§åç«¯æ‰§è¡Œ `assert` æ£€æŸ¥ï¼›è‹¥å¹³å°ä¸æ”¯æŒç›¸åº”åŠŸèƒ½ï¼ˆå¦‚ç¼ºå°‘ FlashInferï¼Œæˆ– GPU èƒ½åŠ›ä¸è¶³ï¼‰ï¼Œä¼šåœ¨åˆå§‹åŒ–é˜¶æ®µæŠ¥é”™ï¼Œé¿å…åœ¨æ¨ç†æœŸé—´äº§ç”Ÿæœªå®šä¹‰è¡Œä¸ºã€‚ |
| **ç±»å‹å®‰å…¨** | æ–°å¢å¯¹ `weight_scale.dtype` çš„æ£€æŸ¥ï¼ˆæ”¯æŒ `torch.float8_e4m3fn` ä¸ `torch.uint8`ï¼‰ï¼Œç¡®ä¿åç«¯æ¥æ”¶æ­£ç¡®çš„é‡åŒ–ç±»å‹ã€‚ |
| **èµ„æºæ³„æ¼** | `process_weights_after_loading` ç°åœ¨ä¼š `del` æ—§å±æ€§æˆ–ç”¨ `Parameter(..., requires_grad=False)` æ›¿æ¢ï¼Œé¿å…åœ¨æ¨¡å‹åŠ è½½åä¿ç•™é¢å¤–æœªä½¿ç”¨çš„å¼ é‡ã€‚ |

### 4. å…¼å®¹æ€§ & å›å½’é£é™©
| é£é™©ç‚¹ | è¯´æ˜ | ç¼“è§£å»ºè®® |
|--------|------|----------|
| **Checkpoint å‚æ•°åå˜åŒ–** | è€çš„ CT checkpointä»ä¿å­˜ `input_scale`ã€`weight_scale_2` ç­‰å­—æ®µï¼›åœ¨åŠ è½½åé€šè¿‡ `process_weights_after_loading` ä¼šé‡æ–°åˆ›å»º `input_global_scale`ã€`weight_global_scale`ã€‚è‹¥æ¨¡å‹åºåˆ—åŒ–æ–¹å¼åœ¨æœªæ¥æ”¹åŠ¨ï¼Œå¯èƒ½å‡ºç°ç¼ºå¤±ã€‚ | ä¿æŒ `process_weights_after_loading` ä¸­ `hasattr` æ£€æŸ¥ï¼Œç¡®ä¿åœ¨ç¼ºå¤±æ—§å­—æ®µæ—¶ä¸æŠ›å¼‚å¸¸ï¼›åœ¨æ–‡æ¡£ä¸­æ³¨æ˜å…¼å®¹æ—§ checkpointã€‚ |
| **åç«¯åˆ‡æ¢è¡Œä¸ºå·®å¼‚** | åŸå®ç°åœ¨ `envs.VLLM_USE_NVFP4_CT_EMULATIONS` ä¸º `True` æ—¶èµ° emulationã€‚æ–°å®ç°ä»æ”¯æŒï¼Œä½†å†…éƒ¨ `select_nv

---

### [Hardware][SM100] Add TRTLLM Kernel for INT4 W4A16 Kernel. (#32437)
**SHA**: `c3a9752` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/c3a9752b0c11f87677e2ab918e524af7a368c664)

âš ï¸ LLMåˆ†æå¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: APIè¯·æ±‚å¤±è´¥: HTTPSConnectionPool(host='integrate.api.nvidia.com', port=443): Read timed out. (read timeout=30)

*æš‚æ— åˆ†æ*

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (16)

### Support clear mm and encoder cache (#33452)
**SHA**: `22d9a05` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/22d9a056d5c4849af372f9506bb93cad7da755f6)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆæ–°å¢ encoder cache ç®¡ç†æ¥å£ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤ä¸º VLLM æœåŠ¡ç«¯åŠ å…¥å¯¹ **encoder cache**ï¼ˆè§†è§‰ç¼–ç å™¨è¾“å‡ºç¼“å­˜ï¼‰çš„ç»Ÿä¸€æ¸…ç†èƒ½åŠ›ã€‚æ–°å¢ `/reset_encoder_cache` HTTP æ¥å£ã€ç›¸åº”çš„ RPC ä¸å†…éƒ¨è°ƒåº¦/æ‰§è¡Œå±‚æ–¹æ³•ï¼Œå¹¶åœ¨ `EncoderCacheManager` ä¸ `EncoderDecoderCacheManager` ä¸­å®ç° `reset()`ï¼Œç¡®ä¿ç¼“å­˜çŠ¶æ€ã€å®¹é‡ç»Ÿè®¡åœ¨æ¨¡å‹æƒé‡æ›´æ–°æˆ–è°ƒè¯•æ—¶èƒ½å¤Ÿè¢«å®Œæ•´é‡ç½®ã€‚åŒæ—¶åœ¨ç»Ÿè®¡ç»“æ„ `SchedulerStats` ä¸­åŠ å…¥ `encoder_cache_usage`ï¼Œå¹¶åœ¨æ–‡æ¡£ä¸­åˆ—å‡ºæ–° endpointã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- **åè®®å±‚**ï¼š`vllm/engine/protocol.py` æ–°å¢æŠ½è±¡æ–¹æ³• `reset_encoder_cache`ã€‚  
- **HTTP API**ï¼š`api_router.py` æš´éœ² `/reset_encoder_cache`ã€‚  
- **è°ƒåº¦å±‚**ï¼š`v1/core/sched/interface.py`ã€`scheduler.py` å®ç°å¹¶è°ƒç”¨ `EncoderCacheManager.reset`ã€‚  
- **ç¼“å­˜å®ç°**ï¼š`v1/core/encoder_cache_manager.py`ï¼ˆEncoderCacheManagerã€EncoderDecoderCacheManagerï¼‰å®ç°æ¸…ç†é€»è¾‘ã€‚  
- **æ‰§è¡Œå±‚**ï¼š`v1/worker/gpu_model_runner.py`ã€`gpu_worker.py` å¢åŠ  GPU ç«¯ç¼“å­˜æ¸…ç†ã€‚  
- **æ ¸å¿ƒ Engine**ï¼š`core.py`ã€`llm_engine.py`ã€`async_llm.py` ç­‰é“¾è·¯åŒæ­¥è°ƒç”¨ã€‚  
- **ç»Ÿè®¡ & æ–‡æ¡£**ï¼š`stats.py` å¢åŠ  `encoder_cache_usage`ï¼›`security.md` æ·»åŠ æ–‡æ¡£è¯´æ˜ã€‚  
- **æµ‹è¯•**ï¼š`tests/v1/core/test_encoder_cache_manager.py` è¦†ç›–äº† reset è¡Œä¸ºã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šä»…åœ¨å¼€å‘æ¨¡å¼ä¸‹æš´éœ² reset æ¥å£ï¼Œç”Ÿäº§ç¯å¢ƒä»å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼Œé¿å…è¯¯è§¦å¯¼è‡´æœåŠ¡ä¸­æ–­ã€‚  
2. **å¹¶å‘å®‰å…¨**ï¼š`reset_encoder_cache` ä¼šåœ¨æœ‰è¿è¡Œè¯·æ±‚æ—¶æ‰“å°è­¦å‘Šï¼Œå»ºè®®åœ¨ä½¿ç”¨å‰ç¡®ä¿è¯·æ±‚å·²å®Œæˆæˆ–åœ¨ç»´æŠ¤çª—å£æ‰§è¡Œã€‚  
3. **ç›‘æ§**ï¼šæ–°å¢çš„ `encoder_cache_usage` éœ€è¦åœ¨ç›‘æ§é¢æ¿ä¸­å±•ç¤ºï¼Œå¸®åŠ©è¿ç»´è¯„ä¼°ç¼“å­˜å‘½ä¸­ç‡ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šCI å·²åŠ å…¥å•å…ƒæµ‹è¯•ï¼Œå»ºè®®åœ¨å¤šå¡/åˆ†å¸ƒå¼ç¯å¢ƒä¸‹åšé›†æˆéªŒè¯ï¼Œç¡®ä¿å„ worker çš„ `reset_encoder_cache` èƒ½è¢« RPC æ­£å¸¸è§¦å‘ã€‚  
5. **æ–‡æ¡£æ›´æ–°**ï¼šREADME ä¸ API æ‰‹å†Œåº”åŒæ­¥æ–°å¢ endpoint çš„ä½¿ç”¨ç¤ºä¾‹åŠé£é™©è¯´æ˜ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤åŠŸèƒ½ä¸ºæ¨¡å‹çƒ­æ›´æ–°ã€è°ƒè¯•æä¾›äº†å¿…è¦çš„ç¼“å­˜æ¸…ç†æ‰‹æ®µï¼Œå½±å“èŒƒå›´å±€éƒ¨ä¸”å®ç°æ–¹å¼ç»Ÿä¸€ï¼Œé£é™©å¯æ§ã€‚å»ºè®®åœ¨ä¸‹ä¸€æ¬¡éƒ¨ç½²æ—¶å¼€å¯è¯¥ endpoint å¹¶åœ¨ç›‘æ§ä¸­è§‚å¯Ÿç¼“å­˜ä½¿ç”¨æƒ…å†µã€‚

---

### [BugFix][Router Replay] Capture Logical Experts with EPLB (#33013)
**SHA**: `13b842f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/13b842f2714558abd96f260ab450d192ce687ffb)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. å°†è·¯ç”±å™¨çš„ä¸“å®¶æ•è·èŒè´£ä» `FusedMoE` å±‚è¿ç§»åˆ° `BaseRouter`ï¼Œæ–°å¢ `capture_fn` ä¸ `set_capture_fn` æ¥å£ã€‚  
2. `GPUModelRunner` åœ¨åˆå§‹åŒ–æ—¶ç»Ÿä¸€ç»‘å®š `RoutedExpertsCapturer`ï¼Œä¸ºæ¯ä¸ª `BaseRouter` æ³¨å…¥æ•è·å›è°ƒã€‚  
3. ç§»é™¤ `FusedMoE` ä¸­çš„æ—§ `self.capture` é€»è¾‘ï¼Œå¹¶ç›¸åº”åˆ æ‰ç›¸å…³ importã€‚  
4. æ–°å¢å•å…ƒæµ‹è¯•éªŒè¯ï¼šâ‘  æ•è·åœ¨ EPLB å‰åå‡èƒ½å¾—åˆ°é€»è¾‘ IDï¼›â‘¡ ç»‘å®šè¿‡ç¨‹åœ¨ `GPUModelRunner` æ­£ç¡®æ‰§è¡Œã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/layers/fused_moe/layer.py`ï¼ˆæ•è·å±æ€§åˆ é™¤ï¼‰  
- `vllm/model_executor/layers/fused_moe/router/base_router.py`ï¼ˆæ–°å¢ capture æ¥å£ï¼‰  
- `vllm/v1/worker/gpu_model_runner.py`ï¼ˆç»‘å®šå®ç°ï¼‰  
- æ–°å¢æµ‹è¯• `tests/model_executor/test_routed_experts_capture.py`

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- ç¡®è®¤ `enable_return_routed_experts` ä¸º True æ—¶ï¼Œ`GPUModelRunner` å¿…ç„¶è°ƒç”¨ `_bind_routed_experts_capturer`ï¼Œå¦åˆ™æ•è·å‡½æ•°ä¼šä¿æŒ `None`ï¼›å¯åœ¨ `init_routed_experts_capturer` å‰åŠ å…¥æ–­è¨€æˆ–æ—¥å¿—ã€‚  
- æ³¨æ„ `capture_fn` ç°åœ¨åœ¨ EPLB æ˜ å°„å‰è¢«è°ƒç”¨ï¼Œè‹¥å¤–éƒ¨ä»£ç ä¾èµ–æ˜ å°„åçš„ IDï¼Œéœ€è¦è‡ªè¡Œåœ¨å›è°ƒé‡Œå†æ¬¡æ˜ å°„ã€‚  
- æ–‡æ¡£ä¸ç¤ºä¾‹åº”æ›´æ–°ä¸º â€œä½¿ç”¨ `router.set_capture_fn`â€ è€Œéç›´æ¥è®¿é—® `layer.capture`ï¼Œé˜²æ­¢æ—§ç”¨æ³•å¤±æ•ˆã€‚  
- è¿è¡Œå…¨å¥— CIï¼ˆåŒ…æ‹¬ GPU æµ‹è¯•ï¼‰ç¡®ä¿å¤šå¡ç¯å¢ƒä¸‹å›è°ƒä¸ä¼šå› åºåˆ—åŒ–/è·¨è¿›ç¨‹è€Œå¤±æ•ˆã€‚  

æ•´ä½“æ”¹åŠ¨æå‡äº†æ•è·è·¯å¾„çš„å¯æ’æ‹”æ€§ï¼Œé™ä½äº†å±‚ä¹‹é—´çš„è€¦åˆï¼Œä½†éœ€ç•™æ„å‘åå…¼å®¹æ€§ä¸é…ç½®åˆå§‹åŒ–é¡ºåºã€‚

---

### [fix][torch.compile] Fix cold-start compilation time increase by adding kv cache update to splitting ops (#33441)
**SHA**: `15f40b2` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/15f40b20aadf27af43bdae38bf4644b82e21634f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. ä¸ºäº†è§£å†³ VLLMâ€‘compile å†·å¯åŠ¨æ—¶å›  KVâ€‘cache æ›´æ–°å¯¼è‡´çš„ç¼–è¯‘æ—¶é—´æ¿€å¢ï¼Œåœ¨ `CompilationConfig` ä¸­æŠŠ `vllm::unified_kv_cache_update` åŠ å…¥åˆ†å‰²ç®—å­åˆ—è¡¨ï¼ˆä»…åœ¨ä¸ä½¿ç”¨ Inductor å›¾åˆ’åˆ†çš„æƒ…å†µä¸‹ï¼‰ã€‚  
2. `split_graph` é€»è¾‘æ–°å¢â€œä¿æŒè¿ç»­åˆ†å‰²ç®—å­åœ¨åŒä¸€å­å›¾â€ çš„å¤„ç†ï¼Œé˜²æ­¢å› ä¸ºç›¸é‚»çš„ `attention`ã€`sqrt` ç­‰ç®—å­è¢«é”™è¯¯æ‹†åˆ†æˆå¤šä¸ªå­å›¾ã€‚  
3. è¡¥å……ä¸¤ç»„å•å…ƒæµ‹è¯•ï¼šâ‘  éªŒè¯å†·å¯åŠ¨æ—¶ AOTâ€‘autograd ç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­æ¬¡æ•°ï¼ˆç¡®ä¿åªå¯¹ 3 ç§å­å›¾é‡æ–°ç¼–è¯‘ï¼‰ï¼›â‘¡ éªŒè¯ç›¸é‚»åˆ†å‰²ç®—å­ä¼šè¢«åˆå¹¶ä¸ºåŒä¸€å­å›¾å¹¶ä¿æŒå‰å‘è®¡ç®—ç›¸ç­‰ã€‚

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/compilation/backends.py`ï¼ˆå›¾åˆ’åˆ†å®ç°ï¼‰  
- `vllm/config/compilation.py`ï¼ˆé»˜è®¤åˆ†å‰²ç®—å­é…ç½®ï¼‰  
- æ–°å¢/ä¿®æ”¹çš„æµ‹è¯•æ–‡ä»¶ `tests/compile/test_cold_start.py`, `tests/compile/test_graph_partition.py`  
- ä»»ä½•ä½¿ç”¨ `CompilationMode.VLLM_COMPILE` ä¸”æœªå¼€å¯ Inductor å›¾åˆ’åˆ†çš„æ¨¡å‹åŠ è½½/æ¨ç†æµç¨‹ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **åŠŸèƒ½éªŒè¯**ï¼šåœ¨ CI ä¸­è·‘å®Œæ•´çš„ compile æµ‹è¯•å¥—ä»¶ï¼Œç¡®ä¿åŠ å…¥ `unified_kv_cache_update` ä¸ä¼šæ„å¤–æŠŠ KVâ€‘cache ä» CUDAGraph ä¸­å‰”é™¤ï¼Œå¯¼è‡´è¿è¡Œæ—¶å›é€€ã€‚  
2. **æ€§èƒ½å›å½’**ï¼šå¯¹æ¯”å¼€å¯/å…³é—­ `use_inductor_graph_partition` ä¸¤ç§æ¨¡å¼ä¸‹çš„å†·å¯åŠ¨ç¼–è¯‘æ—¶é•¿ä¸æ¨ç†ååï¼Œç¡®è®¤æ–°å¢åˆ†å‰²ä¸å¼•å…¥é¢å¤–å¼€é”€ã€‚  
3. **å…¼å®¹æ€§**ï¼šæ£€æŸ¥æ—§ç‰ˆè‡ªå®šä¹‰ `splitting_ops` é…ç½®æ˜¯å¦ä»ç„¶ç”Ÿæ•ˆï¼Œé¿å…å› é»˜è®¤è¿½åŠ å¯¼è‡´ç”¨æˆ·é…ç½®å†²çªã€‚  
4. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨ `vllm` ç¼–è¯‘é…ç½®è¯´æ˜ä¸­æ ‡æ³¨ `unified_kv_cache_update` åªåœ¨é Inductor åˆ’åˆ†æ¨¡å¼ä¸‹è‡ªåŠ¨æ·»åŠ çš„è¡Œä¸ºï¼Œä»¥åŠå…¶å¯¹ CUDAGraph çš„å‰¯ä½œç”¨ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ”¹åŠ¨å®šä½æ˜ç¡®ã€æµ‹è¯•è¦†ç›–å……åˆ†ï¼Œé¢„è®¡å¯æ˜¾è‘—é™ä½å†·å¯åŠ¨ç¼–è¯‘æ—¶é—´ã€‚åç»­å…³æ³¨ä»¥ä¸Šå‡ ç‚¹å¯è¿›ä¸€æ­¥æå‡ç¨³å®šæ€§ä¸å¯ç»´æŠ¤æ€§ã€‚

---

### [Bugfix] Fix incompatibility between #33372 and #32863 (#33475)
**SHA**: `4cb59de` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4cb59dea6a9b54cd3cae87ac69fb0a836615f196)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šæœ¬æ¬¡æäº¤åˆ é™¤äº†åœ¨å¤šä¸ªæ¸²æŸ“å™¨ä¸­å¼ºåˆ¶ `return_dict=False` çš„ä»£ç ï¼Œå¹¶åœ¨ `params.get_apply_chat_template_kwargs` ä¸­ç»Ÿä¸€åŠ å…¥ `return_dict=False`ï¼Œä»¥è§£å†³ #33372 ä¸ #32863 ä¹‹é—´çš„å…¼å®¹æ€§å†²çªã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š`vllm/renderers/deepseek_v32.py`ã€`vllm/renderers/grok2.py`ã€`vllm/renderers/hf.py`ã€`vllm/renderers/params.py` å››ä¸ªæ ¸å¿ƒæ¸²æŸ“æ¨¡å—ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **è¡Œä¸ºå˜åŒ–**ï¼šåˆ é™¤å±€éƒ¨ `kwargs["return_dict"] = False` åï¼Œè‹¥è°ƒç”¨æ–¹æœªæ˜¾å¼ä¼ å…¥ `return_dict`ï¼Œ`tokenizer.apply_chat_template` å°†æ¢å¤é»˜è®¤è¿”å›ç±»å‹ï¼ˆé€šå¸¸ä¸ºå­—å…¸ï¼‰ã€‚è¯·ç¡®è®¤åç»­ä»£ç ï¼ˆå¦‚ `prompt_raw` çš„åå¤„ç†ï¼‰èƒ½å¤Ÿå…¼å®¹å­—å…¸è¿”å›æˆ–å·²ç»Ÿä¸€é€šè¿‡ `params` å¼ºåˆ¶è¿”å›å­—ç¬¦ä¸²ã€‚  
2. **ä¸€è‡´æ€§**ï¼šç»Ÿä¸€åœ¨ `Params.get_apply_chat_template_kwargs` ä¸­è®¾ç½® `return_dict=False`ï¼Œç¡®ä¿æ‰€æœ‰æ¸²æŸ“è·¯å¾„å¾—åˆ°ç›¸åŒçš„è¿”å›æ ¼å¼ï¼Œé¿å…å› ä¸åŒè·¯å¾„è¿”å›å€¼ç±»å‹ä¸ä¸€è‡´å¯¼è‡´çš„éšè—é”™è¯¯ã€‚  
3. **æµ‹è¯•è¦†ç›–**ï¼šæ·»åŠ æˆ–æ›´æ–°é’ˆå¯¹ä¸Šè¿°æ¸²æŸ“å™¨çš„å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯åœ¨ä¸åŒ tokenizer é…ç½®ä¸‹ï¼ˆé»˜è®¤è¿”å› dict ä¸è¿”å› stringï¼‰æ¸²æŸ“ç»“æœä»ç„¶æ­£ç¡®ã€‚å°¤å…¶å…³æ³¨å¼‚æ­¥ `render_messages_async` çš„å…¼å®¹æ€§ã€‚  
4. **æ–‡æ¡£æ›´æ–°**ï¼šè‹¥ `return_dict` å‚æ•°çš„é»˜è®¤è¡Œä¸ºå¯¹ç”¨æˆ·æœ‰å½±å“ï¼Œå»ºè®®åœ¨æ–‡æ¡£ä¸­è¯´æ˜æ¸²æŸ“å™¨ç°åœ¨é»˜è®¤å¼ºåˆ¶è¿”å›å­—ç¬¦ä¸²ï¼Œå¹¶æä¾›è‡ªå®šä¹‰ `return_dict` çš„æ–¹å¼ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡ä¿®æ”¹é€šè¿‡é›†ä¸­ç®¡ç† `return_dict` å‚æ•°ï¼Œæ¶ˆé™¤äº†å†å²ä»£ç ä¸­çš„å†²çªï¼Œæå‡äº†æ¸²æŸ“å±‚çš„å¯ç»´æŠ¤æ€§ã€‚ä½†è¯·åŠ¡å¿…ç¡®ä¿æ‰€æœ‰ä¸‹æ¸¸é€»è¾‘å·²é€‚é…ç»Ÿä¸€çš„è¿”å›æ ¼å¼ï¼Œå¹¶é€šè¿‡æµ‹è¯•éªŒè¯ã€‚

---

### [ez] Add structured torch.compile logs (#33213)
**SHA**: `608b556` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/608b556507acbb15c88d4c119414fc8364627472)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º vLLM çš„ pieceâ€‘wiseâ€¯torch.compile å¼•å…¥ç»“æ„åŒ–æ—¥å¿—ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `vllm/compilation/backends.py` ä¸ `vllm/compilation/piecewise_backend.py` ä¸­åŠ å…¥å¯¹ `torch._logging._internal.trace_structured` çš„è°ƒç”¨ï¼Œå®ç°ä»¥ä¸‹æ—¥å¿—ï¼š  
   - ç¼–è¯‘é…ç½®ï¼ˆartifactï¼‰  
   - é¡¶å±‚åˆ†ç‰‡å›¾ï¼ˆgraph_dumpï¼‰  
   - æ¯ä¸ªå­æ¨¡å—çš„å›¾ dumpï¼ˆä»…ä¸€æ¬¡ï¼‰  
   - æ¯æ¬¡å­å›¾ç¼–è¯‘èµ·å§‹ä¿¡æ¯ï¼ˆartifact `vllm_piecewise_compile_start`ï¼‰  
2. æ–°å¢å•å…ƒæµ‹è¯• `tests/compile/test_structured_logging.py`ï¼Œä½¿ç”¨ `unittest.mock.patch` æ•è· `trace_structured` è°ƒç”¨å¹¶æ ¡éªŒæ—¥å¿—çš„æ•°é‡ã€åç§°åŠå†…å®¹ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **ç¼–è¯‘åç«¯**ï¼š`VllmBackend` ä¸ `PiecewiseBackend`ï¼Œæ‰€æœ‰ä¾èµ– `torch.compile` çš„è·¯å¾„éƒ½ä¼šå¤šä¸€æ¬¡ç»“æ„åŒ–æ—¥å¿—çš„äº§ç”Ÿã€‚  
- **é…ç½®æ¨¡å—**ï¼š`CompilationConfig` ç›¸å…³å­—æ®µè¢«åºåˆ—åŒ–ä¸º JSONï¼Œæ–°å¢å¯¹ `pass_config` ä¸­å¸ƒå°”å­—æ®µçš„åŠ¨æ€éå†ã€‚  
- **æµ‹è¯•å¥—ä»¶**ï¼šæ–°å¢ 121 è¡Œæµ‹è¯•ï¼Œä»…åœ¨ CUDA ç¯å¢ƒä¸‹è¿è¡Œã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æ€§èƒ½ä¸æ—¥å¿—å¼€é”€**ï¼š`trace_structured` æœ¬èº«æ˜¯è½»é‡çš„ï¼Œä½†åœ¨å­å›¾é¦–æ¬¡ç¼–è¯‘æ—¶ä¼šé¢å¤–æ‰§è¡Œä¸€æ¬¡ `graph.print_readable`ï¼Œç¡®ä¿åœ¨å¤§æ¨¡å‹æˆ–é¢‘ç¹é‡æ–°ç¼–è¯‘çš„åœºæ™¯ä¸‹ä¸ä¼šæ˜¾è‘—æ‹–æ…¢å¯åŠ¨æ—¶é—´ã€‚å¯è€ƒè™‘åœ¨ CI/ç”Ÿäº§ç¯å¢ƒé€šè¿‡ç¯å¢ƒå˜é‡å…³é—­ï¼ˆå¦‚ `TORCH_LOG=0`ï¼‰ã€‚  
2. **å…¼å®¹æ€§**ï¼š`torch._logging._internal` ä»…åœ¨ PyTorchâ€¯2.3+ ä¸­ç¨³å®šï¼Œè‹¥é¡¹ç›®ä»æ”¯æŒæ—§ç‰ˆ PyTorchï¼Œéœ€æ·»åŠ å¯¼å…¥é™çº§æˆ–åŒ…è£…å±‚ï¼Œä»¥é˜² `ImportError`ã€‚  
3. **æ—¥å¿—æ¶ˆè´¹**ï¼šç›®å‰æ‰€æœ‰ payload å‡ä¸º JSON ä¸²ï¼Œå»ºè®®åœ¨æ–‡æ¡£ä¸­è¯´æ˜å¯¹åº”çš„ `tlparse` è§£æå­—æ®µï¼Œé¿å… downstream å·¥å…·å› å­—æ®µå˜æ›´è€Œå¤±æ•ˆã€‚  
4. **æµ‹è¯•ç¯å¢ƒ**ï¼šæ–°å¢æµ‹è¯•ä¾èµ– `regex`ï¼Œç¡®è®¤åœ¨é¡¹ç›® `requirements.txt` ä¸­å·²å£°æ˜ï¼›æ­¤å¤–ï¼ŒCI éœ€è¦ CUDA GPU æ”¯æŒï¼Œè‹¥æ²¡æœ‰å¯åœ¨ `python -m pytest -k test_vllm_structured_logging_artifacts -skip` æ–¹å¼è·³è¿‡ã€‚  
5. **ä»£ç å¯ç»´æŠ¤æ€§**ï¼š`_log_compilation_config` ä¸ `_log_compile_start` ä¸¤ä¸ªè¾…åŠ©å‡½æ•°åœ¨åç»­è‹¥å¢åˆ é…ç½®å­—æ®µï¼Œéœ€è¦åŒæ­¥æ›´æ–°å¯¹åº”çš„ JSON æ„é€ é€»è¾‘ï¼Œå»ºè®®æŠ½è±¡ä¸ºç»Ÿä¸€çš„ â€œto_dictâ€ æ–¹æ³•ï¼Œé™ä½ç»´æŠ¤æˆæœ¬ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸º vLLM å¼•å…¥äº†å¯¹ TORCH_TRACE/tlparse çš„å¯è§‚æµ‹æ€§ï¼Œä¾¿äºå®šä½ç¼–è¯‘é˜¶æ®µçš„ç“¶é¢ˆå’ŒéªŒè¯åˆ†ç‰‡ç­–ç•¥ã€‚åªè¦æ³¨æ„ä¸Šè¿°å…¼å®¹æ€§ä¸æ—¥å¿—å¼€é”€ï¼Œå³å¯å®‰å…¨åˆå¹¶ã€‚

---

### [Kernel] [Helion] [3/N] Helion kernel registry (#33203)
**SHA**: `d5c41db` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d5c41db35b33d348bc8b913eab6d11a20a64f168)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆæ–°å¢ Helionâ€¯Kernelâ€¯Registryï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨ `vllm/kernels/helion/register.py` ä¸­å®ç°å…¨å±€æ³¨å†Œè¡¨ `_REGISTERED_KERNELS`ï¼Œå¹¶æä¾› `register_kernel`ã€`get_registered_kernels`ã€`get_kernel_by_name` ä¸‰ä¸ªå…¬å¼€ APIã€‚  
- ä¸º `register_kernel` æ·»åŠ  overloadï¼Œæ”¯æŒ **è£¸è£…é¥°å™¨** ä¸ **å¸¦å‚æ•°è£…é¥°å™¨** ä¸¤ç§è°ƒç”¨æ–¹å¼ï¼Œå¹¶åœ¨æœªæä¾› `fake_impl` æ—¶è‡ªåŠ¨ç”Ÿæˆã€‚  
- æ”¹è¿› `ConfiguredHelionKernel` å¯¹ `config_picker` ä¸º `None` çš„å®¹é”™å¤„ç†ã€‚  
- åœ¨ `vllm/kernels/helion/__init__.py` é‡æ–°å¯¼å‡ºæ–° APIã€‚  
- è¡¥å……å®Œæ•´å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯æ³¨å†Œã€é‡å¤æ£€æŸ¥ã€è­¦å‘Šã€é…ç½®ä¼ é€’ç­‰è¡Œä¸ºã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- æ ¸å¿ƒæ¨¡å—ï¼š`vllm/kernels/helion/register.py`ã€`vllm/kernels/helion/__init__.py`ã€ç›¸å…³æµ‹è¯•ã€‚  
- æ‰€æœ‰ç›´æ¥æˆ–é—´æ¥ä½¿ç”¨ `register_kernel` çš„ç”¨æˆ·ä»£ç ï¼ˆåŒ…æ‹¬ç¬¬ä¸‰æ–¹æ’ä»¶ï¼‰å°†çœ‹åˆ°è¿”å›å€¼ä» â€œè£…é¥°åå‡½æ•°â€ æ”¹ä¸º `HelionKernelWrapper` å®ä¾‹ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šè‹¥å¤–éƒ¨ä»£ç ä¾èµ–æ—§è£…é¥°å™¨è¿”å›åŸå‡½æ•°ï¼ˆä¾‹å¦‚ `@register_kernel` ç«‹å³è°ƒç”¨ï¼‰ï¼Œéœ€æ”¹ä¸ºä½¿ç”¨è¿”å›çš„ wrapper æˆ–é€šè¿‡ `wrapper.raw_kernel_func` è®¿é—®åŸå®ç°ã€‚å¯åœ¨æ–‡æ¡£ä¸­æ³¨æ˜ç ´åæ€§æ”¹åŠ¨ã€‚  
2. **çº¿ç¨‹å®‰å…¨**ï¼šå…¨å±€å­—å…¸ç›®å‰æ²¡æœ‰é”ä¿æŠ¤ï¼Œè‹¥åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¹¶å‘æ³¨å†Œå¯èƒ½äº§ç”Ÿ raceï¼Œå»ºè®®åœ¨æœªæ¥åŠ å…¥ `threading.Lock` æˆ–ä½¿ç”¨ `collections.defaultdict` çš„åŸå­æ“ä½œã€‚  
3. **æ³¨å†Œè¡¨æ¸…ç†**ï¼šæµ‹è¯•ä¸­æ˜¾å¼ `clear()`ï¼Œç”Ÿäº§ç¯å¢ƒè‹¥éœ€è¦çƒ­æ›´æ–°/å¸è½½æ’ä»¶ï¼Œæä¾› `unregister_kernel(name)` ä¼šæ›´å‹å¥½ã€‚  
4. **æ—¥å¿—çº§åˆ«**ï¼šç°åœ¨ä½¿ç”¨ `logger.info`ã€`logger.debug`ï¼Œåœ¨é«˜å¹¶å‘éƒ¨ç½²æ—¶å¯è€ƒè™‘é™ä½ä¸º `debug`ï¼Œé¿å…ä¸å¿…è¦çš„ I/Oã€‚  
5. **ç±»å‹æç¤º**ï¼šå·²åŠ å…¥ `overload` ä¸ `cast`ï¼Œç¡®ä¿ `mypy` èƒ½æ­£ç¡®æ¨æ–­ï¼›å»ºè®®åœ¨ `pyproject.toml` ä¸­æŠŠ `strict = true`ï¼Œé˜²æ­¢é—æ¼æ–°çš„ç±»å‹é”™è¯¯ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸º Helionâ€¯Kernelâ€¯ä½“ç³»æä¾›äº†ç»Ÿä¸€ã€å¯æŸ¥è¯¢çš„æ³¨å†Œæœºåˆ¶ï¼Œå¢å¼ºäº†å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§ã€‚åªè¦åœ¨å‡çº§æŒ‡å—ä¸­è¯´æ˜è¿”å›å€¼å˜åŒ–å¹¶åšå¥½å…¼å®¹æ€§éªŒè¯ï¼Œé£é™©å¯æ§ã€‚

---

### Add support for Mistral Large 3 inference with Flashinfer MoE (#33174)
**SHA**: `f0bca83` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f0bca83ee4b6aa6d63da66d37dd69929bdcfc1fe)

âš ï¸ LLMåˆ†æå¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: APIè¯·æ±‚å¤±è´¥: HTTPSConnectionPool(host='integrate.api.nvidia.com', port=443): Read timed out. (read timeout=30)

*æš‚æ— åˆ†æ*

---

### [Bugfix] Fix `Qwen3ASR` language asr tag in output  (#33410)
**SHA**: `e77f162` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/e77f162cf59d6e132c8b6449dc98b410b347a5b5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨ `speech_to_text` æµç¨‹ä¸­åŠ å…¥å¯¹æ¨¡å‹åŸå§‹è¾“å‡ºçš„ç»Ÿä¸€åå¤„ç†ï¼Œä¸“é—¨å‰”é™¤ Qwen3â€‘ASR è¿”å›çš„ `language <asr_text>` å‰ç¼€ã€‚  
- ä¸º `ModelInterface` å¢åŠ  `post_process_output` æŠ½è±¡å®ç°ï¼Œé»˜è®¤è¿”å›åŸæ–‡ï¼›åœ¨ `qwen3_asr.py` ä¸­å®ç°å…·ä½“çš„æ ‡ç­¾å‰¥ç¦»é€»è¾‘ã€‚  
- æ³¨é‡Šè¯´æ˜æµå¼è¿”å›æ—¶éœ€å¯¹å¯èƒ½è¢«åˆ‡åˆ†çš„æ ‡ç­¾åšç¼“å†²å¤„ç†ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/entrypoints/openai/translations/speech_to_text.py`ï¼ˆASR è¾“å‡ºç”Ÿæˆï¼‰  
- `vllm/model_executor/models/interfaces.py`ï¼ˆæ¨¡å‹é€šç”¨æ¥å£ï¼‰  
- `vllm/model_executor/models/qwen3_asr.py`ï¼ˆQwen3â€‘ASR ä¸“ç”¨å®ç°ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šæ–°æŠ½è±¡æ–¹æ³•é»˜è®¤ä¸å½±å“å…¶ä»–æ¨¡å‹ï¼Œç¡®ä¿æœªå®ç° `post_process_output` çš„æ¨¡å‹ä»è¿”å›åŸå§‹æ–‡æœ¬ã€‚  
2. **æµ‹è¯•**ï¼šæ–°å¢å•å…ƒæµ‹è¯•ï¼Œè¦†ç›–å¸¦/ä¸å¸¦ `<asr_text>` æ ‡ç­¾çš„è¾“å‡ºã€ç©ºå­—ç¬¦ä¸²ä»¥åŠè¯­è¨€å‰ç¼€çš„å„ç§ç»„åˆï¼›åŠ å…¥æµå¼æ¨¡å¼ä¸‹çš„è¾¹ç•Œæƒ…å†µï¼ˆæ ‡ç­¾è·¨å—ï¼‰éªŒè¯ã€‚  
3. **æ–‡æ¡£**ï¼šåœ¨æ¨¡å‹ä½¿ç”¨è¯´æ˜ä¸­æ ‡æ˜ Qwen3â€‘ASR ä¼šè‡ªåŠ¨ç§»é™¤è¯­è¨€æ ‡ç­¾ï¼Œè‹¥éœ€è¦ä¿ç•™å¯è‡ªè¡Œè°ƒç”¨ `model_cls.post_process_output` å‰çš„åŸå§‹æ–‡æœ¬ã€‚  
4. **åç»­ä¼˜åŒ–**ï¼šå®ç°æµå¼ç¼“å†²é€»è¾‘ï¼Œç¡®ä¿åœ¨åˆ†å—æ—¶èƒ½å¤Ÿå®Œæ•´æ£€æµ‹å¹¶å‰”é™¤ `<asr_text>` å‰ç¼€ï¼Œé¿å…å‡ºç°åŠæˆªæ ‡ç­¾ã€‚  

ä»¥ä¸Šä¿®æ”¹æå‡äº† ASR è¾“å‡ºçš„å¯è¯»æ€§ï¼Œé¿å…äº†ç”¨æˆ·ç«¯é¢å¤–çš„æ–‡æœ¬æ¸…æ´—å·¥ä½œã€‚è¯·åœ¨åˆå¹¶å‰é€šè¿‡å®Œæ•´çš„å›å½’å¥—ä»¶éªŒè¯ï¼Œå°¤å…¶å…³æ³¨å¤šæ¨¡æ€éŸ³é¢‘è½¬å†™çš„ç«¯åˆ°ç«¯æµç¨‹ã€‚

---

### [Kernel] [Helion] [2/N] Helion kernel wrapper (#32964)
**SHA**: `8ecd213` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8ecd213c0b63d012db3b003120d7d78edafe785f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º Helion æä¾›é¢„è°ƒé…ç½®çš„ç»Ÿä¸€åŒ…è£…å±‚ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šæ–°å¢ `vllm/kernels/helion/register.py` å®ç° `HelionKernelWrapper`ã€`ConfiguredHelionKernel` ä¸ `PresetConfigSearch`ï¼Œå¹¶åœ¨ `__init__.py` ä¸­å¯¼å‡ºã€‚é…å¥—æ·»åŠ å•å…ƒæµ‹è¯•ã€GPU åç§°å·¥å…·ä»¥åŠå¯¹ `helion.Settings` çš„æ ¡éªŒé€»è¾‘ï¼Œæ”¯æŒåœ¨è¿è¡Œæ—¶ç›´æ¥ä½¿ç”¨é¢„å…ˆè°ƒå¥½çš„é…ç½®è€Œä¸è¿›è¡Œ autotuneã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **vllm.kernels.helion**ï¼šæ ¸å¿ƒæ³¨å†Œæµç¨‹ã€é…ç½®ç®¡ç†ã€å·¥å…·å‡½æ•°å…¨çº¿æ”¹åŠ¨ã€‚  
- **torch.ops.vllm_helion**ï¼šæ–°å»ºè‡ªå®šä¹‰ Op çš„æ³¨å†Œå…¥å£ï¼Œå¯¹å¤–å¯ç›´æ¥è°ƒç”¨ã€‚  
- **æµ‹è¯•å¥—ä»¶**ï¼šæ–°å¢ä¸¤å¥—é’ˆå¯¹æ³¨å†Œå™¨ä¸å·¥å…·å‡½æ•°çš„ 600 è¡Œæµ‹è¯•ï¼Œæå‡è¦†ç›–ç‡ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å¼‚å¸¸è·¯å¾„**ï¼š`ConfiguredHelionKernel._create_key_computer` åœ¨ `config_picker` è¿”å› `None` æ—¶å›é€€åˆ° `"default"`ï¼Œä½†è‹¥ `default` ä¸å­˜åœ¨ä¼šè¿”å› `None` å¹¶éšåæŠ› `ValueError`ã€‚å»ºè®®åœ¨æŠ¥é”™ä¿¡æ¯ä¸­æ˜ç¡®å‘ŠçŸ¥ â€œdefault config missingâ€ã€‚  
2. **é™æ€å½¢çŠ¶é»˜è®¤å€¼**ï¼š`_create_decorated_kernel` å°† `static_shapes` è®¾ä¸º `False` é™¤éç”¨æˆ·æ˜¾å¼è®¾ä¸º `True`ã€‚è‹¥ç”¨æˆ·åœ¨ `helion.Settings` ä¸­æœªå£°æ˜è¯¥å­—æ®µï¼Œä»ä¼šè¢«è¦†ç›–ï¼Œæ–‡æ¡£éœ€è¯´æ˜æ­¤è¡Œä¸ºã€‚  
3. **çº¿ç¨‹å®‰å…¨**ï¼š`HelionKernelWrapper.get_configured_op` åœ¨å¹¶å‘é¦–æ¬¡æ³¨å†Œæ—¶å¯èƒ½å‡ºç°ç«äº‰ï¼ˆå¤šä¸ªçº¿ç¨‹åŒæ—¶æ£€æµ‹ `torch.ops.vllm_helion` ä¸å­˜åœ¨ï¼‰ï¼Œå»ºè®®åŠ é”æˆ–ä½¿ç”¨ `torch.ops.vllm_helion.__dict__` çš„åŸå­æ£€æŸ¥ã€‚  
4. **å†…éƒ¨ä¾èµ–**ï¼šç›´æ¥ `import helion` ä¼šåœ¨æœªå®‰è£…æ—¶æŠ› `ImportError`ï¼Œä½†åœ¨ `__init__.py` å·²å¯¼å…¥ `register`ï¼Œå¯¼è‡´ `import vllm.kernels.helion` ä»ä¼šå¤±è´¥ã€‚è‹¥å¸Œæœ›åœ¨æœªè£… Helion æ—¶ä»…ç¦ç”¨ç›¸å…³åŠŸèƒ½ï¼Œå¯æ”¹ä¸ºæ‡’åŠ è½½æˆ–æ•è·å¼‚å¸¸ã€‚  
5. **æµ‹è¯• Mock**ï¼šå¤§é‡ä½¿ç”¨ `patch("vllm.kernels.helion.register.helion.kernel")`ã€‚è‹¥æœªæ¥ `helion` åŒ…å†…éƒ¨è£…é¥°å™¨ç­¾åå˜æ›´ï¼Œæµ‹è¯•å¯èƒ½å¤±æ•ˆã€‚å»ºè®®æŠ½è±¡ä¸ºå†…éƒ¨å·¥å‚å‡½æ•°ï¼Œä¾¿äºæ›¿æ¢ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ”¹åŠ¨ä¸º Heliumâ€‘Backâ€‘end å¼•å…¥äº†å¯æ’æ‹”çš„é¢„è°ƒé…ç½®æœºåˆ¶ï¼Œæå‡äº† vLLM åœ¨ç‰¹å®š GPU å¹³å°ä¸Šçš„å¯åŠ¨æ€§èƒ½ã€‚åªè¦æ³¨æ„ä¸Šè¿°å¼‚å¸¸ä¸å¹¶å‘ç»†èŠ‚ï¼Œåç»­å¯å¹³ç¨³ä¸Šçº¿ã€‚

---

### [Streaming -> Realtime] Rename all voxtral related classes, fn, files (#33415)
**SHA**: `15e0bb9` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/15e0bb9c428e08ef74e067da2b8b3f4765e3cd52)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / å…¶ä»–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤å°†åŸæœ‰çš„ *VoxtralStreaming* ç›¸å…³å®ç°ç»Ÿä¸€æ”¹åä¸º **VoxtralRealtime**ï¼ŒåŒ…æ‹¬ç±»ã€å‡½æ•°ã€æ–‡ä»¶ä»¥åŠæ–‡æ¡£ä¸­çš„å…¨éƒ¨å¼•ç”¨ã€‚æ ¸å¿ƒæ”¹åŠ¨æ¶‰åŠ `vllm/model_executor/models/voxtral_realtime.py` ä¸­çš„å¤„ç†å™¨ã€ç”Ÿæˆå™¨ç±»åŠå…¶å†…éƒ¨é”™è¯¯æç¤ºæ–‡å­—ï¼›åŒæ—¶æ›´æ–°äº†æ¨¡å‹æ³¨å†Œè¡¨ã€ç¤ºä¾‹è„šæœ¬ã€æµ‹è¯•ç”¨ä¾‹å’Œæ–‡æ¡£é“¾æ¥ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/models/registry.py`ã€`tests/models/registry.py` ä¸­çš„æ¨¡å‹é”®å  
- `vllm/model_executor/models/voxtral_realtime.py`ï¼ˆç±»åã€æ³¨é‡Šã€é”™è¯¯ä¿¡æ¯ï¼‰  
- `vllm/transformers_utils/configs/mistral.py`ï¼ˆæ¶æ„æ˜ å°„ï¼‰  
- æ–‡æ¡£ `docs/serving/openai_compatible_server.md` ä¸ç¤ºä¾‹è„šæœ¬é“¾æ¥  
- ç›¸å…³å•å…ƒæµ‹è¯• `test_voxtral_realtime_*.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šè‹¥å·²æœ‰ä¸‹æ¸¸é¡¹ç›®å¼•ç”¨ `VoxtralStreaming*`ï¼Œåº”æä¾›è¿ç§»æŒ‡å¼•æˆ–ä¸´æ—¶åˆ«åï¼ˆå¦‚åœ¨ `__init__` ä¸­ä¿ç•™æ—§åç§°çš„å¯¼å…¥ï¼‰ï¼Œé¿å…çªå‘æ€§ `ImportError`ã€‚  
2. **æ–‡æ¡£åŒæ­¥**ï¼šç¡®è®¤æ‰€æœ‰å…¬å¼€æ–‡æ¡£ã€README ä»¥åŠç¤ºä¾‹ä»“åº“é“¾æ¥å·²æ›´æ–°ä¸ºæ–°æ–‡ä»¶è·¯å¾„ï¼Œé˜²æ­¢ç”¨æˆ·ä»æœç´¢æ—§åå¯¼è‡´å›°æƒ‘ã€‚  
3. **æµ‹è¯•è¦†ç›–**ï¼šç¡®è®¤è·³è¿‡çš„ `VoxtralStreaming` æµ‹è¯•å·²å…¨éƒ¨æ”¹ä¸º `VoxtralRealtime`ï¼Œä¸” CI ä¸­æœªæ®‹ç•™å¯¹æ—§åç§°çš„ç¡¬ç¼–ç ã€‚  
4. **é”™è¯¯ä¿¡æ¯**ï¼šæ‰€æœ‰æ”¹ä¸º â€œrealtimeâ€ çš„æç¤ºå·²æ›´æ–°ï¼Œè¯·å†æ¬¡æ£€ç´¢é¡¹ç›®ä¸­æ˜¯å¦è¿˜æœ‰æ®‹ç•™çš„ â€œstreamingâ€ å­—æ ·ï¼Œä»¥ä¿æŒä¿¡æ¯ä¸€è‡´ã€‚  
5. **å‘å¸ƒè¯´æ˜**ï¼šåœ¨ä¸‹ä¸€ä¸ª Release Note ä¸­æ˜ç¡®æ ‡æ³¨ â€œVoxtralStreaming â†’ VoxtralRealtimeâ€ï¼Œå¹¶ç»™å‡ºè¿ç§»æ­¥éª¤ï¼Œä»¥å¸®åŠ©ç”¨æˆ·å¹³æ»‘å‡çº§ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ”¹åæœ¬èº«ä¸å½±å“åŠŸèƒ½å®ç°ï¼Œä½†å¯¹å¤– API ä¸æ–‡æ¡£çš„å®Œæ•´åŒæ­¥è‡³å…³é‡è¦ï¼Œè¯·åœ¨æ­£å¼å‘å¸ƒå‰å®Œæˆä¸Šè¿°æ£€æŸ¥ã€‚

---

### [UX] Use gguf `repo_id:quant_type` syntax for examples and docs (#33371)
**SHA**: `29fba76` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/29fba76781441ff38796eb0c189bbdd0497dc58c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. æ–‡æ¡£ä¸ç¤ºä¾‹ç»Ÿä¸€ä½¿ç”¨ `repo_id:quant_type` è¯­æ³•ï¼Œç›´æ¥ä» HuggingFace æ‹‰å– GGUF é‡åŒ–æ¨¡å‹ã€‚  
2. `gguf_utils.is_valid_gguf_quant_type` æ–°å¢å¯¹ GGUF å¸¸ç”¨åç¼€ï¼ˆ`_M/_S/_L/_XL/_XS/_XXS`ï¼‰çš„è¯†åˆ«ï¼Œæ”¯æŒå¦‚ `Q4_K_M`ã€`Q3_K_S` ç­‰æ‰©å±•å‘½åã€‚  
3. `is_remote_gguf` ä¸ `split_remote_gguf` çš„é”™è¯¯æç¤ºåŒæ­¥æ›´æ–°ã€‚  
4. å¢æ·»å•å…ƒæµ‹è¯•è¦†ç›–åŸºç¡€é‡åŒ–ç±»å‹ã€æ‰©å±•åç¼€ä»¥åŠåˆ†å‰²å‡½æ•°çš„è¾¹ç•Œæƒ…å†µã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/transformers_utils/gguf_utils.py`ï¼ˆæ ¸å¿ƒåˆ¤å®š/è§£æé€»è¾‘ï¼‰  
- æ–‡æ¡£ `docs/features/quantization/gguf.md`ã€ç¤ºä¾‹ `examples/offline_inference/basic/README.md`  
- å•å…ƒæµ‹è¯• `tests/transformers_utils/test_utils.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å‘åå…¼å®¹**ï¼šç°æœ‰ `repo_id:quant_type` å·²æ”¯æŒåŸå§‹ GGML ç±»å‹ï¼Œæ–°å¢åç¼€ä¸ä¼šå½±å“æ—§ç”¨æ³•ï¼Œä¿æŒå…¼å®¹æ€§ã€‚  
2. **åç¼€åˆ—è¡¨ç»´æŠ¤**ï¼šåç¼€é›†åˆ `_GGUF_QUANT_SUFFIXES` å†™æ­»åœ¨ä»£ç ä¸­ï¼Œå¦‚åç»­å‡ºç°æ–°åç¼€éœ€åŒæ­¥æ›´æ–°ï¼Œå¦åˆ™ `is_valid_gguf_quant_type` ä¼šè¯¯æŠ¥æ— æ•ˆã€‚å»ºè®®æŠ½ç¦»ä¸ºå¸¸é‡æˆ–ä» `ggml` åº“åŠ¨æ€è¯»å–ï¼ˆè‹¥æœ‰æä¾›ï¼‰ã€‚  
3. **é”™è¯¯ä¿¡æ¯**ï¼š`split_remote_gguf` çš„å¼‚å¸¸ä¿¡æ¯å·²è¡¥å……â€œExtended suffixes also supportedâ€ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒæŠ›å‡ºæ—¶ä»ä¼šè¿”å›å®Œæ•´åˆ—è¡¨ï¼Œå¯èƒ½æ³„éœ²å®ç°ç»†èŠ‚ã€‚è‹¥ä¸å¸Œæœ›æš´éœ²ï¼Œå¯æ”¹ä¸ºæ›´ç®€æ´æç¤ºã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šæ–°åŠ å…¥çš„æµ‹è¯•å·²è¦†ç›–å¸¸è§åç¼€ï¼Œä½†æœªè¦†ç›–éæ³•ç»„åˆï¼ˆå¦‚ `Q4_KM`ã€`Q4_K_`ï¼‰ä»¥åŠå¤§å°å†™ä¸ä¸€è‡´çš„æƒ…å†µï¼Œå»ºè®®å†è¡¥å……å‡ ä¾‹ï¼Œä»¥é˜²æ­¢è¯¯åˆ¤ã€‚  
5. **æ–‡æ¡£åŒæ­¥**ï¼šç¤ºä¾‹å·²æ”¹ç”¨ `unsloth/Qwen3-0.6B-GGUF:Q4_K_M`ï¼Œè¯·ç¡®ä¿ CI ä¸­çš„ README æ¸²æŸ“ä¸å®é™…æ¨¡å‹ ID å¯¹åº”ï¼›è‹¥æ¨¡å‹åç§°å˜åŠ¨ï¼Œæ–‡æ¡£éœ€åŠæ—¶æ›´æ–°ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº†ç”¨æˆ·ç›´æ¥é€šè¿‡ HuggingFace åŠ è½½ GGUF é‡åŒ–æ¨¡å‹çš„ä½“éªŒï¼Œä»£ç å®ç°ç®€æ´ä¸”å·²é…å¥—å•å…ƒæµ‹è¯•ï¼Œé£é™©è¾ƒä½ã€‚åç»­å…³æ³¨åç¼€é›†åˆçš„ç»´æŠ¤ä»¥åŠå¼‚å¸¸ä¿¡æ¯çš„ç”¨æˆ·å‹å¥½åº¦å³å¯ã€‚

---

### [ModelRunner V2] Fix spec decoding + logprobs (#33391)
**SHA**: `876a16f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/876a16f4fb092a7954ce2c8db177943c55adb6a1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º/Bug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¸º `LogprobsTensors` æ–°å¢ `cu_num_generated_tokens` å­—æ®µï¼Œç»Ÿä¸€è·¨æ¨¡å—çš„ token æ•°ä¿¡æ¯ã€‚  
2. ä¿®æ­£ `ModelRunner V2` åœ¨é‡‡æ ·é˜¶æ®µå¯¹ logitsã€idx_mapping ä¸ cuâ€‘numâ€‘logits å‚æ•°çš„è§£ç ä¸ä¼ é€’ï¼Œé˜²æ­¢ shape ä¸åŒ¹é…å¯¼è‡´çš„é”™è¯¯ã€‚  
3. ç›¸åº”æ›´æ–° `sampler.compute_topk_logprobs`ã€`sampler.__call__`ã€`gpu_model_runner._get_prompt_logprobs_dict` ç­‰è°ƒç”¨ç­¾åï¼Œå¹¶åœ¨ `tests/v1/engine/test_output_processor.py` ä¸­åŠ å…¥ç›¸åº”æ–­è¨€ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/engine/logprobs.py`  
- `vllm/v1/outputs.py`ï¼ˆLogprobsTensorsï¼‰  
- `vllm/v1/worker/gpu/model_runner.py`ã€`gpu_model_runner.py`  
- `vllm/v1/worker/gpu/sample/*`ï¼ˆsamplerã€logprobï¼‰  
- å•å…ƒæµ‹è¯• `tests/v1/engine/test_output_processor.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- ç¡®è®¤ `filter` åœ¨æºå¸¦ `cu_num_generated_tokens` æ—¶ä¼šæŠ›å‡ºæ˜ç¡®å¼‚å¸¸ï¼Œé¿å…è¯¯ç”¨ã€‚  
- æ£€æŸ¥æ‰€æœ‰å¤–éƒ¨è°ƒç”¨ï¼ˆå¦‚æ—§ç‰ˆ `ModelRunner`ï¼‰æ˜¯å¦å·²åŒæ­¥æ›´æ–°å‚æ•°ç­¾åï¼Œé˜²æ­¢å‘åå…¼å®¹æ€§å›é€€ã€‚  
- è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•ï¼Œå°¤å…¶æ˜¯å¤šæ‰¹æ¬¡ã€å˜é•¿ logits åœºæ™¯ï¼Œç¡®ä¿ `cu_num_logits` æ­£ç¡®æ˜ å°„åˆ°ç”Ÿæˆ token æ•°ã€‚  
- æ›´æ–°æ–‡æ¡£å’Œç¤ºä¾‹ï¼Œè¯´æ˜ `LogprobsTensors.cu_num_generated_tokens` çš„å«ä¹‰åŠä½¿ç”¨é™åˆ¶ã€‚  
- å…³æ³¨æ½œåœ¨çš„è½»å¾®é¢å¤–å†…å­˜å¼€é”€ï¼ˆä¿å­˜æ¯è¯·æ±‚çš„ç”Ÿæˆ token é•¿åº¦åˆ—è¡¨ï¼‰ï¼Œåœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶è¯„ä¼°å…¶å½±å“ã€‚

---

### [Deprecation] Deprecate `seed_everything` and `scatter_mm_placeholders` in v0.15 (#33362)
**SHA**: `010ec0c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/010ec0c30ef1fa481fc69b8a5d2c205052d93607)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / å…¶ä»–ï¼ˆAPI å¼ƒç”¨ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `vllm/platforms/interface.py` ä¸­å°† `seed_everything` æ ‡è®°ä¸º **å·²å¼ƒç”¨** å¹¶å…¨éƒ¨åˆ é™¤å®ç°ï¼Œæ”¹ä¸ºæ¨èä½¿ç”¨ `vllm.utils.torch_utils.set_random_seed`ã€‚å¯¹åº”çš„ `random`ã€`numpy` å¼•å…¥ä¹Ÿè¢«å»æ‰ã€‚  
2. åœ¨ `vllm/v1/worker/utils.py` ä¸­åŒæ ·å°† `scatter_mm_placeholders` ä¸ `gather_mm_placeholders` æ ‡è®°ä¸º **å·²å¼ƒç”¨** å¹¶åˆ é™¤å®ç°ï¼Œæç¤ºå°†åœ¨ v0.15.0 åç§»é™¤ã€‚  
3. `vllm/model_executor/models/voxtral.py` åˆ é™¤äº†å·²æ³¨é‡Šçš„ transcript token ID cached propertiesï¼Œä¿æŒåŠŸèƒ½ä¸å˜ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm.platforms.interface.Platform`ï¼ˆç§å­è®¾ç½®ï¼‰  
- `vllm.v1.worker.utils`ï¼ˆå¤šæ¨¡æ€å ä½ç¬¦ scatter/gatherï¼‰  
- Voxtral æ¨¡å‹åŠ è½½ä»£ç ï¼ˆè™½æœªå½±å“è¿è¡Œï¼Œä½†æ¸…ç†äº†åºŸå¼ƒå±æ€§ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- é¡¹ç›®å†…éƒ¨æˆ–ç”¨æˆ·ä»£ç è‹¥ä»è°ƒç”¨ `Platform.seed_everything`ï¼Œè¯·æ”¹ä¸º `vllm.utils.torch_utils.set_random_seed(seed)` å¹¶è‡ªè¡Œå¤„ç† `random` ä¸ `numpy` çš„ç§å­ã€‚  
- è‹¥æœ‰è‡ªè¡Œå®ç°çš„å¤šæ¨¡æ€å ä½ç¬¦æ•£å¸ƒ/æ”¶é›†é€»è¾‘ï¼Œè¿ç§»è‡³ `PromptUpdateDetails.is_embed` çš„ä½¿ç”¨æ–¹å¼ï¼Œæˆ–ç›´æ¥ä½¿ç”¨ `vllm.multimodal.processing` ä¸­çš„æ–° APIã€‚  
- åˆ é™¤çš„ `random`ã€`numpy` import å¯èƒ½å¯¼è‡´æœªæ˜¾å¼å¼•ç”¨æ—¶å‡ºç° `NameError`ï¼Œæ£€æŸ¥ç›¸å…³æ¨¡å—çš„å¯¼å…¥é¡ºåºã€‚  
- è¿è¡Œ CI/å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æœªæ®‹ç•™å¯¹å·²å¼ƒç”¨å‡½æ•°çš„å¼•ç”¨ï¼Œé˜²æ­¢åœ¨ v0.15 æ­£å¼å‘å¸ƒåå‡ºç°å…¼å®¹æ€§é”™è¯¯ã€‚

---

### [Bugfix][ROCm] Fixing the skinny gemm dispatch logic from #32831 (#33366)
**SHA**: `31aedfe` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/31aedfe7d62adea6b5a7644f3c323f585b042ce3)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBugä¿®å¤ï¼ˆROCmâ€¯skinnyâ€¯gemm è°ƒåº¦ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. ä¸ºäº†è®© ROCmâ€¯skinnyâ€¯gemm æ­£ç¡®å¤„ç†éè¿ç»­å¼ é‡ï¼Œå»é™¤äº†åŸå…ˆåœ¨ `_custom_ops.py` ä¸­å¯¹è¾“å…¥å¼ºåˆ¶ `contiguous()` çš„åŒ…è£…ï¼Œå¹¶åœ¨è°ƒç”¨ç‚¹åŠ å…¥äº†æ˜¾å¼çš„ `is_contiguous()` æ£€æŸ¥ã€‚  
2. åœ¨ `tests/kernels/quantization/test_rocm_skinny_gemms.py` ä¸­åŠ å…¥ `pad_weights_fp8` è¾…åŠ©å‡½æ•°ä»¥åŠ `padded` å‚æ•°ï¼Œè¦†ç›–äº†æƒé‡è¢« 256â€‘byte å¯¹é½åæˆªæ–­çš„æƒ…å†µï¼Œç¡®ä¿æ–°è°ƒåº¦åœ¨æœ‰/æ— å¡«å……ä¸¤ç§æƒ…å½¢ä¸‹å‡é€šè¿‡ã€‚  
3. å¯¹ `rocm_per_tensor_float_w8a8_scaled_mm_impl`ã€`rocm_unquantized_gemm_impl` æ·»åŠ  `A.is_contiguous()`ã€`x.is_contiguous()` æ–­è¨€ï¼Œä½¿å¾—åªæœ‰åœ¨æ»¡è¶³è¿ç»­æ€§çš„å‰æä¸‹æ‰èµ° skinnyâ€‘gemm è·¯å¾„ã€‚

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/_custom_ops.py`ï¼ˆæ‰€æœ‰ ROCm skinnyâ€¯gemm æ¥å£ï¼‰  
- `vllm/model_executor/layers/quantization/kernels/scaled_mm/rocm.py`ï¼ˆscaledâ€¯mm å®ç°ï¼‰  
- `vllm/model_executor/layers/utils.py`ï¼ˆunquantizedâ€¯gemm è°ƒåº¦ï¼‰  
- ç›¸å…³æµ‹è¯•æ–‡ä»¶ `tests/kernels/quantization/test_rocm_skinny_gemms.py`

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
- å¼€å‘è€…åœ¨ä½¿ç”¨ ROCmâ€¯skinnyâ€¯gemm æ—¶ä»éœ€ä¿è¯è¾“å…¥å¼ é‡è¿ç»­ï¼Œå¦åˆ™ä¼šé€€å›é€šç”¨å®ç°ï¼›å¦‚æœ‰å¿…è¦å¯æ‰‹åŠ¨è°ƒç”¨ `.contiguous()`ã€‚  
- å…³æ³¨åç»­ PR æ˜¯å¦æ¢å¤å¯¹éè¿ç»­å¼ é‡çš„ä¼˜åŒ–æ”¯æŒï¼ˆå¦‚åœ¨ kernel ä¸­åŠ å…¥ stride å¤„ç†ï¼‰ï¼Œä»¥å…å‡ºç°ä¸å¿…è¦çš„æ‹·è´ã€‚  
- CI å·²å¢æ·»å¡«å……/ä¸å¡«å……ä¸¤å¥—è·¯å¾„çš„éªŒè¯ï¼Œæ¨èåœ¨æœ¬åœ°è·‘ `pytest -k rocm_skinny_gemms` ç¡®è®¤å…¼å®¹æ€§ã€‚  
- è‹¥ç”¨æˆ·åœ¨ AMD GPU ä¸Šä»é‡åˆ°æ€§èƒ½å›é€€ï¼Œè¯·æ£€æŸ¥ `torch.cuda.is_available()` ä¸ `torch.backends.cuda.is_built()` æ˜¯å¦æ­£ç¡®æŒ‡å‘ ROCmï¼Œå¹¶ç¡®è®¤ `x.is_contiguous()` ä¸º Trueã€‚

---

### [Kernel] [Helion] [1/N] Add Helion ConfigManager (#32740)
**SHA**: `6c1f9e4` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6c1f9e4c1877cb5cb6a702f5813e606eaba35600)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆæ–°å¢ Helion é…ç½®ç®¡ç†ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨ `vllm/kernels/helion` ç›®å½•æ–°å¢ `config_manager.py`ï¼Œå®ç° `ConfigSet`ï¼ˆå†…å­˜é…ç½®é›†åˆï¼‰å’Œå…¨å±€å•ä¾‹ `ConfigManager`ï¼ˆJSON æ–‡ä»¶çº§çš„åŠ è½½ã€ä¿å­˜ã€ç›®å½•ç®¡ç†ï¼‰ã€‚  
- ä¸ºå…¶ç¼–å†™äº†å®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼ˆ`tests/kernels/helion/test_config_manager.py`ï¼‰ï¼Œè¦†ç›–åˆ›å»ºã€åºåˆ—åŒ–ã€é”™è¯¯å¤„ç†ã€å•ä¾‹è¡Œä¸ºç­‰è·¯å¾„ã€‚  
- åœ¨ `vllm/kernels/__init__.py` ä¸ `vllm/kernels/helion/__init__.py` ä¸­åŠ å…¥ç›¸åº”çš„å¯¼å‡ºå£°æ˜ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- æ–°å¢çš„æ¨¡å—å°†è¢« `vllm.kernels.helion` åŒ…ç›´æ¥å¯¼å‡ºï¼Œä»»ä½•å¼•ç”¨è¯¥å­åŒ…çš„ä»£ç éƒ½ä¼šå¾—åˆ° `ConfigManager`/`ConfigSet`ã€‚  
- ä¾èµ– `helion` åŒ…çš„ç¯å¢ƒï¼ˆé€šè¿‡ `has_helion` æ£€æŸ¥ï¼‰ï¼Œå¦åˆ™ä¼šåœ¨å¯¼å…¥æ—¶æŠ› `ImportError`ã€‚  
- å•ä¾‹å®ç°ä¼šå½±å“è¿›ç¨‹å†…çš„æ‰€æœ‰é…ç½®è¯»å–/å†™å…¥ï¼Œæ¶‰åŠ `base_dir` å‚æ•°çš„å†²çªæ£€æµ‹ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å•ä¾‹çº¿ç¨‹å®‰å…¨**ï¼šå½“å‰å®ç°ä»…åœ¨å•çº¿ç¨‹ä¸‹ä¿è¯å”¯ä¸€æ€§ï¼Œè‹¥æœ‰å¹¶å‘åˆå§‹åŒ–ï¼ˆå¦‚åœ¨å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹ç¯å¢ƒï¼‰å¯èƒ½å‡ºç°ç«äº‰ï¼›å¯è€ƒè™‘ä½¿ç”¨ `threading.Lock` æˆ– `multiprocessing` æ–¹æ¡ˆé˜²æ­¢ raceã€‚  
2. **å¼‚å¸¸ä¿¡æ¯ç»Ÿä¸€**ï¼š`load_config_set` åœ¨ JSON è§£æé”™è¯¯åè¿”å›ç©º `ConfigSet`ï¼Œä½†ä¸ä¼šå‘è°ƒç”¨æ–¹æ˜ç¡®æç¤ºé…ç½®æ–‡ä»¶æŸåï¼›å»ºè®®åœ¨æ—¥å¿—çº§åˆ«ä¹‹å¤–æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸ï¼Œä¾¿äºä¸Šå±‚å®šä½ã€‚  
3. **å¹³å°/é”®å¤§å°å†™**ï¼š`get_config_keys` å¯¹å¹³å°ä½¿ç”¨ `platform.lower()`ï¼Œä½†å…¶å®ƒæ–¹æ³•ç›´æ¥ç”¨åŸå§‹å­—ç¬¦ä¸²ï¼Œå¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ï¼›ç»Ÿä¸€å¤§å°å†™æˆ–åœ¨æ–‡æ¡£ä¸­çº¦å®šå¹³å°ååŒºåˆ†å¤§å°å†™ã€‚  
4. **è·¯å¾„é»˜è®¤ä½ç½®**ï¼šé»˜è®¤ `configs` ç›®å½•ä½äºæºç ç›®å½•ä¸‹ï¼Œå‘å¸ƒçš„ wheel ä¸­å¯èƒ½ä¸å¯å†™ï¼›è‹¥ç”¨æˆ·åœ¨åªè¯»å®‰è£…ç¯å¢ƒä½¿ç”¨ï¼Œ`ensure_base_dir_exists` ä¼šå°è¯•åˆ›å»ºç›®å½•å¤±è´¥ã€‚å»ºè®®åœ¨æ–‡æ¡£ä¸­è¯´æ˜æˆ–æä¾›ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤è·¯å¾„ã€‚  
5. **æµ‹è¯•ä¾èµ–**ï¼šå•å…ƒæµ‹è¯•åªåœ¨ `has_helion()` ä¸ºçœŸæ—¶è¿è¡Œï¼ŒCI ç¯å¢ƒéœ€ç¡®ä¿ `helion` å·²å®‰è£…ï¼Œå¦åˆ™è¿™äº›è¦†ç›–ç‡å°†ç¼ºå¤±ã€‚  
6. **`to_dict` å®ç°**ï¼šå†…éƒ¨ä½¿ç”¨ `json.loads(config.to_json())` è¿›è¡Œè½¬æ¢ï¼Œè‹¥ `helion.Config` å°†æ¥æ”¹å˜è¿”å›æ ¼å¼ï¼Œå¯èƒ½å‡ºç°éšè”½é”™è¯¯ï¼›è€ƒè™‘ç›´æ¥è®¿é—®å±æ€§æˆ–å®ç°ç»Ÿä¸€çš„åºåˆ—åŒ–æ–¹æ³•ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸º Helion è‡ªå®šä¹‰ kernel æä¾›äº†ç»“æ„æ¸…æ™°ã€æ˜“äºç»´æŠ¤çš„é…ç½®ç®¡ç†æ¡†æ¶ï¼Œä»£ç é£æ ¼ä¸é¡¹ç›®ä¸€è‡´ã€‚å…³æ³¨ä¸Šè¿°ç»†èŠ‚åï¼Œå¯æå‡ç¨³å®šæ€§å¹¶é¿å…æ½œåœ¨çš„è¿è¡Œæ—¶å†²çªã€‚

---

### Support FP8 block quant for CompressedTensorsW8A16Fp8 (#33280)
**SHA**: `fd0e377` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/fd0e3772441a32e1c5948d62a8702417fac59979)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
ä¸º `CompressedTensorsW8A16Fp8` æ·»åŠ äº† FP8 **å—é‡åŒ–ï¼ˆblockâ€‘quantï¼‰** æ”¯æŒã€‚`CompressedTensorsW8A16Fp8` ç°åœ¨æ¥å— `QuantizationArgs`ï¼Œèƒ½å¤Ÿæ ¹æ® `QuantizationStrategy.BLOCK` åˆ›å»ºå—å°ºåº¦å‚æ•°ã€æ ¡éªŒå—å°ºå¯¸å¹¶åœ¨åŠ è½½åå¯¹æƒé‡è¿›è¡Œå—â€‘ç‰¹å®šçš„è½¬ç½®/å°ºåº¦å¤„ç†ã€‚ç›¸å…³å·¥å…·å‡½æ•° (`create_fp8_weight_parameter`, `create_fp8_scale_parameter`, `process_fp8_weight_block_strategy` ç­‰) è¿å…¥ `fp8_utils` å¹¶åœ¨ `prepare_fp8_layer_for_marlin` ä¸­åŠ å…¥ `size_k_first` æ§åˆ¶ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`  
- `vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py`  
- `vllm/model_executor/layers/quantization/fp8.py`ï¼ˆå»é™¤å†—ä½™ `set_weight_attrs`ï¼‰  
- `vllm/model_executor/layers/quantization/utils/fp8_utils.py`ï¼ˆæ–°å¢å—ç›¸å…³å·¥å…·ï¼‰  
- ç›¸å…³å‚æ•°ç±» `BlockQuantScaleParameter`ã€`replace_parameter`ã€`set_weight_attrs`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤æ—§æ¨¡å‹ï¼ˆéå—é‡åŒ–ï¼‰åœ¨åŠ è½½æ—¶ä»èµ°åŸè·¯å¾„ï¼Œ`is_static_input_scheme` çš„é»˜è®¤å€¼æœªè¢«è¯¯æ”¹ã€‚  
2. **å•å…ƒæµ‹è¯•**ï¼šæ–°å¢é’ˆå¯¹ `QuantizationStrategy.BLOCK` çš„æƒé‡/å°ºåº¦åˆ›å»ºã€å—å½¢çŠ¶æ ¡éªŒã€`process_weights_after_loading` çš„è¡Œä¸ºæµ‹è¯•ã€‚  
3. **ç¡¬ä»¶ä¾èµ–**ï¼šè¯¥ç‰¹æ€§ä»…åœ¨ Turing åŠä»¥ä¸Š GPUï¼ˆSM75+ï¼‰å¯ç”¨ï¼Œéœ€åœ¨æ–‡æ¡£å’Œè¿è¡Œæ—¶æ£€æŸ¥ä¸­æç¤ºã€‚  
4. **åºåˆ—åŒ–/æ¢å¤**ï¼šç¡®ä¿ `weight_block_size`ã€å—å°ºåº¦ç­‰ä¿¡æ¯åœ¨æ¨¡å‹ä¿å­˜/åŠ è½½æ—¶å®Œæ•´ä¿å­˜ã€‚  
5. **æ€§èƒ½éªŒè¯**ï¼šå¯¹æ¯”å—é‡åŒ–ä¸é€šé“/å¼ é‡é‡åŒ–çš„ååä¸ç²¾åº¦ï¼ŒéªŒè¯ Marlinâ€‘FP8 è·¯å¾„çš„æ­£ç¡®æ€§ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ”¹åŠ¨ä¸º FP8 å—é‡åŒ–é“ºè·¯ï¼Œä»£ç ç»“æ„æ›´æ¸…æ™°ï¼Œä½†éœ€é€šè¿‡æµ‹è¯•å’Œæ–‡æ¡£åŒæ­¥ç¡®ä¿ç¨³å®šè½åœ°ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (21)

### [Doc] Update plugin deprecation notices (#33476)
**SHA**: `793af53` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/793af538a35a7277a2c9ca0ca5f51adb043b97f5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `docs/design/plugin_system.md` ä¸­å°† `seed_everything` æ¥å£çš„å¼ƒç”¨è¯´æ˜æ›´æ–°ä¸ºå·²åœ¨ v0.16.0 ç§»é™¤ï¼Œå»ºè®®æ”¹ç”¨ `vllm.utils.torch_utils.set_random_seed`ã€‚

---

### support return prompt token ids in responses  (#33378)
**SHA**: `6f5e7cd` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6f5e7cda57e973ccb7fca22862e107119515b33a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `ResponsesRequest` çš„å­—æ®µæè¿°ä¸­ï¼Œç²¾ç®€äº†å¯¹è¿”å›æ¶ˆæ¯æ”¯æŒçš„è¯´æ˜ï¼Œå»æ‰äº†ä»…é™éåå°å’Œ gpt-oss çš„é™å®šæªè¾ã€‚æ•´ä½“æ–‡å­—æ›´ç®€æ´ï¼ŒåŠŸèƒ½ä¸å˜ã€‚

---

### [Misc] Replace deprecated interface seed_everything (#33474)
**SHA**: `68feb76` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/68feb76a6f89abd6a52116c99b9b39e538a9a9ac)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ CPU åŸºå‡†æµ‹è¯•è„šæœ¬ä¸­ï¼Œå°†å·²å¼ƒç”¨çš„ `current_platform.seed_everything` æ›¿æ¢ä¸ºç»Ÿä¸€çš„ `set_random_seed` å·¥å…·å‡½æ•°ï¼Œä¿æŒéšæœºç§å­è®¾ç½®çš„ä¸€è‡´æ€§ã€‚

---

### [perf] v1/spec_decode: skip softmax for all-greedy rejection sampling (#32852)
**SHA**: `8980001` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8980001c9333c617f2bc3b2c60926b02434067a7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„/æ€§èƒ½ä¼˜åŒ–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `rejection_sampler.py` ä¸­å°†å¯¹ target logits çš„ softmax è®¡ç®—æ¨è¿Ÿè‡³å¿…è¦æ—¶å†æ‰§è¡Œï¼Œå»é™¤å†—ä½™çš„ softmax æ­¥éª¤ï¼Œé™ä½è®¡ç®—å¼€é”€å¹¶æå‡é‡‡æ ·æ€§èƒ½ã€‚

---

### [ROCM] Enable aiter attn backend for qwen3-next model (#32492)
**SHA**: `527bcd1` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/527bcd14d46595b2bde9e48f3d2f10879623b891)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ ROCM AITER FlashAttention åç«¯æ–‡æ¡£ä¸å®ç°ä¸­ï¼Œå°†æ”¯æŒçš„ kernel block size ç”±ä»… `MultipleOf(16)` æ‰©å±•ä¸º `[16, 32]`ï¼ŒåŒæ­¥æ›´æ–°äº†è¡¨æ ¼è¯´æ˜ã€‚

---

### [BugFix] Add synchronize in CutlassW4A8LinearKernel to ensure data is ready for use. (#33078)
**SHA**: `f68e3ea` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f68e3ea4e135c41b997d8226186509659b3d722c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„ / BugFix  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `CutlassW4A8LinearKernel` ä¸­åŠ å…¥ `torch.cuda.synchronize()`ï¼Œç¡®ä¿åœ¨é‡åŒ–æƒé‡è½¬æ¢å GPU æ•°æ®å·²åŒæ­¥å®Œæ¯•ï¼Œé˜²æ­¢åç»­ä½¿ç”¨æ—¶å‡ºç°æœªå°±ç»ªçš„ç«äº‰é—®é¢˜ã€‚

---

### [CPU][Feat] Enable KleidiAI accelerated int4 dynamic quant with BF16 activations on Arm CPUs (#33122)
**SHA**: `1618e25` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/1618e2549281324100690db3bd8c5f497fa4fc3f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `Dynamic4bitLinearKernel` ä¸­åŠ å…¥å¯¹ Arm CPU ä¸Š KleidiAI åŠ é€Ÿçš„ int4 åŠ¨æ€é‡åŒ–æ”¯æŒï¼Œå…è®¸ä½¿ç”¨ BF16ï¼ˆä»¥åŠ Float32ï¼‰æ¿€æ´»ï¼Œå®ç°æ›´é«˜æ•ˆçš„ 4bitâ€‘8bit çŸ©é˜µä¹˜æ³•ã€‚

---

### Add EAGLE3 support for AFMoE (#33111)
**SHA**: `f3888ac` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f3888aca83edf5125c37d0983f1df94df27d9920)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„/ç‰¹æ€§æ–°å¢  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¸º AFMoE æ¨¡å‹åŠ å…¥ EAGLE3 æ”¯æŒï¼Œæ–°å¢ `SupportsEagle3` æ¥å£ã€è¾…åŠ©éšè—å±‚çŠ¶æ€é‡‡é›†é€»è¾‘ï¼Œå¹¶åœ¨é…ç½®æ ¡éªŒä¸­åŠ å…¥ `"afmoe"`ã€‚

---

### [Bugfix] Handle Asym W4A16 (ConchLinearKernel) for CT (#33200)
**SHA**: `73419ab` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/73419abfae97c0b797c7e1fe997913e9ebfeff68)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `conch.py` ä¸­æ–°å¢å¯¹ zeroâ€‘point æƒé‡çš„è§£åŒ…ä¸é‡æ’é€»è¾‘ï¼Œæ”¯æŒ Asym W4A16 å¹¶åœ¨åŠ è½½æ—¶æ­£ç¡®æ›´æ–°å…ƒæ•°æ®ã€‚  

---

### [Attention] Clarify comment explaining attn_logits +1 dimension (#33427)
**SHA**: `5b55c0b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/5b55c0bea7410e60566e329b28070ce77269a322)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `triton_mla.py` ä¸­å°†å¯¹ `+1` çš„è¯´æ˜ä»æ¨¡ç³Šçš„çŒœæµ‹æ”¹ä¸ºæ˜ç¡®æ³¨é‡Šï¼Œè§£é‡Šè¯¥é¢å¤–ç»´åº¦ç”¨äºå­˜æ”¾ Stage2 æ ¸å¿ƒåˆå¹¶åˆ†ç‰‡æ³¨æ„åŠ›æ—¶çš„ LogSumExp (LSE)ã€‚ä»£ç è¡Œä¸ºæœªå˜ï¼Œä»…æå‡å¯è¯»æ€§ã€‚

---

### [ROCm][CI] Force max_num_seqs=1 on ROCm In test_sharded_state_loader to reduce flakiness (#33277)
**SHA**: `6c64c41` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6c64c41b4abfe10f725088afb1fe143d412dc118)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `test_sharded_state_loader` ä¸­æ–°å¢å¯¹ ROCm å¹³å°çš„æ£€æµ‹ï¼Œè‹¥ä¸º ROCm åˆ™å¼ºåˆ¶ `max_num_seqs=1`ï¼Œä»¥é™ä½æµ‹è¯•ä¸ç¨³å®šæ€§ã€‚

---

### [Misc] offest -> offset in comments and variable names (#33444)
**SHA**: `a2ef06e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a2ef06e1b3a1c0dc4f7bf6587fbdcf1dad287c31)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°†æ³¨é‡Šå’Œå˜é‡ä¸­çš„æ‹¼å†™é”™è¯¯ â€œoffestâ€ ç»Ÿä¸€æ›´æ­£ä¸º â€œoffsetâ€ï¼Œæ¶‰åŠä¸¤ä¸ªæ–‡ä»¶çš„å°‘é‡è¡Œä¿®æ”¹ï¼Œä»£ç åŠŸèƒ½ä¿æŒä¸å˜ã€‚

---

### [BugFix] Fix whisper FA2 + full cudagraphs (#33360)
**SHA**: `0a3c71e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0a3c71e7e5f06507f64677f77d5fca84d9c7cc65)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®å¤ FlashAttention2 åœ¨ encoderâ€‘decoder æ¨¡å‹ä½¿ç”¨ CUDAâ€¯graphs æ—¶çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œåˆ é™¤åŸæœ‰è­¦å‘Šå¹¶åœ¨ CUDAâ€¯graph æ•è·é˜¶æ®µä½¿ç”¨çœŸå®çš„ encoder é•¿åº¦ï¼›ä¸º `_get_encoder_seq_lens` æ·»åŠ  `for_cudagraph_capture` å‚æ•°å¹¶ç›¸åº”ä¼ é€’ã€‚

---

### [Misc] Algin Qwen3-VL-embedding image example outputs with HF repo example (#33419)
**SHA**: `9df152b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9df152bbf6c27db737636960ae791546d65269fc)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `examples/pooling/embed/vision_embedding_offline.py` ä¸­å¼•å…¥å¯¹ `qwen-vl-utils` çš„å¯é€‰ä¾èµ–ï¼Œå®ç°å›¾ç‰‡è‡ªåŠ¨ç¼©æ”¾ï¼›ä¸º `print_embeddings` æ·»åŠ ç±»å‹æ³¨è§£ï¼›åœ¨ `EngineArgs` ä¸­åŠ å…¥ `mm_processor_kwargs` ä»¥ç¦ç”¨å†…éƒ¨è‡ªåŠ¨ç¼©æ”¾ï¼Œä»è€Œå¯¹é½ HF ç¤ºä¾‹è¾“å‡ºã€‚

---

### [Bugfix] Fix typo in read_offset variable name (#33426)
**SHA**: `64a40a7` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/64a40a7ab4d0053830fae04c83763fa67f2183e6)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®å¤ `vllm/v1/engine/detokenizer.py` ä¸­ `read_offest` æ‹¼å†™é”™è¯¯ï¼Œæ”¹ä¸º `read_offset`ï¼Œæ¶ˆé™¤å±æ€§åä¸ä¸€è‡´å¯¼è‡´çš„æ½œåœ¨è¿è¡Œæ—¶é”™è¯¯ã€‚

---

### [CI][HPU]accelerate hpu test by skip python re-install and clean container name (#33286)
**SHA**: `2b46557` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/2b465570e6dd327e8422ef9c87e9b2b1454ceaed)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ HPU CI è„šæœ¬ä¸­ä½¿ç”¨è‡ªå®šä¹‰é•œåƒåå’Œå®¹å™¨åï¼Œè·³è¿‡ Python é‡è£…ï¼Œæ”¹ä¸ºç›´æ¥å®‰è£…ä¾èµ–å¹¶é€šè¿‡ `pip install -e .`ï¼Œå¹¶åœ¨å®¹å™¨å†…è¿è¡Œç¦»çº¿æ¨ç†ç¤ºä¾‹ä»¥åŠ é€Ÿæµ‹è¯•ã€‚

---

### Indicate compile mode in the benchmark results (#32990)
**SHA**: `9ca66ec` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9ca66ecc1012418f6c6d865239835a0adfcae719)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„/åŠŸèƒ½è¡¥å……  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨æ€§èƒ½åŸºå‡†è„šæœ¬ä¸­åŠ å…¥ç¼–è¯‘æ¨¡å¼ä¸ä¼˜åŒ–ç­‰çº§çš„é‡‡é›†å¹¶å†™å…¥ç»“æœå…ƒæ•°æ®ï¼›æ–°å¢ `extract_field`ã€`use_compile` å·¥å…·å‡½æ•°ï¼Œåœ¨ JSON è¾“å‡ºä¸­åŠ å…¥ `compilation_config.mode`ã€`optimization_level` ä¸ `use_compile` æ ‡è®°ã€‚

---

### [Quantization][ROCm] Fix MoE weight loading to be robust (Qwen3_MoE/Qwen3_next as example models) (#33173)
**SHA**: `f451b45` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f451b4558b2bb42dafcdd19f7b5c0fc58194af5a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ MoE æƒé‡åŠ è½½æ—¶å¢åŠ  `loaded_weight.ndim > 0` æ£€æŸ¥ï¼Œé˜²æ­¢å¯¹æ ‡é‡ï¼ˆ0â€‘ç»´ï¼‰å¼ é‡æ‰§è¡Œ `narrow`ï¼Œæå‡åœ¨ Qwen3_MoE/Qwen3_next ç­‰æ¨¡å‹ä¸Šçš„é²æ£’æ€§ã€‚

---

### fix QERL attention import path (#33432)
**SHA**: `3f96fcf` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/3f96fcf64631a100904bb189859f9aa80e8a75d5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®æ­£ QERL æ³¨æ„åŠ›æ¨¡å—çš„å¯¼å…¥è·¯å¾„ï¼Œå°†åŸå…ˆçš„ `vllm.attention.layer` æ›¿æ¢ä¸º `vllm.model_executor.layers.attention`ï¼Œç¡®ä¿æ­£ç¡®å¼•ç”¨å®ç°å¹¶é¿å…æ½œåœ¨çš„ ImportErrorã€‚

---

### Fix encoder-decoder model disabling mm processor cache (#33236)
**SHA**: `67239c4` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/67239c4c4235ae668971625304390ed2078b4dd0)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®æ­£ `ModelConfig.__post_init__` å¯¹ encoderâ€‘decoder æ¨¡å‹çš„å¤šæ¨¡æ€å¤„ç†å™¨ç¼“å­˜ç¦ç”¨é€»è¾‘ï¼Œé¿å…ç›´æ¥ä¿®æ”¹å®ä¾‹å±æ€§å¯¼è‡´åç»­è®¾ç½®å¤±æ•ˆï¼›ç›¸åº”æµ‹è¯•æ”¹ä¸ºå…ˆåˆ›å»ºæ¨¡å‹åå†æ‰‹åŠ¨èµ‹å€¼ `mm_processor_cache_gb`ã€‚

---

### [CI] Qwen3-ASR transcriptios tests (#33414)
**SHA**: `8ece607` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8ece60768f63989db7f26ef81c7f68635f70e081)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¸ºè½¬å½•æ¥å£æ–°å¢å¯¹ Qwen3â€‘ASR æ¨¡å‹çš„å‚æ•°åŒ–æµ‹è¯•ï¼›è°ƒæ•´æ–­è¨€æ–‡æœ¬ä»¥é€‚é…æ–°æ¨¡å‹è¾“å‡ºï¼›å°† gemma æµ‹è¯•æ”¹ä¸ºå‚æ•°åŒ–å¹¶æ›´æ–°æœŸæœ›è½¬å½•å†…å®¹ã€‚

---

