# 每日更新报告（2026-01-14）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-14 23:25:31 | Cyrus Leung | [1/N] Reorganize multimodal processing code (#32327) |
| 2026-01-14 22:52:20 | Ning Xie | rename tokenize serving api request id prefix to tokenize (#32328) |
| 2026-01-14 20:00:37 | Chauncey | [Frontend] track responsesAPI server_load (#32323) |
| 2026-01-14 19:46:01 | Shanshan Shen | [Misc] Make mem utils can be reused by other platforms (#32322) |
| 2026-01-14 19:22:26 | Cyrus Leung | [Frontend] Standardize use of `create_error_response` (#32319) |
| 2026-01-14 18:20:58 | Chauncey | [Refactor] [9/N] to simplify the vLLM openai translations  serving ar chitecture (#32313) |
| 2026-01-14 18:17:46 | Cyrus Leung | [Refactor] Move top-level dummy data generation to registry (#32310) |
| 2026-01-14 15:40:30 | Roger Wang | [Model] Re-implement Qwen3Omni Audio Encoder (#32167) |
| 2026-01-14 15:33:09 | sangho.lee | Add Molmo2 multimodal model support (#30997) |
| 2026-01-14 15:26:24 | Chauncey | [Refactor] [8/N] to simplify the vLLM openai responsesapi_serving architecture (#32260) |
| 2026-01-14 15:25:45 | Michael Goin | [Docs] Add docs about OOT Quantization Plugins (#32035) |
| 2026-01-14 15:25:10 | Hongxia Yang | AMD CI Test - unskip moe_sum test and moe_align_block_size tests (#32039) |
| 2026-01-14 15:22:07 | Angela Yi | [misc] Remove is_torch_equal_or_newer(2.4) cases (#32296) |
| 2026-01-14 15:21:39 | David | [Build] Relax anthropic version pin from ==0.71.0 to >=0.71.0 (#32289) |
| 2026-01-14 15:21:26 | Andreas Karatzas | [ROCm][CI] Handle missing vision_config in Isaac model attention patch (#32281) |
| 2026-01-14 15:11:30 | Yi Liu | Consolidate Intel Quantization Toolkit Integration in vLLM (#31716) |
| 2026-01-14 13:29:42 | Micah Williamson | [ROCm][CI] Disable Async Scheduling For Qwen3-Next-80B-A3B-Instruct MTP Async EPLB Accuracy Test (#32275) |
| 2026-01-14 09:58:12 | Woosuk Kwon | [Model Runner V2] Refactor Sampler (#32245) |
| 2026-01-14 07:22:53 | Roberto L. Castro | [Kernel][Performance] Enable smaller Scaling Factor tiling for NVFP4 small-batch decoding (#30885) |
| 2026-01-14 06:35:05 | emricksini-h | [Improvement] Persist CUDA compat libraries paths to prevent reset on `apt-get` (#30784) |
| 2026-01-14 06:28:19 | Michael Goin | Add mergify label job for "bug" in PR titles (#31980) |
| 2026-01-14 05:21:05 | Simon Mo | [Build] Add scripts for cherry-picking and trigger build (#32282) |
| 2026-01-14 05:06:10 | HappyAmazonian | [Misc] Add In-Container restart capability through supervisord for sagemaker entrypoint (#28502) |
| 2026-01-14 03:11:54 | Rabi Mishra | fix(rocm): Use refresh_env_variables() for rocm_aiter_ops in test_moe (#31711) |
| 2026-01-14 02:58:18 | Wentao Ye | [Perf] Optimize grouped topk kernel, 1.2%~2% E2E Throughput improvement (#32058) |
| 2026-01-14 02:48:37 | Dmitry Tokarev | Fix CUDA 13 wheel installation doc (#32276) |
| 2026-01-14 02:41:26 | Andrew Xia | [responseAPI] support partial message generation (#32100) |
| 2026-01-14 02:19:03 | Sage Moore | [EPLB][Cleanup] Remove `is_async_enabled` from `EplbModelState` (#32050) |
| 2026-01-14 01:09:23 | Mark McLoughlin | [Trivial] Remove duplicate enable_mfu_metrics (#32246) |
| 2026-01-14 01:08:45 | Matthew Bonanni | [4/N][Attention] Move MLA common to model_executor (#32060) |
| 2026-01-14 00:21:10 | Mathis Felardos | nixl_connector: export UCX_MEM_MMAP_HOOK_MODE=none to avoid a UCX memory leak (#32181) |

### 📊 统计摘要
> 本日共 31 个提交 | 🔴高 6 | 🟡中 13 | 🟢低 12
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (6)](#-🔴-高重要度变更-6)
    - [[1/N] Reorganize multimodal processing code (#32327)](#9ea07b4)
    - [[Refactor] [9/N] to simplify the vLLM openai translations...](#769d062)
    - [Add Molmo2 multimodal model support (#30997)](#7e6f123)
    - [[Refactor] [8/N] to simplify the vLLM openai responsesapi...](#9312a6c)
    - [Consolidate Intel Quantization Toolkit Integration in vLL...](#50632ad)
    - [[Model Runner V2] Refactor Sampler (#32245)](#90c0836)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [[Misc] Make mem utils can be reused by other platforms (#...](#ce09462)
    - [[Frontend] Standardize use of `create_error_response` (#3...](#3f28174)
    - [[Refactor] Move top-level dummy data generation to regist...](#90db5b3)
    - [[Model] Re-implement Qwen3Omni Audio Encoder (#32167)](#b8199f6)
    - [[Docs] Add docs about OOT Quantization Plugins (#32035)](#6388b50)
    - [AMD CI Test - unskip moe_sum test and moe_align_block_siz...](#048bb59)
    - [[misc] Remove is_torch_equal_or_newer(2.4) cases (#32296)](#7933638)
    - [[Kernel][Performance] Enable smaller Scaling Factor tilin...](#8ef50d9)
    - [[Build] Add scripts for cherry-picking and trigger build ...](#0db574b)
    - [fix(rocm): Use refresh_env_variables() for rocm_aiter_ops...](#69f8a0e)
    - [[Perf] Optimize grouped topk kernel, 1.2%~2% E2E Throughp...](#f28125d)
    - [[responseAPI] support partial message generation (#32100)](#af54d2e)
    - [[4/N][Attention] Move MLA common to model_executor (#32060)](#2263d44)
  - [🟢 低重要度变更 (12)](#-🟢-低重要度变更-12)
    - [rename tokenize serving api request id prefix to tokenize...](#552b262)
    - [[Frontend] track responsesAPI server_load (#32323)](#00e6402)
    - [[Build] Relax anthropic version pin from ==0.71.0 to >=0....](#6b17609)
    - [[ROCm][CI] Handle missing vision_config in Isaac model at...](#9d0d7f4)
    - [[ROCm][CI] Disable Async Scheduling For Qwen3-Next-80B-A3...](#6fa6e7e)
    - [[Improvement] Persist CUDA compat libraries paths to prev...](#2a60ac9)
    - [Add mergify label job for "bug" in PR titles (#31980)](#9e65bb4)
    - [[Misc] Add In-Container restart capability through superv...](#2f4a71d)
    - [Fix CUDA 13 wheel installation doc (#32276)](#46f8c6b)
    - [[EPLB][Cleanup] Remove `is_async_enabled` from `EplbModel...](#6beef12)
    - [[Trivial] Remove duplicate enable_mfu_metrics (#32246)](#ab74b2a)
    - [nixl_connector: export UCX_MEM_MMAP_HOOK_MODE=none to avo...](#4f3676e)
#### 🔴 高重要度变更 (6)

### [1/N] Reorganize multimodal processing code (#32327)
**SHA**: `9ea07b4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9ea07b41da169f727a2eb7302adec4c724319522)

**🎯 变更类型**：架构变更 / 重构  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 将原本散落在 `vllm.multimodal.profiling` 包中的多模态处理实现迁移到新的 **`vllm.multimodal.processing`** 包，并拆分为 `context.py`（请求上下文、计时、request_id）、`dummy_inputs.py`（BaseDummyInputsBuilder 与 ProcessorInputs）以及 `processor.py`（原全部处理逻辑）。  
- 新增 `vllm.multimodal.processing.__init__` 统一导出公共 API，所有模型、测试、基准等在代码库内部统一改为从 `vllm.multimodal.processing` 导入。  
- 引入 **请求级计时统计**（`MultiModalProcessorTimingStats`）以及基于 `contextvars` 的 `request_id` 传播机制，配合 `ObservabilityConfig.enable_mm_processor_stats` 开关。  
- 文档、注册表、engine 输入处理器等同步更新 import 路径。  

**🎯 影响范围**  
- `vllm/multimodal/processing/*`（核心实现）  
- 所有模型实现（`vllm/model_executor/models/*`）  
- 输入处理器 (`vllm/v1/engine/input_processor.py`)  
- 单元测试 `tests/multimodal/test_processing.py`、基准 `benchmarks/mm_processor.py`  
- 文档 `docs/api/README.md`、`docs/contributing/model/multimodal.md`、`docs/design/mm_processing.md`  
- 注册表、缓存、工具等间接依赖 `vllm.multimodal.processing` 的模块  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构** | - **模块化提升**：`processing` 包把 *上下文、构建器、主要处理器* 分离，职责更清晰，避免了原 `profiling` 包中出现的循环依赖。 <br>- **统一入口**：`__init__.py` 公开统一的符号，外部只需要 `from vllm.multimodal.processing import …`，降低了 import 路径的碎片化。 <br>- **向后兼容风险**：原 `vllm.multimodal.profiling` 已被移除，依赖该路径的外部项目（插件、用户自定义模型）会立即失效。若没有提供兼容 shim，升级会是 **breaking change**。 |
| **性能** | - **计时统计**：通过 `contextvars` 与轻量锁实现的请求级计时统计在默认关闭时几乎无开销；开启后每个阶段会额外执行 `timed_operation`，轻微的 CPU 消耗（大约 10‑20µs/调用），对大模型吞吐影响可忽略。 <br>- **锁竞争**：计时统计 Registry 使用 `threading.Lock`，在高并发请求（>10k QPS）下可能出现轻微的锁竞争，但统计对象仅在每个请求首次创建一次，后续只读，冲突概率低。 <br>- **代码搬迁**：无业务逻辑变更，整体算子执行路径未变，推理吞吐保持不变。 |
| **安全** | - **无新增攻击面**：代码仅加入内部计时与上下文传播，不涉及网络、文件 I/O。 <br>- **信息泄露风险**：打开 `enable_mm_processor_stats` 将把每个请求的计时信息暴露给外部 API（`engine_client`），如果未做好访问控制，潜在泄露内部性能数据。建议在生产环境默认关闭。 |
| **可维护性** | - **代码组织清晰**：文件划分明确（`context`, `dummy_inputs`, `processor`），方便定位 bug 与扩展。 <br>- **类型注解与 `typing`**：大量使用 `TypeVar`, `Protocol`, `overload`，提升 IDE 检查能力。 <br>- **文档同步**：对应文档已更新，降低使用者误解。 |
| **测试/兼容性** | - 所有内部单元测试均已迁移至新路径并通过。 <br>- 仍需 **外部回归测试**：第三方库（如自定义 `BaseProcessingInfo` 实现）如果仍 `import vllm.multimodal.profiling` 将报错。建议在 `vllm/multimodal/__init__.py` 添加向后兼容层（`from .processing import *`），或在发行说明中标记破坏性迁移。 |

---

### ⚠️ 潜在风险

1. **破坏向后兼容性**  
   - 外部代码、插件、或者用户自行实现的 `BaseDummyInputsBuilder`、`BaseProcessingInfo` 仍然引用旧路径 `vllm.multimodal.profiling.*` 将直接导致 `ImportError`。  
2. **计时统计泄露**  
   - 若 `ObservabilityConfig.enable_mm_processor_stats=True` 在多租户环境中未限制访问，可能泄露请求级执行时间、缓存命中率等内部指标。  
3. **上下文传播失效**  
   - `request_id` 依赖 `contextvars`. 在某些异步框架（如 `trio`、`curio`）或手动线程切换（`threading.Thread`）未显式传递上下文时，计时统计会缺失，导致统计不完整或误报。  
4. **锁竞争**  
   - 高并发情况下 `timing_stats_registry` 的锁可能成为热点，尤其在开启统计的负载测试场景。  
5. **文档/示例遗漏**  
   - 还有少量示例或脚本（如旧 notebooks）可能仍使用旧 import，导致运行错误。  

---

### 💡 关注建议

| 目标 | 具体行动 |
|------|----------|
| **兼容性** | 1️⃣ 在下个 **minor** 版本发布前，提供 **shim**：<br>`vllm/multimodal/profiling/__init__.py` 只做 `from vllm.multimodal.processing import *` 并添加 `DeprecationWarning`。<br>2️⃣ 在 Release Notes 中明确注明 **breaking change**，并给出迁移指南。 |
| **安全** | - 默认在 `ObservabilityConfig` 中将 `enable_mm_processor_stats=False`。<br>- 为公开 API（如 `engine_client.get_timing_stats`）加入权限校验或仅在 `admin` 模式可用。 |
| **性能监控** | - 在高并发集群做压测，重点关注 `timed_operation` 中的锁争用。如果争用显著，可改为 `collections.defaultdict` + `threading.RLock` 或使用 `concurrent.futures.thread.Lock` 替代。 |
| **文档/示例** | - 更新所有外部文档、示例 notebooks、GitHub Action scripts。<br>- 在 `docs/api/README.md` 中加入新 import 路径示例。 |
| **测试** | - 增加 **向后兼容性测试**：在 CI 中临时 `import vllm.multimodal.profiling`，确保 shim 正常工作。<br>- 为计时统计增设单元测试：开启/关闭 `enable_mm_processor_stats`，验证 `get_all_timing_stats` 的

---

### [Refactor] [9/N] to simplify the vLLM openai translations  serving ar chitecture (#32313)
**SHA**: `769d062` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/769d0629e1c55d0a92dbf7244e7a6de4103c6ba4)

**🎯 变更类型**：重构 / 架构变更  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 将 OpenAI 音频（Transcription / Translation）相关的协议、服务实现、路由统一抽离到 `vllm.entrypoints.openai.translations` 包中。  
- `api_server.py` 中的音频端点实现被删掉，改为在 `translations/api_router.py` 中重新注册，同步在 `build_app` 时通过 `register_translations_api_router(app)` 加载。  
- 对 `engine/protocol.py`、`engine/serving.py` 的 import 进行整理，统一使用新协议模块。  
- 新增 `translations/protocol.py`（完整的请求/响应模型、采样参数转换与验证）和 `translations/serving.py`、`translations/speech_to_text.py` 以及对应的路由注册文件。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/api_server.py`（核心 FastAPI 入口）  
- `vllm/entrypoints/openai/engine/protocol.py` 与 `engine/serving.py`（内部协议/服务）  
- 新增的 `vllm/entrypoints/openai/translations/*`（协议、服务、路由）  
- 依赖音频 API 的用户代码（直接 import `TranscriptionRequest`、`TranslationRequest` 等）  

**🔍 技术洞察**  

- **架构影响**  
  - **模块化提升**：音频相关代码从主入口中抽离，形成独立子包，职责更清晰，后续维护、功能扩展（如新增音频模型）更易定位。  
  - **统一路由注册**：通过 `translations/api_router.py` 的 `attach_router` 将路由注入 FastAPI，避免在 `api_server.py` 中散布大量重复装饰器代码。  
  - **状态对象**：`request.app.state.openai_serving_transcription` 与 `openai_serving_translation` 必须在启动阶段（`OpenAIServing` 初始化）正确挂载，否则音频端点会返回 “model does not support” 错误。  

- **性能影响**  
  - **运行时开销基本不变**：新路由层仅是 FastAPI 的一次 `include_router`，与原先直接在 `api_server` 中定义的路径等价；请求路径解析与依赖注入成本相同。  
  - **代码体积下降**：`api_server.py` 减少约 600 行，使得模块加载更快，且消除了大量未使用的导入与变量。  

- **安全考虑**  
  - **输入校验迁移**：`TranscriptionRequest.validate_transcription_request` 与 `TranslationRequest.validate_stream_options` 仍保留在新协议文件中，使用 `HTTPException` 与自定义 `VLLMValidationError` 抛出明确错误码，未引入新安全风险。  
  - **文件上传处理**：仍通过 `UploadFile` 读取二进制，未增加额外的文件大小检查或内容检测，需要在上层或部署环境自行限制。  

**⚠️ 潜在风险**  

1. **向后兼容性**  
   - 外部项目如果直接 `from vllm.entrypoints.openai.engine.protocol import TranscriptionRequest`（旧路径）将会报 `ImportError`；同理对 `api_server` 中删除的端点路径（如 `/v1/audio/transcriptions`）的直接调用仍可工作，但前置依赖 `openai_serving_transcription` 必须已在 `app.state` 中注册。  

2. **应用状态初始化**  
   - 若在自定义启动脚本中未调用 `OpenAIServing`（或未把 `openai_serving_transcription/translation` 加入 `app.state`），音频端点会返回通用错误，导致生产环境不可用。  

3. **路由重复注册**  
   - `build_app` 中已经显式 `register_translations_api_router(app)`；若后续再次在插件或外部代码中手动 `include_router`，会导致路径冲突或 FastAPI 警告。  

4. **测试用例失效**  
   - 所有原先基于旧 `api_server` 中端点的单元/集成测试需更新路由注册路径或导入路径。  

**💡 关注建议**  

- **文档与迁移指南**  
  - 在 vLLM 官方文档中添加 “Audio API 重构”章节，说明新导入路径（`vllm.entrypoints.openai.translations.*`）并给出兼容性 shim（如在 `vllm/entrypoints/openai/__init__.py` 中重新导出旧名称的可选实现）。  

- **确保状态挂载**  
  - 在 `OpenAIServing` 初始化代码中显式创建 `app.state.openai_serving_transcription` 与 `openai_serving_translation`，并在异常路径下给出清晰日志。  

- **回归测试**  
  - 添加针对 `translations/api_router.py` 的端到端测试，覆盖：  
    1. 正常转写/翻译请求返回 JSON 与流式响应。  
    2. `stream` 参数错误时抛出 `VLLMValidationError`。  
    3. 模型不支持音频时返回统一错误信息。  

- **监控与限流**  
  - 考虑在新路由层加入请求大小检查或限流中间件，防止恶意大文件上传导致 OOM。  

- **渐进式发布**  
  - 若社区仍有大量直接使用旧导入路径的代码，可提供临时兼容层（例如在 `vllm/entrypoints/openai/engine/protocol.py` 中保留 `TranscriptionRequest = translations.protocol.TranscriptionRequest` 并标记为 deprecated），并在下一个 major 版本中移除。  

通过上述模块化重构，项目的代码结构更加清晰、可维护性提升；只要在发布说明中明确迁移路径并确保状态初始化，风险可被可控地管理。

---

### Add Molmo2 multimodal model support (#30997)
**SHA**: `7e6f123` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7e6f12381092cd0b457ad10c57fa48ca73c415a7)

**🎯 变更类型**：功能增强 / 多模态模型集成  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 在 vLLM 中新增对 **Molmo2** 多模态模型（支持图像 & 视频）的完整实现，包含模型加载、前向、权重映射、视觉/视频处理模块以及对应的注册、配置与示例。  
- 同步更新文档、示例脚本、测试用例以及模型注册表，新增 `max_num_batched_tokens` 配置项，以满足 Molmo2 Prefix‑LM 的批次约束。  

**🎯 影响范围**  
- `vllm/model_executor/models/molmo2.py`（核心模型实现）  
- `vllm/model_executor/models/registry.py`（模型注册表）  
- `vllm/config/model.py`（MM Prefix‑LM 判定）  
- `vllm/config/multi_modal.py`（`max_num_batched_tokens` 选项）  
- `vllm/multimodal/video.py`（新增 Molmo2 视频后端）  
- `vllm/multimodal/processing.py`（新增 `select_token_ids` 用于 token 级别替换）  
- 示例目录 `examples/offline_inference/`（Molmo2 示例）  
- 测试目录 `tests/models/`（Molmo2 初始化 & 处理路径）  
- 文档 `docs/models/supported_models.md`（列出 Molmo2）  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 在模型层级新增 `Molmo2VisionBackbone`、`Molmo2TextModel`、`Molmo2DecoderLayer` 等子模块，完整遵循 vLLM 的 **PP / TP**（张量并行、流水线并行）框架。<br>- 通过 `is_mm_prefix_lm` 将 Molmo2 标记为 “Prefix‑LM”，以便在 `Engine` 中使用 `max_num_batched_tokens` 限制。</br>- 新增 `Molmo2VideoBackend`，统一视频解码入口，兼容现有 `VideoLoaderRegistry`。<br>- `select_token_ids` 为多模态 Prompt 更新提供了基于 token ID 匹配的通用方式，提升了对特殊占位符（如 `<|image|>`）的处理灵活性。 |
| **性能影响** | - Molmo2 使用 **ViT‑L** 视觉 backbone（27 层）和 **16‑头** self‑attention，显著增加 **GPU 显存** 与 **计算 FLOPs**（尤其在多图/视频场景下）。<br>- 新增的 `max_num_batched_tokens=31872`（在注册表中）限制了单次推理的 token 上限，防止因 Prefix‑LM 的 **大 batch** 触发 OOM。<br>- 视频加载实现通过 **后端选择** 与 **帧抽样**（最大 8 fps）降低解码开销；但仍会在 `load_bytes` 中产生额外 CPU‑to‑GPU copy。 |
| **安全考虑** | - `EngineArgs.trust_remote_code=True` 是对 Molmo2 **CustomModel** 必要的，但会执行模型作者的 `modeling_*.py`，存在 **远程代码执行** 风险。<br>- 新增的 `Molmo2VideoBackend` 读取二进制视频数据并使用 OpenCV，若未限制输入来源，可能导致 **恶意视频** 触发 CVE（如 buffer overflow）。<br>- 文档与示例未显式提醒用户审计第三方代码，需在 release notes 中强化安全警示。 |
| **可维护性** | - 代码量 ~2800 行，逻辑集中在单文件，符合 vLLM 现有模型实现模式（如 Llama、Gemma）。<br>- 使用统一的 **`WeightsMapper`**、**`AutoWeightsLoader`**，便于后续权重切分与 LoRA 兼容。<br>- 引入大量 `@cached_property` 与 `@support_torch_compile`，保持与旧模型的一致性。<br>- 测试覆盖仅加入了 **registry** 与 **token‑patch**，缺少对 **Vision/Video 前处理** 的端到端单元测试，后续需补齐。 |

**⚠️ 潜在风险**  
1. **显存溢出**：Molmo2 在多图/视频推理时，`pixel_values`、`token_pooling` 维度会随图像数量线性增长，若 `max_num_batched_tokens` 配置不足，仍可能触发 OOM。  
2. **远程代码执行**：`trust_remote_code=True` 直接执行第三方仓库代码，若模型仓库被篡改会影响部署安全。  
3. **视频解码兼容性**：OpenCV 的 **backend 选择** 在不同平台（Linux/Windows/macOS）可能不同，导致解码失败或性能波动。  
4. **并行切分不完整**：`Molmo2VisionBackbone.load_weights` 中的 `stacked_params_mapping` 可能遗漏新加入的层（如 `image_pooling_2d`），在 TP/PP 环境下可能出现 **missing shard** 错误。  
5. **测试覆盖不足**：当前仅覆盖模型注册与 token 处理，缺少对 **多模态前处理 → token_pooling** 的完整回归，升级后可能出现 unseen bugs。  

**💡 关注建议**  
- **显存管理**：在文档与示例中明确提示用户根据实际图像/视频数量调节 `max_num_batched_tokens` 与 `max_num_seqs`；建议在 `EngineArgs` 中加入自动检测并给出警告。  
- **安全审计**：在发布说明中加入 “**仅在可信环境**使用 `trust_remote_code=True`” 的提醒；可考虑在 `EngineArgs` 添加 `allow_remote_code` 白名单校验。  
- **跨平台验证**：在 CI 中加入 OpenCV backend 检测脚本，确保 `Molmo2VideoBackend` 在常见 Linux 镜像、MacOS、Windows（如果支持）均能正常加载。  
- **权重加载验证**：为 `Molmo2` 编写专门的 **weights‑sharding 单元测试**，验证在 TP=2、PP=2 等组合下所有参数均被正确映射。  
- **端到端多模态测试**：新增 `tests/multimodal/molmo2_*`，涵盖图像单张、批量图像、短视频（2‑3 帧）路径的完整前向推理，确保 `token_pooling`、`image_projector` 与 LLM 交互正确。  
- **性能基准**：提供基准脚本对比 Molmo2 与 Molmo（单模态）在相同硬件下的吞吐量、延迟与显存占用，帮助用户评估是否开启 Prefix‑LM。  

---  

**结论**：此次 PR 为 vLLM 引入了功能强大的多模态模型 Molmo2，扩展了项目的模型覆盖范围并完善了视频处理能力。它在架构层面与现有框架高度兼容，但因模型体量大、依赖自定义代码及视频解码实现，需重点关注显存管理、远程代码安全与跨平台兼容性。通过上述建议的风险缓解与测试补齐，可将该功能的上线风险降至最低，并为用户提供可靠的多模态推理体验。

---

### [Refactor] [8/N] to simplify the vLLM openai responsesapi_serving architecture (#32260)
**SHA**: `9312a6c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9312a6c03a87e100fdfc2679900221f9c6489a63)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交将原先 **engine.protocol** 中与 *Responses API* 相关的模型、序列化函数以及异常处理抽离到全新的 **openai.responses.protocol** 模块，并在 **openai.responses** 包中实现独立的路由 (**api_router**) 与服务实现 (**serving**) 。随后在所有依赖方（测试、解析器、工具、推理等）统一改为导入新的路径，同时在主服务入口 **api_server.py** 中挂载新的路由。整体目标是简化 *vLLM* 对 OpenAI *Responses* API 的架构，提升代码可维护性和模块化程度。

**🎯 影响范围**  
- `vllm/entrypoints/openai/engine/protocol.py`（大幅删减）  
- 新增 `vllm/entrypoints/openai/responses/protocol.py`（完整实现 Responses 请求/响应模型）  
- 新增 `vllm/entrypoints/openai/responses/api_router.py`（SSE 流式转发）  
- `vllm/entrypoints/openai/responses/serving.py`（服务实现迁移）  
- `vllm/entrypoints/openai/api_server.py`（移除旧路由、挂载新路由）  
- 多处测试、解析器、工具、推理模块的 import 路径改动（约 30+ 文件）  
- 依赖 `vllm/entrypoints/openai/engine/serving.py` 与 `vllm/entrypoints/openai/engine/protocol` 的代码也同步更新。

**🔍 技术洞察**  

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | - 将 *Responses* 相关职责从 `engine` 包拆分为独立的 `responses` 包，实现 **职责单一、模块自治**。<br>- `api_server` 只需在启动阶段一次性挂载 `responses` 路由，避免之前在 `engine` 中混杂的多余路径与大量冗余 import。<br>- 新增 `responses/api_router` 与 `responses/serving`，与原有 `completion`、`chat`、`embedding` 等路由保持一致的组织方式，后续扩展（如新增 SSE 事件或自定义中间件）更具可插拔性。 |
| **性能影响** | - 运行时的 import 结构略有变化（新增模块），对启动时间的影响可以忽略（< 100 ms）。<br>- 业务路径逻辑保持不变，核心推理路径（`OpenAIServingResponses.create_responses` 等）未改动，**推理吞吐、延迟基本不受影响**。<br>- SSE 转换函数 `_convert_stream_to_sse_events` 被抽取到路由文件，保持原有的异步生成器行为，性能保持一致。 |
| **安全考虑** | - 新增的路由同样使用 `validate_json_request` 与统一的错误模型 `ErrorResponse`，保持原有的请求体验证。<br>- 通过 `responses` 函数获取 `app.state.openai_serving_responses`，若服务未实例化则返回统一的错误响应，避免出现 *None* 调用导致的 500。<br>- 迁移过程中对 **OpenAI 客户端版本兼容**（`ResponseTextConfig`/`ResponseFormatTextConfig`）的兼容代码保持不变，未引入新的安全风险。 |
| **可维护性** | - 大幅削减 `engine.protocol` 的体积（从 531 行删至 1 行），降低未来维护成本。<br>- 所有 *Responses* 相关的 Pydantic 模型、序列化、校验均集中在 `responses/protocol.py`，便于统一升级 OpenAI SDK 版本。<br>- 测试文件已更新对应 import，保持 CI 覆盖率不受影响。 |

**⚠️ 潜在风险**  
1. **向后兼容性**：外部插件或用户直接 import `vllm.entrypoints.openai.engine.protocol.ResponsesRequest`（旧路径）将在升级后失效，可能导致 *ImportError*。<br>2. **路由注册遗漏**：如果在自定义构建 `FastAPI` 实例时忘记调用 `register_responses_api_router(app)`，*Responses* 接口将不可达，返回错误信息。<br>3. **状态对象命名冲突**：`api_server` 中原先的 `openai_serving_tokenization` 与新添加的 `openai_serving_responses` 必须在 `FastAPI` 启动阶段正确赋值；若两者顺序错误，可能出现 `None` 调用。<br>4. **依赖版本差异**：`ResponseTextConfig` 的兼容层仍依赖 OpenAI SDK 的内部实现，若未来 SDK 移除该类或更改导入路径，需同步更新 `responses/protocol.py`。<br>5. **测试覆盖不足**：虽然大多数单元测试已改为新路径，但仍有少量集成测试（如在 `examples` 中直接使用旧路径）未更新，CI 通过后仍可能在生产环境触发错误。  

**💡 关注建议**  

- **迁移指南**：在项目的 `README` 或发行说明中明确注明 `Responses*` 相关模型已从 `engine.protocol` 移动到 `responses.protocol`，建议用户使用新路径。<br>- **兼容层**：可以在 `vllm/entrypoints/openai/engine/protocol.py` 中提供向后兼容的 alias（如 `ResponsesRequest = responses.protocol.ResponsesRequest`），在下一个大版本前平滑过渡。<br>- **启动检查**：在 `FastAPI` 启动时加入日志校验，确保 `app.state.openai_serving_responses` 已被初始化；若未初始化，抛出显式错误以便快速定位。<br>- **持续集成**：增加一项检查，搜索旧路径的 import 并警告（linter 或 CI 脚本），防止遗漏。<br>- **文档更新**：在 API 文档（OpenAPI schema）中确保 `responses` 路由的 `tags` 与其它路由保持一致，帮助用户在 UI 中快速发现。  

总体来看，此次 **模块化重构** 明确了 *Responses API* 的边界，提升了代码组织和长期可维护性，且对现有核心功能（推理性能、错误处理）影响极小。只要在升级过程中注意向后兼容与路由注册，即可安全发布。

---

### Consolidate Intel Quantization Toolkit Integration in vLLM (#31716)
**SHA**: `50632ad` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/50632adc583fe1af8e2ee112b48c61c14153574f)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交将 Intel 量化工具链的实现统一到 `inc`（Intel Neural Compressor）模块，删除独立的 `auto-round` 量化实现并在文档、配置、代码中把原 `auto‑round` 替换为 `inc`。同时对量化配置注册、加载逻辑以及测试用例做了相应更新，使 vLLM 在 CPU/XPU/Intel GPU/Gaudi 上统一使用 INC 提供的混合‑位/混合‑类型量化方案。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/quantization/`（`inc.py` 与 `auto_round.py`）  
- 量化方法注册表 `vllm/model_executor/layers/quantization/__init__.py`  
- `vllm/config/model.py`（新增 `inc` 验证）  
- `vllm/engine/arg_utils.py`（删除对 `inc` 的在线量化判定）  
- 文档 `docs/features/quantization/*`（更名、表格更新）  
- 测试 `tests/quantization/test_auto_round.py`（改为普通量化路径）  
- 权重加载工具 `vllm/model_executor/model_loader/weight_utils.py`（去除 `inc` 与 GGUF 的特殊分支）  

---

### 🔍 技术洞察  

#### 架构影响  
- **统一量化入口**：原先的 `auto-round` 作为独立的 `QuantizationMethods` 被完全移除，所有 `auto-round` 相关的配置、层转换逻辑现在由 `INCConfig` 承担。  
- **量化方法映射**：`INCConfig.override_quantization_method` 为模型配置中的 `auto-round` 自动切换为 `inc`，保持向后兼容但强制走同一实现路径。  
- **加载流程简化**：`arg_utils.create_load_config` 中原有 `device="cpu" if is_online_quantization(self.quantization) else None` 被删除，`inc` 不再被视作“在线量化”，避免在模型加载时强制使用 CPU 设备。  
- **模块注册**：`__init__.py` 中 `auto-round` 条目被剔除，仅保留 `inc`，防止用户直接指定已废弃的名称。  

#### 性能影响  
- **后端统一**：INC 在 CPU/XPU/Intel GPU/Gaudi 上已经实现了高效的低位（2‑4 bit）和混合位（W4A16、W8A16）量化，统一后可直接复用已有的 Marlin、AWQ、GPTQ、IPEX 实现，减少重复代码和分支检查，理论上在启动与推理阶段的分支预测与指令缓存命中率会略有提升。  
- **编译时间**：`arg_utils` 中对 “online quantization” 的判断已移除，避免在 `inc` 场景下出现不必要的 CPU 编译路径，降低模型加载时的编译开销。  
- **实际吞吐**：因为 `inc` 已经在 Gaudi/Intel GPU 上针对 FP8、INT4/INT8 等提供专属 kernel，统一后用户可直接在 `vllm serve --quantization inc` 中使用这些高效 kernel，吞吐与前置 `auto-round` 基本持平或略有提升（取决于后端实现）。  

#### 安全考虑  
- **环境变量**：原有 `QUANT_CONFIG` 环境变量仍被保留用于 INC 的校准文件，不涉及新安全风险。  
- **代码执行路径**：删除了 `auto-round` 动态导入路径，减小了潜在的代码注入面向攻击面（仅保留官方维护的 INC 实现）。  
- **兼容性检查**：`INCConfig.override_quantization_method` 中只在 `quant_method == "auto-round"` 时改写为 `inc`，不会误改写其它自定义量化配置，保持安全的显式声明原则。  

---

### ⚠️ 潜在风险  

| 风险点 | 可能影响 | 缓解措施 |
|--------|----------|----------|
| **向后兼容性**：已有用户在 CLI/代码中显式使用 `quantization="auto-round"` 可能在未更新至最新文档前遇到 “unsupported quantization method” 错误。 | 运行时报错，服务不可用。 | 在发布说明中强调已废弃 `auto-round`，新增一段兼容警告（如在 `get_quantization_config` 中加入临时别名或提示）。 |
| **配置文件迁移**：`quantization_config.json` 中仍使用 `auto-round` 关键字，旧模型加载可能失效。 | 模型加载失败。 | `INCConfig.override_quantization_method` 已实现自动替换，但仅在 `quant_method` 字段为 `auto-round` 时生效；确保所有文档和示例更新。 |
| **测试覆盖不足**：仅有 `test_auto_round.py` 小幅修改，未覆盖 `inc` 的完整功能（AWQ、GPTQ、Marlin 等混合场景）。 | 隐蔽回归。 | 增加针对 `inc` 的全量化方法测试矩阵（bits、group_size、backend 组合），尤其是 Gaudi/Intel GPU 环境。 |
| **文档同步**：README 表格已更新，但仍保留对 “INC (W8A8)” 的旧行，可能导致误导。 | 用户误解支持范围。 | 在文档中明确 “所有 Gaudi 相关量化已迁移至 vllm‑gaudi”。 |
| **运行时依赖**：`inc` 依赖的 `auto-round` 包已被移除，若用户仍安装旧版 `auto-round` 可能出现冲突。 | 环境冲突。 | 在 `requirements.txt` 中去除 `auto-round`，并在发行版说明中提醒更新依赖。 |

---

### 💡 关注建议  

1. **发布前兼容提示**：在 `vllm` 的入口（如 `vllm/__init__.py` 或 `arg_utils.create_load_config`）加入对 `quantization="auto-round"` 的一次性警告，提醒用户改用 `inc`。  
2. **完善文档**：  
   - 在 **Quantization** 页面顶部添加 “`auto-round` 已被合并到 `inc`，请使用 `inc`”。  
   - 表格中删除已迁移的 Gaudi 行，或在脚注中说明迁移路径。  
3. **加强测试**：  
   - 为每种后端（CPU、XPU、GPU、Gaudi）添加 `inc` 的端到端推理基准测试。  
   - 包含 “force‑eager” 选项的验证（文档已提示）。  
4. **监控部署**：在正式发布后关注社区反馈，特别是使用 `auto-round` 迁移到 `inc` 的用户，收集报错日志，快速回滚或发布补丁。  
5. **安全审计**：虽然改动不直接影响安全，但建议在 CI 中加入对环境变量（如 `QUANT_CONFIG`）的合法性检查，防止路径注入或恶意配置文件被加载。  

---

**总体结论**：此次合并消除了 `auto-round` 与 `inc` 两套重复实现之间的维护成本，提升了代码可维护性和执行效率。只要妥善处理向后兼容警示并补齐相应的测试用例，风险可控，建议尽快合并至主分支并在下一个正式版本中发布。

---

### [Model Runner V2] Refactor Sampler (#32245)
**SHA**: `90c0836` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/90c0836902daf1766205f1644322231c274c3d7e)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：此次提交将原有的 `SamplingMetadata` 单一结构拆分为多个专职状态对象（`SamplingStates`、`PenaltiesState`、`LogitBiasState`），并在 `Sampler` 中实现分阶段写入、GPU‑UV‑A（Unified Virtual Address）后端管理。`ModelRunner` 与相关子模块同步改为使用新的 API，删除了大量冗余的张量复制与 `SamplingMetadata` 实例化逻辑，提升了内存使用效率并为后续功能扩展奠定基础。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`（核心执行路径）  
- `vllm/v1/worker/gpu/sample/*`（Sampler、Penalties、LogitBias、States）  
- `vllm/v1/worker/gpu/spec_decode/eagle.py`（采样相关的温度/种子传递）  
- 删除 `vllm/v1/worker/gpu/sample/metadata.py`（旧 dataclass）  

**🔍 技术洞察**  

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | 1. **职责划分明确**：`SamplingStates` 负责温度、top‑k/p、min‑p、seed、logprobs 等 per‑request 参数；`PenaltiesState` 负责重复、频率、出现惩罚及其统计计数；`LogitBiasState` 负责 token‑level bias。<br>2. **统一的分阶段写入**：`apply_staged_writes` 在一次 GPU‑CPU 同步后统一复制至 UV‑A，减少多次 `copy_to_uva` 带来的同步开销。<br>3. **去中心化**：不再通过 `SamplingMetadata` 进行一次性包装，避免在每次采样时创建大量小对象，降低 Python GC 负担。 |
| **性能影响** | - **内存占用**：`UvaBackedTensor` 让大量 per‑request 参数常驻 GPU，且只在需要时同步到 CPU，显著削减了在 `model_runner` 中的临时张量拷贝（原先 `make_dummy`、`make_sampling_metadata` 等）。<br>- **吞吐提升**：`Sampler.__call__` 现在直接使用已同步好的张量进行 `apply_penalties_and_temperature`、`apply_min_p`、`apply_top_k/top_p`，减少了 `torch.gather`/`torch.copy_` 的中间步骤。<br>- **潜在开销**：首次 `UvaBackedTensor.copy_to_uva` 会触发一次 CPU→GPU 同步，若请求数频繁波动可能出现短暂的同步瓶颈。整体预计 **10%‑20%** 的推理吞吐提升（需实测验证）。 |
| **安全考虑** | - **随机种子管理**：种子现在统一由 `SamplingStates.seeds` 负责生成，避免了过去在不同路径上使用 `np.random` 产生不一致的种子。<br>- **日志与异常**：删除 `SamplingMetadata` 并未引入新的外部输入，风险主要在 **数组越界**（`idx_mapping_np` 与内部状态长度不匹配）和 **未初始化的 UV‑A**（如果 `copy_to_uva` 被遗漏）。建议在关键路径加入 `assert` 检查。 |
| **可维护性** | - **代码组织更清晰**：每个子模块只处理自己的职责，后续如新增 “token‑frequency decay” 只需在 `PenaltiesState` 中扩展。<br>- **测试粒度**：原来的 `SamplingMetadata` 统一测试已被拆分，需要为每个新类补齐单元测试。 |

**⚠️ 潜在风险**  

1. **同步错误**：`apply_staged_writes` 如果在 `Sampler.__call__` 前未被调用，可能导致使用到旧的参数（温度、top‑k 等）产生错误的采样分布。  
2. **数组映射不匹配**：`idx_mapping_np` 必须与 `max_num_reqs` 一致；任何越界访问会触发 CUDA 异常或产生错误的惩罚统计。  
3. **遗留代码依赖**：除 `model_runner` 外，项目中仍可能有隐式引用 `SamplingMetadata`（如第三方插件或旧的 notebook 示例），在未同步的情况下会抛 `ImportError`。  
4. **GPU‑UV‑A 兼容性**：在旧版 CUDA/驱动或不支持 UV‑A 的环境中，`UvaBackedTensor` 可能退化为普通张量，导致性能倒退甚至内存碎片。  
5. **日志概率（logprobs）处理**：`max_num_logprobs` 现在返回 `-1` 表示不计算，若上层代码仍检查 `None` 可能导致逻辑分支错误。  

**💡 关注建议**  

- **单元/集成测试**：为 `SamplingStates.add_request`、`apply_staged_writes`、`PenaltiesState.apply_penalties_and_temperature`、`LogitBiasState.apply_logit_bias` 分别编写覆盖所有分支的测试。包括 `top_k/top_p/min_p` 均为默认值、非默认值的组合。  
- **性能基准**：在同一硬件上对比重构前后 `model_runner.execute_model + sample_tokens` 的吞吐与显存占用，特别关注 **大 batch（>256）** 与 **多 token‑length** 场景。  
- **安全检查**：在 `Sampler.__call__` 与 `ModelRunner.sample_tokens` 中加入 `assert idx_mapping_np.max() < self.sampler.sampling_states.max_num_reqs` 之类的边界校验，防止因索引错误导致未定义行为。  
- **文档升级**：更新开发者文档，说明 `Sampler` 构造函数新参数含义、`add_request` 必须在 `ModelRunner.update_states` 中被调用的顺序，以及如何在自定义插件中获取 `temperature`、`seeds` 等。  
- **兼容层**：若项目仍需支持老版插件，可在 `vllm/v1/worker/gpu/sample/metadata.py` 中保留一个轻量包装类，将内部调用转发到新的状态对象，避免突兀的破坏式更新。  
- **GPU‑UV‑A 兼容性检测**：在启动阶段检测 `torch.cuda.get_device_capability` 是否支持 UV‑A，若不支持则回退到普通张量并记录警告，以免在不兼容环境中出现崩溃。  

---  

**总体结论

---

#### 🟡 中重要度变更 (13)

### [Misc] Make mem utils can be reused by other platforms (#32322)
**SHA**: `ce09462` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ce0946249d28f263930f2789186e49db242d1834)

**🎯 变更类型**：功能增强（为内存工具加入平台抽象，使其可在除 CUDA 之外的后端复用）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- `vllm/platforms/interface.py` 的 `__getattr__` 逻辑改为只有当属性值不为 `None` 时才直接返回，否则统一走警告路径并返回 `None`。  
- `vllm/utils/mem_utils.py` 大幅重构：从直接调用 `torch.cuda.*`，改为统一走 `current_platform` 提供的接口（`memory_stats`、`mem_get_info`、`memory_reserved`、`empty_cache`、`reset_peak_memory_stats` 等），并在文件顶部一次性导入 `current_platform`。  
- `gpu_worker.py` 中手动的 `torch.cuda.empty_cache()`、`reset_peak_memory_stats()` 与 `gc.collect()` 被删除，改由 `memory_profiling` 内部的统一平台实现完成。  
- `MemorySnapshot.__repr__` 中的字段名由固定的 `cuda_memory` 改为 `{current_platform.device_name}_memory`，提升跨平台可读性。

**🎯 影响范围**  
- **平台抽象层**（`vllm/platforms`）  
- **内存工具**（`vllm/utils/mem_utils.py`）  
- **GPU/CPU worker**（`vllm/v1/worker/gpu_worker.py`）  
- 可能受影响的 **单元测试、监控脚本以及日志解析**（原先硬编码的 `cuda_memory` 字段名）

**💡 关注建议**  

1. **平台实现完整性**  
   - 确保所有已支持的平台（CUDA、ROCm、CPU‑only 等）在 `vllm/platforms` 中实现以下方法：  
     `get_current_memory_usage`, `memory_stats`, `mem_get_info`, `memory_reserved`, `empty_cache`, `reset_peak_memory_stats`, `device_name`。  
   - 若某平台缺失上述任意一项，`__getattr__` 会返回 `None` 并打印警告，这很可能导致 `MemorySnapshot.measure()` 抛出 `AttributeError` 或产生错误的内存统计。建议在抽象基类中声明为 **abstractmethod**，或在缺失时抛出更明确的异常。

2. **`__getattr__` 行为变更**  
   - 现在即使属性存在但值为 `None`（如某平台的 `torch.device` 可能返回 `None`），也会走警告分支。若业务代码本来依赖这种 “存在但空” 的语义，需额外判断 `hasattr` 而非直接访问。建议在文档中说明该细节，或在 `__getattr__` 中保留对 `None` 的直接返回（仅在确定不会误报的情况下）。

3. **日志/解析兼容性**  
   - `MemorySnapshot.__repr__` 的字段名已从 `cuda_memory` 变为 `{device}_memory`。如果外部监控或日志解析脚本仍假设字段名为 `cuda_memory`，需要同步更新或在迁移期间保持向后兼容（例如保留旧字段别名）。

4. **单元测试与持续集成**  
   - 添加跨平台的 `mem_utils` 测试：在不依赖 CUDA 的环境中，使用模拟平台对象验证 `memory_profiling`、`MemorySnapshot.measure` 等函数的行为。  
   - 对 `gpu_worker.determine_available_memory` 的路径进行覆盖，以确保移除的 `torch.cuda.empty_cache()`/`gc.collect()` 不影响内存测算的准确性。

5. **性能与资源**  
   - 逻辑上仅是调用层的切换，对性能影响微乎其微。但请关注 `current_platform.memory_stats` 与 `torch.cuda.memory_stats` 在不同后端实现上的开销差异，必要时在平台实现中加入缓存或懒加载。

整体来看，此次改动为 vLLM 引入了平台无关的内存管理抽象，提升了代码在多硬件后端的可移植性。只要平台实现齐全、文档同步更新并补足相应的测试，即可顺利上线。

---

### [Frontend] Standardize use of `create_error_response` (#32319)
**SHA**: `3f28174` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3f28174c6a6d16dd42405016986a36f9d17e57c0)

**🎯 变更类型**：重构/功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：统一所有 OpenAI/Pooling/Serve 入口的异常处理方式，去除直接抛 `HTTPException`，改为调用 `handler.create_error_response`，并在 `create_error_response` 中扩展对 `OverflowError`、`NotImplementedError` 等异常的映射以及对异常信息的 `sanitize_message`。此外对 `vllm.entrypoints.utils` 做了 TYPE_CHECKING 条件导入，减轻运行时依赖。

**🎯 影响范围**  
- 多个 API Router（`api_server.py、chat_completion/api_router.py、responses/api_router.py、pooling/*/api_router.py、serve/*/api_router.py`）的异常路径统一。  
- `vllm/entrypoints/openai/engine/serving.py` 与 `serving_models.py` 中的 `create_error_response` 逻辑升级。  
- `vllm/entrypoints/utils.py` 引入条件导入，影响到参数校验、日志和 LoRA 处理等工具函数。

**💡 关注建议**  
1. **行为验证**：确认返回的 `ErrorResponse` 能被 FastAPI 正确序列化并返回相应的 HTTP 状态码，尤其是 `BadRequest`、`NotImplemented` 场景。  
2. **测试覆盖**：为 `OverflowError`、`NotImplementedError` 以及异常信息被 `sanitize_message` 截断的路径补充单元/集成测试，防止信息泄漏或状态码回退。  
3. **日志与追踪**：现有 `create_error_response` 仍保留 `traceback.print_*`，建议改为统一的日志记录，以便在生产环境中统一审计。  
4. **向后兼容**：如果外部代码仍捕获 `HTTPException`，需确认其不会因改为返回 `JSONResponse` 而导致意外行为。可考虑在文档中说明异常处理已迁移。  
5. **性能检查**：大量入口函数改为返回 `ErrorResponse` 再由统一代码包装，理论上影响不大，但建议在高并发场景下跑一次基准，确认没有意外的额外序列化开销。  

总体而言，此次改动提升了错误处理的一致性和安全性，只要在上述关键点做好回归测试，即可安全合入。

---

### [Refactor] Move top-level dummy data generation to registry (#32310)
**SHA**: `90db5b3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/90db5b31e4eeb8fc8bfb3260791bf795fda4f633)

**变更概述**  
本次提交将多模态模型的「dummy」数据生成从各处理器内部抽离，统一交由 `MULTIMODAL_REGISTRY` 实现。新增 `Registry.get_dummy_mm_inputs`（返回 `MultiModalInputs`），并在内部使用处理器的 `dummy_inputs.get_dummy_processor_inputs` 再调用 `processor.apply` 完成数据填充；对应的 `BaseDummyInputsBuilder.get_decoder_dummy_data` 与 `DummyDecoderData` 被移除。相关测试和 `gpu_model_runner` 也同步改为调用新接口。

**影响范围**  
- `vllm/multimodal/registry.py`：核心逻辑迁移，API 由 `get_decoder_dummy_data` 改为 `get_dummy_mm_inputs`。  
- `vllm/multimodal/profiling.py`：删除旧的 `DummyEncoderData / DummyDecoderData`，保留 `BaseDummyInputsBuilder` 供处理器使用。  
- `vllm/multimodal/processing.py`：仅调整导入顺序。  
- `vllm/v1/worker/gpu_model_runner.py`：改为使用新返回结构的 `mm_kwargs`。  
- 测试目录：相应更新调用方式，确保旧接口不再出现。

**关注建议**  
1. **兼容性检查**：确认项目其他位置（如外部插件或自定义处理器）是否仍依赖已删除的 `get_decoder_dummy_data` / `DummyDecoderData`，必要时提供向后兼容包装。  
2. **缓存行为**：`get_dummy_mm_inputs` 现在会在 `processor.apply` 时写入缓存，而 `gpu_model_runner` 中的注释表明未读取缓存。请确认此行为在并发或多进程场景下不会导致重复计算或缓存冲突。  
3. **错误信息**：原先在 `get_decoder_dummy_data` 中会在 dummy token 数不足时抛异常，迁移后改为在 `registry.get_dummy_mm_inputs` 中补齐 token。若有业务依赖异常，请在文档中说明差异。  
4. **文档更新**：更新 API 文档、示例代码以及 README 中关于 “profiling dummy data” 的使用说明，指明新的入口函数及返回类型。  
5. **回归测试**：运行全部多模态相关单元测试，特别是 `test_profiling` 与 `test_limit_mm_per_prompt_dummy`，确保生成的 `mm_placeholders` 与 `mm_kwargs` 完整对应。  

总体来看，此次重构提升了 dummy 数据生成的统一性和可维护性，但需注意已删除接口的潜在外部依赖以及缓存交互的细节。若后续仍需对 decoder‑side dummy 数据进行细粒度控制，可考虑在 `BaseDummyInputsBuilder` 中提供专用的包装函数。

---

### [Model] Re-implement Qwen3Omni Audio Encoder (#32167)
**SHA**: `b8199f6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b8199f604931012c433e018980aac22d44ad30b5)

**🎯 变更类型**：功能增强（在 vLLM 中重新实现 Qwen‑3‑Omni 音频编码器）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 删除对 HuggingFace `Qwen3OmniMoeAudioEncoder` 的直接引用，改为在 vLLM 内部实现完整的音频编码器，包括卷积前端、位置嵌入、Transformer 编码层以及输出投射。  
- 新增 `SinusoidsPositionEmbedding、Qwen3OmniMoeAudioAttention、Qwen3OmniMoeAudioEncoderLayer、Qwen3OmniMoeAudioEncoder` 四大组件，并使用 vLLM 自研的 `MMEncoderAttention`、`ColumnParallelLinear`、`RowParallelLinear`、`QKVParallelLinear` 支持张量并行与 Flash‑Attention。  
- 调整 `Qwen3OmniMoeThinker` 初始化：`audio_tower` 现在接受 `multimodal_config` 与前缀，去掉强制开启 `flash_attention_2` 的旧逻辑。  
- 实现自定义权重加载映射，兼容 HuggingFace 的参数命名规则。  

**🎯 影响范围**：  
- `vllm/model_executor/models/qwen3_omni_moe_thinker.py`（核心实现）  
- 依赖的底层算子：`vllm/model_executor/layers/attention/mm_encoder_attention.py`、并行线性层等  
- 多模态配置 `MultiModalConfig`（音频‑视觉注意力后端选择）  

**💡 关注建议**  
1. **兼容性检查**：确认模型权重的 `qkv` 拆分与 `QKVParallelLinear` 的 shard 标识一致，防止加载时维度错位。  
2. **后端选择**：`compute_attn_mask_seqlen` 仅在 Flash‑Attention/ROCM 后端返回 `max_seqlen`，若用户关闭这些后端，注意 `max_seqlen` 为 `None`，避免 `MMEncoderAttention` 报错。  
3. **并行与显存**：新实现在大规模张量并行时会划分 `num_local_heads`，建议在多节点/TP 环境下验证 `get_tensor_model_parallel_world_size` 正确返回。  
4. **性能回归**：对比原 HF 实现的输出（尤其在 fp16 下的 clamp 逻辑）和新实现的数值误差；必要时在 `torch.compile` 场景下检查 `support_torch_compile` 装饰器兼容性。  
5. **单元测试**：增加对 `SinusoidsPositionEmbedding`、分块卷积路径（`conv_chunksize`）以及 `cu_seqlens` 生成的边界情况（例如极短/极长音频） 的覆盖，防止出现 OOM 或序列截断错误。  

总体来看，此次改动为 Qwen‑3‑Omni 引入了原生、可并行的音频前端，提升了可定制性与潜在性能。上线前务必完成权重兼容、后端切换以及显存/数值回归测试。

---

### [Docs] Add docs about OOT Quantization Plugins (#32035)
**SHA**: `6388b50` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6388b50058eaffe357f94ff0ab3dd0734c86b0e4)

**🎯 变更类型**：文档/轻量功能扩展  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `docs/features/quantization/README.md` 中新增“Out‑of‑Tree Quantization Plugins”章节，提供自定义量化插件的完整使用、实现指南以及示例代码。  
- 在 `vllm/model_executor/layers/quantization/__init__.py` 的 `__all__` 中加入 `register_quantization_config`，对外暴露插件注册装饰器。

**🎯 影响范围**  
- **文档子系统**：新增约 150 行说明，影响阅读体验。  
- **量化模块公开接口**：`register_quantization_config` 现在是公共 API，任何外部包都可以通过 `import vllm.model_executor.layers.quantization as q` 使用该装饰器。  
- **插件生态**：为第三方自定义量化方法提供正式入口，间接影响 `LLM` 实例化时的 `quantization` 参数解析。

**💡 关注建议**  

| 方向 | 建议 |
|------|------|
| **兼容性** | 确认 `register_quantization_config` 在 `__init__.py` 顶部已被 `from .registry import register_quantization_config`（或等价导入）引入，防止导入时报 `NameError`。 |
| **安全性** | 插件注册是全局状态，建议在文档中强调“在创建 `LLM` 前必须先 import 插件模块”，并在实现中使用线程安全的 `dict`（如 `collections.defaultdict`）防止并发冲突。 |
| **错误提示** | 当插件返回 `None` 或不实现必须的 `QuantizeMethodBase` 时，现有代码会回退为未量化。可以在 `register_quantization_config` 中加入冲突检测（同名插件警告）以及缺失实现的显式异常，以帮助开发者快速定位问题。 |
| **测试** | 新增单元测试：<br>1. 注册两个不同名称的插件，确认 `get_quantization_config` 能正确返回对应类。<br>2. 同名插件二次注册应抛出或覆盖（文档需说明）。 |
| **文档** | ① 在 CHANGELOG 中添加 “支持 OOT Quantization Plugins”。<br>② 在 README 或快速入门中加入“一行导入即启用自定义量化”示例，降低使用门槛。 |
| **生态** | 鼓励社区提交 `my_quant` 示例仓库，并在 `docs/plugins` 中列出已知插件列表，形成良性循环。 |

**总体评估**  
此次 PR 主要是对外暴露已有的注册装饰器并补充完整文档，对核心运行时逻辑几乎没有影响，风险相对可控。但因涉及全局注册机制，建议在代码层面加上冲突检测及明确的错误信息，并补充对应的自动化测试，以防后续插件生态出现隐藏的注册冲突或未实现方法导致的运行时错误。

---

### AMD CI Test - unskip moe_sum test and moe_align_block_size tests (#32039)
**SHA**: `048bb59` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/048bb597286cc7cb1e30e03b366aef1e2b393b39)

**🎯 变更类型**：其他（测试解锁 & 平台适配）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交将原本在 ROCm 环境下被 `skipif` 跳过的两组 MoE（Mixture‑of‑Experts）单元测试取消，恢复在 AMD GPU 上的完整验证；同时在 `vllm/platforms/rocm.py` 中新增 `import_kernels` 方法，确保在 ROCm 环境启动时尝试加载 `vllm._rocm_C` 扩展，若不存在则静默忽略。  

**🎯 影响范围**  
- `tests/kernels/moe/`：`test_moe.py`、`test_moe_align_block_size.py` 两个文件的测试用例现在会在 ROCm CI 上运行，涉及 `moe_sum`、`moe_align_block_size`、`moe_align_block_size_with_expert_map` 等核心 kernel。  
- `vllm/platforms/rocm.py`：平台检测与 kernel 导入路径变化，可能影响所有在 ROCm 上初始化 vLLM 的代码路径。  

**💡 关注建议**  
1. **CI 通过率**：启用这些测试后，CI 可能会因 ROCm 设备兼容性或 `_rocm_C` 编译问题出现失败。建议在 AMD GPU 镜像上先手动跑一遍，确认 `vllm._rocm_C` 能被成功编译/加载。  
2. **回退机制**：`import_kernels` 使用 `contextlib.suppress(ImportError)`，如果扩展缺失会默默跳过，可能导致运行时功能缺失而不易定位。建议在日志中加入一次 `warning`，提醒用户该扩展未加载。  
3. **文档同步**：既然已恢复 ROCm 下的 MoE 测试，需在 README/CONTRIBUTING 中更新对 AMD GPU 的支持说明，尤其是对 `bitsandwords`、`bitsandbytes` 等量化特性的可用性描述。  
4. **平台检测**：`RocmPlatform.is_rocm()` 仍然用于跳过测试的旧装饰器被移除，确保没有其他地方仍依赖该条件进行功能分支，否则可能出现未预期的行为。  

总体来看，此次改动提升了 AMD GPU 上的测试覆盖，若配合相应的 ROCm 编译链与扩展库，可在多平台间实现更一致的功能验证。开发者在提交新 kernel 前，务必在 AMD 环境下跑完整套 MoE 测试，避免因平台差异导致回归。

---

### [misc] Remove is_torch_equal_or_newer(2.4) cases (#32296)
**SHA**: `7933638` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/79336380518a95db1b2b78edf156b88a5605f190)

**🎯 变更类型**：重构 / 其他  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 删除了对 `torch>=2.4` 的版本检测（`is_torch_equal_or_newer("2.4")`）以及对应的 `supports_dynamo()`、`supports_custom_op()` 判断。  
2. `decorators.py`、`gpu_model_runner.py` 中不再因缺少 Dynamo 或自定义 Op 支持而跳过编译路径。  
3. `parallel_state.py` 直接注册所有自定义算子，去掉了 “如果不支持则不注册” 的分支。  

**🎯 影响范围**  
- `vllm.compilation.decorators` – 编译开关逻辑变化。  
- `vllm.distributed.parallel_state` – 自定义 Op 无条件注册。  
- `vllm.utils.torch_utils` – 移除 `supports_dynamo`、`supports_custom_op` 实现。  
- `vllm.v1.worker.gpu_model_runner` – `load_model` 中的 Dynamo 检查被删掉。  

**💡 关注建议**  
1. **兼容性风险**：在 PyTorch <2.4（尤其是 2.3 及以下）上，`torch.library.custom_op` 与 `FakeScalarType` 仍不可用，强制注册会抛 `AttributeError` 或运行时错误。建议在入口处加上明确的错误提示或 fallback（如使用旧的实现或直接 abort）。  
2. **文档同步**：README/Installation 中应说明 vLLM 现在默认要求 PyTorch ≥2.4，或提供旧版兼容指引。  
3. **测试覆盖**：加入 CI 对旧版 PyTorch（例如 2.3）进行负向测试，确保出现的异常能够被捕获并给出友好提示。  
4. **代码可维护性**：如果未来仍希望保留对低版本的支持，建议把统一的 “torch_version_check” 工具封装起来，避免散落的硬编码判断导致回滚困难。  

总体而言，此次重构简化了编译路径并去除了不必要的版本判断，但对低于 2.4 的 PyTorch 环境会产生回归，需通过文档、错误处理及测试来降低用户升级的阻力。

---

### [Kernel][Performance] Enable smaller Scaling Factor tiling for NVFP4 small-batch decoding (#30885)
**SHA**: `8ef50d9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8ef50d9a6b91b7800e69a846219069a29a0298a4)

**🎯 变更类型**：性能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 NVFP4 小批量解码引入 8×4 规模因子 (Scaling Factor) tiling，降低了最小块大小（原 128×4 → 8×4），并在量化/反量化路径、FlashInfer 接口以及模型执行层中加入相应的布局切换逻辑。  

**🎯 影响范围**  
- **量化/反量化实现**：`tests/kernels/quantization/nvfp4_utils.py`、`vllm/_custom_ops.py`、`vllm/utils/flashinfer.py` 以及 `vllm/model_executor/layers/quantization/*`。  
- **配置与环境**：`vllm/envs.py` 新增 `VLLM_NVFP4_GEMM_BACKEND` 选项。  
- **测试**：新增/扩展了 NVFP4 相关单元测试，覆盖 8×4 与 128×4 两种布局。  
- **CI 流程**：Buildkite 流水线加入 `tests/models/quantization/test_nvfp4.py`。  

**💡 关注建议**  

1. **布局切换条件**  
   - 当前使用 `backend == "trtllm" and m <= 32` 判断是否走 8×4 布局，逻辑相对硬编码。建议抽象为专用函数或配置项，防止未来新增后端或不同尺寸阈值导致误判。  

2. **向后兼容**  
   - `scaled_fp4_quant` 参数默认值仍为 `"none"`，但多数调用已传入 `self.backend`。若用户未显式设置后端，仍会走旧路径（128×4），需要在文档中说明默认行为或在函数内部给出兼容警告。  

3. **Fake Op 与真实实现一致性**  
   - `flashinfer_nvfp4_quantize_fake` 返回的 `block_scale` 大小为 `(rounded_m, rounded_n)`（uint8），而真实实现返回 `(rounded_m, rounded_n // 4)`（int32）后再 view 为 `float8_e4m3fn`。确保形状在 JIT/torch.compile 场景下保持一致，否则可能触发形状不匹配错误。  

4. **性能验证**  
   - 新增的 8×4 tiling 只在“小 batch”(m≤32) 时生效，建议在 CI 中加入对不同 m 值的基准测试，确认在常见推理场景下的吞吐提升与内存占用变化。  

5. **文档与示例**  
   - 在 README/Quantization 章节补充 “NVFP4 8×4 布局开启方式” 与 “何时使用 `flashinfer-trtllm` 后端” 的使用说明，帮助用户快速上手。  

总体来看，改动围绕 NVFP4 小批量解码的性能瓶颈，加入了灵活的布局切换与对应的测试，风险点主要在条件判断的硬编码与 Fake‑Op 与真实实现的形状保持上。只要按以上建议完善文档与兼容性检查，改动对现有功能的影响应可控。

---

### [Build] Add scripts for cherry-picking and trigger build (#32282)
**SHA**: `0db574b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0db574b185ffa90a86569bad9a014b9c03f27f53)

**🎯 变更类型**：其他（构建/CI 脚本新增）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交向 `.buildkite/scripts/` 目录新增两套 Bash 脚本：  
1. `cherry-pick-from-milestone.sh` – 自动从 GitHub 某里程碑获取已合并 PR 的 merge‑commit，判断哪些尚未出现在当前分支，并以 `git cherry-pick` 命令的形式输出，支持 dry‑run 与实际执行。  
2. `trigger-ci-build.sh` – 使用 Buildkite 官方的 `bk` CLI 为当前提交和分支触发 CI 构建，默认 dry‑run，默认注入 `RUN_ALL=1`、`NIGHTY=1` 环境变量。  

**🎯 影响范围**  
- **CI/CD 流程**：`trigger-ci-build.sh` 直接影响 Buildkite pipeline 的触发方式，主要用于手动或自动化触发 nightly/全量测试。  
- **开发者工作流**：`cherry-pick-from-milestone.sh` 为维护里程碑特性的 cherry‑pick 提供便利，涉及 `git` 与 `gh` CLI 的交互。  
- **文档与 CI 环境**：需要在项目文档和 CI 镜像中确保 `bash`、`git`、`gh`、`bk` 均已安装并可用。

**💡 关注建议**  

1. **依赖检查与错误信息**  
   - 两个脚本均通过 `command -v` 检查工具是否存在，已算完备。建议在 `cherry-pick-from-milestone.sh` 对 `gh` 版本做最小限制（如 2.0+），防止 API 行为变更导致解析失效。  
   - `trigger-ci-build.sh` 在检验提交是否已推送到远程时，仅使用 `git branch -r --contains`，在大仓库或多远程情形下可能产生误判，可考虑加上 `git ls-remote origin ${COMMIT}` 进一步确认。

2. **安全与幂等**  
   - 两个脚本默认 **dry‑run**，符合安全原则。若使用 `--execute`，建议在脚本内部再次提示用户确认（`read -p "Proceed? (y/n)"`），防止误操作。  
   - `cherry-pick-from-milestone.sh` 在实际执行时一次性 `git cherry-pick $sha`，若提前出现冲突会中断并要求手动解决，这已明确提醒。可再提供 `--continue-on-conflict` 选项，自动暂停并记录冲突文件。

3. **跨平台兼容性**  
   - 脚本使用 Bash 特性（`set -euo pipefail`、数组等），在 Windows Git‑Bash 环境基本可运行，但最好在 README 中注明 “Linux/macOS (bash ≥ 4)”。  
   - 颜色码在不支持 ANSI 的终端会出现乱码，可加入检测 `[[ -t 1 ]] &&` 决定是否启用颜色。

4. **日志与可测性**  
   - 所有 `log_*` 函数统一前缀，便于 CI 日志过滤。可以考虑提供 `--quiet` 参数以抑制信息输出，供自动化脚本调用。  
   - 为防止误删临时文件，`trap "rm -f $PR_DATA"` 已设置，良好。

5. **文档**  
   - 将脚本用法、前置条件、示例写入项目根目录的 `CONTRIBUTING.md` 或单独的 `scripts/README.md`，并在 CI 文档中注明 `bk` 与 `gh` 的配置步骤。  
   - 在 `.buildkite/pipeline.yml` 中加入这两个脚本的调用示例，确保新成员能够直接使用。

6. **测试**  
   - 建议在 CI 中加入 **smoke test**：在 pull‑request 环境执行 `--dry-run`，检查返回码 0 并验证输出格式。  
   - 对 `cherry-pick-from-milestone.sh` 可使用 `gh api` 的 mock（如 `gh` 的 `--repo` 参数配合本地仓库）进行单元测试，防止 GitHub API 变动导致脚本失效。

**总体结论**  
本次提交为 vLLM 项目引入了实用的内部工具，提升里程碑维护与手动 CI 触发的效率。只要在文档、依赖版本约束及安全提示上再做少量补充，即可安全上线。

---

### fix(rocm): Use refresh_env_variables() for rocm_aiter_ops in test_moe (#31711)
**SHA**: `69f8a0e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/69f8a0ea37a018b2e54897c44f8832de51a2bf5c)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `tests/kernels/moe/test_moe.py` 中，原先通过 `importlib.reload(rocm_aiter_ops)` 强行重新加载模块以读取新环境变量，现改为统一使用 `rocm_aiter_ops.refresh_env_variables()`，并在设置 `VLLM_ROCM_USE_AITER` 前统一写入 env。去除了对 `sys`、`importlib` 的不必要依赖，简化了环境变量的刷新流程。

**🎯 影响范围**  
- `tests/kernels/moe/test_moe.py`（ROCm MoE 相关单元测试）  
- 可能影响其它依赖 `rocm_aiter_ops` 环境变量的测试或运行时代码，因为现在环境变量的刷新方式已统一。

**💡 关注建议**  
1. **确保 `refresh_env_variables` 的实现是幂等且线程安全**，避免在并行测试中出现状态残留。  
2. 完整跑一遍 CI（包括 CPU、CUDA 与 ROCm 测试），确认不再出现因旧缓存导致的错误。  
3. 若项目中还有手动 `importlib.reload` 的旧用法，建议统一迁移到 `refresh_env_variables`，保持行为一致。  
4. 文档中补充说明：在测试或运行时改变 `VLLM_ROCM_USE_AITER`，务必调用 `refresh_env_variables()` 使改动生效。  

此改动提升了代码可维护性，降低了对 Python 模块加载机制的依赖，推荐在后续开发中继续沿用该刷新方式。

---

### [Perf] Optimize grouped topk kernel, 1.2%~2% E2E Throughput improvement (#32058)
**SHA**: `f28125d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f28125d87b6a5976a1584267077d4f0287ce6ce2)

**🎯 变更类型**：性能优化 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原先的两阶段 *top‑k‑with‑k2* + *group‑idx‑and‑topk* 实现合并为单个 `grouped_topk_fused_kernel`，通过改写 WarpSelect、引入共享内存布局、添加 `WarpSelect::get_val/get_idx` 接口，去掉临时 `group_scores` 张量并优化 SM 用量，实现 1.2%–2% 的端到端吞吐提升。同步更新 Python 包装层，兼容 Kimi 配置字段 `num_expert_group` / `num_experts_per_token`。

**🎯 影响范围**  
- `csrc/moe/grouped_topk_kernels.cu`（GPU kernel、WarpSelect 逻辑）  
- `vllm/moe` Python 接口 `grouped_topk`（调用签名变化、去除 `group_scores` 参数）  
- 单元测试 `tests/models/utils.py`（新增对 Kimi 配置字段的兼容）  

**💡 关注建议**  
1. **兼容性**：`invokeNoAuxTc` 的函数签名已删除 `group_scores` 参数，确保所有内部调用和可能的外部插件已同步更新；若仍需保留旧接口，可提供轻量包装层。  
2. **正确性**：合并后所有线程必须统一调用 `WarpSelect::add`，请在不同 `n_group`（尤其 1、2、4、8、16、32）下跑完整的数值回归（包括极端 `topk > topk_group * (num_experts / n_group)` 场景），验证 `topk_indices` 与 `topk_values` 与旧实现完全一致。  
3. **共享内存**：新 kernel 使用两段共享内存（内部 staging + 用户 buffer），动态 SM 大小计算依赖 `warp_topk::round_up_to_multiple_of<256>`。建议在不同 GPU 架构（如 A100、H100、RTX 4090）上检查 `cudaFuncSetAttribute(..., cudaFuncAttributeMaxDynamicSharedMemorySize)`，防止因 SM 超限导致 launch 失败。  
4. **性能基准**：除整体吞吐提升外，关注 **warp_id >= n_group** 的提前返回路径是否在高 `n_group`（如 32）情况下产生不必要的空 block；可通过 `nvprof`/`nsight` 统计 warp idle 时间，进一步微调 block‑size 与 warp‑per‑group 设定。  
5. **文档与测试**：更新模块说明，明确 `grouped_topk` 只支持 `topk ≤ 32`、`n_group ≤ 32`，并在 README / API doc 中标记新字段 `num_expert_group`、`num_experts_per_token` 的兼容逻辑。扩展 CI 用例覆盖这些字段。  

总体而言，本次改动显著简化了 MoE Top‑K 实现并提升了 GPU 端性能，但请务必在多卡、多模型配置下完成回归验证，防止共享内存或 warp‑同步失误导致稀疏路由错误。

---

### [responseAPI] support partial message generation (#32100)
**SHA**: `af54d2e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/af54d2e2d0f6d8a3bb3bd3789961810203a1fc40)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：新增 `should_continue_final_message` 用于检测输入中最后一条是否为 “in_progress”/“incomplete” 的 assistant 消息或 reasoning，进而在 OpenAI‑compatible 接口中开启 partial‑completion 模式；相应在 `serving_responses.py` 传递 `add_generation_prompt=False`、`continue_final_message=True`；并补充了大量单元测试覆盖各种对象、字典及异常情况。  

**🎯 影响范围**：  
- `vllm/entrypoints/responses_utils.py`（核心实现）  
- `vllm/entrypoints/openai/serving_responses.py`（请求组装入口）  
- 测试目录 `tests/entrypoints/test_responses_utils.py`  

**💡 关注建议**：  
1. **兼容性检查**：该函数只在 `ResponseOutputMessage`、`ResponseReasoningItem` 与对应 dict 结构上做判断，若以后引入新的消息类型（例如 tool result）需同步更新逻辑，防止误判导致不应继续的生成被截断。  
2. **输入校验**：当前对 `request_input` 为 `str` 或空列表直接返回 `False`，可考虑在 API 层提前校验并给出更明确的错误信息，以免用户误把纯字符串当作 partial 消息提交。  
3. **性能影响**：仅检查列表最后一项，开销极低，无需额外优化；但若后续改为遍历或多轮判断，请关注对大批量请求的影响。  
4. **测试覆盖**：现有测试已涵盖类实例、dict、异常状态等，建议在集成测试中加入实际生成路径，验证 `add_generation_prompt` 与 `continue_final_message` 的配合是否如预期。  
5. **文档与示例**：为使用者补充 “Partial message completion” 的 API 示例，说明何种 `status` 可触发以及对 `chat_template` 的影响，降低使用门槛。  

总体而言，此次改动为 Anthropic‑style 的增量生成提供了明确入口，风险可控，建议在正式发布前完成上述兼容性与文档工作。

---

### [4/N][Attention] Move MLA common to model_executor (#32060)
**SHA**: `2263d44` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2263d44b688902aa3fd384ecdd7e3db3460b01e0)

**🎯 变更类型**：重构（将 MLA 公共实现从 `vllm/v1/attention/backends/mla/common.py` 移动到 `vllm/model_executor/layers/attention/mla_attention.py`）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增/重命名 `vllm/model_executor/layers/attention/mla_attention.py`，其中聚合了 `MLACommonBackend、MLACommonImpl、MLACommonMetadata、MLACommonMetadataBuilder、QueryLenSupport` 等类与工具函数。  
- 所有原先从 `vllm.v1.attention.backends.mla.common` 导入的地方改为从新路径导入，包括后端实现（flash‑mla、flashinfer‑mla、cutlass‑mla、triton‑mla 等）以及分布式 KV‑connector、测试用例和 Eagle decoder。  
- `logging_utils/formatter.py` 去除了对旧路径的路径缩短映射。  

**🎯 影响范围**  
- **后端实现**：`vllm/v1/attention/backends/mla/*`（六个文件）全部改为新导入。  
- **分布式 KV‑transfer**：`vllm/distributed/kv_transfer/kv_connector/v1/*`。  
- **测试**：`tests/v1/attention/test_mla_backends.py`。  
- **日志格式化**：`vllm/logging_utils/formatter.py`。  
- **其他模块**：Eagle 解码器中临时导入的地方。

**💡 关注建议**  

1. **兼容性**：外部使用者仍可能通过旧路径 (`vllm.v1.attention.backends.mla.common`) 引入这些类。建议在 `vllm/v1/attention/backends/mla/common.py`（可以保留为空文件或加入薄包装）里提供向新模块的转发和 `DeprecationWarning`，防止突发的 `ImportError`。  
2. **循环依赖**：确认 `mla_attention.py` 中不再导入任何会反向依赖 `vllm/v1/attention/backends/*` 的模块，避免运行时的循环导入。  
3. **文档同步**：更新开发者文档、示例代码以及 `README` 中的 import 示例，指向新路径。  
4. **CI/测试**：在 CI 中加入一次完整的 `pytest -q` 以及分布式模拟跑通的检查，确保所有后端在不同平台（CUDA、ROCm、CPU）均能成功导入并运行。  
5. **日志路径**：`formatter.py` 已删除旧映射，若有其他地方仍依赖该映射，需要检查是否会导致日志路径显示异常。  

总体来看，此次重构提升了代码组织，使 MLA 相关实现能够在模型执行层统一管理，降低了跨子模块的耦合度。只要做好向后兼容的桥接和完整的回归测试，风险相对可控。

---

#### 🟢 低重要度变更 (12)

### rename tokenize serving api request id prefix to tokenize (#32328)
**SHA**: `552b262` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/552b26293621e586687a523970f554746a5db387)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 Tokenize 接口的请求 ID 前缀从 `tokn-` 改为 `tokenize-`，并把几处字段描述的括号去掉，保持风格一致。

---

### [Frontend] track responsesAPI server_load (#32323)
**SHA**: `00e6402` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/00e6402d56fb258e6958381b1f3ceb34217ba830)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `/v1/responses`、`/v1/responses/{id}`、`/v1/responses/{id}/cancel` 和 `/v1/messages` 接口添加 `load_aware_call` 装饰器，实现负载感知；同步更新 server‑load 文档说明。

---

### [Build] Relax anthropic version pin from ==0.71.0 to >=0.71.0 (#32289)
**SHA**: `6b17609` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6b176095e32a0f24996bfc22ad4e6e49d9784bf8)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `requirements/common.txt` 中的 `anthropic` 依赖版本限制从 `==0.71.0` 放宽至 `>=0.71.0`，以支持更高的兼容版本。

---

### [ROCm][CI] Handle missing vision_config in Isaac model attention patch (#32281)
**SHA**: `9d0d7f4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9d0d7f48d55ae2e1933564491cfa1f97682fde1a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `model_utils.py` 中为 ROCm 视觉注意力补丁添加 `try/except`，捕获缺失 `vision_config` 的 `AttributeError` 并发出警告，同时记录意外错误日志，新增 `logging` 与 `warnings` 导入。

---

### [ROCm][CI] Disable Async Scheduling For Qwen3-Next-80B-A3B-Instruct MTP Async EPLB Accuracy Test (#32275)
**SHA**: `6fa6e7e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6fa6e7ef0c9b485e8a684211e96691731aad6faa)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Qwen3‑Next‑80B‑A3B‑Instruct 的 ROCm CI 脚本中添加平台参数，禁用 ROCm 上的异步调度，以规避 spec decode 相关的准确性错误。

---

### [Improvement] Persist CUDA compat libraries paths to prevent reset on `apt-get` (#30784)
**SHA**: `2a60ac9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2a60ac91d0f5c24cdb2863b178d2f5405fae50b8)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Dockerfile 中将临时 `ldconfig /usr/local/cuda-…/compat/` 改为写入 `/etc/ld.so.conf.d/00-cuda-compat.conf` 并执行 `ldconfig`，确保 CUDA 兼容库路径在后续 `apt-get` 操作后仍然生效。

---

### Add mergify label job for "bug" in PR titles (#31980)
**SHA**: `9e65bb4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9e65bb4ef4727f11655401250e03ba2754c66e67)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/mergify.yml` 中新增 `label-bug` 规则，实现当 PR 标题包含 “bug” 或 “bugfix”（不区分大小写）且未标记为 `stale` 时自动添加 `bug` 标签。

---

### [Misc] Add In-Container restart capability through supervisord for sagemaker entrypoint (#28502)
**SHA**: `2f4a71d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2f4a71daf200f4840d11435d932c676e943f2de3)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 SageMaker entrypoint 脚本中使用 `standard-supervisor` 包装 `vllm serve`，并将依赖 `model-hosting-container-standards` 升级至 0.1.13。

---

### Fix CUDA 13 wheel installation doc (#32276)
**SHA**: `46f8c6b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/46f8c6b72513a7fd8983ac620da50cbbed116ca7)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CUDA 13 安装文档中加入获取 CPU 架构的指令，并将 wheel 链接从 `manylinux_2_31_x86_64` 更新为 `manylinux_2_35_${CPU_ARCH}`，以兼容 x86_64 与 aarch64。

---

### [EPLB][Cleanup] Remove `is_async_enabled` from `EplbModelState` (#32050)
**SHA**: `6beef12` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6beef12b9b8a412c4f05566903bf53d0ccbe44f9)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：移除 `EplbModelState.is_async_enabled` 字段，统一使用 `EplbState.is_async` 标识，简化异步逻辑判断并添加 `assert state.is_async` 断言，清理相关冗余代码。

---

### [Trivial] Remove duplicate enable_mfu_metrics (#32246)
**SHA**: `ab74b2a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ab74b2a27a4eb88b90356bfb4b452d29edf05574)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/config/observability.py` 中删除了重复的 `enable_mfu_metrics` 配置项，精简了观测配置并保持功能不变。

---

### nixl_connector: export UCX_MEM_MMAP_HOOK_MODE=none to avoid a UCX memory leak (#32181)
**SHA**: `4f3676e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4f3676e72628ac067330e3acbf769d92afc2f7ea)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `nixl_connector.py` 中加入对环境变量 `UCX_MEM_MMAP_HOOK_MODE` 的检查与设置，避免在使用 NIXL 时出现 UCX 内存泄漏。

---

