# 每日更新报告（2026-02-07）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-07 22:18:16 | Cyrus Leung | [Doc] Fix run_batch docs (#34056) |
| 2026-02-07 21:33:11 | Hashem Hashemi | Perf tuning and expansion of cases covered for wvSplitKrc (#33493) |
| 2026-02-07 21:30:49 | Jiang Wu | Make directory exist ok for ray spinning up multiple replicas on a single instance (#33604) |
| 2026-02-07 21:30:22 | zifeitong | Update DeepGEMM version pin in Dockerfile to match #32479 (#33935) |
| 2026-02-07 21:30:09 | Rohan Potdar | move checks out of `unified_kv_cache_update` custom op (#33943) |
| 2026-02-07 21:26:05 | whx | [PluggableLayer][3/N] Apply PluggableLayer to mamba layers. (#33660) |
| 2026-02-07 21:25:24 | Jee Jee Li | [Model] Enable Step3p5ForCausalLM testing (#33755) |
| 2026-02-07 21:24:57 | Pooya Davoodi | [Frontend]Add support for transcriptions and translations to run_batch (#33934) |
| 2026-02-07 21:24:52 | TundeAtSN | Enable Eagle3 speculative decoding for Mistral3ForConditionalGeneration to support eagle3 (#33939) |
| 2026-02-07 21:24:48 | Richard Zou | [torch.compile] Stop compiling identical artifacts (#34003) |
| 2026-02-07 21:24:44 | Mohammad Miadh Angkad | [Kernel] Add KernelConfig flag to enable/disable FlashInfer autotune (#34006) |
| 2026-02-07 21:24:40 | Cyrus Leung | [Renderer] Define `render_cmpl` and `render_chat` (#34039) |
| 2026-02-07 21:24:35 | wang.yuqi | [CI][Build]  Pin grpcio-tools==1.78.0 (#34048) |
| 2026-02-07 16:59:49 | Cyrus Leung | [Misc] Simplify `get_max_tokens` (#34036) |
| 2026-02-07 15:58:50 | lukec | Fix spelling errors (#33978) |
| 2026-02-07 14:21:08 | Andreas Karatzas | [ROCm][CI] Pinning lm-eval version to resolve multi-modal small eval bug (#34038) |
| 2026-02-07 13:30:17 | Cyrus Leung | [Misc] Make `PlaceholderRange.get_num_embeds` a method (#34035) |
| 2026-02-07 12:28:01 | Vel | [Kernel] Add enable_sm120_or_later for SM121 (DGX Spark) CUTLASS support (#33517) |
| 2026-02-07 12:14:45 | Wentao Ye | [Revert] Add util `handle_deprecated` back (#33998) |
| 2026-02-07 11:37:02 | 果冻虾仁 | fix description in plugin_system.md (#33999) |
| 2026-02-07 11:11:05 | Nick Hill | [ModelRunner V2] Revert token rank comparison difference for now (#34017) |
| 2026-02-07 11:01:41 | kourosh hakhamaneshi | [Misc] Add backward-compatible import aliases for renamed translations module (#34015) |
| 2026-02-07 10:45:59 | Xin Yang | [Bugfix] Fix _fused_moe_lora_expand signature mismatch (#33821) |
| 2026-02-07 10:43:25 | rasmith | [CI][AMD]Bugfix] Check that model_config is not None in enable_norm_pad_fusion (#34007) |
| 2026-02-07 10:42:52 | Nicolò Lucchesi | [Bugfix] Fix Whisper tokenization (#34011) |
| 2026-02-07 10:27:33 | Ikenna | [Bugfix] Fix QK Norm+RoPE fusion pattern matching on B200+FP8 (#33967) |
| 2026-02-07 08:08:58 | Aaron Hao | [Feat][RL] Pause and Resume with keep requests for single engine (#32351) |
| 2026-02-07 06:17:55 | kourosh hakhamaneshi | [bugfix] [ROCm] Fix premature CUDA initialization in platform detection (#33941) |
| 2026-02-07 06:03:34 | Dimitrios Bariamis | Fix RoutingMethodType logic (#33919) |
| 2026-02-07 04:33:40 | Sumanth R Hegde | [Fix] Fix `logprobs=0` handling for `/inference/v1/generate` endpoint (#34010) |
| 2026-02-07 03:11:32 | xuebwang-amd | [Bugfix] Fix no attribute error of SharedFusedMoE (DeepSeek-V3.1 as test model) (#33993) |
| 2026-02-07 03:09:59 | Charlie Fu | [Rocm][Bugfix] Fix dtype not same for gemm_a4w4 op (#33734) |
| 2026-02-07 02:57:06 | Wentao Ye | [Refactor] Remove align block size logic in `moe_permute` (#33449) |
| 2026-02-07 02:56:48 | zhrrr | [Model Runner V2] support apply penalty for spec decode (#33251) |
| 2026-02-07 02:05:35 | vllmellm | [DOC] [ROCm] Update docker deployment doc (#33971) |
| 2026-02-07 01:58:21 | Seiji Eicher | [KV Connector] Add missing method overrides to MultiConnector (#33292) |
| 2026-02-07 01:49:56 | Wentao Ye | [Log] Optimize duplicate startup log (#33944) |
| 2026-02-07 01:23:44 | Chauncey | [Bugfix] Fix the issue where tool calling does not work when using fast detokenization with dsv32 (#33964) |
| 2026-02-07 00:00:59 | Eldar Kurtić | [Docs] Update link to Benchmark CLI documentation (#33254) |

### 📊 统计摘要
> 本日共 39 个提交 | 🔴高 1 | 🟡中 20 | 🟢低 18
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (1)](#-🔴-高重要度变更-1)
    - [[Feat][RL] Pause and Resume with keep requests for single...](#89a385d)
  - [🟡 中重要度变更 (20)](#-🟡-中重要度变更-20)
    - [Perf tuning and expansion of cases covered for wvSplitKrc...](#ed17f54)
    - [move checks out of `unified_kv_cache_update` custom op (#...](#de3869b)
    - [[PluggableLayer][3/N] Apply PluggableLayer to mamba layer...](#ce9b3cd)
    - [[Model] Enable Step3p5ForCausalLM testing (#33755)](#db4ede9)
    - [[Frontend]Add support for transcriptions and translations...](#2cb2340)
    - [[torch.compile] Stop compiling identical artifacts (#34003)](#81fe69c)
    - [[Kernel] Add KernelConfig flag to enable/disable FlashInf...](#dd6a6e1)
    - [[Renderer] Define `render_cmpl` and `render_chat` (#34039)](#edb359c)
    - [[Misc] Simplify `get_max_tokens` (#34036)](#11a4c9d)
    - [Fix spelling errors (#33978)](#15a0b9e)
    - [[Misc] Make `PlaceholderRange.get_num_embeds` a method (#...](#48312e5)
    - [[Misc] Add backward-compatible import aliases for renamed...](#1c3b220)
    - [[Bugfix] Fix QK Norm+RoPE fusion pattern matching on B200...](#9060771)
    - [[bugfix] [ROCm] Fix premature CUDA initialization in plat...](#4a2d00e)
    - [Fix RoutingMethodType logic (#33919)](#207c3a0)
    - [[Refactor] Remove align block size logic in `moe_permute`...](#77c09e1)
    - [[Model Runner V2] support apply penalty for spec decode (...](#16786da)
    - [[DOC] [ROCm] Update docker deployment doc (#33971)](#aaa2efb)
    - [[KV Connector] Add missing method overrides to MultiConne...](#aca5967)
    - [[Log] Optimize duplicate startup log (#33944)](#67a746e)
  - [🟢 低重要度变更 (18)](#-🟢-低重要度变更-18)
    - [[Doc] Fix run_batch docs (#34056)](#b956cdf)
    - [Make directory exist ok for ray spinning up multiple repl...](#860981d)
    - [Update DeepGEMM version pin in Dockerfile to match #32479...](#52181ba)
    - [Enable Eagle3 speculative decoding for Mistral3ForConditi...](#4df44c1)
    - [[CI][Build]  Pin grpcio-tools==1.78.0 (#34048)](#6ed5eda)
    - [[ROCm][CI] Pinning lm-eval version to resolve multi-modal...](#c490d8c)
    - [[Kernel] Add enable_sm120_or_later for SM121 (DGX Spark) ...](#bc32444)
    - [[Revert] Add util `handle_deprecated` back (#33998)](#18e8545)
    - [fix description in plugin_system.md (#33999)](#6f7adc5)
    - [[ModelRunner V2] Revert token rank comparison difference ...](#40218a8)
    - [[Bugfix] Fix _fused_moe_lora_expand signature mismatch (#...](#3920caf)
    - [[CI][AMD]Bugfix] Check that model_config is not None in e...](#ec28784)
    - [[Bugfix] Fix Whisper tokenization (#34011)](#55aeec0)
    - [[Fix] Fix `logprobs=0` handling for `/inference/v1/genera...](#ae2e93f)
    - [[Bugfix] Fix no attribute error of SharedFusedMoE (DeepSe...](#9e9acce)
    - [[Rocm][Bugfix] Fix dtype not same for gemm_a4w4 op (#33734)](#fe54382)
    - [[Bugfix] Fix the issue where tool calling does not work w...](#7bec435)
    - [[Docs] Update link to Benchmark CLI documentation (#33254)](#5c52644)
#### 🔴 高重要度变更 (1)

### [Feat][RL] Pause and Resume with keep requests for single engine (#32351)
**SHA**: `89a385d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/89a385d79fb511f439f8ebe9d38d6811b5b24a91)

**🎯 变更类型**：功能增强、架构变更、API扩展  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 为 `AsyncLLM` 与底层 `EngineCore` 引入 **pause / resume** 功能，支持三种模式：`abort`（立即中止并返回 `abort` 原因）、`wait`（等待在处理的请求完成）和 `keep`（冻结调度队列，仅在后续 `resume` 时继续）。  
- 新增 `PauseMode` 类型别名、对应的协议参数、REST 接口 `/pause` 与 `/resume`，并在 `vllm/engine/protocol.py`、`api_router.py` 中完成向后兼容的参数迁移。  
- 在 `EngineCore` 与 `EngineCoreClient` 中实现调度器的暂停/恢复逻辑（仅在单进程/单客户端模式下可用），并对 `pause_generation` 参数 `wait_for_inflight_requests`、`clear_cache` 做了 **DEPRECATED** 标记。  
- 补充了大量 **单元/集成测试**（基础、abort、wait、keep 单请求及多请求），以及 **示例脚本** `examples/offline_inference/pause_resume.py` 用于手动验证。  

**🎯 影响范围**  
- `vllm/v1/engine/async_llm.py`、`vllm/v1/engine/core.py`、`vllm/v1/engine/core_client.py`（调度器暂停相关实现）  
- `vllm/engine/protocol.py`、`vllm/entrypoints/serve/rlhf/api_router.py`（对外 API）  
- `tests/v1/engine/test_async_llm.py`（新测试）  
- 示例脚本 `examples/offline_inference/pause_resume.py`  

---

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 1. 在 `EngineCore` 添加了 `_scheduler_paused` 状态以及 `pause_scheduler` / `resume_scheduler` 方法，调度循环在 `step` / `step_with_batch_queue` 入口处提前返回，确保在 `keep` 模式下不产生新 token。<br>2. `AsyncLLM` 新增 `_client_count` 用于检测多实例（`--api-server-count > 1`）情况下的限制，防止跨进程调度器状态不一致。<br>3. `EngineClient` 协议层扩展 `pause_generation` 参数签名，引入 `PauseMode`，兼容旧参数并给出弃用提示。 |
| **性能影响** | - **abort / wait**：行为与之前相同，仅在 `pause_generation` 前会额外检查 `_paused` 标志，性能影响可忽略。<br>- **keep**：不清除 KV 缓存（`clear_cache=False` 默认），恢复后可直接继续生成，提升 **恢复时延**，但在暂停期间调度器完全空转（`step` 直接返回），CPU 资源占用极低。<br>- 多请求 `keep` 场景下，所有请求保持在调度队列中，暂停期间不会产生任何 I/O，整体吞吐保持不变，但总体延迟会增加对应的暂停时长。 |
| **安全考虑** | 变更仅涉及内部调度控制，无外部代码执行或资源泄露风险。唯一需要关注的是 **异常路径**：如果 `pause_scheduler_async` 在数据并行模式下被调用，会抛 `NotImplementedError`，若未捕获可能导致服务异常退出。建议在前端 API 层返回明确错误信息（已实现）。 |
| **可维护性** | - 新增的 `PauseMode` 通过 `Literal` 实现，类型检查友好。<br>- `wait_for_inflight_requests` 与 `clear_cache` 已标记为废弃，后续可在下一个 major 版本中移除。<br>- 代码分支（`if mode == "keep"` vs 其它）保持简洁，避免在 `AsyncLLM` 中混入过多状态判断。<br>- 单元测试覆盖全部模式，降低回归风险。 |

---

### ⚠️ 潜在风险  

1. **多实例不兼容**：当 `--api-server-count > 1`（多进程/多节点部署）时，仅 `keep` 模式被允许，其他模式会直接抛 `NotImplementedError`。如果用户在集群模式下仍使用 `abort`/`wait`，会导致服务异常。  
2. **Race 条件**：`_paused` 与 `_scheduler_paused` 两套状态在极端并发（同时 `pause`、`resume`、`generate`）下可能出现短暂不一致，尤其在 `keep` 与 `abort` 混用时。现有测试已覆盖基本并发，但未模拟高并发/高负载下的竞争。  
3. **废弃参数兼容**：老客户端仍可能使用 `wait_for_inflight_requests`/`clear_cache`，当前实现仍兼容，但未来移除时需做好迁移指南。  
4. **资源泄漏**：`pause_scheduler_async` 在 `DPAsyncMPClient` 中抛 `NotImplementedError`，若某些数据并行部署误调用该方法，可能导致未释放的请求占用内存。  
5. **日志/监控缺失**：暂停/恢复操作未在日志中记录详细上下文（如请求数、模式），运维排障时难以快速定位。  

---

### 💡 关注建议  

- **文档与迁移指南**：在官方文档中明确标出 `pause_generation` 新增的 `mode` 参数、各模式语义、以及不支持多实例的限制；提供废弃参数的迁移示例。  
- **监控埋点**：在 `pause_scheduler` / `resume_scheduler` 以及 `pause_generation` / `resume_generation` 中加入日志（例如 `engine.pause.generation mode=keep count=xx`），并通过 Prometheus 暴露相应指标，便于运维监控。  
- **容错处理**：在 `api_router.pause` 中捕获 `NotImplementedError`，返回统一的错误码（如 `400 Bad Request`），防止未捕获异常导致服务崩溃。  
- **并发安全**：考虑在 `AsyncLLM` 中对 `_pause_cond` 加锁前检查 `_scheduler_paused`，或在 `EngineCore.step` 前统一使用 `asyncio.Lock` 防止同一时间多次 `pause`/`resume` 竞争。  
- **跨进程支持**：若后续计划支持 `keep` 之外的模式在多实例环境下使用，需要在 `EngineCoreClient` 实现分布式的调度器状态同步（如通过共享内存或 RPC 广播）。  
- **回归测试**：加入 **高并发压力测试**（如 100 并发生成请求同时触发 `pause`/`resume`），验证不会出现死锁或泄漏。  

---  

**结论**：本次改动显著提升了 vLLM 在 RLHF、模型微调等场景下的可控性与灵活性，尤其是 `keep` 模式在不清除缓存的前提下实现了快速恢复。但需要注意多实例部署限制、并发状态一致性以及即将废弃的老参数。通过加强文档、日志与监控，以及在高并发环境下的额外测试，可将风险降至可接受水平。

---

#### 🟡 中重要度变更 (20)

### Perf tuning and expansion of cases covered for wvSplitKrc (#33493)
**SHA**: `ed17f54` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ed17f54c8bccc2f1a19f83f65b36d837698b8792)

**🎯 变更类型**：性能调优 & 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 `csrc/rocm/skinny_gemms.cu` 中对 Wave‑SplitK‑Reduce‑Count（wvSplitKrc）实现做了大量重构：引入 `CHUNKK` 参数以在 K 维度上做子块划分，简化 LDS 大小计算，统一了原先 “大 A” 与 “小 CUs” 两套代码路径，改为统一的原子‑reduce 方案并加入显式 `s_waitcnt`。  
2. 单元测试 `tests/kernels/quantization/test_rocm_skinny_gemms.py` 也同步改写，采用更细粒度的 N、K、M 参数集合，并在运行时根据当前 CU 数量动态跳过超出容量的组合。  
3. `vllm/model_executor/layers/utils.py` 中对 `rocm_unquantized_gemm_impl` 添加了对 wvSplitKrc 可行性的预判（根据 N、M、K、CU 数量），并在满足条件时使用该 kernel 替代原有实现。  

**🎯 影响范围**：  
- 核心 ROCm GEMM 实现（skinny_gemms.cu）  
- 相关单元测试（quantization‑tests）  
- VLLM 运行时的 GEMM 调度路径（model_executor）  

**💡 关注建议**  
1. **正确性**：大量手动索引与取模运算改动后，尤其是 `bigB_`、`bigA`、原子计数 `cntr` 的计算，需要在多种尺寸下（包括奇数、非 16 对齐）做回归验证，防止越界或重复累加。  
2. **原子计数**：`doRdc` 被硬编码为 `true`，如果后续出现 K 小于阈值的情况，可能产生不必要的同步开销或计数错误。建议保留原始判断或加注释说明何时会被禁用。  
3. **同步**：仅依赖编译器插入的 `s_waitcnt` 并未保证所有工作项已完成写入 LDS，建议在关键段加显式 `__syncthreads()` 或 `asm volatile ("s_waitcnt ...")`，防止稀疏硬件调度导致数据竞争。  
4. **可维护性**：`CHUNKK` 参数的意义和取值范围分散在多个宏和模板实例化处，建议在文件顶部添加统一说明或 typedef，避免后续误用。  
5. **测试覆盖**：当前测试仅在 gfx950 上跑，且会在 CU 不足时直接 `skip`。建议补充：
   - 非 gfx950（如 gfx1100）上的兼容性检查。  
   - 对极端 N（<10）和 K（非 8 的倍数）组合的负向用例，确保 fallback 到普通 GEMM。  
6. **文档**：在 `README` 或 VLLM 的 ROCm 章节注明 `VLLM_ROCM_USE_SKINNY_GEMM` 环境变量的使用前提条件（N、M、K 的取值范围以及 CU 数量要求），帮助用户避免误配置。  

总体来看，此次改动显著提升了在典型 LLM 推理场景下的 GEMM 吞吐，但因实现细节高度依赖硬件属性，务必在多个 GPU、不同驱动版本上做好回归，以防出现难以复现的性能回退或数值偏差。

---

### move checks out of `unified_kv_cache_update` custom op (#33943)
**SHA**: `de3869b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/de3869bb4db76658bc4775ba4ba5fa6ef546237a)

**🎯 变更类型**：重构（把 KV‑Cache 更新前的检查从 `unified_kv_cache_update` 自定义算子中抽出来）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `attention.py`、`cross_attention.py`、`whisper_causal.py` 中加入 `self.kv_sharing_target_layer_name`、`key`、`value` 的显式判空，只有在不共享 KV‑Cache 且 KV 均有效时才调用统一缓存更新。相应地，各后端实现（flash、rocm、triton）去掉了原先的 `kv_sharing_target_layer_name` 与 `key/value` 检查，统一在上层完成判断，简化了 `do_kv_cache_update` 逻辑。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/attention/attention.py`  
- `vllm/model_executor/layers/attention/cross_attention.py`  
- `vllm/model_executor/models/whisper_causal.py`  
- 所有后端 KV‑Cache 更新实现：`flash_attn.py、rocm_aiter_unified_attn.py、rocm_attn.py、triton_attn.py`  

**💡 关注建议**：  
1. **功能验证**：在开启 KV‑Cache 共享（如多层的 Multi‑Query Attention）时，确保缓存不再被误更新；在普通单层场景下仍保持原有行为。  
2. **性能回归**：由于检查提前到 Python 层，确认对热点路径的开销没有负面影响（推荐对比 `torch.ops.vllm.unified_kv_cache_update` 前后的吞吐）。  
3. **文档更新**：补充 `kv_sharing_target_layer_name` 的使用说明，明确何时会跳过缓存更新。  
4. **测试覆盖**：新增或扩展单元/集成测试，覆盖跨注意力、Whisper 模型以及不同后端（Flash, ROCm, Triton）在 KV‑Cache 共享与不共享两种情形。  

总体来说，此次重构提升了代码可读性与可维护性，但在 KV‑Cache 共享的特殊模型上需要重点回归测试。

---

### [PluggableLayer][3/N] Apply PluggableLayer to mamba layers. (#33660)
**SHA**: `ce9b3cd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ce9b3cd3e91c2f6c901f12b038ee7797aaa5c829)

**🎯 变更类型**：功能增强（实现可插件化的 Mamba 层）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将原先 `CustomOp` 替换为新的 `PluggableLayer`，并在 `mamba_mixer.py、mamba_mixer2.py、plamo2.py` 中重新注册相应层。  
2. 将原有的 `forward_native / forward_cuda` 接口统一为 `forward_impl`，并在 `*_mixer` 的调用方（`*_mixer_fake`、`plamo2_mamba_mixer` 等）改为调用 `forward_impl`。  
3. 移除空实现的 `forward_native`（仅保留 `pass`）以及对应的 CUDA stub，避免无意义的占位代码。

**🎯 影响范围**  
- **模型执行器层**：`vllm/model_executor/layers/mamba/*` 以及 `vllm/model_executor/models/plamo2.py`。  
- **注册机制**：`PluggableLayer` 的注册表取代了 `CustomOp`，影响所有通过 `CustomOp.register` 动态加载的自定义算子。  
- **前向上下文**：`ForwardContext.no_compile_layers` 中的实例现在必须实现 `forward_impl`，否则会在运行时抛出属性错误。

**💡 关注建议**  
1. **接口兼容性**：确认 `PluggableLayer` 与 `CustomOp` 在属性 & 方法（`layer_name`、`prefix` 等）上的兼容，防止老版本插件在升级后失效。  
2. **实现完整性**：`forward_impl` 仍需在 CUDA 编译路径中提供实际实现，检查对应的 C++/CUDA kernel 是否已绑定到 `PluggableLayer`。  
3. **单元测试**：补充针对 `mamba_mixer*` 与 `plamo2_mamba_mixer` 的回归测试，尤其是 `no_compile_layers` 走 Python 实现的分支。  
4. **文档更新**：在开发者手册中说明 `PluggableLayer` 的使用方式、注册方式以及 `forward_impl` 的签名，避免混淆。  
5. **回退方案**：若库在某些环境（如不支持 CUDA）仍需使用原生实现，考虑在 `PluggableLayer` 中提供 fallback 到 `CustomOp` 或纯 Python 实现的 shim。  

做好上述检查后，此次可插件化改造能够提升层的可扩展性和维护性，同时保持运行时的兼容性。

---

### [Model] Enable Step3p5ForCausalLM testing (#33755)
**SHA**: `db4ede9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/db4ede97434300e2dea211f7234024dd7fe34bfc)

**🎯 变更类型**：功能增强 / 轻量重构（新增对 `Step3p5ForCausalLM` 的测试支持，简化模型初始化逻辑）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. **文档**：将 `Step3p5ForCausalLM` 对应的 HuggingFace 仓库名由 `stepfun-ai/step-3.5-flash` 修正为 `stepfun-ai/Step-3.5-Flash`，保持大小写一致。  
2. **测试注册表**：在 `tests/models/registry.py` 中加入了对 `SmolLM3ForCausalLM、StableLMEpochForCausalLM、StableLmForCausalLM、Starcoder2ForCausalLM` 等模型的在线可用性声明，并为 `Step3p5ForCausalLM`/`Step3p5MTP` 添加了 `use_original_num_layers=True`、`hf_overrides={"num_hidden_layers": 4}`，以及对 MoE 层的显式初始化要求。  
3. **代码实现**（`vllm/model_executor/models/step3p5.py`）：  
   - 移除对 LoRA 配置的特殊处理，统一使用 `config.vocab_size` 构造 `ParallelLMHead`。  
   - 删除对 `DEFAULT_VOCAB_PADDING_SIZE`、LoRA‑相关 vocab padding 的依赖。  
   - `LogitsProcessor` 初始化改为仅接受 `vocab_size`（不再需要 `unpadded_vocab_size`）。  
   - 将 MoE 层的搜集逻辑从构造函数前部迁移到后部，使其在模型实例化后统一处理。  
   - 代码结构更简洁，但保留了对 `FusedMoEBlock` 的断言与权重列表准备。

**🎯 影响范围**  
- **模型执行器**：`Step3p5ForCausalLM` 及其多阶段（MTP）实现。  
- **测试套件**：新增/修改的模型注册表测试，用于验证在线可用性和 MoE 初始化。  
- **文档**：`supported_models.md` 中的模型列表。  
- **可能受影响的用户**：使用 LoRA 对 `Step3p5` 进行微调的场景、依赖 vocab padding 的自定义推理脚本。

**💡 关注建议**  

1. **LoRA 兼容性**  
   - 当前提交直接剔除了 LoRA 相关 vocab‑padding 逻辑。若社区有需求在 `Step3p5` 上使用 LoRA，这将导致加载错误。建议在提交说明或文档中明确标注 “`Step3p5ForCausalLM` 暂不支持 LoRA”，或保留向后兼容的分支（例如在 `vllm_config.lora_config` 为 `None` 时走新路径）。  

2. **LogitsProcessor 参数变更**  
   - `LogitsProcessor` 的构造函数已从 `(unpadded_vocab_size, vocab_size)` 变为仅 `(vocab_size)`。请确认所有调用点（包含自定义插件或旧版代码）已同步更新，否则会抛出 `TypeError`。  

3. **MoE 层初始化顺序**  
   - 之前的 MoE 收集在 `__init__` 前部完成，现迁移至后部。若后续在 `self.model` 初始化期间有副作用（例如层被提前释放），需要确保 `self.moe_layers` 的收集在所有层都已完全构造后执行。建议加入单元测试，验证在多 GPU/TPU 环境下 MoE 权重的正确加载。  

4. **测试覆盖**  
   - 新增的 `use_original_num_layers=True` 与 `hf_overrides={"num_hidden_layers": 4}` 仅针对 `Step3p5` 的 MoE 层。请确保这些覆盖不影响其他模型的默认 `num_hidden_layers`，并在 CI 中加入对 `num_hidden_layers` 参数被覆盖时的断言。  

5. **文档同步**  
   - 文档已更新模型仓库名，建议在 `CHANGELOG.md` 中添加对应说明，以免用户在查找模型时仍使用旧的大小写路径。  

6. **回滚风险**  
   - 由于改动涉及模型加载流程的核心路径（vocab size、LoRA），建议在正式发布前在多个硬件平台（单卡、TPU、跨节点多卡）跑一次完整的推理基准，确认没有隐藏的分布式同步错误。  

总体来看，此次改动的目标是让 `Step3p5` 在测试套件中可被准确实例化，并简化不常用的 LoRA 处理路径，提升代码可维护性。但在正式上线前，需要特别关注 LoRA 兼容性和 `LogitsProcessor` 参数的全局一致性。

---

### [Frontend]Add support for transcriptions and translations to run_batch (#33934)
**SHA**: `2cb2340` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2cb2340f7a91a2f6629aacc45563af20897812c0)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 `vllm run‑batch` 引入音频转写（transcriptions）与翻译（translations）支持。新增 `BatchTranscriptionRequest`、`BatchTranslationRequest`（使用 `file_url`），实现 data‑url 与 HTTP URL 下载、转为 `UploadFile` 并调用原有转写/翻译服务；在 `run_batch` 中通过统一的 endpoint registry 统一路由，新增对应测试用例。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/run_batch.py`（核心批处理逻辑）  
- `vllm/entrypoints/openai/speech_to_text/*`（转写/翻译服务）  
- `vllm/entrypoints/openai/api_server.py`（endpoint 匹配）  
- 测试 `tests/entrypoints/openai/test_run_batch.py`  
- 新增 `vllm/assets/audio.py`（用于获取示例音频 URL）  

**💡 关注建议**  
1. **兼容性**：批处理 JSON 中仍只能使用 `file_url`，若误传 `file` 将触发验证错误，请在文档中明确说明。  
2. **安全**：`download_bytes_from_url` 直接支持任意 http/https URL，生产环境建议加入白名单或超时/大小限制，防止 SSRF 或 OOM。  
3. **性能**：下载音频会在批处理任务中同步进行，若批量大量文件建议在前置层做并发控制或缓存。  
4. **错误处理**：当前将下载异常包装为 `BadRequestError`，可考虑在日志中记录原始异常堆栈，以便排查。  
5. **测试**：新增的 `test_transcription_http_url` 依赖外网音频，CI 环境可能不具备网络，请使用本地 `data:` URI 或 Mock。  
6. **文档**：更新 CLI `run‑batch` 示例，说明 `audio/transcriptions`、`audio/translations` 支持的 JSON schema 与 `response_format`、`language`、`to_language` 参数。  

开发者在提交新 batch 文件时，确保音频 URL 可访问或使用 base64 data URL；用户在使用 `vllm run‑batch` 时可通过 `--model openai/whisper‑large‑v3`（或 `whisper‑small`）调用转写/翻译功能。

---

### [torch.compile] Stop compiling identical artifacts (#34003)
**SHA**: `81fe69c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/81fe69cae5cb0d3aff23934064c639e9c9815a81)

**🎯 变更类型**：功能增强（torch.compile 编译去重）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `vllm/compilation/backends.py` 中为编译后端引入了 `loaded_artifacts` 缓存并通过 monkey‑patch `torch._functorch._aot_autograd.autograd_cache.autograd_cache_key` 实现“相同子图只编译一次”。  
- 为此新增 `StopCompiling` 异常用于提前返回已有的编译结果，并在 `tests/compile/test_cold_start.py` 中相应修改断言，验证仅会编译 3 个唯一子图。  

**🎯 影响范围**  
- **编译后端**：`BackendCompiler.compile` 逻辑及缓存管理。  
- **测试套件**：`tests/compile/test_cold_start.py`。  
- **运行时**：首次冷启动时的磁盘 I/O 与编译时间将显著下降，后续重复子图直接复用内存中的 artifact。  

**💡 关注建议**  
1. **异常安全**：`StopCompiling` 继承自 `BaseException`，会绕过普通的 `except Exception` 捕获，建议改为自定义 `Exception`（或在文档中明确使用方式），避免意外捕获不到。  
2. **线程安全**：`loaded_artifacts` 目前是普通 dict，若在多线程/多进程环境下并发编译，同一 `cache_key` 可能产生竞争，建议加锁或使用 `collections.defaultdict` 配合原子操作。  
3. **兼容性**：依赖了内部私有 API `torch._functorch._aot_autograd.autograd_cache.autograd_cache_key`，未来 PyTorch 迁移可能破坏该实现，建议在 `torch` 正式暴露 `cache_key` 前做好回退路径或封装层。  
4. **测试覆盖**：当前仅验证了唯一子图数量，建议加入对 `loaded_artifacts` 内容的断言以及异常路径（如 `cache_key` 为 `None`）的测试，确保不影响非等价子图的正常编译。  

总体而言，此次改动显著降低了冷启动时的重复编译开销，但需注意异常层级、并发安全以及对私有 API 的依赖，以免在后续升级 PyTorch 时产生回归。

---

### [Kernel] Add KernelConfig flag to enable/disable FlashInfer autotune (#34006)
**SHA**: `dd6a6e1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dd6a6e119062027fae8964335c525796f71cdc2b)

**变更类型**：功能增强  
**重要程度**：🟡 中  

**核心变更**  
1. 新增 `KernelConfig`（`vllm/config/kernel.py`），提供 `enable_flashinfer_autotune` 开关。  
2. 在 `VllmConfig` 中加入 `kernel_config`，并在所有优化层级默认配置里写入对应布尔值。  
3. `EngineArgs` 与 CLI 新增 `--enable-flashinfer-autotune` 与 `--kernel-config` 参数，实现与 `KernelConfig` 的双向映射。  
4. `create_engine_config` 中加入对该标记的冲突检查与覆盖逻辑。  
5. `kernel_warmup` 根据 `worker.vllm_config.kernel_config.enable_flashinfer_autotune` 决定是否执行 FlashInfer autotune。  

**影响范围**  
- 配置子系统 (`vllm/config/*`)  
- 命令行入口 (`vllm/engine/arg_utils.py`)  
- 引擎创建路径 (`vllm/engine/arg_utils.py`、`vllm/model_executor/warmup/kernel_warmup.py`)  
- 默认优化层级配置  

**关注建议**  
- **默认行为**：在未显式设置时会在优化层级里填充 `False/True`，但仍会在 `VllmConfig.__post_init__` 检查 `None`，确保所有入口（JSON、CLI、Python）都能赋值。  
- **互斥校验**：`--enable-flashinfer-autotune` 与 `kernel_config.enable_flashinfer_autotune` 不能同时出现，保持报错信息清晰。  
- **文档与示例**：在 README/CLI 帮助中标明该选项的适用场景（仅 Hopper/Blackwell GPU），以及关闭后的性能影响。  
- **单元/集成测试**：添加覆盖 `KernelConfig` 默认、显式设置、冲突校验以及 warmup 分支的测试，防止未来改动导致隐式 `None` 抛异常。  
- **哈希一致性**：目前 `compute_hash` 返回空因子，若后续实现使该标记影响计算图，请同步更新哈希实现，避免缓存失效。  

总体而言，此次改动为 FlashInfer autotune 提供了显式开关，兼容性良好，只需关注配置传播和文档同步即可。

---

### [Renderer] Define `render_cmpl` and `render_chat` (#34039)
**SHA**: `edb359c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/edb359cce460de499331abb8f9c202aa40bbb335)

**🎯 变更类型**：功能增强（统一渲染/标记流程）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm.renderers` 中新增 `render_cmpl` / `render_cmpl_async` 与 `render_chat` / `render_chat_async`，把“解析‑渲染‑标记”三步合并为一个入口，统一处理 `prompt_extras`（如 `mm_processor_kwargs`、`cache_salt`）。  
2. 抽取 `extract_target_prompt`，让 `extract_prompt_components` 与 `extract_prompt_len` 复用，简化多模态模型的 prompt 提取逻辑。  
3. `entrypoints/llm.py`、`entrypoints/openai/engine/serving.py` 以及相应测试文件改用新的渲染接口，删除了旧的 `SingletonDictPrompt` 相关代码。  

**🎯 影响范围**  
- `vllm.renderers`（核心渲染、参数处理）  
- `vllm.entrypoints.llm`、`vllm.entrypoints.openai.engine.serving`（API 层调用链）  
- `vllm.renderers.inputs.preprocess`（prompt 抽取工具）  
- 多模态相关测试（phi‑4mm、qwen2‑vl、awq）  

**💡 关注建议**  
1. **兼容性**：`render_cmpl` 仍返回 `Sequence[DictPrompt|TokPrompt]`，但旧代码可能仍依赖 `list` 类型，建议在文档中明确返回容器不保证可变。  
2. **多模态特殊路径**：当前 `render_cmpl` 对多模态模型直接返回 `dict_prompts`，未调用 `tokenize_prompts`，若后续需要在线 token 化，需补充对应实现或在注释中提醒。  
3. **异常处理**：新增的 `_apply_prompt_extras` 直接 `update` 目标 prompt，若目标结构不是 `dict`（如自定义 Prompt 类）会触发运行时错误，建议加入 `isinstance(target_prompt, dict)` 检查并抛出友好错误。  
4. **测试覆盖**：已删除 “无图像” 参数化用例，确认所有多尺度图像路径仍被完整测试；如果未来恢复对空图像的支持，需补充对应测试。  
5. **性能**：一次性渲染‑标记可减少多余的循环与临时列表，建议在基准测试中加入新版与旧版的吞吐量对比，以验证提升是否符合预期。  

总体而言，此次改动提升了渲染流程的可组合性和代码可维护性，只要注意上述兼容性与异常处理，即可安全合入主线。

---

### [Misc] Simplify `get_max_tokens` (#34036)
**SHA**: `11a4c9d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/11a4c9d30d205ee4cb8e9a92e34834fc6585fce0)

**🎯 变更类型**：功能增强（API 简化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `utils.get_max_tokens` 从接受完整请求对象改为直接接收 `max_tokens` 参数，去除内部 `isinstance`/`getattr` 的分支逻辑。  
- 相应地在 `chat_completion、completion、engine、responses` 四个入口处改为显式传入对应字段（`max_completion_tokens / max_tokens / max_output_tokens`）。  
- 删除对 `ChatCompletionRequest、CompletionRequest、ResponsesRequest` 的 import 与类型占位，精简 `utils.py`。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/chat_completion/serving.py`  
- `vllm/entrypoints/openai/completion/serving.py`  
- `vllm/entrypoints/openai/engine/serving.py`  
- `vllm/entrypoints/openai/responses/serving.py`  
- `vllm/entrypoints/utils.py`（核心工具函数）  

**💡 关注建议**  
1. **行为保持**：确保所有调用在未显式提供 `max_tokens` 时仍会得到 `None`，从而使用 `default_max_tokens = max_model_len - input_length` 的旧逻辑。可在单元测试中加入 `max_tokens=None` 场景，验证返回值与旧实现一致。  
2. **类型与文档**：为 `get_max_tokens` 增加完整的 docstring 与 `int | None` 类型提示，说明 `max_tokens` 为业务层已解析的值，函数不再处理请求对象。  
3. **冗余参数**：当前 `default_sampling_params` 参数在函数体内未被使用，若未来不再需要，应同步清理调用处以避免误导。  
4. **向后兼容**：虽然该函数为内部工具，但若有外部插件直接调用，需在迁移指南中说明参数签名的变化。  
5. **代码风格**：删除已废弃的 `getattr` 代码块后，建议运行一次 `ruff/flake8` 检查，确保未留下未使用的 import（如 `FlexibleArgumentParser`）或变量。  

总体而言，此次改动提升了函数职责的单一性，降低了运行时属性查找开销，对外行为保持不变，只要相应的单元测试覆盖 `None`、合法正数以及超出模型长度的极端值，即可安心合入。

---

### Fix spelling errors (#33978)
**SHA**: `15a0b9e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/15a0b9e570dc8bfe716a7d76d50716898123dbae)

**🎯 变更类型**：其他（拼写修正）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交主要纠正了多个文件中出现的拼写错误，包括注释中的 “Intialize/Intialize” → “Initialize”、常量名 “NO_REASONING_QUICK_THROUGHT” → “NO_REASONING_QUICK_THOUGHT”，以及变量名 “is_availble” → “is_available”。同时更新了相应的引用，以确保代码能够通过 lint 检查。

**🎯 影响范围**  
- `tests/kernels/moe/`、`tests/reasoning/`、`vllm/model_executor/layers/fused_moe/oracle/fp8.py`：仅测试代码和注释，功能不变。  
- `vllm/v1/attention/ops/flashmla.py`：变量名更正，仅在本文件内部使用，运行逻辑保持不变。  

**💡 关注建议**  
1. **运行 CI**：虽然变更只涉及拼写，但请在提交后完整跑一遍单元测试，确保更名的常量和变量没有遗漏引用。  
2. **保持一致**：后续新增代码请使用统一的拼写（如 `Initialize`、`AVAILABLE_BACKENDS`、`is_available`），并在 IDE 中启用拼写检查插件，以防类似错误再现。  
3. **文档同步**：若 README、开发文档或注释中出现相同拼写错误，请同步修正，提升代码库整体可读性。  

总体来看，此次修改对功能、性能或 API 没有实质影响，属于质量提升类的细节优化，风险极低。

---

### [Misc] Make `PlaceholderRange.get_num_embeds` a method (#34035)
**SHA**: `48312e5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/48312e579a6de16b3d15a40796ef165ed141c8dc)

**🎯 变更类型**：功能改进（将 `PlaceholderRange.get_num_embeds` 从属性改为普通方法）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 取消 `PlaceholderRange.get_num_embeds` 的 `@cached_property` 装饰，改为普通实例方法 `def get_num_embeds(self) -> int`。  
- 项目中所有对该属性的访问改为调用方法，包括测试、预算计算、调度器、输入处理、请求对象以及 GPU 执行路径。  

**🎯 影响范围**：  
- `vllm/multimodal/inputs.py` 中 `PlaceholderRange` 实现。  
- 多处业务代码（`scheduler.py、input_processor.py、request.py、gpu_model_runner.py`）以及单元测试文件。  

**💡 关注建议**：  
1. **性能**：`@cached_property` 被移除后，每次调用都会重新计算 `embeds_cumsum`（若 `is_embed` 为 `None` 则直接返回 `length`），但计算开销极小，影响可忽略。若后续出现频繁调用的热点，可考虑手动缓存或恢复属性形式。  
2. **向后兼容**：公开为方法后，外部代码若仍以属性方式访问会触发 `AttributeError`。建议在文档和发布说明中明确迁移步骤，或在 `PlaceholderRange` 中保留一个兼容属性（如 `@property def get_num_embeds(self): return self.get_num_embeds()`）以平滑过渡。  
3. **测试**：所有受影响的单元测试已更新并通过，建议在 CI 中加入对未迁移代码的灰度检测，防止遗漏。  
4. **代码规范**：统一使用 `()`. 调用方式在新版 API 中更显式，也符合 PEP‑8 对 “方法应使用括号” 的建议。  

总体而言，此次改动集中在接口层面，影响范围有限且已在代码库中全面替换，风险较低。后续关注性能监控和向后兼容性即可。

---

### [Misc] Add backward-compatible import aliases for renamed translations module (#34015)
**SHA**: `1c3b220` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1c3b22058f3407c0bf517ef05b72d03a192974b3)

**🎯 变更类型**：重构 / 向后兼容  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `vllm.entrypoints.openai.translations` 包下新增了一套占位模块，统一对原先已改名为 `vllm.entrypoints.openai.speech_to_text` 的子模块提供向后兼容的 import 别名，并在导入时抛出 `DeprecationWarning`，提示用户在 0.17+ 之前完成迁移。

**🎯 影响范围**  
- `vllm.entrypoints.openai.translations`（新建的 5 个文件）  
- 任何直接使用旧路径 `vllm.entrypoints.openai.translations.*` 的用户代码、示例或第三方插件。  

**💡 关注建议**  
1. **用户侧**：尽快将 import 改为 `vllm.entrypoints.openai.speech_to_text.*`，否则在 0.17 之后将导致 `ImportError`。可以利用 `warnings.filterwarnings('ignore', category=DeprecationWarning, module='vllm.entrypoints.openai.translations')` 暂时压制警告。  
2. **库维护**：确认文档、示例、README 中的旧 import 已同步更新；若有 CI 检查 `DeprecationWarning`，需要在 0.17 前移除这些别名文件。  
3. **兼容性测试**：建议在发布前跑一次全链路测试，确保旧别名仍能正确转发全部公开 API（包括 `__all__` 与类型提示）。  
4. **版本策略**：在 0.16.x 发行说明中标记 “deprecated translations module”，并在 0.17 正式删除对应文件，避免用户在升级后遇到突兀的破坏性错误。  

总体来看，此次改动只涉及 import 重定向，业务逻辑未受影响，风险低。但务必在文档和迁移指南中做好提示，以防用户在下一大版本升级时出现不可用的情况。

---

### [Bugfix] Fix QK Norm+RoPE fusion pattern matching on B200+FP8 (#33967)
**SHA**: `9060771` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/906077181b21128576018604562e55fbc7f70c34)

**🎯 变更类型**：Bugfix + 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
1. 修复在 B200 + FP8 环境下 QK‑Norm + RoPE 融合的模式匹配失效问题。  
2. 新增 `SplitCoalescingPass`，在图优化阶段把对同一张量、相同 split‑sizes 的多次 `split_with_sizes` 合并为唯一节点，使后续的融合 passes 能正确识别。  
3. 相应地在 `PassManager` 中把该 pass 插入到 `QKNormRoPEFusionPass` 前，更新测试以覆盖普通 split 与“散布式” split 两种写法，并新增 `test_split_coalescing.py` 验证合并效果。

**🎯 影响范围**  
- **编译调度层**：`vllm/compilation/passes/utility/split_coalescing.py`、`pass_manager.py`。  
- **融合层**：`QKNormRoPEFusionPass`（依赖合并后的 split）。  
- **模型实现**：`tests/compile/passes/test_qk_norm_rope_fusion.py` 中对 `qkv.split` 的改写，新增 `test_scattered_split` 选项。  
- **测试套件**：新增 `tests/compile/passes/test_split_coalescing.py`，以及对已有测试的参数化修改。

**💡 关注建议**  

1. **Pass 顺序**：`SplitCoales化Pass` 必须在任何依赖唯一 split 的融合 pass 之前执行。当前已放在 `QKNormRoPEFusionPass` 前，建议在文档或注释中明确说明依赖顺序，以免后续新增 pass 时破坏此链。  

2. **兼容性检查**：`split_with_sizes` 合并仅在所有用户都是 `getitem` 的情况下进行。若后续出现 `split` 的结果直接用于其他算子（如 `cat`、`view`）可能被误漏，建议在合并前记录未合并的原因，或在日志里给出提示，便于定位潜在遗漏。  

3. **性能验证**：合并节点会减少算子计数，但也可能改变 Graph 中的节点拓扑，对后续的内存布局优化产生连锁影响。建议在 CI 中加入对大型模型（如 Qwen‑3‑30B‑FP8）全流程的基准测试，确保不会引入额外的编译时开销或性能回退。  

4. **测试覆盖**：  
   - 已覆盖 `scatter split` 与普通 split 两种写法，且验证 `aten.split_with_sizes` 调用次数为 1。  
   - 建议再加入跨层共享同一 `qkv` 张量的情形，确保 CSE 在更复杂的子图中仍能被正确捕获。  
   - 在非 CUDA/ROCM 平台上也跑一遍（skip‑logic 已在测试中），防止平台特异性错误。  

5. **文档与 changelog**：在 `CHANGELOG.md` 中明确记录 “B200 + FP8 QK‑Norm‑RoPE 融合修复” 以及 “新增 SplitCoalescingPass”，并在 `docs/compilation` 章节补充该 Pass 的作用与使用限制。  

6. **代码可维护性**：`SplitCoalescingPass` 只依赖 `torch.fx` 基础 API，代码简洁。若以后出现 `torch.ops.aten.split`（非 `split_with_sizes`）的类似冗余，考虑将两者统一抽象成“DuplicateOpCoalescingPass”。  

总体来说，此次变更解决了实际硬件上导致融合失效的关键问题，并通过通用的图合并 Pass 提供了更稳健的底层支撑。只要注意上述顺序、兼容性与性能回归验证，风险可控。祝后续合并顺利！

---

### [bugfix] [ROCm] Fix premature CUDA initialization in platform detection (#33941)
**SHA**: `4a2d00e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4a2d00eafd45d06a63936fcd4461b129382e783b)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 修复 ROCm 平台检测在导入时提前初始化 CUDA/ROCM 的问题；通过 `amdsmi` 查询 GCN 架构，只有在查询失败时才回退到 `torch.cuda`（会触发初始化）。  
2. 新增 CI 步骤和两组独立脚本/测试，验证 `vllm.platforms` 导入不导致 CUDA 初始化，以及 `torch.cuda.device_count()` 在设置 `CUDA_VISIBLE_DEVICES` 后仍然生效。  

**🎯 影响范围**  
- `vllm/platforms/rocm.py`（架构检测、`use_rocm_custom_paged_attention`）  
- CI 配置 `.buildkite/*.yaml`（新增测试用例）  
- 新增测试文件 `tests/cuda/test_platform_no_cuda_init.py` 及其脚本  

**💡 关注建议**  
1. **依赖管理**：`amdsmi` 是可选依赖，确保在没有该库的环境下仍能平滑回退而不抛异常。可在 `setup.cfg/pyproject.toml` 中标记为 `extras_require["rocm"]`。  
2. **缓存行为**：`_get_gcn_arch_via_amdsmi` 使用 `@cache`，首次调用若触发 CUDA 初始化后会一直保持该状态。建议在文档中明确说明该函数的副作用以及何时安全调用。  
3. **回退警告**：当前在 `amdsmi` 失效时会 `logger.warning_once`，但仍会导致 CUDA 初始化。若用户依赖“仅查询而不初始化”，应提供显式开关或更细粒度的错误提示。  
4. **测试可靠性**：脚本在无 GPU 环境下会直接 `exit(0)`，这会让 CI 在 CPU‑only runner 上跳过实际检查。建议在 CI 中使用带 GPU 的 runner，或在测试中 mock `torch.cuda.is_initialized` 以确保逻辑被执行。  
5. **兼容性回归**：更改后部分基于旧实现（直接 `torch.cuda.get_device_properties`）的代码路径可能会因 `amdsmi` 失败而触发 CUDA 初始化，进而影响 Ray‑multi‑GPU 场景。建议在发布说明中明确此行为变化，并提供迁移指引（如提前设置 `CUDA_VISIBLE_DEVICES` 或使用 `ROCR_VISIBLE_DEVICES`）。  

总体来看，此次改动解决了关键的多进程启动 bug，影响范围主要在 ROCm 平台检测与相关注意力特性，建议在正式发布前对无 `amdsmi` 环境做完整回退测试，并在文档中加入使用说明。

---

### Fix RoutingMethodType logic (#33919)
**SHA**: `207c3a0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/207c3a0c20c0fee6f375e37e067074b95053094f)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
1. 将 Docker 镜像及依赖中的 FlashInfer 版本从 **0.6.2 → 0.6.3**，保持与 upstream 同步。  
2. 在 `vllm/model_executor/layers/fused_moe/config.py` 中新增 `get_routing_method_type`，统一根据 `scoring_func、top_k、renormalize` 推断 `RoutingMethodType`，并在 `fused_topk_router`、`fused_topk_bias_router` 中使用。  
3. `router_factory` 增加对 `GroupedTopKRouter` 的回退逻辑：当路由方式为 `Unspecified` 且仅单组时，自动降级为普通 top‑k 路由，以避免出现不可用的路由组合。  
4. `flashinfer_trtllm_moe.py` 为兼容 FlashInfer 0.6.3 添加 `activation_type=ActivationType.Swiglu` 参数（临时补丁）。  

**🎯 影响范围**  
- **Docker 镜像**：所有基于 `vllm` 的容器将使用新版 FlashInfer，可能带来算子性能/精度提升。  
- **Fused MoE 代码路径**：`RoutingMethodType` 的判定从硬编码改为工具函数，涉及 `fused_topk_router`, `fused_topk_bias_router`, `router_factory`。  
- **量化/FP8 路由**：`_supports_routing_method` 里恢复了 `Renormalize/RenormalizeNaive` 支持。  

**💡 关注建议**  
1. **兼容性测试**：在升级 FlashInfer 后，跑完整的单元/集成测试，尤其关注 FP8 量化模型的输出与之前版本对比，防止因 `ActivationType.Swiglu` 参数导致的细微数值差异。  
2. **路由行为验证**：对比 `get_routing_method_type` 与原有硬编码（Renormalize vs Default）在不同 `scoring_func/top_k/renormalize` 组合下的实际路由选择，确保未出现意外的 `Unspecified` 路由被错误使用。  
3. **回退逻辑审查**：`router_factory` 中的“降级为普通 top‑k”路径在多组场景下不触发，确认业务侧不依赖 `GroupedTopKRouter` 的 `Unspecified` 行为；如有特殊需求，请显式设定 `routing_method`。  
4. **文档同步**：更新 README/CHANGELOG 中关于 FlashInfer 版本及 MoE 路由选项的说明，提醒用户在自行构建镜像时同步 `versions.json`。  

总体来看，此次改动修复了路由方法判定的漏洞，提升了与 FlashInfer 新版的兼容性，但涉及底层算子和路由策略的细微变化，建议在生产环境前完成回归测试。

---

### [Refactor] Remove align block size logic in `moe_permute` (#33449)
**SHA**: `77c09e1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/77c09e1130661197ccac2d968a28cd4a557922d5)

**核心变更概览**  
本次 PR 将 Mixture‑of‑Experts（MoE） 的 `moe_permute` 逻辑中与 **align block size**（DeepGEMM 需要的 128‑对齐）相关的代码全部删掉，简化了 CUDA kernel 与 Python 包装层：

1. **CUDA 实现**  
   - `csrc/moe/moe_permute_unpermute_op.cu`：删除 `align_block_size` 参数、`getMIndices` 调用以及对 `expert_first_token_offset` 的对齐处理。`expandInputRowsKernel` 只保留 `expert_first_token_offset`，去掉 `aligned_expert_first_token_offset` 与 `ALIGN_BLOCK_SIZE` 编译分支。  
   - 相关头文件、内部实现 (`moe_permute_unpermute_kernel.*`) 同步删减 `ALIGN_BLOCK_SIZE` 模板参数与分支调度。  

2. **Python 接口**  
   - `torch_bindings.cpp`、`fused_moe/moe_permute_unpermute.py` 移除 `align_block_size`、`fill_invalid_expert` 参数，删除对 `m_indices`（DeepGEMM 用的分组标记）的创建和返回。  
   - `moe_permute` 的返回值从 6 项降为 5 项（去掉 `m_indices`）。  

3. **测试与基准**  
   - `tests/kernels/moe/test_moe_permute_unpermute.py`、benchmark 脚本相应删除 `align_block_size` 参数及对齐路径的验证。  

**影响范围**  
- **核心计算**：MoE‐permute/unpermute 的所有调用点（模型执行、后端插件）现在只能走 **CUTLASS** 或其他不需要对齐的 GEMM 实现；若项目仍依赖 DeepGEMM，则会失去对齐优化。  
- **API 兼容性**：外部代码若显式传入 `align_block_size`（或依赖 `m_indices`）会因参数不存在而报错。  
- **性能基准**：去掉对齐逻辑后在 DeepGEMM 路径上可能出现性能回退，需要在 CI 中确认 CUTLASS 已经是默认实现。  

**建议与注意事项**  

| 方向 | 建议 |
|------|------|
| **向后兼容** | 在 `moe_permute` 的 Python 包装层加入 **参数去除的 Deprecation Warning**，提醒用户 `align_block_size` 已不再支持。 |
| **文档更新** | 更新 `vllm` 文档、API 手册以及示例代码，去除 `align_block_size` 参数的说明，明确仅返回 `permuted_idx` 而非 `m_indices`。 |
| **性能验证** | 在不同 GPU、CUDA 12+ 环境下跑完整的模型吞吐基准，确保 CUTLASS/其它实现的性能不低于原 DeepGEMM 对齐路径（尤其是大 batch、FP8 场景）。 |
| **代码路径检查** | 搜索仓库其他地方（例如自定义插件或者第三方库）是否仍有 `align_block_size` 参数的调用，必要时添加适配层或迁移指南。 |
| **错误信息** | 对 `moe_permute_unpermute_supported` 保持一致，保证在不支持的 CUDA 版本上仍能给出明确错误，而不是因缺少 `align_block_size` 导致隐藏的 `assert`。 |
| **单元测试** | 当前删除了对齐相关的 `parameterize`，建议增加一次 **全量回归**（包括不同 `topk`、`ep_size`、FP8）以防隐藏的维度对齐假设被遗漏。 |

总体而言，此次重构显著简化了 MoE permute 的实现，降低了维护成本，但需确保所有依赖 DeepGEMM 对齐的场景已被迁移至 CUTLASS 或者明确标记不再支持。完成上述兼容性与性能检查后，可安全合并。

---

### [Model Runner V2] support apply penalty for spec decode (#33251)
**SHA**: `16786da` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/16786da7357327f44f3a8f23d17e3c84235d2952)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Speculative Decoding（草稿解码）加入 token‑level penalty 计算。新增 `expanded_local_pos` 记录每个 logit 在请求中的相对位置，并将 `input_ids`、位置信息以及 `num_speculative_tokens` 传递到采样、惩罚与采样器层，修改 Triton kernel 使重复、频率、出现惩罚能够累计草稿阶段的 token 统计。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/input_batch.py` – 扩展 batch 数据结构、扩展 `expand_idx_mapping` 返回值。  
- `vllm/v1/worker/gpu/model_runner.py` – 在准备输入、采样流程中传递新字段。  
- `vllm/v1/worker/gpu/sample/penalties.py` – Triton 惩罚核重构，加入 `token_ids`、`expanded_local_pos`、`MAX_SPEC_LEN` 计算累计草稿计数。  
- `vllm/v1/worker/gpu/sample/sampler.py` – Sampler 新增 `num_speculative_tokens` 参数并把新张量向下传递。  

**💡 关注建议**  
1. **一致性检查**：确保所有调用 `expand_idx_mapping` 的路径（包括未来的分支）都已更新以接受 `expanded_local_pos`，避免因返回值拆包错误导致运行时异常。  
2. **性能评估**：新增的 `expanded_local_pos` 与 `token_ids` 读取会在 Triton kernel 中产生额外内存访问，建议在大批量推理时对 `MAX_SPEC_LEN`（即 `num_speculative_tokens`）进行基准测试，确认对吞吐量的影响在可接受范围。  
3. **边界条件**：`expanded_local_pos` 采用 `tl.load` 直接读取，若 `pos` 超出 `MAX_SPEC_LEN`（草稿步数）会出现未定义行为，建议在调用前加入断言或在 kernel 中使用安全的 `mask`。  
4. **回退兼容**：老版模型或未开启 speculative 解码的路径仍会使用 `expanded_local_pos` 的 zero 初始化，保持向后兼容；但若未来移除该特性，请同步清理相关默认值。  
5. **单元测试**：补充包含草稿解码‑+‑penalty 场景的 pytest，特别是验证 `repetition_penalty` 在草稿阶段累计的计数是否与预期一致。  
6. **文档更新**：在 `Speculative Decoding` 与 `Penalty` 章节加入参数说明（`num_speculative_tokens`、`expanded_local_pos`），帮助使用者正确配置 `SamplingParams`。  

总体来说，此次改动为 Speculative Decoding 引入了完整的惩罚机制，提升生成质量，但牵涉到多个 GPU 计算入口，务必在不同硬件/批量规模下进行充分验证。

---

### [DOC] [ROCm] Update docker deployment doc (#33971)
**SHA**: `aaa2efb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aaa2efbe9861200056a8f2dc31d2482fec89b880)

**📝 变更概览**  
本次 PR 主要对文档进行结构与内容的统一整理，涉及 `docs/deployment/docker.md`、`docs/getting_started/installation/gpu.*.inc.md` 以及对应的入口文件 `docs/getting_started/installation/gpu.md`、`gpu.rocm.inc.md`。核心工作包括：

1. 将原先分散在 `deployment/docker.md` 的 Docker 使用说明重新抽取为 **预置镜像**、**自建镜像** 两大块，统一纳入 “GPU 安装指南”。  
2. 引入 Markdown **include 指令**（`--8<--`）实现跨文件复用，减少重复内容。  
3. 为 NVIDIA、ROCm 两个平台分别补充了最新的官方镜像标签 (`vllm/vllm-openai`, `vllm/vllm-openai-rocm`) 以及相应的运行示例。  
4. 简化了 ARM64、Podman、pre‑compiled wheel 等细节的说明，并在需要的地方保留了原有 *note*、*tip*。  
5. 为所有入口文件新增 `toc_depth` 前置块，统一侧边目录深度。  

**⚡ 影响范围**  
- **文档渲染层**：`mkdocs`/`sphinx`（项目实际使用的是 `mkdocs-material`）在构建时需要解析 `--8<--` include，若路径或块标识有误会导致构建失败。  
- **外部链接**：官方 Docker Hub、AMD Infinity Hub 等 URL 已更新，需确保这些链接在 CI 中保持可达。  
- **跨文件引用**：`gpu.cuda.inc.md`、`gpu.rocm.inc.md` 等文件被多个页面引用，任何语法错误（如缺失的结束标记）都会在所有使用该块的页面中呈现异常。  
- **版本信息**：文档中仍保留了针对 v0.4.1/0.4.2 的 root‑user 注意事项，若后续不再维护这些老版本，建议加上 `deprecated` 标记或移除，以免误导用户。  

**🔍 关键检查点**  
1. **Include 语法**：确认所有 `--8<-- "path:section"` 的路径相对于项目根目录正确，且对应文件中存在相同的 `# --8<-- [start:...]`/`# --8<-- [end:...]` 块。  
2. **Markdown 兼容性**：加入的 `--- toc_depth: X` front‑matter 必须紧贴文件开头，且不与已有的 YAML 冲突。  
3. **链接有效性**：`https://hub.docker.com/r/vllm/vllm-openai-rocm`、`https://rocm.github.io/` 等外链建议在 CI 中做一次 HEAD 检查。  
4. **代码块标记**：部分 `bash`/`console` 块在改动后出现了不匹配的缩进或缺少结束反引号，需在本地预览或 `mkdocs serve` 确认渲染无误。  
5. **兼容性文档**：`gpu.md` 仍通过 `=== "NVIDIA CUDA"`、`=== "ROCm"` 切换视图，确保对应的 `include` 块在两种视图下均能正确展开。  

**💡 建议**  

| 目标 | 操作 |
|------|------|
| **防止 CI 文档构建失败** | 在 CI pipeline 中加入 `mkdocs build --strict`，确保所有 include、链接、YAML 前置块均通过校验。 |
| **保持文档同步** | 将 Docker 镜像标签（如 `vllm/vllm-openai:latest`）与项目的发布脚本关联，自动在发布时更新文档中的版本号占位符。 |
| **清理陈旧信息** | 将针对 0.4.x 版本的 “root user” 说明标记为已废除或移动至 “历史版本” 章节，避免新手误判。 |
| **提升可读性** | 对长代码块添加简短注释或分段注释（如 `# mount cache`, `# expose port`），帮助阅读者快速定位关键参数。 |
| **多平台一致性** | 与 `gpu.rocm.inc.md` 对照，确保 NVIDIA 与 ROCm 两套文档在结构上保持对等，避免某个平台缺失 “pre‑built images” 或 “build from source” 的章节。 |

总体而言，此次改动是一次 **文档结构优化**，没有直接影响代码运行。只要在 CI 中通过一次完整的文档构建检查，即可确保对外发布的指南保持连贯、准确。祝发布顺利 🚀。

---

### [KV Connector] Add missing method overrides to MultiConnector (#33292)
**SHA**: `aca5967` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aca596741673df92922565f08189d01ea228f605)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `MultiConnector` 补全了多个在 `KVConnectorBase_V1` 中声明的抽象方法（`set_host_xfer_buffer_ops`、`handle_preemptions`、`get_handshake_metadata`、`set_xfer_handshake_metadata`、`get_finished_count`），实现统一调度至内部子连接器。  
2. 在单元测试中新增：  
   - 对 `MultiConnector` 初始化过程的事件顺序更细致的断言。  
   - 集成测试 `test_multi_connector_handle_preemptions_integration`，验证 `handle_preemptions` 在所有子连接器上被正确转发。  
   - 新增检查 `MultiConnector` 是否已覆盖基类所有公开方法的测试。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py`（核心调度层）  
- `tests/v1/kv_connector/unit/test_multi_connector.py`（单元/集成测试）  

**💡 关注建议**  

1. **方法签名一致性**：新增的 `set_host_xfer_buffer_ops`、`handle_preemptions` 等已在基类声明为抽象方法，务必确认其参数、返回值与基类完全匹配，防止运行时类型错误。  
2. **异常传播**：当前实现对每个子连接器直接调用，若某个子连接器抛异常会中断后续调用。建议在循环中捕获并聚合异常或记录日志，以保证其它子连接器仍能执行。  
3. **性能开销**：`get_handshake_metadata` 采用“返回第一个非空”策略，若子连接器数量增多，遍历成本仍可接受，但后续若需要合并多连接器元数据，请评估 O(N) 的影响。  
4. **测试清理**：`test_multi_connector_handle_preemptions_integration` 通过临时目录创建存储路径，使用 `finally` 删除。确保在异常路径下 `shutil.rmtree` 不会因文件占用而失败，可考虑 `ignore_errors=True`（已使用）或 `pathlib.Path.rmdir` 的递归安全方式。  
5. **未实现的 TODO**：  
   - `get_kv_connector_kv_cache_events` 仍为 TODO，后续需实现聚合逻辑或在 `INHERITED_OK` 中标记。  
   - `get_finished_count` 目前返回 `None`，若上层逻辑依赖该信息，需要在对应子连接器实现后补全。  

总体来看，此次改动完善了 `MultiConnector` 与各子连接器的接口一致性，提升了调度层在多连接器场景下的可靠性。建议在下一轮发布前补全剩余 TODO，避免在生产环境出现未实现的方法调用。

---

### [Log] Optimize duplicate startup log (#33944)
**SHA**: `67a746e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/67a746e87f52d8728a40fbead123c7c17c94d4dc)

**🎯 变更类型**：其他（日志去重/行为优化）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在多个核心模块中将 `logger.info` 替换为 `logger.info_once` 并显式指定 `scope`（`local`/`global`），旨在防止同一信息在进程启动或循环中被重复打印，提升日志可读性与启动效率。  
**🎯 影响范围**：`vllm/compilation/backends.py`（编译缓存日志）、`vllm/utils/deep_gemm.py`（DeepGEMM 启用/禁用日志）、`vllm/v1/worker/gpu_worker.py`（V2 Model Runner 启动日志）。  

**💡 关注建议**  
- **开发者**：确认 `logger.info_once` 的实现已支持 `scope` 参数，避免因 scope 匹配错误导致日志意外被过滤。若有自定义日志处理器，需要同步更新以识别 `scope`。  
- **测试**：加入启动日志去重的单元/集成测试，确保在多 GPU 多进程场景下仍能看到关键一次性提示。  
- **用户**：日志行为略有变化，常用的 “使用 V2 Model Runner” 等信息现在仅出现一次，若依赖日志监控脚本，请检查是否需要调整匹配规则。  

整体来看，此次改动仅影响日志输出，不会改变业务逻辑或性能指标，风险较低。建议在下一个发布周期进行回归验证后合并。

---

#### 🟢 低重要度变更 (18)

### [Doc] Fix run_batch docs (#34056)
**SHA**: `b956cdf` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b956cdf818dbb05f4dc327cc76b46c18a1e37cf8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `run_batch.py` 中引入 `WrapperFn` 类型别名，统一 `wrapper_fn` 参数类型，并为 `make_transcription_wrapper` 添加返回类型注解，提升类型明确性和文档一致性。

---

### Make directory exist ok for ray spinning up multiple replicas on a single instance (#33604)
**SHA**: `860981d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/860981d8d85bbf810d8bdf14d799407adad67b1d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `runai_utils.py` 中创建模型缓存目录的逻辑改为 `os.makedirs(dir_name, exist_ok=True)`，避免不必要的删除与重建，提高并发实例启动时的目录安全性。

---

### Update DeepGEMM version pin in Dockerfile to match #32479 (#33935)
**SHA**: `52181ba` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/52181baaea01f8cc64e2ffde7761a3ff3b7e4d3e)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 Dockerfile、versions.json 与 install_deepgemm 脚本中 DeepGEMM 的 Git 提交哈希从 `594953a…` 更新为 `477618c…`，保持版本一致性。

---

### Enable Eagle3 speculative decoding for Mistral3ForConditionalGeneration to support eagle3 (#33939)
**SHA**: `4df44c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4df44c16ba8c4e44aeb7bf0dd622933c693d7613)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `Mistral3ForConditionalGeneration` 中新增 `SupportsEagle3` 接口，实现 `set_aux_hidden_state_layers` 与 `get_eagle3_aux_hidden_state_layers` 方法，以支持 Eagle3 推测解码。

---

### [CI][Build]  Pin grpcio-tools==1.78.0 (#34048)
**SHA**: `6ed5eda` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6ed5eda30009a25b6ca827064880b20a26c9afc2)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `grpcio-tools` 统一固定为 `1.78.0`，并在 `pyproject.toml`、各 `requirements/*.txt` 中更新对应依赖，确保构建与测试环境版本一致。

---

### [ROCm][CI] Pinning lm-eval version to resolve multi-modal small eval bug (#34038)
**SHA**: `c490d8c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c490d8cc7389860c1c6f9ae4181c023c7350738f)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `requirements/rocm-test.txt` 中将 `lm-eval[api]` 的版本约束由 “>=0.4.9.2” 固定为 “==0.4.9.2”，以解决多模态小评估的兼容性问题。

---

### [Kernel] Add enable_sm120_or_later for SM121 (DGX Spark) CUTLASS support (#33517)
**SHA**: `bc32444` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bc32444b238d2ec3726f599cf3fc67dbaf51a6c6)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `enable_sm120_family` 模板，兼容 SM120 与 SM121（DGX Spark）GPU 的 CUTLASS 内核，并在 FP8 blockwise kernel 中改用该模板。

---

### [Revert] Add util `handle_deprecated` back (#33998)
**SHA**: `18e8545` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/18e85452979d2f974f2c193d159816a893fbc253)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/config/utils.py` 中新增 `handle_deprecated` 函数，实现对已废弃配置项的警告提示并自动迁移至新命名。

---

### fix description in plugin_system.md (#33999)
**SHA**: `6f7adc5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6f7adc533a3ad6b45d8e5a92cfa2e0094daa702b)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将插件示例代码的文件路径说明从 `vllm_add_dummy_model.py` 修正为 `vllm_add_dummy_model/__init__.py`，以匹配实际结构。

---

### [ModelRunner V2] Revert token rank comparison difference for now (#34017)
**SHA**: `40218a8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/40218a82bad0bc772d75551a8009799f1a001db7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `logits > x` 改为 `logits >= x`，恢复原始 token 排名比较，避免因比较方式不同导致的排名偏差。

---

### [Bugfix] Fix _fused_moe_lora_expand signature mismatch (#33821)
**SHA**: `3920caf` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3920cafdd6a8b20a76fa4805e010ae5da8f3a45f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 `_fused_moe_lora_expand` 的函数签名，去除多余的 `b_intermediate_cache1` 参数，以消除签名不匹配的错误。

---

### [CI][AMD]Bugfix] Check that model_config is not None in enable_norm_pad_fusion (#34007)
**SHA**: `ec28784` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ec28784fdc4dd1eb0141e9629b3f2d7cfce5c114)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `enable_norm_pad_fusion` 中新增 `cfg.model_config is not None` 检查，防止空模型配置导致属性访问错误。

---

### [Bugfix] Fix Whisper tokenization (#34011)
**SHA**: `55aeec0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/55aeec04f52a9d347c3299c4f4d0df2683ffac00)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Whisper 处理器中排除 `truncation`、`max_length` 参数，防止在提供音频数据时特征提取器误将其当作音频长度进行截断。

---

### [Fix] Fix `logprobs=0` handling for `/inference/v1/generate` endpoint (#34010)
**SHA**: `ae2e93f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ae2e93f89b06529dba41b5c0adc1a6d27e921320)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 `/inference/v1/generate` 接口在 `logprobs=0` 时的处理逻辑，确保返回结构完整；新增对应单元测试验证不同 `logprobs` 参数的返回。

---

### [Bugfix] Fix no attribute error of SharedFusedMoE (DeepSeek-V3.1 as test model) (#33993)
**SHA**: `9e9acce` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9e9acce577cc8d6daf6db7aacc24a939c08391ca)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `FusedMoE` 初始化中新增 `has_shared_experts` 参数，避免在模块未完全构造前访问 `self.shared_experts`，并在 `SharedFusedMoE` 中传递该标记，以修复 DeepSeek‑V3.1 测试模型的属性错误。

---

### [Rocm][Bugfix] Fix dtype not same for gemm_a4w4 op (#33734)
**SHA**: `fe54382` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fe5438200b1cd5e496cf50f095d208eb1fb18fdd)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `gemm_a4w4` 调用中将 `weight` 强制转为 `x_q` 的数据类型，确保 `gemm_a4w4` 的输入 dtype 一致，修复了类型不匹配的 Bug。

---

### [Bugfix] Fix the issue where tool calling does not work when using fast detokenization with dsv32 (#33964)
**SHA**: `7bec435` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7bec4351305f7b99ff6385b778760d301e29a6f0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `deepseekv32_tool_parser.py` 中添加 `adjust_request`，在使用 fast detokenization 时强制 `skip_special_tokens=False`，修复工具调用失效的问题。

---

### [Docs] Update link to Benchmark CLI documentation (#33254)
**SHA**: `5c52644` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5c52644b103126c140f56fb67f01918bdde9a4b0)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `benchmarks/README.md` 中将 Benchmark CLI 文档链接更新为新的地址 `https://docs.vllm.ai/en/latest/benchmarking/cli/#benchmark-cli`，旨在指向更准确的章节。

---

