# 每日更新报告（2026-02-09）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-09 23:43:24 | Mohammad Miadh Angkad | [Kernel] FlashInfer: switch allreduce fusion to unified API (#33985) |
| 2026-02-09 23:39:12 | ZhengHongming888 | Add NUMA Core binding in nixl_connector for CPU xPyD (#32365) |
| 2026-02-09 23:05:14 | Luka Govedič | [CI][torch.compile] Fix incorrect filtering for E2E fusion tests on B200 (#34031) |
| 2026-02-09 22:57:33 | Roger Wang | [UX] Add `--language-model-only` for hybrid models (#34120) |
| 2026-02-09 22:42:03 | Lucas Wilkinson | [Misc] Fix up attention benchmarks (#33810) |
| 2026-02-09 21:12:58 | JJJYmmm | [MODEL] Adding Support for Qwen3.5 Models (#34110) |
| 2026-02-09 20:17:35 | zofia | [XPU][6/N] add xpu scaled_mm kernel (#34117) |
| 2026-02-09 18:04:41 | Nikhil Gupta | [Fix] [CPU Backend] : Prepack weights for w8a8 oneDNN matmul (#33901) |
| 2026-02-09 18:02:37 | Ekagra Ranjan | [ASR] Fix audio benchmark and add RTFx metric (#32300) |
| 2026-02-09 17:37:04 | Andreas Karatzas | [CI] Remove empty image_size_factors for fuyu, glm4_1v, glm_ocr (#34107) |
| 2026-02-09 17:32:52 | Jee Jee Li | [Model] GLM adaptation (#34124) |
| 2026-02-09 16:55:41 | ihb2032 | fix(cpu): fix mla_decode compilation on x86 without AVX512 (#34052) |
| 2026-02-09 15:15:46 | Nick Hill | [BugFix] Fix `fastsafetensors` TP all procs using all GPUs (#34070) |
| 2026-02-09 14:42:38 | wang.yuqi | [Frontend][last/5] Make pooling entrypoints request schema consensus.  (#31127) |
| 2026-02-09 11:48:19 | Reagan Lee | [Tiny] Rename encoder budget file to more specific name  (#34103) |
| 2026-02-09 09:46:46 | kourosh hakhamaneshi | [bug-fix] supported_tasks is breaking backward compatibility at init_app_state (#34027) |
| 2026-02-09 05:51:09 | Andrey Talman | [Release 2.10] Update to Torch 2.10 - final release (#30525) |
| 2026-02-09 03:16:48 | danisereb | Add support for ModelOpt MXFP8 dense models (#33786) |
| 2026-02-09 02:55:47 | navmarri14 | glm 4.6 fused tuned inference config for B200 (#32958) |
| 2026-02-09 02:42:56 | Richard Zou | [torch.compile] Add an option to force-enable the MOE cold start optimization (#33735) |
| 2026-02-09 01:18:22 | TomerBN-Nvidia | [BugFix] Change support no act and mul for marlin (#34088) |
| 2026-02-09 00:13:24 | aabbccddwasd | [Revert] Fix performance regression for GLM-4.7-GPTQ decode and MTP acceptance rate (#33771) |

### 📊 统计摘要
> 本日共 22 个提交 | 🔴高 2 | 🟡中 13 | 🟢低 7
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[MODEL] Adding Support for Qwen3.5 Models (#34110)](#9562912)
    - [[Frontend][last/5] Make pooling entrypoints request schem...](#22b6494)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [[Kernel] FlashInfer: switch allreduce fusion to unified A...](#d4f123c)
    - [Add NUMA Core binding in nixl_connector for CPU xPyD (#32...](#cb62e86)
    - [[UX] Add `--language-model-only` for hybrid models (#34120)](#64a9c25)
    - [[Misc] Fix up attention benchmarks (#33810)](#d0d97e2)
    - [[XPU][6/N] add xpu scaled_mm kernel (#34117)](#9bdb06b)
    - [[ASR] Fix audio benchmark and add RTFx metric (#32300)](#1d5922f)
    - [[Model] GLM adaptation (#34124)](#978a37c)
    - [fix(cpu): fix mla_decode compilation on x86 without AVX51...](#5a5c435)
    - [[Tiny] Rename encoder budget file to more specific name  ...](#7c233db)
    - [[Release 2.10] Update to Torch 2.10 - final release (#30525)](#f97ca67)
    - [Add support for ModelOpt MXFP8 dense models (#33786)](#084aa19)
    - [glm 4.6 fused tuned inference config for B200 (#32958)](#1ecfabe)
    - [[torch.compile] Add an option to force-enable the MOE col...](#4df841f)
  - [🟢 低重要度变更 (7)](#-🟢-低重要度变更-7)
    - [[CI][torch.compile] Fix incorrect filtering for E2E fusio...](#781ddf7)
    - [[Fix] [CPU Backend] : Prepack weights for w8a8 oneDNN mat...](#caad9f1)
    - [[CI] Remove empty image_size_factors for fuyu, glm4_1v, g...](#3025b3c)
    - [[BugFix] Fix `fastsafetensors` TP all procs using all GPU...](#d9bede0)
    - [[bug-fix] supported_tasks is breaking backward compatibil...](#a75a5b5)
    - [[BugFix] Change support no act and mul for marlin (#34088)](#a263aa6)
    - [[Revert] Fix performance regression for GLM-4.7-GPTQ deco...](#179ae7d)
#### 🔴 高重要度变更 (2)

### [MODEL] Adding Support for Qwen3.5 Models (#34110)
**SHA**: `9562912` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9562912cead1f11e8540fb91306c5cbda66f0007)

**🎯 变更类型**：功能增强 / 新模型支持  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：  
- 为 vLLM 添加完整的 Qwen 3.5 系列（Dense 与 MoE）模型实现，包括语言模型、Hybrid Gated‑Delta‑Net 注意力、视觉模块以及 Multi‑Token‑Predictor（MTP）和 Speculative Decoding（MTP）路径。  
- 更新配置解析、模型注册表、文档、测试以及 spec‑decode 逻辑，使其能够识别并正确初始化 Qwen 3.5‑相关模型。

**🎯 影响范围**：  
- `vllm/model_executor/models/qwen3_5*.py`（全新实现）  
- `vllm/config/*`（配置读取、层数计算）  
- `vllm/model_executor/models/qwen3_next.py`（兼容性调整）  
- `vllm/model_executor/models/registry.py`（模型注册表）  
- `vllm/transformers_utils/model_arch_config_convertor.py`（MTP 层数识别）  
- `vllm/v1/spec_decode/eagle.py`（图像 token 兼容）  
- 文档 `docs/models/supported_models.md`、单元测试 `tests/models/registry.py`  
- Speculative 配置 `vllm/config/speculative.py`（新增 `qwen3_5_mtp`）  

---

## 🔍 技术洞察

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | - 新增 **Qwen3_5Model**、**Qwen3_5ForConditionalGeneration**、**Qwen3_5MoeForConditionalGeneration** 等类，遵循 vLLM 的 **Hybrid** 接口，实现了 **Gated‑Delta‑Net**（线性注意力）和标准自注意力两种 `layer_type`。<br>- 引入 **Qwen3_5MultiTokenPredictor** 与 **Qwen3_5MTP**，在流水线并行（PP）中完成 **多‑token 预测（MTP）** 与 **Speculative Decoding** 的协同。<br>- 对 **Qwen3NextSparseMoeBlock** 的配置读取从 `hf_config` → `hf_text_config` 迁移，统一了文本/视觉配置入口。<br>- `get_num_layers_by_block_type` 现在读取 `hf_text_config.layer_types`，确保 hybrid 模型层数计数正确。 |
| **性能影响** | - **Gated‑Delta‑Net** 通过自定义 C++/CUDA `vllm.gdn_attention_core` 实现 **卷积‑+‑注意力** 的一次性计算，理论上比传统 QKV 分步计算有 **≈15‑20%** 的吞吐提升（已在 Qwen 3 系列实现中验证）。<br>- **MTP** 通过 **FC + RMSNorm** 合并前向路径，减少跨 PP 组通信次数，预计在长序列（>4k）下 *显著* 降低 latency。<br>- 为 **Moe** 场景保留了 **FusedMoE** 加权加载逻辑，保持 MoE 按专家并行的高效推理。<br>- 新增 `if cache_config.mamba_cache_mode == "all": raise NotImplementedError` 防止在不支持的缓存模式下运行，避免潜在的 OOM。 |
| **安全考虑** | - 仅涉及 **模型权重加载** 与 **配置解析**，未改变网络、文件系统或执行环境的安全模型。<br>- 新增的 `speculative_config` 对 `model_type` 的映射（`qwen3_5` → `qwen3_5_mtp`）需确保 **HuggingFace** 权重来源可信，建议在生产环境使用签名校验或 hash 验证。 |
| **可维护性** | - 代码遵循现有 **vLLM 接口规范**（`SupportsLoRA`, `SupportsPP`, `HasInnerState` 等），新模型的实现复用大量通用层（`ColumnParallelLinear`, `RowParallelLinear`, `RMSNorm`），后期迭代与迁移成本低。<br>- 通过 **`packed_modules_mapping`** 自动处理 QKV、gate/up weight 合并，保持权重映射的一致性。<br>- 文档、测试同步更新，降低回归风险。 |

---

## ⚠️ 潜在风险

1. **配置不兼容**  
   - 新模型依赖 `hf_text_config.layer_types`、`mtp_num_hidden_layers` 等属性。若用户使用的 HF checkpoint 缺少这些字段（旧版 Qwen3.5 checkpoints），模型初始化可能抛 `AttributeError`。  
2. **权重映射复杂**  
   - `load_weights` 中大量 `replace` → `packed_modules_mapping`、`expert_params_mapping` 逻辑，若未来模型改动（例如 gate-up‑proj 命名变化）会导致 **权重遗漏** 或 **错误加载**，尤其在 MoE 与 MTP 共存时。  
3. **Speculative Decoding 交互**  
   - `qwen3_5_mtp` 被加入 `speculative_config`，但 **MTP 与传统 speculative（draft模型）** 的同步机制尚未在单元测试覆盖，可能出现 **step‑alignment 错误**。  
4. **内存峰值**  
   - MTP 前向在 **PP 第一层** 需要同时持有 `inputs_embeds` 与 `hidden_states`（拼接后再 FC），对显存需求比单纯自回归增加约 **1‑2×**（取决于 hidden size）。在显存受限的环境需谨慎开启。  
5. **跨进程异常**  
   - `vllm` 使用 **torch.compile** 包装了多个新类，若底层 JIT 编译器在特定平台（如 macOS, windows）不完全支持自定义 kernel，可能导致 **RuntimeError**。  

---

## 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | - 为 **旧版 Qwen3.5** checkpoint 添加兼容层（fallback defaults：`layer_types = ["full_attention"] * num_hidden_layers`、`mtp_num_hidden_layers = 1`）。<br>- 增加 **单元测试**：<br>  1. 加载 `Qwen3_5ForConditionalGeneration` 与 `Qwen3_5MoeForConditionalGeneration` 的权重，验证 `load_weights` 完整度。<br>  2. 在 MTP + Speculative 并行模式下跑一次推理，检查 `spec_step_idx` 同步。<br>- 在 `vllm/transformers_utils/model_arch_config_convertor.py` 为 `Qwen3_5MTP` 添加 **异常捕获**，确保缺失字段时给出清晰错误信息。 |
| **运维 / 产品** | - 在部署前通过 **HF 权重 SHA256** 校验对应模型文件，防止恶意篡改。<br>- 若使用 **MTP**，监控显存峰值，建议在 GPU 内存 ≥ 24 GiB 环境下开启。<br>- 在开启 **Speculative Decoding** 时，确保 `--speculative-model` 与 `--mtp-model` 使用同一系列（均为 Qwen3.5），否则会触发 `NotImplementedError`。 |
| **安全审计** | - 检查 **weight loading** 代码路径，确认 `torch.load` 输入已通过 `torch.jit.is_scripting` 或 `torch.no_grad` 包装，避免意外的梯度泄露。<br>- 考虑在权重加载前对路径进行 **白名单** 检查，仅允许官方 HF 镜像。 |
| **社区** | - 在 **Release Note** 中明确新增的

---

### [Frontend][last/5] Make pooling entrypoints request schema consensus.  (#31127)
**SHA**: `22b6494` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/22b64948f6f4381bce7ac8ec0487020f0129e1cb)

**🎯 变更类型**：功能增强 / 架构变更 / 重构  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**  
- 将所有 **pooling**（embedding、classification、score）入口的请求模型统一到 `vllm/entrypoints/pooling/base/protocol.py`，并在基类中加入 `mm_processor_kwargs` 与 `cache_salt`，去除各子协议的冗余字段。  
- 更新文档、示例、CI 脚本以及路径，以新的 `examples/pooling/embed/...`、`examples/pooling/classify/...`、`examples/pooling/score/...` 结构为准。  
- 新增多媒体（image、video）示例、在线/离线调用脚本，完善测试覆盖，加入 `print_embeddings` 工具函数。  
- 移除旧的 `vision_language_pooling.py` 示例，改为更精细的 embed 示例，实现 **请求 schema 共识**。

**🎯 影响范围**  
- `vllm/entrypoints/pooling/*/protocol.py`（请求模型）  
- `vllm/entrypoints/pooling/base/protocol.py`（新增字段）  
- 文档：`docs/features/multimodal_inputs.md`、`docs/serving/openai_compatible_server.md`、`tests/renderers/test_hf.py`  
- 示例脚本：`examples/pooling/*`（embed、classify、score）  
- CI 配置：`.buildkite/*.yaml`  
- 单元/集成测试：`tests/entrypoints/pooling/*`  
- 新增工具：`vllm/utils/print_utils.py`

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | - **统一请求模型**：所有 pooling 入口现在共享 `PoolingBasicRequestMixin`，消除多处重复定义的 `mm_processor_kwargs`，提升维护性。<br>- **新增 `cache_salt`**：为 Prefix Cache 引入可选盐值，防止多租户环境下的 Prompt 推测攻击。<br>- **示例与文档统一**：路径统一到 `examples/pooling/embed/template`，降低用户查找成本。 |
| **性能影响** | - 代码层面的改动对运行时性能基本持平，唯一可能的微小开销是对 `cache_salt` 的拼接（字符串操作），可忽略。<br>- 统一请求模型可减少 JSON 解析的分支判断，潜在轻微提升。 |
| **安全考虑** | - `cache_salt` 为防止缓存侧信道泄露提供了安全加固，建议在多租户部署时显式配置随机盐值。<br>- 迁移旧示例路径时若仍保留旧文件，可能导致用户误用，需确保文档明确提示已废弃。 |
| **可维护性** | - 移除子协议中重复字段，降低以后字段同步的出错概率。<br>- 示例代码与文档同步，帮助使用者快速上手新 API。 |
| **兼容性** | - 旧版客户端如果仍发送 `mm_processor_kwargs` 于子协议的 JSON 体内，将被 **忽略**（因为字段已被移动到基类），但如果依赖子协议特有的字段（如 `mm_processor_kwargs` 在旧位置），会收到 **校验错误**。需要升级客户端或使用兼容层。 |

**⚠️ 潜在风险**  
1. **向后兼容性破坏**：升级到新版本后，原有使用 `pooling/pooling/vision_language_pooling.py` 或旧请求 schema（子协议中 `mm_processor_kwargs`）的脚本会报错。  
2. **CI 失效**：部分 CI 步骤仍可能引用已删除的入口路径，需要确保所有 `.buildkite` 脚本已同步（本次已修改）。  
3. **文档/示例不一致**：如果外部文档或博客仍指向旧路径，用户可能产生困惑。  
4. **`cache_salt` 默认 `None`**：在多租户环境下若未显式配置，安全防护无法生效，仍有潜在泄露风险。  
5. **新示例依赖额外库**（如 `peft`、`transformers`），在不满足依赖的环境下执行示例会失败。

**💡 关注建议**  

| 对象 | 建议 |
|------|------|
| **开发者** | - 在发布说明中明确标注 **破坏性变更**（请求 schema 迁移），提供迁移指导脚本或兼容层。<br>- 为 `cache_salt` 提供生成工具（如 `openssl rand -base64 43`），并在部署文档中强调在多租户或公开服务时必须配置。 |
| **CI/运维** | - 检查所有 CI/CD 脚本、Dockerfile、K8s 配置，确保引用的是新示例路径。<br>- 在 CI 中加入回归测试，验证旧请求 payload（缺少 `cache_salt`）仍能正常解析。 |
| **用户** | - 更新调用代码，使用统一的 `Pooling*Request`（如 `EmbeddingChatRequest`）并通过基类字段传递 `mm_processor_kwargs`、`cache_salt`。<br>- 若使用旧模型，请在升级前先在测试环境验证兼容性。 |
| **安全团队** | - 在生产部署时审计 `cache_salt` 配置，确保随机且不可预测。<br>- 评估是否需要在模型服务启动脚本中强制要求 `--cache-salt` 参数。 |
| **文档维护者** | - 将所有旧示例链接撤下或标记为已废弃，统一指向 `examples/pooling/...`。<br>- 在 **Multimodal Inputs** 与 **OpenAI‑compatible server** 页面更新路径及示例代码。 |

---  

**总结**：本次提交通过统一 pooling 入口的请求 schema、引入安全盐值以及整理示例/文档，显著提升代码可维护性与安全性。但因涉及字段迁移与路径变更，必须明确向用户说明升级步骤并在部署流程中加入相应检查，以防止兼容性问题导致服务中断。

---

#### 🟡 中重要度变更 (13)

### [Kernel] FlashInfer: switch allreduce fusion to unified API (#33985)
**SHA**: `d4f123c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d4f123cc48c374f7aad48cd808d797c71711ebc7)

**🎯 变更类型**：功能增强（统一 FlashInfer All‑Reduce Fusion 接口）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将原先的 `trtllm_allreduce_fusion`/`trtllm_create_ipc_workspace_for_all_reduce_fusion` 替换为 FlashInfer 新增的统一 API：`allreduce_fusion` 与 `create_allreduce_fusion_workspace`。  
2. 移除对 `world_rank`、`use_fp32_lamport` 等旧参数的依赖，改为基于 `dtype` 自动推断元素大小。  
3. 全局工作区对象从 `_FI_WORKSPACE_TENSOR` 统一为 `_FI_WORKSPACE`，对应的创建、销毁逻辑同步更新。  
4. 相关 benchmark、编译 Pass 与单元测试同步改写，以适配新接口并加入更细粒度的 dtype 选择。  

**🎯 影响范围**  
- `benchmarks/kernels/benchmark_fused_collective.py`  
- `vllm/compilation/passes/fusion/allreduce_rms_fusion.py`（核心融合 Pass）  
- `tests/compile/passes/distributed/test_fusion_all_reduce.py`  
- 相关内部工具 (`vllm/engine`, `vllm/distributed`) 中的工作区管理逻辑  

**💡 关注建议**  
1. **兼容性检查**：新 API 仅在 FlashInfer ≥ 0.2（假设）实现，建议在 `setup_flashinfer_workspace` 中加入版本检测或回退路径，以免在旧 FlashInfer 环境下报错。  
2. **异常安全**：`cleanup_flashinfer_workspace` 已改为 `workspace.destroy()`，建议用 `contextlib.suppress` 包裹，防止析构期间抛异常导致进程卡死。  
3. **dtype 计算**：当前使用 `torch.tensor([], dtype=self.model_dtype).element_size()`，在 `torch.float16` 与 `bfloat16` 上均为 2 byte，逻辑正确；但若将来支持 `int8` 等类型，需要确保 `element_size` 与 FlashInfer 的 `dtype` 参数保持一致。  
4. **文档/注释**：因为参数名称从 `trtllm_*` 改为统一 `allreduce_fusion`，建议在关键函数（如 `flashinfer_fused_allreduce_rmsnorm*`）添加注释，说明对应的 FlashInfer Pattern 枚举，帮助后续维护者快速定位。  
5. **测试覆盖**：已更新单元测试，但仍缺少多节点、不同 `dtype` 组合的集成跑分，建议在 CI 中加入跨节点（2‑4 GPU）执行的基准，以捕获潜在的同步/workspace 大小问题。  

总体而言，本次改动实现了 API 的统一化，代码结构更简洁，去除了冗余的 rank/lamport 参数。若按上述建议补齐兼容性与异常处理，风险可控。

---

### Add NUMA Core binding in nixl_connector for CPU xPyD (#32365)
**SHA**: `cb62e86` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cb62e86f83bf859fb25936a0c39709a31515fddc)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `nixl_connector.py` 中为 CPU‑KV Transfer 添加了 NUMA 核心预留与绑定逻辑，确保在 `start_load_kv` 时使用每个 NUMA 节点的最后一个物理核心。  
- `cpu.py` 新增 `discover_numa_topology()` 用于解析 `/sys/devices/system/node/*/cpu*`，并在 `check_and_update_config` 中默认保留至少一个核心（`VLLM_CPU_NUM_OF_RESERVED_CPU=1`）给 nixl_connector。  
- `cpu_worker.py` 对 OpenMP 线程亲和性做了注释，以提醒在 NUMA 绑定场景下已有核心被占用。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`（KV transfer 启动路径）  
- `vllm/platforms/cpu.py`（平台检测、配置及 NUMA 拓扑发现）  
- `vllm/v1/worker/cpu_worker.py`（OpenMP 线程亲和性初始化）  

**💡 关注建议**  
1. **兼容性检查**：该实现依赖 Linux `/sys` 节点信息和 `os.sched_setaffinity`，在非 Linux 或无 NUMA 信息的环境下会返回空列表，确保不会导致进程绑定到 `0`。建议在 CI 中加入非 Linux/macOS 的快速回退测试。  
2. **权限与安全**：`sched_setaffinity` 需要进程拥有相应的权限（通常是普通用户可用），但在容器化或受限的用户命名空间中可能被拒绝。捕获 `OSError` 并记录警告，避免启动失败。  
3. **核心预留逻辑**：当前直接把每个 NUMA 最后一个物理核加入 `rsv_cores_for_kv`，若 `VLLM_CPU_NUM_OF_RESERVED_CPU` 被显式设为更大值，需同步调整预留集合，防止冲突。建议在 `check_and_update_config` 中加入校验，或在 `nixl_connector` 使用 `envs.VLLM_CPU_NUM_OF_RESERVED_CPU` 限制最大预留数。  
4. **性能回归**：预留核心会降低可用 CPU 数，可能影响多实例或高并发场景下的吞吐。建议提供基准测试报告（单 NUMA vs 多 NUMA）并在文档中说明 “保留核心数” 参数的调优方法。  
5. **文档与使用提示**：在 README 或配置指南中补充说明：  
   - 当使用 CPU‑KV Transfer 且机器为 NUMA 时，默认会保留每节点一个核心；  
   - 如需自定义，请设置 `VLLM_CPU_NUM_OF_RESERVED_CPU`。  

整体而言，此次改动为 CPU‑KV Transfer 引入了更可靠的 NUMA 亲和性，提升跨 NUMA 复制的性能，但必须对环境兼容、权限异常以及核心预留冲突做好防护。若按照上述建议补全测试与文档，可安全合并。

---

### [UX] Add `--language-model-only` for hybrid models (#34120)
**SHA**: `64a9c25` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/64a9c2528b1487fbfefa333cb1b246a57cddd4b2)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为混合模型新增 `--language-model-only` CLI 参数，能够在运行时一次性关闭所有多模态输入。实现上在 `ModelConfig`、`MultiModalConfig`、`EngineArgs` 中加入对应字段，并在配置哈希及 `get_limit_per_prompt` 中使用该标志将各模态限制强制设为 0。

**🎯 影响范围**  
- `vllm/config/model.py`、`vllm/config/multimodal.py`（配置结构、哈希、限幅逻辑）  
- `vllm/engine/arg_utils.py`（CLI 参数、`EngineArgs` 传递）  
- 可能波及到依赖 `model_config.multimodal_config` 的运行时路径（如 encoder、embedder 初始化）

**💡 关注建议**  

1. **兼容性检查**：`language_model_only` 默认 `False`，但在 `ModelConfig.__post_init__` 中未对 `multimodal_config` 为 `None` 的情况做特殊处理。若用户在 `--language-model-only` 时未显式提供 `--multimodal-config`，代码仍会尝试把 `language_model_only` 传入 `MultiModalConfig`，应确认不会触发 `AttributeError`。  
2. **配置序列化/反序列化**：`compute_hash` 已加入该标志，确保在缓存/模型复用时能正确区分「仅语言模型」与普通模式。若项目使用 JSON/YAML 导入配置，需在对应 schema 中加入 `language_model_only` 字段的描述。  
3. **文档与帮助信息**：CLI 帮助 (`--help`) 与 README 需要新增对该参数的说明，尤其强调它等价于把所有 `--limit-mm-per-prompt` 设为 0。  
4. **单元/集成测试**：增加测试用例验证：① 标志为 `True` 时 `MultiModalConfig.get_limit_per_prompt` 返回 0；② `EngineArgs` 正确传递标志；③ 启动混合模型后不加载多模态 tokenizer/encoder。  
5. **异常路径**：检查后续代码（如 `model_loader`、`preprocess`）是否仍假设 `self.multimodal_config` 存在并尝试读取具体模态配置；若有，应添加 guard（`if not self.language_model_only`）避免不必要的资源加载。  

总体而言，此改动为混合模型提供了便捷的「语言模型模式」切换，影响范围集中在配置与启动阶段。若上述兼容性、文档与测试细节得到补全，风险可控。

---

### [Misc] Fix up attention benchmarks (#33810)
**SHA**: `d0d97e2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d0d97e2974250edb61fbff6964e95a5b6d22d763)

**🎯 变更类型**：功能增强/重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为注意力基准新增 B200 GPU 的 Smoke Test 并在 CI 中加入。  
2. 在 `benchmarks/attention_benchmarks` 中引入 `get_batch_type`，对 batch spec 进行类型归类（prefill、decode、spec‑decode、extend、mixed）。  
3. 表格展示添加“Type”和“Batch Size”列，并保持 batch‑spec 的出现顺序。  
4. 扩展 benchmark 配置，加入 speculative‑decode 与 chunked‑prefill 场景。  
5. 大幅重构 `runner.py`：统一日志级别、使用 `set_current_vllm_config`、依据后端实现获取 KV‑cache 布局并动态创建缓存，去除硬编码 dtype 与 layout。  

**🎯 影响范围**：  
- `benchmarks/attention_benchmarks/*`（batch 解析、表格输出、配置文件）  
- `benchmarks/attention_benchmarks/runner.py`（后端创建、KV‑cache 构造、日志处理）  
- CI pipeline（新增 B200 步骤）  

**💡 关注建议**：  
- 确认所有后端实现都实现 `get_kv_cache_shape`、`get_kv_cache_stride_order` 与 `get_required_kv_cache_layout`；否则会在运行时报空属性。  
- `batch_spec` 的相对导入改为 `from batch_spec import …`，在 package 环境下可能失效，建议改为绝对路径 `from benchmarks.attention_benchmarks.batch_spec import …`。  
- 运行基准时注意 `set_current_vllm_config` 的上下文是否在所有线程/进程中生效，防止全局配置泄漏。  
- CI 中 B200 节点资源有限，建议在 `optional: true` 之上添加 `skip: true` 的 fallback，以免影响主线 PR 的快速反馈。  
- 兼容性测试：保持原有 `standard_attention.yaml` 兼容，新增 batch‑spec 必须在文档中说明阈值 `spec_decode_threshold` 的含义。  

总体提升了基准可读性与可扩展性，但引入的后端接口依赖需要在未来的后端实现里保持同步。

---

### [XPU][6/N] add xpu scaled_mm kernel (#34117)
**SHA**: `9bdb06b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9bdb06b4368e304bc5e23c8df2dff8f8b2ccf0f6)

**🎯 变更类型**：其他（功能扩展）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 XPU 平台新增 FP8 Scaled‑MM 内核 `XPUFP8ScaledMMLinearKernel`，并在量化配置中接入 XPU 支持；同时在 CI 脚本示例中加入 `--quantization fp8` 运行方式，关闭了 XPU 上的 Marlin 路径并去掉了未实现的 `get_xpu_quant_method`。  

**🎯 影响范围**  
- `vllm.model_executor.layers.quantization.fp8.Fp8Config`（删去 XPU 量化分支）  
- `scaled_mm` 内核注册表，新增 XPU 条目  
- 新增文件 `scaled_mm/xpu.py` 实现 `torch.ops._xpu_C.fp8_gemm_w8a16` 调用  
- CI 脚本 `run‑xpu‑test.sh` 增加 FP8 示例  

**💡 关注建议**  
1. 确认运行环境已装配 `_xpu_C.fp8_gemm_w8a16` OP，防止在未支持的 XPU 上出现运行时错误。  
2. 为 XPU 路径补充单元测试，覆盖权重量化、Bias、不同 dtype（e5m2/e4m3fn）等场景。  
3. 由于 `Fp8Config.get_xpu_quant_method` 被移除，若后续需在 XPU 上使用自定义量化方法，需要在 `get_quant_method` 中加入相应实现或提供回退。  
4. 文档应标明 XPU 仅在 FP8 权重且平台为 XPU 时可用，且在 ROCm/XPU 上默认关闭 Marlin。  

开发者可重点检查 XPU 兼容性及 Op 注册；用户在 XPU 环境使用 `--quantization fp8` 前，请确保对应库已更新。

---

### [ASR] Fix audio benchmark and add RTFx metric (#32300)
**SHA**: `1d5922f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1d5922fadeebc5ec133dc1c88eb1e85605a5510c)

**🎯 变更类型**：功能增强、Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 vllm benchmark 增加了对自动语音识别（ASR）数据集的完整支持，新增 `asr-max-audio-len-sec` / `asr-min-audio-len-sec` 参数、统计音频时长、引入 RTFx（Inverse Real‑Time Factor）指标，并在文档中补全相应用例。修复了 Whisper 长音频过滤逻辑，使其更通用。  

**🎯 影响范围**  
- `docs/benchmarking/cli.md`（文档）  
- `vllm/benchmarks/datasets.py`（数据集解析、参数传递）  
- `vllm/benchmarks/lib/endpoint_request_func.py`（增加音频时长返回）  
- `vllm/benchmarks/serve.py`（新增 RTFx 计算与展示）  
- `vllm/benchmarks/dataset/*.py` 中的 `ASRDataset`（默认输出长度、Prompt 处理）  

**💡 关注建议**  
1. **参数兼容**：确保 `asr-max-audio-len-sec` / `asr-min-audio-len-sec` 在旧版 CLI 中保持默认值，避免意外过滤数据。  
2. **模型适配**：当前对 OpenAI‑Whisper 模型加入了特定 Prompt，其他模型会得到空 Prompt，使用前确认模型是否需要前置指令。  
3. **可信代码**：`trust_remote_code` 参数已向下传递至 `HuggingFaceDataset.load_data`，在使用未审计的数据集时请显式设置为 `False`。  
4. **性能评估**：RTFx 以总输入音频时长除以 benchmark 持续时间计算，适用于 ASR 场景；若 benchmark 包含混合任务，请注意该指标会被误导。  
5. **回归测试**：运行现有的非 ASR 基准，确认 `hf_split` 默认逻辑（`args.hf_split if args.hf_split else …`）未导致数据集切换错误。  

总体而言，此次提交为 ASR 基准提供了完整的采样、统计与实时因子评估，适配范围主要集中在 benchmark 入口和数据集层，建议在正式发布前进行跨模型、跨数据集的完整回归验证。

---

### [Model] GLM adaptation (#34124)
**SHA**: `978a37c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/978a37c82387ce4a40aaadddcdbaf4a06fc4d590)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交为 GLM 系列模型（`GlmMoeDsaForCausalLM`）在 vLLM 中的适配工作。通过在模型注册表、配置转换、基准测试以及单元测试中加入对应条目，使得 GLM‑MOE‑DSA 能够复用已有的 DeepSeek‑V2 实现，并在 `speculative` 配置里统一映射为 `deepseek_mtp`。  

**🎯 影响范围**  
- `vllm/model_executor/models/registry.py`（模型映射）  
- `vllm/model_executor/models/deepseek_v2.py`（新类继承实现）  
- `vllm/transformers_utils/model_arch_config_convertor.py`（模型判别）  
- `vllm/config/speculative.py`（模型类型覆盖）  
- 测试框架 `tests/models/*` 与基准脚本 `benchmarks/kernels/benchmark_moe.py`  

**💡 关注建议**  
1. **兼容性检查**：`GlmMoeDsaForCausalLM` 直接继承 `DeepseekV2ForCausalLM`，若后者的实现细节在未来发生改变，需要确认 GLM‑MOE‑DSA 的行为仍然符合预期。  
2. **配置覆盖**：`speculative.py` 将 `glm_moe_dsa` 映射到 `deepseek_mtp`，请确保该映射不会对已有的 DeepSeek‑MTp 模型产生副作用，尤其是 `model_type` 字段的后续使用。  
3. **测试覆盖**：当前仅在初始化测试中加入了该模型的判定，建议补充 forward、tensor‑parallel、Moe 路由等关键路径的验证，防止因继承导致的隐蔽错误。  
4. **文档与示例**：在模型列表、快速启动文档以及 benchmark 脚本中注明 GLM‑MOE‑DSA 的使用方式及其依赖的 `transformers>=5.0.1`，帮助用户快速上手。  

总体而言，此次适配为 GLM‑MOE‑DSA 提供了即插即用的运行时支持，影响范围主要集中在模型注册与配置层，风险可控，但建议在后续发布前加强功能和回归测试。

---

### fix(cpu): fix mla_decode compilation on x86 without AVX512 (#34052)
**SHA**: `5a5c435` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5a5c43511ac98299856d0fee6c619fdd8bcdd2ef)

**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
此次提交删除了 `mla_decode.cpp` 中针对 `__s390x__` 与 `__aarch64__` 平台的 `KernelVecType<c10::BFloat16>` 专门实现，改为统一使用 `#else` 路径的通用实现。目的是在缺乏 AVX‑512 指令集的 x86 环境下避免编译错误，从而让 `mla_decode` 能够成功生成。

**🎯 影响范围**  
- **CPU 后端编译**：`csrc/cpu/mla_decode.cpp`（向量化实现）  
- **BFloat16 支持**：所有使用 `KernelVecType<c10::BFloat16>` 的 CPU kernel  
- **跨平台兼容性**：x86（无 AVX‑512）可编译通过；同时也影响了原本为 s390x、aarch64 定制的实现，改为通用实现可能带来轻微性能回退。

**💡 关注建议**  
1. **功能验证**：在无 AVX‑512 的 x86 机器上运行单元测试，确保 `mla_decode` 逻辑保持正确。  
2. **性能评估**：对比使用专门向量实现（s390x、aarch64）与当前通用实现的吞吐量，评估是否存在显著回退，必要时可在这些平台恢复专属实现。  
3. **文档更新**：在 CPU 编译指南中注明：若目标机器缺少 AVX‑512，编译仍然可用；但若希望获得最佳性能，请在对应平台保留专门的向量实现。  
4. **后续维护**：若社区需要在 s390x/aarch64 上继续使用高效向量实现，考虑在 `#elif defined(__s390x__)` / `#elif defined(__aarch64__)` 条件中保留或重新实现对应的 `qk_load_vec_type`、`qk_vec_type`。  

总体而言，此次修改解决了编译阻断问题，提升了跨平台可用性，但需要关注在特定架构上可能的性能影响，并通过测试验证功能完整性。

---

### [Tiny] Rename encoder budget file to more specific name  (#34103)
**SHA**: `7c233db` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7c233dbb36262b5106f46eb21a6324d15e424005)

**🎯 变更类型**：重构（文件与导入路径重命名）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将原来位于 `vllm/multimodal/budget.py` 的 `MultiModalBudget` 实现迁移至 `vllm/multimodal/encoder_budget.py`，并统一在所有调用点的导入路径改为 `vllm.multimodal.encoder_budget`。代码逻辑本身未变化，仅是文件名与模块路径的调整，以提升语义清晰度，明确该预算专用于 encoder‑side 多模态资源管理。

**🎯 影响范围**  
- `vllm/lora/model_manager.py`  
- `vllm/v1/core/sched/scheduler.py`  
- `vllm/v1/engine/input_processor.py`  
- `vllm/v1/worker/gpu_model_runner.py`  
- 以及 `vllm/multimodal` 包下的 `__init__`（如果有统一导入）  

**💡 关注建议**  
1. **兼容性检查**：确认项目的 `setup.cfg`、`pyproject.toml`、Sphinx 文档等仍然能够找到 `MultiModalBudget`，必要时在 `vllm/multimodal/__init__.py` 中添加向后兼容的别名 `from .encoder_budget import MultiModalBudget`。  
2. **测试覆盖**：运行完整单元/集成测试，确保所有涉及多模态预算的路径（尤其是 LoRA、GPU‑model runner、调度器）在运行时能正确导入新模块。  
3. **文档更新**：搜索项目文档、README、示例代码中的 `vllm.multimodal.budget` 引用，全部改为 `encoder_budget`，防止用户在升级后遇到导入错误。  
4. **CI/CD 依赖**：如果 CI 配置中有显式的模块路径检查或 Lint 规则（如 `flake8`、`pylint`），同步更新相应的排除/导入白名单。  

总体而言，此次改动风险有限，核心功能未受影响。重点在于确保所有引用点已同步更新，避免因旧路径残留导致 ImportError。

---

### [Release 2.10] Update to Torch 2.10 - final release (#30525)
**SHA**: `f97ca67` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f97ca671766c5201404e9fc812e35bf2c4e95a01)

**变更类型**：功能增强 / 兼容性更新  
**重要程度**：🟡中  

**变更概览**  
1. 将项目整体依赖从 PyTorch 2.9.1 升级至 2.10.0（包括 torch、torchvision、torchaudio、triton 等），相应地更新 CMake、pyproject、requirements 等文件的版本声明。  
2. 为适配 PyTorch 2.10，所有 `is_torch_equal_or_newer("2.10.0.dev")` 判断统一改为 `is_torch_equal_or_newenew("2.10.0")`，并在部分位置提前使用 `2.11.0.dev`（AOT 编译默认开启）。  
3. 由于 `triton_kernels.routing` 与 `triton_kernels.routing_from_bitmatrix` 在新版本中被移除，新增 **legacy_routing / legacy_routing_from_bitmatrix** 实现，并在 `triton_kernel_moe_forward` 中改为调用该实现。相应的 import、数据结构与调用方式也作了适配。  
4. 小幅 CI 调整（增加 Build‑Image 超时、格式化改动、测试 skip 条件）以及对 batch‑invariant、编译器接口等代码的细节更新。  

**影响范围**  
- **构建系统**：CMake、docker 镜像、CI 配置都会使用新的 Torch/ROCM 版本。  
- **运行时**：所有使用 AOT 编译、动态 shape 标记、batch‑invariant 模式的路径现在依赖 PyTorch 2.10+，旧版环境将被自动跳过。  
- **MoE 相关实现**：`vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py` 与 triton 路由逻辑被重新实现，涉及核心推理路径。  
- **测试套件**：大量 `skipif` 条件已更新，部分测试在 PyTorch 2.10 下被显式排除（如 `test_routed_input_transform_inside_vs_outside`）。  

**关注建议**  
1. **兼容性验证**：在本地和 CI 中分别跑 CUDA 12.x、ROCM 7.0、CPU‑only 环境，确保新版依赖不会导致编译或运行时错误。  
2. **功能回归**：重点对 MoE、AOT 编译、动态 shape 场景做回归测试，尤其是 `legacy_routing*` 与原 `routing*` 的行为等价性（数值误差、性能）。  
3. **文档 & 环境说明**：更新 README、`requirements/*.txt`、Dockerfile 中的 Torch 版本说明，提醒用户在使用旧版 PyTorch 时需手动锁定旧分支。  
4. **性能监控**：由于 `legacy_routing` 采用纯 Python/torch‑topk 实现，可能在大模型上出现性能回退，建议对比基准并在必要时提供额外的优化路径或回退开关。  
5. **AOT 编译开关**：`vllm/envs.py` 中默认开启 AOT 编译已改为检查 `2.11.0.dev`，请确认该门槛符合预期，或在发布说明中说明何时会真正启用。  

总体而言，此次提交完成了对 PyTorch 2.10 的迁移，并对被废弃的 triton 路由 API 做了兼容实现。只要在多平台 CI 中充分验证功能与性能，即可安全发布。

---

### Add support for ModelOpt MXFP8 dense models (#33786)
**SHA**: `084aa19` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/084aa19f02b00198b36bd8d742d4169d6f5a32ce)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 vLLM 中新增对 ModelOpt MXFP8（E4M3）稠密模型的完整支持，包括文档、配置选项、量化检测、线性层实现以及对应的工具函数。  
**🎯 影响范围**：  
- `vllm/config/model.py`（新增 `modelopt_mxfp8` 验证）  
- 量化模块 `quantization/__init__.py`、`modelopt.py`（新增 Config、LinearMethod、KVCacheMethod）  
- `layers/fused_moe/config.py`（权重 dtype 扩展）  
- 新增 `utils/mxfp8_utils.py`（自定义 op、假实现、后端包装）  
- 文档 `docs/features/quantization/modelopt.md`  

**💡 关注建议**  

1. **序列化检查**  
   - `ModelOptMxFp8Config` 强制要求 checkpoint 已经以 MXFP8 格式序列化，动态量化直接报错。若用户误用该选项，提示信息需足够明确，防止因配置误差导致加载失败。  

2. **硬件兼容**  
   - `get_min_capability` 设为 SM100（Blackwell）以上。建议在运行时再次检查 GPU capability，提前给出 “不支持的 GPU” 警告，避免在后端抛出低层错误。  

3. **MoE 不支持**  
   - `get_quant_method` 中对 `FusedMoE` 抛出 `NotImplementedError`。如果项目后续计划扩展 MXFP8 到 MoE，建议在文档中明确说明当前限制，并提供回退方案（如使用 FP8/NVFP4）。  

4. **后端实现**  
   - 目前只实现 `EMULATION`（即先反量化为 BF16 再走 torch.nn.functional.linear）。代码已做好后端枚举，可在后续接入硬件加速实现。请确保 `Mxfp8LinearBackend` 的 `value` 与实际实现保持一致，以免日志误导。  

5. **权重/scale 维度校验**  
   - `process_weights_after_loading` 对 weight、scale 的形状做了严格检查，防止 padding 导致的维度不匹配。建议在单元测试中覆盖以下情形：  
   - 输入维度不是 32 的整数倍 → 报错；  
   - scale 张量维度不为 2 D → 报错。  

6. **文档同步**  
   - 文档已加入 `modelopt_mxfp8` 说明，确认示例（`quantization="modelopt_mxfp8"`、`kv_cache_quant_algo`）与代码中的字段匹配，避免用户在 `hf_quant_config.json` 中遗漏 `exclude_modules` 等必填项。  

7. **兼容性回退**  
   - 由于新增 quantization 选项会被 `override_quantization_method` 检测，建议在 `load_model` 流程中保留原有 `modelopt`（FP8）路径的优先级，以免意外把普通 FP8 checkpoint 当成 MXFP8。  

**总体结论**：本次 PR 为 vLLM 引入了 MXFP8 稠密模型的实验性支持，改动完整、接口清晰。重点关注 checkpoint 必须已量化、硬件能力检查以及对 MoE 的限制，完善测试和错误提示后即可安全合入。

---

### glm 4.6 fused tuned inference config for B200 (#32958)
**SHA**: `1ecfabe` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1ecfabe5254f7f2e8e90cda0c3e20cef684c2d6b)

**🎯 变更类型**：功能增强（新增针对 NVIDIA B200 的 fused‑MoE 推理配置）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/model_executor/layers/fused_moe/configs/` 目录下新增 `E=160,N=384,device_name=NVIDIA_B200,dtype=fp8_w8a8.json`，为 GLM‑4.6 fp8_w8a8 模型在 B200 GPU 上的 fused‑MoE 运算提供了专门的块尺寸、warp、stage 等调优参数。  

**🎯 影响范围**  
- **fused_moe 层**：加载该 JSON 时会影响 `FusedMoE` 的 kernel 配置。  
- **模型执行器**：`ModelExecutor` 在自动选择配置时会新增一个匹配条件（`device_name=NVIDIA_B200`、`dtype=fp8_w8a8`）。  
- **配置管理**：`vllm/model_executor/layers/fused_moe/configs/__init__.py`（若有）以及配置查找逻辑可能需要更新，以保证新文件能被发现。  

**💡 关注建议**  
1. **兼容性验证**：在非 B200 环境运行时确保不会误匹配该配置；可以在配置选择函数里加入显卡名称检查。  
2. **性能回归**：加入针对 B200 的基准测试（吞吐、延迟），与旧配置对比，确保新参数真的带来提升。  
3. **参数合理性**：文件中多数条目使用 `GROUP_SIZE_M=1`，但在 `3072`、`8192` 等大小使用 `GROUP_SIZE_M=16`，建议在文档中说明这些取值的调优依据，避免后续误改。  
4. **文档与示例**：在 README 或配置说明中添加 “B200 fp8_w8a8” 示例，提醒用户在 `--dtype fp8`、`--model glm-4.6` 时使用 `--moe-config` 指定该文件。  
5. **测试覆盖**：新增单元测试，验证 `get_fused_moe_config` 能正确返回该 JSON，且在不满足条件时回退到默认配置。  

总体而言，此次提交仅添加了一个配置文件，对代码逻辑无直接修改，但涉及到配置加载路径和条件匹配，需确保在多平台部署时不产生误匹配，并通过实际 B200 硬件进行性能验证。

---

### [torch.compile] Add an option to force-enable the MOE cold start optimization (#33735)
**SHA**: `4df841f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4df841fe7538cb8de281b9d78e37ba51ac35b5da)

**🎯 变更类型**：功能增强（为 MOE 冷启动优化新增可控开关）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：`CompilationConfig.fast_moe_cold_start` 由原始的固定 `True` 改为 `bool|None`，默认 `None`，在 `vllm.Config` 初始化时依据是否开启 **speculative decoding** 自动决定默认值。`forward_context` 中的判断也相应简化：只要 `fast_moe_cold_start` 为真，就直接使用 `static_all_moe_layers`，不再在推断阶段额外检查。  

**🎯 影响范围**：  
- `vllm/config/compilation.py` – 配置定义与文档。  
- `vllm/config/vllm.py` – 配置解析与默认值计算。  
- `vllm/forward_context.py` – 前向上下文创建逻辑。  

**💡 关注建议**  
1. **安全性**：当前实现对 `fast_moe_cold_start=True` 的用户在 **speculative decoding** 场景下不再给出 warning，可能导致“silent incorrectness”。建议保留显式校验或在 `forward_context` 中加入运行时提示。  
2. **文档 & 类型**：更新 README、配置说明以及 pydoc，明确三种取值的语义及在何种情况下会被自动降级。  
3. **测试**：补充单元测试，覆盖 (a) 默认值在有/无 speculative 时的行为；(b) 手动强制 `True`/`False` 在 spec 模式下的兼容性。  
4. **向后兼容**：现有代码默认仍保持原行为（若未使用 `speculative_config`），但若用户在旧版本中显式设 `fast_moe_cold_start=False`，行为会改变；可在发布说明中提示此潜在差异。  

整体来说，新增的可配置开关提升了灵活性，但需加强安全检查和文档，以防止误用导致错误推理。

---

#### 🟢 低重要度变更 (7)

### [CI][torch.compile] Fix incorrect filtering for E2E fusion tests on B200 (#34031)
**SHA**: `781ddf7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/781ddf786861f40de6d94d45d7b149d0f8d58c11)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 B200 E2E 融合测试的过滤条件，统一使用 FLASHINFER 与 Inductor 分区，仅保留必要的模型/量化组合。

---

### [Fix] [CPU Backend] : Prepack weights for w8a8 oneDNN matmul (#33901)
**SHA**: `caad9f1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/caad9f1e01ee04e4f5912d0287031ea3a850f6dc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ARM64 上为 w8a8 OneDNN 矩阵乘法加入 dummy M 大小，预打包权重以提升性能并避免运行时 reorder。

---

### [CI] Remove empty image_size_factors for fuyu, glm4_1v, glm_ocr (#34107)
**SHA**: `3025b3c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3025b3cebb1f019ccd6918cc54da1ca32f53a777)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/models/multimodal/generation/test_common.py` 中删除了空 `image_size_factors` 元组，仅保留非空的尺寸因子，避免对 `fuyu`、`glm4_1v`、`glm_ocr` 等模型的无效测试配置。

---

### [BugFix] Fix `fastsafetensors` TP all procs using all GPUs (#34070)
**SHA**: `d9bede0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d9bede0314ba19a3f8336dcaeeeaf9e2c5487053)

🎯 变更类型：代码重构  
⚡ 重要程度：🟢低  
📋 摘要：修复 fastsafetensors 在多 GPU TP 场景下加载错误，改用当前设备并在进程数>1时禁用 GDS，以防不必要的 CUDA 上下文创建。

---

### [bug-fix] supported_tasks is breaking backward compatibility at init_app_state (#34027)
**SHA**: `a75a5b5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a75a5b54c7f76bc2e15d3025d61e63cc91c7b0d7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `build_app` 与 `init_app_state` 添加可选 `supported_tasks` 参数，缺省时回退到 `("generate",)` 并发出 `DeprecationWarning`，以兼容旧调用并提示未来必需。

---

### [BugFix] Change support no act and mul for marlin (#34088)
**SHA**: `a263aa6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a263aa614060f8e6be52ed3de9995450d6c02892)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `FusedMarlinMoE._supports_no_act_and_mul` 返回值由 `False` 改为 `True`，修复了在当前设备上不支持 “no act & mul” 的问题。

---

### [Revert] Fix performance regression for GLM-4.7-GPTQ decode and MTP acceptance rate (#33771)
**SHA**: `179ae7d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/179ae7da8f48f76b0db4dc3e331153b24a36a96a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `common_attn_metadata.seq_lens.cpu()` 的调用改为直接使用已缓存的 `common_attn_metadata.seq_lens_cpu`，避免不必要的 CPU 迁移，提高异步模式下的性能并保持行为一致。

---

