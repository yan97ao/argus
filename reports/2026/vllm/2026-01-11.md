# 每日更新报告（2026-01-11）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-11 23:11:11 | Jiangyun Zhu | [CI] fix `test_concat_and_cache_mla_rope_fused` (#32117) |
| 2026-01-11 22:01:18 | Ning Xie | fix offline inference chat response prompt (#32088) |
| 2026-01-11 21:01:53 | maang | [FixBug] Improve exception string in `tensorizer.py` (#31680) |
| 2026-01-11 21:01:16 | rongfu.leng | [Misc] fix this log format not space (#32112) |
| 2026-01-11 21:01:12 | Cyrus Leung | [CI/Build] Separate out flaky responses API tests (#32110) |
| 2026-01-11 16:18:57 | Isotr0py | [Misc] Make `scipy` as optional audio/benchmark dependency  (#32096) |
| 2026-01-11 16:05:36 | Or Ozeri | [KVConnector] OffloadingConnector: Fix bug in handling of preemptions (#29870) |
| 2026-01-11 15:19:46 | Matt | [Hardware][AMD][CI][Bugfix] Fix AMD Quantization test group (#31713) |
| 2026-01-11 15:15:49 | Fadi Arafeh | [CPU][BugFix] Disable AOT Compile for CPU (#32037) |
| 2026-01-11 15:15:46 | Laith Sakka | make assume_32_bit_indexing configurable (#32044) |
| 2026-01-11 15:14:54 | Andy Liu | [MTP][GLM][Bugfix] Fixed .weight_scale loading logic that dropped MTP prediction accuracy with fp8+mtp (#32101) |
| 2026-01-11 12:27:27 | Cyrus Leung | [Benchmark][2/2] Use spline interpolation to tune SLA variables (#32095) |
| 2026-01-11 06:25:08 | Or Ozeri | [BugFix] Wait for compute before offloading KV to CPU (#31341) |
| 2026-01-11 04:40:09 | RickyChen / 陳昭儒 | [Bugfix] Fix Qwen3-VL-Reranker model loading for sequence classification (#32089) |
| 2026-01-11 04:40:05 | Vadim Gimpelson | [MISC] Add strict contiguity check for FlashInfer attention tensors (#32008) |
| 2026-01-11 04:40:02 | Vensen | [Bugfix][Quantization] Ensure input contiguity in per_token_quant_int8 (#31637) |
| 2026-01-11 04:39:59 | shyeh25 | Revert "[Kernels][FI] Skip trtllm attention when num_kv_heads=1 (#308… (#31617) |
| 2026-01-11 04:39:25 | Or Ozeri | [BugFix] scheduler: Fix resuming of preempted requests after async load (#31583) |
| 2026-01-11 04:36:45 | gnovack | fused_moe_kernel - cast accumulator after applying router weights (#32002) |
| 2026-01-11 03:04:18 | Xin Yang | [LoRA][Perf] Improve FusedMoE LoRA performance for small rank (#32019) |
| 2026-01-11 02:13:44 | jvlunteren | [Kernel] Optimize Sliding Window Attention in 3D Triton Kernel (#31984) |

### 📊 统计摘要
> 本日共 21 个提交 | 🔴高 0 | 🟡中 10 | 🟢低 11
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🟡 中重要度变更 (10)](#-🟡-中重要度变更-10)
    - [[CI/Build] Separate out flaky responses API tests (#32110)](#a374532)
    - [[Misc] Make `scipy` as optional audio/benchmark dependenc...](#cee7436)
    - [[KVConnector] OffloadingConnector: Fix bug in handling of...](#4c16ba6)
    - [[Hardware][AMD][CI][Bugfix] Fix AMD Quantization test gro...](#bde57ab)
    - [[Benchmark][2/2] Use spline interpolation to tune SLA var...](#ef96fa3)
    - [[BugFix] Wait for compute before offloading KV to CPU (#3...](#2a4dbe2)
    - [[MISC] Add strict contiguity check for FlashInfer attenti...](#e15a5ff)
    - [Revert "[Kernels][FI] Skip trtllm attention when num_kv_h...](#1c46dea)
    - [[BugFix] scheduler: Fix resuming of preempted requests af...](#0285997)
    - [[LoRA][Perf] Improve FusedMoE LoRA performance for small ...](#543c23b)
  - [🟢 低重要度变更 (11)](#-🟢-低重要度变更-11)
    - [[CI] fix `test_concat_and_cache_mla_rope_fused` (#32117)](#3df619a)
    - [fix offline inference chat response prompt (#32088)](#d74132c)
    - [[FixBug] Improve exception string in `tensorizer.py` (#31...](#a34abc4)
    - [[Misc] fix this log format not space (#32112)](#d70249e)
    - [[CPU][BugFix] Disable AOT Compile for CPU (#32037)](#9103ed1)
    - [make assume_32_bit_indexing configurable (#32044)](#46eb30f)
    - [[MTP][GLM][Bugfix] Fixed .weight_scale loading logic that...](#0dd6363)
    - [[Bugfix] Fix Qwen3-VL-Reranker model loading for sequence...](#8020a60)
    - [[Bugfix][Quantization] Ensure input contiguity in per_tok...](#6ea001c)
    - [fused_moe_kernel - cast accumulator after applying router...](#d1fd802)
    - [[Kernel] Optimize Sliding Window Attention in 3D Triton K...](#b8bf5c4)
#### 🟡 中重要度变更 (10)

### [CI/Build] Separate out flaky responses API tests (#32110)
**SHA**: `a374532` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a37453211120bfd7b94042d84f519918d0a3698f)

**🎯 变更类型**：其他（CI/测试结构优化）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原本与其它 OpenAI API 测试混在一起的 “responses” 测试单独抽离为独立的 CI 步骤；相应更新了 `.buildkite/*.yaml`，新增 `Entrypoints Integration Test (Responses API)` 阶段；对该目录下的测试文件做了路径重命名并统一使用 `from ....utils import RemoteOpenAIServer` 的相对导入。  

**🎯 影响范围**  
- CI 配置：`.buildkite/test‑amd.yaml、.buildkite/test‑pipeline.yaml、.buildkite/test_areas/entrypoints.yaml`  
- 测试套件：`tests/entrypoints/openai/responses/*`（新增 `__init__.py` 并重命名若干文件）  
- 入口点代码：无直接功能改动，仅修改了测试的 import 路径。  

**💡 关注建议**  
1. **本地运行验证**：开发者在本地执行 `pytest entrypoints/openai/responses`，确认相对导入 `from ....utils` 能在项目根目录下成功。若使用 `pip install -e .`，确保 `tests` 包被包含在 `PYTHONPATH`。  
2. **CI 稳定性**：新增的独立步骤可能首次出现资源竞争或超时（50 min 限制），建议监控该步骤的执行时间，必要时调优 `timeout_in_minutes` 或 `fast_check` 参数。  
3. **文档与覆盖**：在项目的 CI/测试文档中加入 “Responses API” 测试的说明；若使用覆盖率报告，记得在相应的 CI 配置中添加该目录的收集，以免导致覆盖率下降。  
4. **回滚风险**：若新步骤频繁失败（尤其是 flaky），可以临时在 CI 中加入 `--ignore=entrypoints/openai/responses` 回退到原来的统一跑法，待修复后再恢复。  

总体来看，此次改动提升了测试的可维护性和定位 flaky 的能力，对业务功能没有直接影响，但需要确认本地与 CI 环境的 import 路径以及执行时长，避免因新步骤导致整体 CI 失效。

---

### [Misc] Make `scipy` as optional audio/benchmark dependency  (#32096)
**SHA**: `cee7436` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cee7436a26fef900402213f1e0190ea3331048bf)

**🎯 变更类型**：功能增强（将 *scipy* 设为可选依赖）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 从 `requirements/common.txt` 中去掉了 `scipy`，改为在 `setup.py` 的 `bench` 与 `audio` extras 中声明。  
- `vllm/multimodal/audio.py` 中对 `scipy.signal` 的导入改为懒加载，并在缺失时使用 `PlaceholderModule`，防止导入错误导致模块加载失败。  

**🎯 影响范围**  
- **audio**：`resample_audio_scipy` 现在在没有安装 `scipy` 时仍能导入模块，只是相关功能不可用。  
- **bench**：基准测试脚本若使用 `scipy`（如信号处理）需要显式安装 `vllm[bench]`。  
- **依赖管理**：用户默认 `pip install vllm` 不再自动拉取 `scipy`，减小安装体积。  

**💡 关注建议**  
- 检查 `PlaceholderModule` 的实现，确保在缺少 `scipy` 时抛出清晰错误或提供友好提示。  
- 在文档/README 中标明 `audio` 与 `bench` 功能需要额外安装 `vllm[audio]`、`vllm[bench]`。  
- 运行 CI/单元测试时覆盖 `scipy` 缺失的场景，防止运行时 `AttributeError`。  
- 若未来在其它模块使用 `scipy`，请同样采用懒加载或放入相应 extras。  

---

### [KVConnector] OffloadingConnector: Fix bug in handling of preemptions (#29870)
**SHA**: `4c16ba6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4c16ba617f76b342dd0e62deba1f96ed6cee74fa)

**变更类型**：Bug 修复（在 KV Offloading 场景下正确处理请求被抢占的情况）  
**重要程度**：🟡 中  

**核心改动**  
1. **Connector 接口**  
   - 在 `KVConnectorBase` 与 `OffloadingConnector` 中新增 `handle_preemptions`，在调度产生 `preempted_req_ids` 时被调用。  
2. **Worker 层**  
   - 为所有 `OffloadingHandler` 实现 `wait(job_ids)`，利用 `torch.Event` 在 `CpuGpuOffloadingHandler` 中实现阻塞等待。  
   - `OffloadingWorker` 统一转发 `wait` 调用。  
3. **调度/执行路径**  
   - `gpu_model_runner.execute_model` 在检测到抢占并且 KV Transfer 组存在时，触发 `handle_preemptions`。  
   - `OffloadingConnector.build_connector_meta` 现在在构造元数据时完成被抢占请求的未完成 store，并在 `handle_preemptions` 中提交/等待相应的异步存储任务。  
4. **测试**  
   - 扩展 `test_offloading_connector` 与 `test_offloading_worker`，验证抢占后 **flush**、**store**、**load** 的完整性。  
   - 新增 `test_request_preemption` 场景，模拟 KV 缓存耗尽、抢占、恢复等全过程。  

**影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/*`（基类、offloading_connector）  
- `vllm/v1/kv_offload/worker/*`（CPU‑GPU 处理器、抽象 worker）  
- `vllm/v1/worker/gpu_model_runner.py`（调度入口）  
- 相关单元测试文件。  

**建议**  
- **文档**：在 KV Transfer 章节补充抢占流程及 `handle_preemptions` 的使用说明。  
- **并发安全**：`_transfer_events` 目前在 `wait` 与 `get_finished` 中并发访问，建议加锁或使用线程安全容器防止潜在 race。  
- **兼容性**：老的自定义 `OffloadingHandler` 仍需实现 `wait`；可在基类提供默认实现（遍历 `get_finished`）以降低实现门槛。  
- **性能**：抢占时一次性提交所有 pending store，避免频繁小批量提交；如果业务场景中抢占频繁，可考虑批量 `wait` 的聚合策略。  

总体而言，此次修改修复了在 KV 缓存空间不足导致的抢占后数据不一致问题，并为后续异步 KV Transfer 扩展提供了统一的抢占处理入口。代码结构保持清晰，测试覆盖面已提升。建议在正式发布前完成并发安全校验并更新使用手册。

---

### [Hardware][AMD][CI][Bugfix] Fix AMD Quantization test group (#31713)
**SHA**: `bde57ab` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bde57ab2edb67533158a762823fba31e32f71a85)

**🎯 变更类型**：功能增强 / Bugfix（针对 AMD/ROCM 环境的量化测试及平台适配）  
**⚡ 重要程度**：🟡 中（影响 CI 稳定性及多平台量化功能）  

**📋 变更摘要**  
1. CI 配置新增 `amdproduction` 镜像，使 AMD 生产硬件参与量化测试。  
2. 大量测试用例加入 `current_platform` 的条件判断（CUDA / ROCM），包括跳过非 CUDA 平台、在 ROCm 上限制 `marlin`、`fp8` 等特性。  
3. `tests/quantization/utils.py` 与 `quantization/__init__.py` 增加平台对自定义量化的自动支持检测。  
4. `scaled_mm/triton.py`、`ptpc_fp8.py` 等实现中加入对权重尺度的通道化转换及对 FP8 dtype 的统一包装。  
5. `rocm.py` 扩展 `supported_quantization` 列表，预留对 `*_marlin` 的兼容。  

**🎯 影响范围**  
- CI 运行脚本 `.buildkite/test-amd.yaml`  
- 量化测试套件：`test_compressed_tensors.py、test_configs.py、test_fp8.py、test_gptq_dynamic.py、test_ptpc_fp8.py` 等  
- 平台抽象 `vllm/platforms/*`（尤其 `rocm.py`）  
- 量化层实现 `vllm/model_executor/layers/quantization/*`（ScaledMM、PTPC‑FP8）  

**💡 关注建议**  
1. **平台兼容性**：新增的条件跳过可能导致在非 CUDA 环境下的回归测试缺失，建议在 AMD‑ROCm CI 中补全相应的正向测试，确保 `marlin`、`fp8` 等路径仍被覆盖。  
2. **`supported_quantization` 处理**：在 `__init__.py` 中直接把自定义配置写入 `current_platform.supported_quantization`，要防止在多平台并行运行时出现列表共享导致误判，建议复制后再追加（避免全局副作用）。  
3. **权重尺度转换**：`scaled_mm/triton.py` 引入 `convert_to_channelwise`，务必检查 `layer.logical_widths` 在所有 fused module 上的正确性，防止出现维度不匹配的 RuntimeError。  
4. **FP8 dtype 统一**：多处改为 `current_platform.fp8_dtype()`，确保 `rocm.py` 正确实现 `fp8_dtype()`（包括 e4m3fn/e4m3fnuz 区分），否则会在非 CUDA 环境下触发 `AttributeError`。  
5. **文档与示例**：更新 README/QUANTIZATION 部分，说明在 AMD/ROCm 上哪些量化方案被支持、哪些会被自动降级为 `ERROR`，避免用户在生产环境中误用不兼容的配置。  

总体来看，变更提升了 AMD/ROCm 的 CI 覆盖与平台感知，但需要在多硬件 CI 中保持对所有关键路径的测试，防止因条件跳过而引入隐藏回归。适当增加平台‑specific 单元测试并审慎管理全局状态，可进一步稳固此次改动。

---

### [Benchmark][2/2] Use spline interpolation to tune SLA variables (#32095)
**SHA**: `ef96fa3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ef96fa3f1f15e08769e70ee3e335b5b4d7e6a6ee)

**变更概览**  
本次提交用 **样条插值（PCHIP）** 替代原来的二分搜索，实现了 `solve_sla`（原 `_find_sla_value`）以及 `SLAHistory` 辅助类。文档、测试均同步改写，去除了 `_estimate_sla_bounds`、`_estimate_sla_value` 等老旧实现，并把 `scipy.interpolate.PchipInterpolator` 设为可选依赖。

**核心影响**  
- **benchmark/sweep**：SLA 调参流程从“先二分再细化”改为“先在两端取点、用样条求根、不断收敛”。  
- **依赖**：新增对 `scipy` 的运行时依赖；在缺失时会以占位模块报错，需要在 CI/用户环境中确保 `scipy` 可用。  
- **返回值**：`solve_sla` 现在返回 `(sla_data, SLAHistory)`，而调用方 `search_sla` 已相应改为取 `history.get_max_passing()`；若 `run_sla` 为 `None`（dry‑run）直接返回 `None`，保持原行为。  
- **测试**：原 `_estimate_sla_bounds/_find_sla_value` 测例被全部替换为围绕 `solve_sla` 的调用，覆盖线性、凹/凸、二次、平方根等多种曲线形状，验证了根的求取与历史记录的正确性。

**潜在风险 / 注意事项**  
1. **插值精度**：PCHIP 在数据点稀疏或噪声较大时可能出现多根或根不存在的情况，代码已在根不存在时回退二分，但仍需要监控对极端曲线的收敛速度。  
2. **历史记录边界**：`SLAHistory` 的 `get_max_passing` / `get_min_failing` 默认返回 `min_value`、`max_value`，若首次测得的点全部不满足/全部满足，返回值可能与预期不同，请在上层做好异常处理。  
3. **性能**：每轮均调用 `run_sla`，插值本身开销极低，整体性能提升主要来自减少二分迭代次数。但在极端 `max_value=8192` 时仍会进行多次测跑，建议在 CI 中加入超时检测。  
4. **兼容性**：原有 `search_sla` 接口未暴露 `sla_inf_value` 参数，改动后默认使用 `8192`（近似无限 QPS）。若外部代码依赖旧的 `sla_inf_value` 参数，需要同步修改或提供包装层。  

**建议**  
- 在 `setup.cfg` / `pyproject.toml` 中显式声明 `scipy>=1.8` 为运行时依赖，避免占位模块在生产环境误报。  
- 为 `solve_sla` 添加完整的 docstring，说明插值失败时的二分回退策略以及返回值结构。  
- 在 `search_sla` 中加入对 `history.get_max_passing() == history.get_min_failing()`（收敛失败）的日志或异常，防止死循环。  
- 考虑在 `tests/benchmarks/sweep` 增加噪声模拟（随机波动）场景，验证插值在不平滑数据上的鲁棒性。  

总体来看，改动提升了 SLA 调参的收敛效率并让代码结构更清晰，唯一需要关注的是对 `scipy` 的依赖和插值边界的稳健处理。

---

### [BugFix] Wait for compute before offloading KV to CPU (#31341)
**SHA**: `2a4dbe2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2a4dbe24eadcb8e0354e47f608b53399aec52c43)

**🎯 变更类型**：BugFix（等待计算完成后再将 KV Off‑load 到 CPU）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `OffloadingConnector` 中新增 `unsubmitted_stores_count`，并将待写回的 store 任务延迟到下一个 engine 步才提交，避免在 token 采样期间阻塞生成。  
2. `OffloadingWorker` 取消了原先的 `priority` 参数，改为在 `transfer_async` 中使用 `stream.wait_stream(torch.cuda.current_stream())`，实现 “compute‑then‑offload” 的同步。  
3. 单元测试相应更新，验证延迟提交的 store 在计算完成后才触发，并保持原有的 off‑load 行为。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`（任务调度逻辑）  
- `vllm/v1/kv_offload/worker/cpu_gpu.py`（单向转移实现）  
- 相关单元测试 `tests/v1/kv_connector/unit/test_offloading_connector.py`  

**💡 关注建议**  
1. **同步正确性**：`stream.wait_stream(torch.cuda.current_stream())` 依赖当前 CUDA 主流是否始终为计算流，建议在文档或注释中说明前置条件，防止在自定义流场景下出现隐藏的 race。  
2. **未提交 Store 的生命周期**：`_unsubmitted_store_jobs` 在 `start_kv_transfers` 前清空，确保无残留任务导致泄漏或重复提交；可在 `get_finished` 中加入断言检测未意外丢失的 job。  
3. **向后兼容**：删除 `priority` 参数后，若外部代码仍传递该参数会触发 TypeError，建议保持向后兼容的包装或在 `__init__` 中捕获多余参数。  
4. **测试覆盖**：现有测试已覆盖基本路径，建议再增加并发请求、跨层 KV 以及异常（如 `transfer_async` 失败）情形，确保延迟提交不会影响错误传播。  
5. **性能监控**：延迟 offload 可能增加一次 engine 步的内存占用，建议在部署时监控 GPU‑CPU KV 队列长度，以防出现突发的内存峰值。  

总体而言，此次修改通过在计算完成后再触发 KV off‑load，解决了 token 生成延迟的回归问题，代码结构清晰，但需注意同步与兼容性细节。

---

### [MISC] Add strict contiguity check for FlashInfer attention tensors (#32008)
**SHA**: `e15a5ff` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e15a5ff07b89646090458ce6ae7908bcbae82b90)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/utils/torch_utils.py` 中新增 `is_strictly_contiguous`，用于同时检查 Tensor 是否连续且其 stride 完全符合 C‑contiguous 布局（没有退化 stride）。在 FlashInfer 后端的前置/解码路径里，将原来的 `tensor.is_contiguous()` 断言全部替换为该函数，以防止某些 CUDA kernel 在出现退化 stride 时出现错误。  

**🎯 影响范围**：  
- `vllm/utils/torch_utils.py`（新增工具函数）  
- `vllm/v1/attention/backends/flashinfer.py`（前置与解码两段代码的断言）  

**💡 关注建议**：  
1. **兼容性**：`is_strictly_contiguous` 在连续但存在 size‑1 维度的非标准 stride 时会报错，可能会导致已有调用者在某些 reshape / squeeze 场景下触发 AssertionError。建议在文档或 README 中说明该约束，或为用户提供 `torch.contiguous()` 的显式调用示例。  
2. **错误信息**：当前使用 `assert`，触发时仅能得到 “AssertionError”。建议改为抛出自定义异常并附带期望 stride 与实际 stride 对比信息，便于定位问题。  
3. **测试覆盖**：新增单元测试，覆盖：① 完全连续的 Tensor；② size‑1 维度且 stride 正常；③ size‑1 维度且 stride 退化的情况，确保异常被捕获。  
4. **性能影响**：函数遍历全部维度并做乘法，对大 Tensor 开销可忽略不计，但在高频调用路径上仍建议在 Debug 模式下才做检查，或者通过环境变量 `VLLM_STRICT_CONTIGUOUS=0` 关闭。  
5. **文档同步**：在 `utils/torch_utils.md`（若有）或相应的 API 文档中添加该函数的使用说明和实现细节。  

总体而言，此改动提升了 FlashInfer 对输入张量布局的鲁棒性，避免了潜在的 CUDA kernel 崩溃风险；但需注意向后兼容性和错误可读性，适当补充测试与文档。

---

### Revert "[Kernels][FI] Skip trtllm attention when num_kv_heads=1 (#308… (#31617)
**SHA**: `1c46dea` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1c46dea0017a7689c2d9fbc0f7d1f1357ca6c7de)

**🛠️ 变更类型**：重构 / 代码回退  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交撤销了在 *FlashInfer* 与 *TRT‑LLM* Attention 交叉路径中对 `num_kv_heads=1`（即 MQA）进行的显式排除。`can_use_trtllm_attention` 与 `use_trtllm_attention` 的实现再次接受 `num_kv_heads=1`，对应的单元测试也被删除。  

**🎯 影响范围**  

| 模块 | 影响点 |
|------|--------|
| `vllm/utils/flashinfer.py` | 1️⃣ `can_use_trtllm_attention` 的返回条件从 `has_trtllm and (num_qo_heads % num_kv_heads == 0) and (num_kv_heads != 1)` 改为 `has_trtllm and (num_qo_heads % num_kv_heads == 0)`；<br>2️⃣ `use_trtllm_attention` 删除了对 `num_kv_heads==1` 的提前返回与警告。 |
| `tests/kernels/attention/test_flashinfer_trtllm_attention.py` | 删除了专门验证 “`num_kv_heads=1` 被拒绝” 的测试用例。 |

**💡 关注建议**  

1. **潜在回归**：`num_kv_heads=1` 时会产生 **degenerate KV‑cache stride**（`stride_heads == stride_batch`），导致 CUDA `cuTensorMapEncodeTiled` 在构建 TMA 描述符时失败。撤销排除逻辑后，用户在 Blackwell 及以上 GPU 上开启 TRT‑LLM Attention 可能会遭遇 **RuntimeError / CUDA illegal memory access**。  
2. **恢复检测**：建议在 `can_use_trtllm_attention` 中保留对该情况的检测，即使不直接返回 `False`，也可以在 `use_trtllm_attention` 中加入 **运行时检测 + 报错**（或 fallback 到 FlashInfer），防止硬件层面的崩溃。  
3. **日志与文档**：若保持接受 `num_kv_heads=1`，请在 `vllm` 文档中明确说明此配置在 **Blackwell 之外** 或 **特定 CUDA 版本** 可能不被支持，并在代码中保留 `logger.warning_once`，提醒用户。  
4. **单元测试**：重新加入或新建对 `num_kv_heads=1` 的行为验证（例如：在不支持的平台返回 `False`，在支持的平台捕获异常并回退），确保后续改动不再意外破坏此约束。  
5. **兼容性检查**：如果项目对外提供 `--attention-config.use_trtllm_attention=1` 的强制开关，仍需在 `use_trtllm_attention` 中对 `num_kv_heads=1` 给出明确的 **warning + fallback**，防止用户因强制开启而遇到不可预料的崩溃。  

总体而言，回退删除了对已知硬件限制的防护，可能导致在特定配置上出现运行时错误。建议在恢复或新增检测逻辑的同时，补全对应的测试用例，以保持库的稳健性。

---

### [BugFix] scheduler: Fix resuming of preempted requests after async load (#31583)
**SHA**: `0285997` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/028599739d617b76c55179a9dabddb458e1fe035)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 修复在 KVConnector 为异步模式时，被抢占（preempt）的请求在恢复后仍保持 `WAITING` 状态的问题，改为在有抢占记录时转回 `PREEMPTED` 状态，以便后续重新调度。  
2. 为 `Request` 新增 `num_preemptions` 的注释，澄清其含义为“被抢占的次数”。  
3. 扩展单元测试 `test_kv_connector_handles_preemption`，加入 `is_async` 参数，覆盖同步/异步两种路径，并在异步场景下检查 `waiting`、`running`、`scheduled_cached_reqs` 等状态的正确性。

**🎯 影响范围**  
- `vllm/v1/core/sched/scheduler.py`：调度器状态迁移逻辑。  
- `vllm/v1/request.py`：`Request` 对象的字段注释（不影响运行时行为）。  
- `tests/v1/core/test_scheduler.py`：新增对异步 KVConnector 的测试覆盖。  

**💡 关注建议**  
- 开发者在实现自定义 KVConnector 时，确认其 `is_async` 标记与调度器的抢占恢复逻辑保持一致，避免出现请求卡在 `WAITING`。  
- 关注 `num_preemptions` 的累加时机，确保只有真正的抢占才会增加，否则可能导致不必要的 `PREEMPTED` 状态切换。  
- 现有用户的 API 未变，升级后可直接受益于抢占恢复的正确行为；若在自定义调度器或监控中依赖 `Request.status`，请验证在异步加载完成后状态是否按预期从 `PREEMPTED` 转回 `WAITING`。  
- 继续运行完整的单元测试，特别是新增的参数化测试，以确保在不同 KVConnector 配置下调度器行为一致。

---

### [LoRA][Perf] Improve FusedMoE LoRA performance for small rank (#32019)
**SHA**: `543c23b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/543c23be789cef54bcb8facacaf7417f42fb95cf)

**🎯 变更类型**：功能增强 / 性能优化  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交为 LoRA‑enabled FusedMoE 添加了专门的配置推导器 `try_get_optimal_moe_lora_config`，在 `rank` 较小（如 4、8、16）时自动调低 `BLOCK_SIZE_N`、`BLOCK_SIZE_K`，并在 `utils.get_lora_op_configs` 中使用对应的默认块大小。这样能够避免在小秩情况下因块太大导致的算子空闲或显存浪费，从而提升收敛速度和吞吐量。

**🎯 影响范围**  
- `vllm/lora/layers/fused_moe.py` – 通过新函数获取 shrink/expand 配置。  
- `vllm/lora/layers/utils.py` – 新增 `try_get_optimal_moe_lora_config` 实现。  
- `vllm/lora/ops/triton_ops/utils.py` – 为 LoRA shrink/expand ops 添加专属默认块大小。  
- 依赖 `vllm.model_executor.layers.fused_moe.fused_moe` 的所有 LoRA‑MoE 路径。

**💡 关注建议**  

1. **兼容性检查**：新配置函数在 `op_type` 不匹配时会回退到原 `try_get_optimal_moe_config`，理论上不影响已有模型，但请在 CI 中跑一次全量回归，确认没有意外的块尺寸导致的 shape 错误。  
2. **性能验证**：建议在不同 `rank`（4~64）以及不同 `top_k`、`batch_size` 下跑基准，记录显存占用和吞吐率，确保改动在小秩场景下带来预期提升且在大秩场景下不产生退化。  
3. **配置可见性**：`_get_lora_moe_configs` 会返回 `shrink_config`、`expand_config`，若用户自定义 `op_type` 或手动覆盖块大小，需注意新默认值可能被覆盖。文档中应说明 “LoRA‑MoE 自动调节 BLOCK_SIZE_N/K”。  
4. **代码维护**：`try_get_optimal_moe_lora_config` 直接复制并修改原配置，若后续 `try_get_optimal_moe_config` 逻辑变化，需要同步更新此函数的实现，避免两者行为不一致。可以考虑将公共块调节逻辑抽取为内部工具函数。  

总体来看，此改动在 LoRA 低秩场景下提供了细粒度的块大小调优，预计可显著降低算子启动开销，对整体推理性能有正向影响。开发者在合并后应重点关注配置一致性与回归测试，用户则可直接升级以获得更快的 LoRA‑MoE 推理。

---

#### 🟢 低重要度变更 (11)

### [CI] fix `test_concat_and_cache_mla_rope_fused` (#32117)
**SHA**: `3df619a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3df619ac94bc60d14625cbc3405b051bc92c1c6e)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_rotary_embedding_mla_cache_fused.py` 中改用 `set_random_seed` 替代 `current_platform.seed_everything`，并新增 `default_vllm_config` 参数。

---

### fix offline inference chat response prompt (#32088)
**SHA**: `d74132c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d74132ca3b8aea4840478bfbca56fd59273b325e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在离线推理示例中，引入 `RequestOutput` 类型并返回对话上下文；`print_outputs` 兼容多轮对话，修正打印提示的逻辑，使示例更易阅读与调试。

---

### [FixBug] Improve exception string in `tensorizer.py` (#31680)
**SHA**: `a34abc4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a34abc49b7553d4af8d60fdeb78b5b5312e4f350)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：优化 `tensorizer.py` 中对不支持的 LoRA 适配器文件的异常信息，改为更明确的错误提示并限定文件类型为 `.safetensors` 或 `.bin`。

---

### [Misc] fix this log format not space (#32112)
**SHA**: `d70249e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d70249e2e9b15655020efe08f3ef46fd908758aa)

**🎯 变更类型**：代码重构（微调日志字符串）  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `core_client.py` 中为超时异常信息的字符串末尾添加了一个空格，使日志内容更易阅读，未改动功能逻辑。

---

### [CPU][BugFix] Disable AOT Compile for CPU (#32037)
**SHA**: `9103ed1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9103ed16967390f3bbd6df104dcd162db43d3148)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/envs.py` 中新增平台检测，CPU 上强制关闭 AOT 编译，以修复相关 Bug。

---

### make assume_32_bit_indexing configurable (#32044)
**SHA**: `46eb30f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/46eb30f5196edf5691376febb7923bfa620649da)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `assume_32_bit_indexing` 配置项，默认 true，并在 `patched_inline_call` 中使用该配置来控制是否启用 32 位索引。

---

### [MTP][GLM][Bugfix] Fixed .weight_scale loading logic that dropped MTP prediction accuracy with fp8+mtp (#32101)
**SHA**: `0dd6363` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0dd63639be24c67a0bccc9e3b0f0fd6ab039455d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 `.weight_scale` 加载逻辑，跳过模型未暴露的权重量化尺度，防止 KeyError，恢复 fp8+MTP 预测精度。

---

### [Bugfix] Fix Qwen3-VL-Reranker model loading for sequence classification (#32089)
**SHA**: `8020a60` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8020a60402597740c7fbd0beba1d5f253983f82f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修复 Qwen3‑VL‑Reranker 在序列分类时的加载错误，改用语言模型子模块访问 `lm_head`，并在权重映射后正确清除已加载的 `lm_head.weight`。

---

### [Bugfix][Quantization] Ensure input contiguity in per_token_quant_int8 (#31637)
**SHA**: `6ea001c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6ea001cfb7549265e9053569b4b17126ebcfc2d0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `per_token_quant_int8` 中确保输入为连续张量，并在二维以上时先展平，改用显式的 (M, N) shape 创建输出和尺度张量，最后恢复原始维度，防止因非连续内存导致的错误。

---

### fused_moe_kernel - cast accumulator after applying router weights (#32002)
**SHA**: `d1fd802` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d1fd802fa3df9641e9b6cc838f055f81c7619dc3)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：调整 `fused_moe_kernel` 中的累计值转换逻辑，统一在最后统一 cast 到计算类型，简化量化分支并消除不必要的类型转换。

---

### [Kernel] Optimize Sliding Window Attention in 3D Triton Kernel (#31984)
**SHA**: `b8bf5c4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b8bf5c45bbbb1249bd1656557603da71e9d4c3b9)

**🎯 变更类型**：代码重构/性能优化  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 3D Triton 注意力内核中加入滑动窗口裁剪逻辑，限制遍历的 tile 范围以提升大序列场景下的计算效率。

---

