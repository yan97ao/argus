# 每日更新报告（2026-01-30）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-30 23:50:05 | Kyle Sayers | [QeRL] Layerwise Reloading (#32133) |
| 2026-01-30 23:27:42 | Danielle Robinson | [BugFix][LoRA] TritonExperts is ModularMoEPath for FP8 models (#33393) |
| 2026-01-30 23:00:46 | Frank Wang | Disable Cascade Attention for Batch Invariance (#32561) |
| 2026-01-30 22:23:33 | Julien Denize | Improve Mistral format checks. (#33253) |
| 2026-01-30 22:03:25 | Harry Mellor | Fix `test_moe.py` for Transformers v5 (#33413) |
| 2026-01-30 21:36:20 | Nathan Weinberg | [Doc] Enhance documentation around CPU container images (#32286) |
| 2026-01-30 21:31:17 | 杨朱 · Kiki | [Misc] Clean up HIDDEN_DEPRECATED_METRICS after metric removal (#33323) |
| 2026-01-30 19:48:15 | Harry Mellor | Remove deprecated `reasoning_content` message field (#33402) |
| 2026-01-30 19:06:08 | vllmellm | [Doc] [ROCm] Update Documentation to reflect v0.15.0 release (#33388) |
| 2026-01-30 18:52:02 | Julien Denize | [BUGFIX] Pixtral cannot be loaded with --limit-mm-per-prompt 0 (#33406) |
| 2026-01-30 18:41:29 | Patrick von Platen | [Realtime API] Adds minimal realtime API based on websockets (#33187) |
| 2026-01-30 17:56:59 | 杨朱 · Kiki | [Misc] Replace Optional[X] with X \| None syntax (#33332) |
| 2026-01-30 17:27:31 | Cyrus Leung | [Refactor] Move MM item count validation outside of processor (#33396) |
| 2026-01-30 16:23:14 | tianshu-Michael-yu | fix: allow LFM2 MoE prefix caching (align) (#33376) |
| 2026-01-30 15:54:27 | hujiaxin0 | [model] Add support for openPangu7B-VL (#32449) |
| 2026-01-30 15:27:04 | Harry Mellor | Explicitly set `return_dict` for `apply_chat_template` (#33372) |
| 2026-01-30 14:43:32 | Lucas Kabela | [CI] Enable mypy import following for `vllm/spec_decode` (#33282) |
| 2026-01-30 14:18:41 | Harry Mellor | Move decode context parallel validationn to `ParallelConfig` (#33239) |
| 2026-01-30 13:39:53 | Ryan Rock | [CI][AMD] Skip 4 GPUs testgroup ray tests (#33305) |
| 2026-01-30 13:31:20 | Isotr0py | [Models] Refactor Kimi-K2.5 weight loading (#33346) |
| 2026-01-30 12:40:19 | Harry Huang | [BugFix] Disable async scheduling for Mamba prefix caching (#33352) |
| 2026-01-30 11:37:39 | Harry Mellor | Fix `tie_word_embeddings` for multimodal models in Transformers v5 (#33359) |
| 2026-01-30 11:01:29 | Wang Haoyu | [Model][Multimodal] Add explicit MusicFlamingo adapter (#32696) |
| 2026-01-30 06:12:35 | Aidan Reilly | [Docs] Adding links and intro to Speculators and LLM Compressor (#32849) |
| 2026-01-30 04:15:17 | Michael Goin | [Bugfix] Enable Triton MoE for FP8 per-tensor dynamic (#33300) |
| 2026-01-30 04:09:35 | Kevin H. Luu | [release] Minor fixes to release annotation and wheel upload (#33129) |
| 2026-01-30 03:21:33 | danisereb | Add Triton fused MoE config for B200 (Nemotron Nano) (#32804) |
| 2026-01-30 02:40:11 | CarstyYou | [Bugfix][Kernel] Fix negative memory offset in GDN Triton kernel (#33326) |
| 2026-01-30 02:00:13 | Linda | [NVIDIA] [feat] Integrate flashinfer Trtllmgen bf16 moe (#32954) |
| 2026-01-30 01:56:30 | Chendi.Xue | [BUGFIX][XPU] fix memory check after XPU reuse GPU_worker  (#33358) |
| 2026-01-30 00:54:31 | Cyrus Leung | [Chore] Move `MediaConnector` to `vllm.multimodal.media` (#33324) |
| 2026-01-30 00:47:05 | Angela Yi | [ez] Delete torch25_custom_graph_pass (#33287) |

### 📊 统计摘要
> 本日共 32 个提交 | 🔴高 4 | 🟡中 13 | 🟢低 15
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [[QeRL] Layerwise Reloading (#32133)](#f857a03)
    - [[Realtime API] Adds minimal realtime API based on websock...](#10152d2)
    - [[model] Add support for openPangu7B-VL (#32449)](#ba45bed)
    - [[Chore] Move `MediaConnector` to `vllm.multimodal.media` ...](#831453f)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [Disable Cascade Attention for Batch Invariance (#32561)](#8f5d512)
    - [Improve Mistral format checks. (#33253)](#ae5b7af)
    - [Remove deprecated `reasoning_content` message field (#33402)](#c5113f6)
    - [[Misc] Replace Optional[X] with X | None syntax (#33332)](#1a7894d)
    - [[Refactor] Move MM item count validation outside of proce...](#c87eac1)
    - [fix: allow LFM2 MoE prefix caching (align) (#33376)](#f45870b)
    - [Explicitly set `return_dict` for `apply_chat_template` (#...](#9432ed8)
    - [[CI] Enable mypy import following for `vllm/spec_decode` ...](#726d897)
    - [[Models] Refactor Kimi-K2.5 weight loading (#33346)](#8bfc8d5)
    - [[Model][Multimodal] Add explicit MusicFlamingo adapter (#...](#c46b0cd)
    - [[release] Minor fixes to release annotation and wheel upl...](#2284461)
    - [Add Triton fused MoE config for B200 (Nemotron Nano) (#32...](#8e2a469)
    - [[NVIDIA] [feat] Integrate flashinfer Trtllmgen bf16 moe (...](#0493d89)
  - [🟢 低重要度变更 (15)](#-🟢-低重要度变更-15)
    - [[BugFix][LoRA] TritonExperts is ModularMoEPath for FP8 mo...](#74898a7)
    - [Fix `test_moe.py` for Transformers v5 (#33413)](#a11bc12)
    - [[Doc] Enhance documentation around CPU container images (...](#58cb55e)
    - [[Misc] Clean up HIDDEN_DEPRECATED_METRICS after metric re...](#cf896ae)
    - [[Doc] [ROCm] Update Documentation to reflect v0.15.0 rele...](#174f167)
    - [[BUGFIX] Pixtral cannot be loaded with --limit-mm-per-pro...](#8e2ad97)
    - [Move decode context parallel validationn to `ParallelConf...](#d334dd2)
    - [[CI][AMD] Skip 4 GPUs testgroup ray tests (#33305)](#070c811)
    - [[BugFix] Disable async scheduling for Mamba prefix cachin...](#ec51831)
    - [Fix `tie_word_embeddings` for multimodal models in Transf...](#80b918f)
    - [[Docs] Adding links and intro to Speculators and LLM Comp...](#1337657)
    - [[Bugfix] Enable Triton MoE for FP8 per-tensor dynamic (#3...](#bfb9bda)
    - [[Bugfix][Kernel] Fix negative memory offset in GDN Triton...](#23591e6)
    - [[BUGFIX][XPU] fix memory check after XPU reuse GPU_worker...](#8c8ebeb)
    - [[ez] Delete torch25_custom_graph_pass (#33287)](#5a66c9c)
#### 🔴 高重要度变更 (4)

### [QeRL] Layerwise Reloading (#32133)
**SHA**: `f857a03` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f857a03f6b71101ac3f67b766f9a165c8b617f7b)

**🎯 变更类型**：功能增强 / 架构变更 / 安全修复  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 引入 **layerwise weight reloading**（`vllm/model_executor/model_loader/reload/*`），实现了在模型已加载后逐层、逐权重地重新装载并在必要时进行量化后处理。  
2. 为 `GPUModelRunner.reload_weights` 添加了完整的 **可选迭代器/磁盘路径** 接口，并配合新实现的 **layerwise 初始化‑处理‑收尾** 流程。  
3. 删除了原来的 `online_quantization.py`，用全新的 `torchao_decorator` 与 `layerwise` 体系统一处理 torch‑ao 在线量化的重新加载。  
4. 扩展了测试套件（`test_reload.py`、`test_torchao` 等）以验证 meta‑tensor 转换、层级恢复、权重量化以及完整的多模型、多 TP 场景下的权重切换。  
5. 为安全序列化提供 `VLLM_ALLOW_INSECURE_SERIALIZATION=1` 环境开关的 fixture，防止 `LLM.apply_model` 在内部 `pickle` 时产生意外错误。  

**🎯 影响范围**  
- **模型加载子系统**：`model_loader`、`model_loader/utils.py`、`model_loader/reload/*`、`model_loader/torchao_decorator.py`。  
- **运行时核心**：`v1/worker/gpu_model_runner.py`、`v1/worker/gpu_worker.py`、`vllm/llm_engine`（间接通过 RPC 调用）。  
- **量化路径**：torch‑ao 相关（`torchao_decorator`、`support_quantized_model_reload_from_hp_weights`）。  
- **单元测试**：`tests/model_executor/model_loader/...`、`tests/quantization/test_torchao.py`、`tests/v1/worker/test_gpu_model_runner.py`。  

---

## 🔍 技术洞察  

### 架构影响
| 维度 | 影响描述 |
|------|----------|
| **模块划分** | 新增 `reload` 包，形成“元‑tensor → meta‑恢复 → layerwise 加载 → 量化后处理”四阶段流水线，解耦了**权重恢复**与**量化**逻辑，原有 `online_quantization` 被彻底移除。 |
| **模型状态管理** | 使用 **WeakKeyDictionary** (`LAYERWISE_INFO`) 存储每层的 `LayerReloadingInfo`，在模型对象被 GC 时自动清理，避免内存泄漏。 |
| **Meta‑Tensor 机制** | `to_meta_tensor`、`materialize_meta_tensor` 保持原 `torch.Tensor` 类和属性，兼容自定义子类（如 `SharedWeightParameter`）。这使得在 **CPU → GPU**、**TP** 环境下的 lazy‑allocation 成为可能。 |
| **量化兼容** | 通过 `torchao_decorator.set_torchao_reload_attrs` 与 `support_quantized_model_reload_from_hp_weights`，在 `model.load_weights` 前后自动打开/关闭 `_do_torchao_reload` 标记，确保仅在需要时走 layerwise 逻辑。 |
| **RPC 接口** | `GPUModelRunner.reload_weights` 现在接受 **weights_iterator**、**weights_path**、**is_checkpoint_format** 参数，允许 **增量权重**（如 RLHF）或 **完整 checkpoint** 重新加载。 |
| **依赖关系** | `model_loader/utils.initialize_model` 在模型实例化后立即 `record_metadata_for_reloading`，确保后续 `reload_weights` 能够在 meta‑tensor 基础上恢复。 |

### 性能影响
| 场景 | 预期收益 | 潜在开销 |
|------|----------|----------|
| **一次性全模型 reload** | - 只在需要时 materialize层，**显存占用峰值降低**（权重先在 meta 设备占位）<br>- 通过 `get_numel_loaded` 统计实际加载的元素，避免多余 `copy_`。 | - 包装/unwrap weight loader 的函数调用层会产生微小的 Python 开销（可通过 `torch.jit`/C++ 实现进一步减低）。 |
| **增量微调 (RLHF)** | - 只 materialize 被修改的层，**重载时间与改动比例线性**，显著快于全模型 `torch.save/load`。 | - 需要在每层完成 `load_numel >= load_numel_total` 时触发 `_layerwise_process`，若 **层内部有 padding**（如某些量化方法），会产生 **延迟处理**，导致额外的 `copy_` 与 `materialize_layer`。 |
| **多 TP / EP 环境** | - `record_metadata_for_reloading` 在模型构建阶段一次完成，后续各 rank 只需读取 meta 信息，不会重复解析 checkpoint。 | - `WeakKeyDictionary` 在高并发（TP>8）下的锁争用仍然很小，但需关注 **跨进程共享**（当前仅在单进程内有效）。 |
| **torch‑ao 量化** | - 只在 `reload_weights` 时一次性跑 `quant_method.process_weights_after_loading`，避免在每次增量加载时重复量化。 | - `materialize_layer` 会把所有权重先 materialize 到目标设备，再量化，可能在 **GPU 显存** 上产生临时峰值（尤其是 FP8/FP16 转换）。 |

### 安全考虑
| 项目 | 说明 |
|------|------|
| **不安全序列化** | 新增 `enable_pickle` fixture 设置环境变量 `VLLM_ALLOW_INSECURE_SERIALIZATION=1`，允许 `LLM.apply_model` 使用 `pickle` 序列化自定义函数。若生产环境误开启，可能导致 **任意代码执行**。建议在正式部署时保持默认关闭，仅在测试/研发阶段显式打开。 |
| **元数据泄露** | `to_meta_tensor` 与 `materialize_meta_tensor` 会复制 `__dict__`，若模型中携带 **敏感属性**（如 API 密钥、路径），仍会在 meta‑tensor 中保存。理论上 meta‑tensor 位于 CPU 内存，仍受进程权限控制；但建议审计模型子类的 `__dict__`，避免泄露。 |
| **WeakKeyDictionary** | 因为键为 `nn.Module` 实例，若有 **循环引用**（未通过 `sanitize_layer_refs` 清理），会导致字典无法回收，从而产生 **内存泄漏**。已实现 `sanitize_layer_refs` 去除 `weight_loader.__self__` 引用，风险已降低。 |
| **文件路径** | `reload_weights(weights_path=…)` 直接使用外部路径加载 checkpoint，若路径被篡改可能导致 **加载恶意模型**。调用方应对路径进行白名单或校验（hash、签名）。 |

---

## ⚠️ 潜在风险

1. **层级加载进度误判**  
   - QKV‑ParallelLinear 等量化层在 `load_numel_total` 与实际加载元素不匹配时，会触发 “Excessive loading” 警告，导致 **延迟处理** 或 **未复制的权重量**。若新量化方法产生额外 padding，可能导致显存泄漏或权重不一致。  
   - **建议**：在新增量化实现时，确保 `get_layer_size` 能准确反映真实 `copy_` 元素数，或在 `make_online_process_loader` 中加入 `padding` 计数逻辑。

2. **torch‑ao 重新加载的递归标记**  
   - `model._original_do_torchao_reload` 与 `model._do_torchao_reload` 在 `initialize_layerwise_reload` / `finalize_layerwise_reload` 之间切换，若异常提前返回，标记可能残留导致后续 `load_weights` 进入错误路径。  
   - **建议**：使用 `try/finally` 包裹 `initialize_layerwise_reload`/`finalize_layerwise_reload`，确保状态恢复。

3. **多进程/分布式环境**  
   - 当前实现依赖 **单进程 WeakKeyDictionary**，在 `torch.distributed`（TP、EP）场景下，每个进程会各自持有一份 `LAYERWISE_INFO`，若模型在 **跨进程共享**（例如 `torch

---

### [Realtime API] Adds minimal realtime API based on websockets (#33187)
**SHA**: `10152d2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/10152d2194c32f6e312c99ed51fd62b01f55598e)

**🎯 变更类型**：功能增强 / 架构变更 / 安全修复  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**：  
本次提交为 vLLM 项目新增 **Realtime API**（基于 WebSocket），实现了服务器端的实时音频流转录能力。核心工作包括：  
1. 完整的 HTTP / WebSocket 路由、状态初始化与日志；  
2. 新增 `RealtimeConnection` 管理音频流、事件分发、异步生成任务及错误处理；  
3. 定义统一的 JSON 协议（`protocol.py`），包括 `session.created`、`input_audio_buffer.append`、`transcription.delta` 等；  
4. 为模型层新增 `SupportsRealtime` 接口以及 Voxtral 相关实现，实现 **流式音频缓冲 → Prompt → StreamingInput** 的转换；  
5. 在 `EngineClient`、`AsyncLLM`、`GPUModelRunner` 等核心模块中引入 `StreamingInput`，并在任务列表中加入 `realtime`。  
6. 补充文档、示例（文件/麦克风）以及单元/集成测试。  

预期效果是用户可以通过 `ws://host/v1/realtime` 发送 base64‑PCM16 音频块，实时获得增量文字（`transcription.delta`）并在结束时得到完整转录与使用统计（`transcription.done`），全链路保持与现有 OpenAI 兼容 API 的统一错误格式和日志体系。

---

### 🎯 影响范围
- **入口层**：`vllm/entrypoints/openai/api_server.py`、`vllm/entrypoints/openai/realtime/api_router.py`、`vllm/entrypoints/launcher.py`  
- **协议层**：`vllm/entrypoints/openai/realtime/protocol.py`、`vllm/entrypoints/openai/engine/protocol.py`  
- **连接层**：`vllm/entrypoints/openai/realtime/connection.py`（WebSocket 生命周期、音频队列）  
- **服务层**：`vllm/entrypoints/openai/realtime/serving.py`（Audio → StreamingInput）  
- **模型层**：`vllm/model_executor/models/interfaces.py`（`SupportsRealtime` 接口）、`vllm/model_executor/models/voxtral.py`、`vllm/model_executor/models/voxtral_streaming.py`（实时缓冲实现）  
- **调度层**：`vllm/v1/engine/async_llm.py`、`vllm/v1/worker/gpu_model_runner.py`（任务发现）  
- **数据层**：`vllm/inputs/data.py`（`StreamingInput` dataclass）  
- **测试/示例**：`tests/entrypoints/openai/test_realtime_validation.py`、`examples/online_serving/openai_realtime_*.py`、文档 `docs/serving/openai_compatible_server.md`  

---

### 🔍 技术洞察

| 维度 | 洞察 |
|------|------|
| **架构影响** | - **新增路由层**：通过 `APIRouter` 将 `/v1/realtime` 添加到 FastAPI 应用，保持与其它 OpenAI 接口统一的启动流程。<br>- **连接抽象**：`RealtimeConnection` 负责 WebSocket 接收、事件分发、音频队列（`asyncio.Queue`）以及生成任务的生命周期管理，保持单例 per‑connection，易于水平扩展（每个连接独立）。<br>- **模型接口**：`SupportsRealtime` 用 `Protocol` 定义必需的 `buffer_realtime_audio` 异步生成器，使后端模型只需实现该方法即可被实时 API 调用，遵循 **开闭原则**。<br>- **StreamingInput** 从 `vllm/inputs/data.py` 抽离为通用数据结构，`AsyncLLM.generate` 已支持 `StreamingInput`，因此所有已有流式生成逻辑（文本、图片等）无需改动。 |
| **性能影响** | - **音频流分块**：采用 `asyncio.Queue` 与 `asyncio.Task`，音频块和生成任务并行，理论上能够在音频到达的同时启动推理，降低端到端延迟。<br>- **缓冲实现**：`VoxtralRealtimeBuffer` 预分配 30 s 缓冲区，避免频繁 realloc、拷贝；但是在极端长流或高并发时仍可能触发重新分配，需监控内存占用。<br>- **生成器切片**：每次 `commit` 触发一次 `engine_client.generate`，内部仍走完整的调度路径（pre‑fill、decode），与 Batch 推理的吞吐对齐；对单连接的吞吐略有下降（因为每次仅处理单个音频块），但对实时交互的 **延迟** 明显提升。<br>- **CPU/GPU 负载**：音频解码（PCM16 → float32）在 Python 层完成，开销极小；主要负载仍在模型前向；如果使用多 GPU，需要保证 `engine_client` 能够在同一 Scheduler 中混合 `generate` 与 `realtime` 任务（已在 `GPUModelRunner.get_supported_generation_tasks` 中加入）。 |
| **安全考虑** | - **输入校验**：在 `RealtimeConnection.handle_event` 对 base64 解码、PCM16 长度、最大文件大小（`VLLM_MAX_AUDIO_CLIP_FILESIZE_MB`）进行校验，防止 OOM 与恶意大文件攻击。<br>- **异常捕获**：所有 JSON 解析错误、音频解码错误、模型未支持错误统一包装为 `error` 事件返回客户端，避免泄露内部异常栈。<br>- **WebSocket 关闭**：在 `WebSocketDisconnect` 与异常路径均执行 `cleanup`，确保队列、任务被安全取消，防止资源泄露。<br>- **权限**：现阶段与其他 OpenAI 接口共用同一鉴权（若有），未引入额外风险。 |
| **可维护性** | - **模块化**：协议、连接、服务层分离，代码可读性好；使用 Pydantic 统一序列化/反序列化，降低手写 JSON 的错误率。<br>- **测试覆盖**：新增 `test_realtime_validation.py` 覆盖连接验证、分块流、错误路径；示例脚本提供快速手动验证。<br>- **向后兼容**：仅在 `supported_tasks` 包含 `"realtime"` 时才会挂载路由与状态对象，默认 `vllm serve` 参数不变，已有部署不受影响。 |
| **依赖变更** | - 新增 `vllm[audio]` 额外依赖（`librosa`, `numpy`, `websockets`, `gradio`），通过 `extras_require` 引入，避免对不需要音频的用户产生硬性安装。 |

---

### ⚠️ 潜在风险

| 风险点 | 描述 | 缓解措施 |
|--------|------|----------|
| **内存泄露 / 缓冲增长** | `VoxtralRealtimeBuffer` 按需扩容，若客户端持续发送音频而未发送 `final=True`，缓冲会继续增长至 OOM。 | - 在 `RealtimeConnection` 设置最大累计音频时长或字节数阈值；<br>- 超限后返回 `error` 并关闭连接。 |
| **并发连接数** | 每个 WebSocket 会创建独立 `asyncio.Task` 与 `Queue`，在高并发（千级）时可能导致过多协程和队列对象，调度器的锁竞争增大。 | - 在 `serve_http` 中对 `max_concurrency` 做限制；<br>- 监控 `Task` 数量，必要时采用连接池或限流。 |
| **模型不支持实时** | 若用户在 `session.update` 中指定不具 `SupportsRealtime` 的模型，`_check_model` 只返回错误 JSON，但后续仍可能走到 `start_generation`。 | - 在 `handle_event` 的 `session.update` 分支提前返回错误并终止后续操作。 |
| **音频格式不匹配** | 只支持 PCM16 @16kHz，若客户端发送其它采样率或位深度，解码后音频会失真或报错。 | - 在文档/示例中强制说明；在 `RealtimeConnection` 增加采样率校

---

### [model] Add support for openPangu7B-VL (#32449)
**SHA**: `ba45bed` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ba45bedfd15ab01af2be5ae28af03888f3683063)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交为 vLLM 项目新增对 OpenPangu‑VL（7B）多模态大模型的完整支持。实现了模型专属的视觉 Transformer、交叉模态 RoPE（M‑RoPE + interleaved）实现，并在注册表、文档、示例及测试中加入对应入口。  
**🎯 影响范围**：  
- `vllm/model_executor/models/openpangu_vl.py`（新模型实现）  
- `vllm/model_executor/layers/rotary_embedding/`（新增 `mrope_interleaved.py`，改造 `__init__.py`）  
- `vllm/model_executor/models/openpangu.py`（RoPE 参数适配）  
- `vllm/model_executor/models/registry.py`（模型注册）  
- 示例脚本、文档、模型注册测试  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **新增多模态支路**：在 `OpenPanguVLForConditionalGeneration` 中组合 `OpenPanguVisionTransformer`（视觉特征提取）+ 语言模型，遵循 vLLM 的 **MultiModal** 接口，实现 `embed_multimodal`、`iter_mm_grid_thw`、`get_mrope_input_positions` 等方法。<br>- **跨模态 RoPE**：在通用 `get_rope` 中新增 `scaling_type="openpangu"`，若配置了 `mrope_interleaved=True` 则实例化 `MRotaryEmbeddingInterleaved`，实现 **分段‑交错** 的多模态 RoPE。<br>- **模型映射**：在 `registry.py` 中声明 `"OpenPanguVLForConditionalGeneration"`，并在 `hf_to_vllm_mapper` 里提供前缀映射，保证权重加载能够无缝对齐。 |
| **性能影响** | - **视觉 Transformer**：引入 3D 卷积、块级注意力以及多阶段合并，显著提升显存占用（≈1–2 GB 额外）以及前向算子时间，尤其在多帧视频输入时。<br>- **M‑RoPE Interleaved**：自定义 `forward` 里手动切分并拼接 `cos/sin`，不走高度优化的 CUDA kernel，可能比原生 `MRotaryEmbedding` 慢 10‑30%。<br>- **后端限制**：仅在 `FLASH_ATTN` 可用时通过，否则直接抛异常，限制了在不支持 Flash‑Attention 的硬件上的可用性。<br>- **缓存复用**：`rotary_pos_emb` 仍使用缓存机制，但在 `interleaved` 场景会重新排列，增加一次额外的 `torch.cat`、`reshape` 操作。 |
| **安全考虑** | - **trust_remote_code**：示例与模型注册均开启 `trust_remote_code=True`，若用户自动下载 `FreedomIntelligence/openPangu-VL-7B`，潜在执行第三方代码的风险。<br>- **强制 eager**：`enforce_eager=True` 关闭了 CUDA graph 加速，降低了图优化错误的可能性，安全上更可控。<br>- **权重加载**：使用 `AutoWeightsLoader` + `WeightsMapper`，只映射已知前缀，防止意外加载恶意参数。 |
| **可维护性** | - 新增 185 行 `mrope_interleaved.py`，代码结构与原 `MRotaryEmbedding` 保持一致，易于后续复用。<br>- 文档、示例、测试同步更新，降低使用者入门门槛。<br>- 通过 `@MULTIMODAL_REGISTRY.register_processor` 完整注册，遵循现有插件化模式，后续扩展（如新模态）可直接在该文件中实现。 |

---

### ⚠️ 潜在风险

1. **性能回退**  
   - 交叉模态 RoPE 实现没有专门的 CUDA kernel，推理吞吐可能下降约 10‑30%。在大批量或高分辨率视频场景下，可能成为瓶颈。  
2. **显存激增**  
   - `OpenPanguVisionTransformer` 的 `patch_embed`、多层 `VisionBlock` 以及 `merger` 在每帧都保持完整特征图，显存需求比纯文本模型高出约 2‑3 GB（取决于 `max_num_seqs` 与 `limit_mm_per_prompt`）。  
3. **后端依赖**  
   - 仅支持 `FLASH_ATTN`；在不支持此后端的 GPU（如旧版显卡或 CPU）会直接抛异常，导致模型不可用。  
4. **安全风险**  
   - `trust_remote_code=True` 对未审计的第三方仓库引入隐蔽执行风险，特别是在生产环境中。  
5. **配置错误**  
   - 若 `rope_parameters` 中未提供 `mrope_section` 或 `mrope_interleaved` 为 `True`，会触发 `ValueError("Pangu mrope lacks necessary parameters.")`，导致模型启动失败。  

---

### 💡 关注建议

| 目标 | 建议 |
|------|------|
| **开发者** | - 在模型配置文件（`hf_config.json`）中明确 `rope_parameters`：`{"scaling_type":"openpangu", "mrope_interleaved":true, "mrope_section":[2,2]}`，避免启动错误。<br>- 为关键路径（`MRotaryEmbeddingInterleaved.forward`）实现基于 **CUDA kernel** 的加速或使用 `torch.compile`（PyTorch 2.0）进行 JIT 优化。 |
| **部署/运维** | - 运行前确认 GPU 驱动、CUDA 与 Flash‑Attention 兼容；若不支持，请回退到标准 `MRotaryEmbedding`（可通过关闭 `mrope_interleaved`）。<br>- 对启用 `trust_remote_code` 的节点做白名单审计，或在受信网络环境下拉取模型权重。 |
| **性能调优** | - 调整 `max_num_seqs` 与 `limit_mm_per_prompt`，在显存紧张时降低并发数。<br>- 对视频输入，建议使用 **稀疏抽帧** 或增大 `temporal_patch_size` 以减小序列长度。 |
| **测试** | - 为新模型补充 **多帧视频** 的端到端验证，确保 `get_mrope_input_positions` 正确生成交错位置信息。<br>- 在 CI 中加入 **显存基准**（`torch.cuda.max_memory_allocated`）对比 OpenPangu‑VL 与已有多模态模型的差距。 |

> **总结**：此次 PR 为 vLLM 引入了一条全新的多模态模型路径——OpenPangu‑VL，涉及视觉特征抽取、交叉模态 RoPE 以及模型注册全链路。它扩大了 vLLM 对国产/开源视觉语言模型的覆盖，但也带来了显存、性能和安全方面的可观风险。团队在上线前应重点验证 RoPE 的效率、显存占用以及 Remote‑

---

### [Chore] Move `MediaConnector` to `vllm.multimodal.media` (#33324)
**SHA**: `831453f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/831453fcef69f76f3266ed2a0d8f193b86e786ad)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：将原先位于 `vllm.multimodal.utils` 中的 `MediaConnector` 与相关注册表 `MEDIA_CONNECTOR_REGISTRY` 拆分迁移到新的子包 `vllm.multimodal.media`，并在旧位置提供惰性兼容层及废弃警告。相应的 import 路径在多个模块（tests、entrypoints、inputs 等）同步调整，同时对 `vllm.transformers_utils.tokenizer` 的属性访问实现补全。  

**🎯 影响范围**：  
- `vllm.multimodal.media`（新模块、connector 实现）  
- `vllm.multimodal.utils`（被大幅裁剪，仅留下辅助函数）  
- `vllm.multimodal.inputs`（新增 `MediaWithBytes` 的直接导入）  
- `vllm/entrypoints/chat_utils.py`、测试文件、模型解析代码等所有直接引用 `MediaConnector` 与 `MEDIA_CONNECTOR_REGISTRY` 的地方  
- 对外用户代码仍可通过旧路径 `vllm.multimodal.utils.MEDIA_CONNECTOR_REGISTRY` 访问，但会收到 `DeprecationWarning`  

---

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 1. **职责分离**：`MediaConnector` 作为媒体加载的核心逻辑，搬入 `media` 子包后，使 `utils` 仅保留与 I/O、编码/解码等无状态工具，遵循单一职责原则。<br>2. **模块层级清晰**：`media` 子包现在聚合 `audio、image、video、base、connector` 等相关实现，便于未来扩展（如新增 `text` 媒体）以及文档化。<br>3. **向后兼容**：通过 `__getattr__` 动态导入并抛出 `DeprecationWarning`，保证老代码在短期内仍能运行，避免突发破坏。 |
| **性能影响** | 1. **加载成本**：迁移本身不改变运行时逻辑；`MediaConnector` 仍使用全局线程池 `global_thread_pool`，保持原有并发读取性能。<br>2. **首次导入成本**：由于新模块在 `media/__init__.py` 中一次性 `import` 多个子模块，首次导入可能稍慢，但仅在进程启动阶段触发，对整体吞吐影响极小。<br>3. **废弃警告**：如果用户开启 `-Werror`，警告会导致运行错误，需要在部署脚本中加以抑制。 |
| **安全考虑** | 1. **域/路径限制**：`MediaConnector` 中对 `allowed_media_domains` 与 `allowed_local_media_path` 的校验保持不变，安全边界未受影响。<br>2. **Deprecation 警告**：通过 `warnings.warn(..., DeprecationWarning, stacklevel=2)`，不泄露实现细节，安全可控。<br>3. **异常处理**：保持对 `UnidentifiedImageError` → `ValueError` 的转换，保证上层统一错误处理。 |
| **可维护性** | 1. **代码组织**：将大量与媒体加载相关的实现集中，后续维护/单元测试的定位更直接。<br>2. **注册表统一**：`MEDIA_CONNECTOR_REGISTRY` 仍为单例 `ExtensionManager`，只在 `media` 包中声明，避免在不同位置出现同名实例导致注册冲突。<br>3. **文档/API 迁移**：需要更新公开文档、示例以及 `__all__` 列表，以反映新路径。 |

---

### ⚠️ 潜在风险  

1. **向后兼容性破坏**  
   * 外部项目直接使用 `from vllm.multimodal.utils import MediaConnector` 或 `MEDIA_CONNECTOR_REGISTRY`，在未捕获 `DeprecationWarning` 的情况下，可能因 CI 中 `-Werror` 导致构建失败。  
2. **循环导入**  
   * `media/__init__.py` 同时导入 `connector` 与其他子模块，若子模块内部再次 `import vllm.multimodal.media` 可能触发循环。当前代码未出现循环，但后续新增子模块时需注意。  
3. **线程池资源泄漏**  
   * `global_thread_pool` 在 `media/connector.py` 中创建并在 `atexit` 退出时关闭。若项目使用多进程（如 `torch.multiprocessing`）并在子进程中重复导入，可能出现已关闭的池子被再次使用的情况。  
4. **警告抑制不当**  
   * 项目或用户可能在全局使用 `warnings.filterwarnings("ignore")` 抑制所有 warning，导致废弃提示被无意隐藏，延迟迁移排期。  

---

### 💡 关注建议  

| 对象 | 建议 |
|------|------|
| **开发者** | - 在代码中尽快改为 `from vllm.multimodal.media import MediaConnector, MEDIA_CONNECTOR_REGISTRY`。<br>- 对 CI 中的警告策略进行审视，若使用 `-Werror`，在迁移完成前添加 `filterwarnings("ignore", category=DeprecationWarning, module="vllm.multimodal.utils")`。 |
| **维护者** | - 在下一个 LTS（v0.17）中彻底删除 `vllm.multimodal.utils.MEDIA_CONNECTOR_REGISTRY` 的兼容层。<br>- 为 `media` 子包补齐 `__all__` 并在项目文档、CHANGELOG、示例代码中明确迁移路径。 |
| **用户 / 第三方库** | - 检查依赖是否仍通过旧路径引用 `MediaConnector`，并适当更新 `requirements` 或 `import`。<br>- 若在多进程环境使用，确保每个子进程在启动前已完成 `import vllm.multimodal.media.connector`，避免使用已关闭的线程池。 |
| **安全审计** | - 复核 `allowed_media_domains` 与 `allowed_local_media_path` 参数的默认值（空列表、None），确保在生产环境中显式配置，防止意外加载不受信任的远程媒体。 |
| **测试** | - 在单元测试中加入对 `DeprecationWarning` 的捕获断言，防止未来警告被误当作错误。<br>- 添加针对 `media.__getattr__` 的覆盖，用于验证旧路径仍能返回注册表实例。 |

---  

**结论**：此次迁移显著提升了项目的模块化与可维护性，风险主要集中在向后兼容性与警告处理上。只要在 CI 与部署环境中适当管理 `DeprecationWarning`，并在代码基底完成 import 更新，影响可控且收益（清晰的架构、易扩展的媒体加载框架）远大于迁移成本。建议在下一个 minor 版本前完成全部迁移并在 v0.17 彻底移除旧入口。

---

#### 🟡 中重要度变更 (13)

### Disable Cascade Attention for Batch Invariance (#32561)
**SHA**: `8f5d512` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8f5d51203b56e8f80cca01dc775e6bf3804ddd55)

**🎯 变更类型**：功能增强 / Bug 修复（为 *VLLM_BATCH_INVARIANT* 环境变量提供完整支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 当 `VLLM_BATCH_INVARIANT` 被打开时，自动在模型配置上强制 `disable_cascade_attn=True`，并在启动时给出一次性警告。  
2. 暂时把 FlashInfer 后端从批量不变性测试中剔除（CTA 大小不兼容），并在 `batch_invariant.override_envs_for_invariance` 中同步。  
3. 为 MoE Router Gate 层的线性运算新增批量不变性路径 `linear_batch_invariant`，并在 `Linear.apply` 中检测后使用。  
4. 在测试中放宽 batch 大小、延长生成长度，以更好覆盖 batch‑invariance 场景；同时对 prompt 填充逻辑做了小幅调整并留下 TODO。  

**🎯 影响范围**  
- `vllm/config/vllm.py`（模型配置初始化）  
- `vllm/model_executor/layers/batch_invariant.py`（后端过滤）  
- `vllm/model_executor/layers/linear.py`、`utils.py`（MoE Gate 线性层）  
- `tests/v1/determinism/*`（确定性/批量不变性测试）  

**💡 关注建议**  
1. **功能完整性**：确认 `linear_batch_invariant` 实现已在对应文件 `batch_invariant.py` 中提供，并对不同 dtype、quantization、TP size 做过单元测试。  
2. **兼容性**：`disable_cascade_attn` 的自动修改可能与用户手动配置冲突，建议在文档中明确说明 `VLLM_BATCH_INVARIANT` 会强制关闭 cascade attention。  
3. **性能回归**：关闭 FlashInfer 可能导致在支持该后端的机器上性能下降，建议在 CI 中加入对比基准或在 Release Note 中标注。  
4. **测试覆盖**：当前测试仅使用 32 条相同长度的短 prompt，未触发 chunked‑prefill。考虑实现注释中的 ragged‑length prompt，以确保真实性能/正确性验证。  
5. **代码可维护性**：`is_layer_moe_router_gate` 的硬编码后缀列表应保持同步（如新增 MoE gate 类型），可考虑将其迁移到常量模块或通过配置驱动。  

总体来看，此次提交为 batch‑invariance 特性提供了必要的后端兼容与 MoE Gate 支持，但需在文档、基准和更完整的测试上进一步完善，以避免意外的性能或行为回退。

---

### Improve Mistral format checks. (#33253)
**SHA**: `ae5b7af` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ae5b7aff2bbca60b335190f76f4c637119035db2)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交为 Mistral 模型的配置检查增加专门的 `config_format` 参数，并在 repo 工具层实现了 `any_pattern_in_repo_files` 与 `is_mistral_model_repo` 两个抽象函数。相应地，`ModelArchConfigConvertorBase.get_torch_dtype` 在读取 safetensors 元数据前会临时补丁 HuggingFace Hub 常量，以支持 Mistral 的 `consolidated*.safetensors` 命名。Tokenizer 注册、模型配置加载以及单元测试均同步改造，统一通过新工具判断模型仓库类型。

**🎯 影响范围**  
- `vllm/config/model.py` – 传递 `config_format`、dtype 检查入口。  
- `vllm/transformers_utils/config.py`、`repo_utils.py` – 新增仓库判定函数。  
- `vllm/tokenizers/registry.py` – 自动选择 Mistral、Grok2 tokenizer。  
- `vllm/transformers_utils/model_arch_config_convertor.py` – 添加 `_maybe_patch_hf_hub_constants` 上下文管理器并改造 dtype 获取。  
- 测试目录 `tests/*` – 新增/更新对应单元测试。

**💡 关注建议**  
1. **向后兼容**：`config_format` 默认仍为 `"hf"`，但在实例化 `ModelConfig` 时若旧代码未显式传参，请确认 `ModelConfig` 的 `__post_init__` 中已保留默认值，以免出现意外 `TypeError`。  
2. **文档更新**：在 vLLM 配置说明和 tokenizer 使用指南中补充 “Mistral config format” 与新检测方法的说明，避免用户误用。  
3. **常量恢复**：`_maybe_patch_hf_hub_constants` 已使用 `try/finally` 恢复，建议在 CI 中加入并发调用的压力测试，以确保多线程/多进程情况下不会出现常量交叉污染。  
4. **性能评估**：`any_pattern_in_repo_files` 通过一次 `list_filtered_repo_files` 完成判断，若仓库文件非常多可能产生额外 I/O，建议在 `list_filtered_repo_files` 中加入早停（只要匹配到一个即可返回），可进一步提升检测效率。  
5. **测试覆盖**：现有测试已覆盖基本路径，建议补充对 `config_format="mistral"` 场景下异常路径（如缺少 `consolidated*.safetensors`）的断言，确保回退至 HF 逻辑。  

总体而言，此次改动提升了对 Mistral 模型的兼容性与自动化检测能力，但需注意文档同步与潜在的并发安全性。

---

### Remove deprecated `reasoning_content` message field (#33402)
**SHA**: `c5113f6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c5113f60f2ca39334cdbe1657d5a8fd512767e04)

**🎯 变更类型**：其他（去除已废弃字段）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交彻底删除了 `reasoning_content` 字段，统一改为使用 `reasoning`。相关模型、解析、渲染、单元测试以及工具链均已同步改写，删除了旧字段的兼容代码和校验。  

**🎯 影响范围**  
- `vllm.entrypoints.openai.chat_completion.protocol.ChatMessage`、`DeltaMessage`、`engine.protocol` 中的字段声明与校验。  
- 各解析函数（`_parse_chat_message_content`、`harmony_utils`、`deepseek_v32_encoding.render_message`、`parse_message_from_completion_text` 等）对 `reasoning_content` 的读取全部改为 `reasoning`。  
- 流式返回、日志、工具调用以及测试用例中对应的属性名也同步修改。  
- 兼容层（`model_validator`）被删除，意味着如果外部仍然发送 `reasoning_content` 将会被丢弃或触发错误。  

**💡 关注建议**  
1. **向后兼容**：如果已有用户或第三方库仍在使用 `reasoning_content`，需要在文档中明确迁移窗口，并在发布说明里给出强制升级的时间点。  
2. **验证路径**：确认所有入口（REST、OpenAI‑compatible API、Harmony 接口）在接受请求时仅保留 `reasoning`，避免意外的 `KeyError`。可以在请求过滤层加入警告日志，帮助用户发现残留字段。  
3. **测试覆盖**：当前已修改的单元测试覆盖了大多数代码路径，建议再补充 **集成测试**，包括：  
   - 通过 `curl`/SDK 发送含 `reasoning_content` 的请求，验证返回错误或警告。  
   - 检查 `ChatCompletion` 流式输出中不再出现 `reasoning_content` 键。  
4. **文档更新**：更新 OpenAI‑compatible API schema、Harmony 协议说明以及示例代码，删除 `reasoning_content` 的描述。  

总体而言，此次改动简化了消息模型，降低了冗余。只要在升级前做好兼容提示与测试验证，风险可控。祝项目顺利发布！

---

### [Misc] Replace Optional[X] with X | None syntax (#33332)
**SHA**: `1a7894d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1a7894dbdfd025da846b0bbfd19ecc4c6e9fcc1b)

**🟢 变更类型**：代码重构（语法升级）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将项目中大量使用的 `typing.Optional[T]` 替换为 Python 3.10+ 原生并集写法 `T | None`。涉及 `vllm` 以及子模块的类型注解、返回值、属性定义等 130 余处改动。  
**🎯 影响范围**：  
- **核心模型层**：`beam_search.py`、`v1/engine`、`v1/request`、`v1/worker` 等。  
- **分布式 KV‑Cache 迁移**：`distributed/kv_transfer/kv_connector/*`、`parallel_state.py`、`kv_transfer_state.py`。  
- **量化、LoRA、Multimodal、平台抽象**等多处子模块。整体几乎所有涉及 `Optional` 的文件均被覆盖。  

**💡 关注建议**  

1. **Python 版本**  
   - 代码已依赖 `X | None` 语法，最低运行时需 Python 3.10。请在 `setup.cfg`/`pyproject.toml` 中锁定 `python_requires=">=3.10"`，并在 CI 环境验证。  

2. **类型检查兼容性**  
   - `mypy`、`pyright` 等工具已支持新语法，但若项目仍在使用旧版 type‑checker（如 `pytype`）可能报错。建议在 CI 中加入 `pip install -U mypy` 并运行 `mypy vllm`，确保所有文件通过。  

3. **运行时行为**  
   - `| None` 与 `Optional` 完全等价，仅是语法层面变更，不会影响运行时性能或功能。但请注意在 **runtime‑only** 环境（如 `torchscript`）中，有时对 `|` 表达式的解析会产生轻微差异，建议跑一次完整的单元测试/集成测试，以捕获潜在的序列化或 JIT 编译问题。  

4. **文档与示例**  
   - 项目文档（README、API docs）若手动列出示例代码，需同步更新为新写法，避免阅读者混淆。  

5. **依赖库的类型提示**  
   - 部分外部库（如 `torch`, `msgspec`, `safetensors`）的 `.pyi` 仍使用 `Optional[...]`，这不会冲突，但在统一风格上可考虑在项目内部对外部类型进行 `typing.cast`，或在 `py.typed` 中声明。  

6. **回退策略**  
   - 若后续需要兼容 Python 3.9 以下的部署环境，可考虑在 `vllm/__init__.py` 中加入一个小型 shim：  
     ```python
     import sys
     if sys.version_info < (3, 10):
         from typing import Optional as _Opt
         __builtins__["|"] = lambda a, b: _Opt[a] if b is None else a  # 简化示例
     ```  
   - 但更推荐直接在部署环境升级 Python。  

**结论**：本次改动主要是现代化类型注解写法，提升代码可读性并与 PEP 604 保持一致。只要保证运行环境为 Python 3.10+，并在 CI 中执行完整的类型检查与测试，即可安全合并。无需额外功能回归测试。

---

### [Refactor] Move MM item count validation outside of processor (#33396)
**SHA**: `c87eac1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c87eac18f7d0169e527631772fd081cc08aad01a)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将多模态输入数量的校验从 `MultiModalProcessor` 移到其配套的 `ProcessingInfo`（`info`）对象，统一通过 `info.validate_num_items`、`info.allowed_mm_limits`、`info.supported_mm_limits` 访问。  
- 为这几个属性加入 `cached_property`，并在旧属性上标记 `@deprecated`，保持向后兼容。  
- 相应地更新了入口、Lora、注册表、worker 等调用方以及单元测试的实现。  

**🎯 影响范围**  
- `vllm/multimodal/processing/context.py`（核心实现）  
- `vllm/multimodal/processing/processor.py`（去除冗余校验逻辑）  
- `vllm/multimodal/registry.py`、`vllm/entrypoints/chat_utils.py`、`vllm/lora/model_manager.py`、`vllm/v1/worker/utils.py`（调用方）  
- `tests/multimodal/test_processing.py`（测试适配）  

**💡 关注建议**  
1. **兼容性**：已用 `@deprecated` 包装旧属性，确保 v0.16 仍可运行；建议在下个次要版本（v0.17）前彻底清理这些属性并更新文档。  
2. **性能**：`cached_property` 能避免重复计算 `supported_mm_limits`、`allowed_mm_limits`，但请确认 `ProcessingInfo` 生命周期足够长，否则频繁创建可能导致重复缓存。  
3. **错误信息**：`validate_num_items` 现在统一生成错误提示，确保所有入口（如 `ChatUtils.add`、`LoraModelManager`）得到一致报错。  
4. **测试**：现有单元测试已修改，仅检查 `info.get_supported_mm_limits`，建议新增直接调用 `info.validate_num_items` 的边界测试，以防未来误删。  
5. **文档**：更新用户手册中关于 `--limit-mm-per-prompt` 与多模态限制的说明，指明新推荐的 API `processor.info.*`。  

整体来看，此次重构提升了职责划分，使校验逻辑集中在 `ProcessingInfo`，代码可维护性与可测试性得到改善，风险较低。请在发布前完成兼容性验证并同步文档。

---

### fix: allow LFM2 MoE prefix caching (align) (#33376)
**SHA**: `f45870b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f45870b53fc4cab2b747b04d5d3c47a68377d548)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 将 Lfm2Moe 对前缀缓存的原先硬性断言改为在 `cache_config.mamba_cache_mode="all"` 时抛出 `NotImplementedError`，并提示使用 `--mamba-cache-mode=align`。  
2. 为 Lfm2Vl 模型新增 `MambaStateCopyFunc` 与 `MambaStateCopyFuncCalculator` 的导入，并实现 `get_mamba_state_copy_func`，返回短卷积状态复制函数，以配合 “align” 前缀缓存模式。

**🎯 影响范围**：  
- `vllm/model_executor/models/lfm2_moe.py`（Lfm2MoE模型初始化）  
- `vllm/model_executor/models/lfm2_vl.py`（Lfm2VL模型的 Mamba 状态复制逻辑）  
- 相关的缓存配置 (`VllmConfig.cache_config`) 与 Mamba 相关工具 (`mamba_utils`)。

**💡 关注建议**：  
1. **兼容性**：新版不再支持 `mamba_cache_mode=all`，运行旧命令行或脚本会直接报 `NotImplementedError`，请提前在部署文档或 CI 中加入检查/迁移提示。  
2. **功能验证**：确保 `MambaStateCopyFuncCalculator.short_conv_state_copy_func()` 在所有 GPU/CPU 环境下返回有效的复制函数；建议新增单元测试覆盖 `get_mamba_state_copy_func` 与实际的状态拷贝行为。  
3. **错误信息**：当前错误信息较为直白，若后续计划实现 “all” 模式，可考虑使用 `NotImplementedError` 的占位提示，避免误导用户。  
4. **文档更新**：在 `vllm` 的使用手册和 `README` 中补充 “align” 前缀缓存模式的说明，以及不再支持 “all” 模式的原因与迁移步骤。  
5. **性能观察**：开启 `align` 模式后，监控前缀缓存的内存占用与推理时延，确保新实现没有引入意外的拷贝开销。  

总体而言，此次改动解锁了 Lfm2MoE 在 “align” 前缀缓存下的可用性，同时为 Lfm2Vl 添加了必要的状态复制函数，属于功能增强。后续需关注配置兼容、文档同步以及对应的单元/集成测试。

---

### Explicitly set `return_dict` for `apply_chat_template` (#33372)
**SHA**: `9432ed8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9432ed8c7e158def084e0770fd02838292bf57e4)

**🎯 变更类型**：Bug 修复 / 兼容性提升  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在多个示例、测试以及内部渲染/Tokenizer实现中统一显式传入 `return_dict=True`（或在内部统一强制为 `False`），以适配 Transformers 5.x 默认返回字典的行为，避免 `apply_chat_template` 直接返回 `Tensor` 时产生属性访问错误 (`.input_ids`)。  

**🎯 影响范围**  
- `examples/*`：所有直接调用 `tokenizer.apply_chat_template` 的示例脚本。  
- `tests/entrypoints/openai/*`：对 OpenAI 接口的单元测试。  
- `vllm/model_executor/models/isaac.py`、`vllm/renderers/*`、`vllm/tokenizers/grok2.py`、`vllm/transformers_utils/processors/*`：内部统一将 `return_dict` 设为 `False`，确保返回纯 Tensor。  
- `vllm/renderers/hf.py` 的 `safe_apply_chat_template` 也加入该默认。  

**💡 关注建议**  
1. **保持向后兼容**：当前做法对旧版 Transformers（默认返回 Tensor）仍然有效，因为显式传参会覆盖默认值。若未来再次更改默认，建议在库层统一封装 `apply_chat_template`，避免散落的显式参数。  
2. **文档更新**：在 `vllm` 使用指南和各示例的注释中说明 `return_dict` 参数的必要性，帮助使用者了解此行为变化。  
3. **测试覆盖**：已加入对 `return_dict` 的单元测试，建议在 CI 中继续跑一次完整的示例脚本，以捕获潜在的运行时错误。  
4. **性能考量**：强制返回字典再取 `input_ids` 会产生一次额外的对象包装，影响可忽略，但若在高频调用路径上仍可考虑在内部实现 `apply_chat_template` 时直接返回 Tensor。  

总体而言，此次修改解决了 Transformers 5.x 升级后 `apply_chat_template` 接口不兼容的问题，风险有限，兼容性明显提升。请确保在发布说明中标明此行为变更，以免用户在自行封装时产生冲突。

---

### [CI] Enable mypy import following for `vllm/spec_decode` (#33282)
**SHA**: `726d897` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/726d89720ca7ad4109c30fe7c5ce44456affc365)

**🎯 变更类型**：重构 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交主要在 *v1/spec_decode* 相关模块中统一对 `speculative_config` 的访问方式，去除对 `vllm_config.speculative_config` 的直接引用，并在多个位置加入了显式的 `assert` 检查和类型注解（如 `cast`、`SupportsMultiModal`）。同时在 `tools/pre_commit/mypy.py` 中把 `vllm/v1/spec_decode` 排除出 mypy 检查，以防止因新改动导致的类型检查失败。

**🎯 影响范围**  
- `vllm/v1/spec_decode/draft_model.py`  
- `vllm/v1/spec_decode/eagle.py`  
- `vllm/v1/spec_decode/medusa.py`  
- `vllm/v1/spec_decode/suffix_decoding.py`  
- `tools/pre_commit/mypy.py`（CI 配置）

**💡 关注建议**  
1. **配置完整性**：新增的 `assert vllm_config.speculative_config is not None` 防止空指针，但在生产环境中若配置缺失会直接抛异常，建议在上层捕获或提供友好的错误提示。  
2. **类型安全**：对 `get_layers_from_vllm_config` 添加 `# type: ignore[type-abstract]` 与 `cast`，确保 mypy 能通过；后续如有更细粒度的抽象层，可考虑在 `model_executor` 中统一声明抽象基类，减少局部 `cast`。  
3. **多模态兼容**：在 `eagle.py` 中对 `target_model` 的 multimodal 支持加入了 `hasattr` 检查和 `SupportsMultiModal` 接口，确保在不同模型实现间的一致性，后续若新增模型实现，请实现该接口。  
4. **CI 配置**：`tools/pre_commit/mypy.py` 将 `vllm/v1/spec_decode` 排除，暂时规避类型检查。建议在后续迭代中逐步补全对应类型注解，恢复完整的 mypy 检查以提升代码质量。  

总体来看，此次改动提升了代码的可读性与安全性，但需要关注配置缺失时的错误处理以及后续完善类型注解。

---

### [Models] Refactor Kimi-K2.5 weight loading (#33346)
**SHA**: `8bfc8d5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8bfc8d5600ed2c3c22835225b70c72b5b39fdd9f)

**🎯 变更类型**：功能增强（模型权重加载重构）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将 Kimi‑K2.5 模型的权重加载逻辑从手写的逐层匹配迁移到统一的 `AutoWeightsLoader` + `WeightsMapper` 框架。  
- 删除了原有的 `load_weights`、`get_expert_mapping`、`logits_processor` 等专属实现，改用通用加载器并通过 `weights_mapper` 完成名称映射。  
- 通过 `_mark_tower_model` / `_mark_language_model` 上下文管理器隐藏了视觉塔和语言模型的构造细节，并使用 `init_vllm_registered_model` 统一创建语言模型实例。  
- 调整了视觉投影层前缀的拼接方式，删除了不再需要的 import。  

**🎯 影响范围**  
- `vllm/model_executor/models/kimi_k25.py`（核心模型类）  
- `vllm/model_executor/models/kimi_k25_vit.py`（多模态投影层前缀）  
- 相关通用工具 `vllm/model_executor/models/utils.py`（新增 `AutoWeightsLoader`、`WeightsMapper`、`init_vllm_registered_model`）  
- 可能波及的单元测试、模型转换脚本以及使用 PP（pipeline parallel） 的部署场景。  

**💡 关注建议**  
1. **权重映射完整性**：确认 `WeightsMapper` 中的 `orig_to_new_prefix` 列表覆盖了所有旧 checkpoint 中的前缀变更（如 `mm_projector.proj.*`），防止加载时出现 “missing key” 警告。  
2. **PP 缺失层兼容**：`AutoWeightsLoader` 仍会调用 `is_pp_missing_parameter`，但原有 `PPMissingLayer` 已被移除，需确保在 pipeline‑parallel 环境下缺失的 `lm_head` 能够被正确跳过。  
3. **Logits 缩放**：原来的 `LogitsProcessor` 负责 `logit_scale`，现在直接返回 `self.language_model.compute_logits`，若模型配置里仍有 `logit_scale`，请在 `DeepseekV2Model` 或上层加入等价的 scaling。  
4. **测试覆盖**：新增/更新权重加载单元测试，覆盖 HF checkpoint、safetensors、以及分布式（TP/PP）三种情况。  
5. **文档/示例**：在模型注册说明中补充 “使用 `init_vllm_registered_model` 加载 DeepseekV2 架构” 的示例，避免用户仍按照旧方式手动实例化 `ParallelLMHead`。  

总体而言，此次重构提升了代码可维护性与加载统一性，但在分布式部署和特殊权重映射场景下需进行充分验证。若验证通过，建议合入主线。

---

### [Model][Multimodal] Add explicit MusicFlamingo adapter (#32696)
**SHA**: `c46b0cd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c46b0cd0af778e5678c52347c5b4957c1e8a0715)

**🎯 变更类型**：功能增强（为 MusicFlamingo 添加显式适配器）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 文档、示例、测试与模型注册表中新增对 `MusicFlamingo`（`nvidia/music-flamingo-2601-hf`）的支持。  
2. 在 `audioflamingo3.py` 中加入 `pos_emb` dummy 参数以兼容 MusicFlamingo 检查点，并把 `layer` 调用改为 `layer(hidden_states, attention_mask, None)`。  
3. 新增 `musicflamingo.py`：复用 AudioFlamingo3 的实现，同时在处理信息层面通过可选的 `MusicFlamingoConfig/Processor` 实现自动兼容。  

**🎯 影响范围**  
- `vllm/model_executor/models/*`（AudioFlamingo3 修改、全新 MusicFlamingoAdapter）  
- `vllm/model_executor/models/registry.py`（模型注册）  
- `vllm/multimodal`（处理器注册）  
- 示例脚本 `examples/offline_inference/audio_language.py` 与对应测试  
- 文档 `docs/models/supported_models.md`  

**💡 关注建议**  
- **兼容性**：`pos_emb` 仅为占位，确保在加载 checkpoint 时不会覆盖真实位置编码；如后续发布正式 MusicFlamingo 权重，请检查是否仍需此占位层。  
- **依赖**：MusicFlamingo 的 `Config/Processor` 仅在 `transformers>=5.0.0.dev` 时可用，使用时请锁定对应版本或保持回退到 AudioFlamingo3。  
- **示例**：`run_musicflamingo` 使用 `<sound>` 作为音频占位符，确认 tokenizer 已注册该 token，避免出现 “token not in vocab” 错误。  
- **测试**：CI 需要包含 `MusicFlamingoForConditionalGeneration` 检查点的加载路径，确保 `check_available_online` 能正确识别模型。  

总体而言，此次改动为 MusicFlamingo 提供了完整的端到端支持，需关注依赖版本与占位参数的潜在冲突。

---

### [release] Minor fixes to release annotation and wheel upload (#33129)
**SHA**: `2284461` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2284461d02e22c51a7f5ac02f0f287402ba61989)

**🎯 变更类型**：其他（CI/CD 流程调整）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 Release 流水线的 “Upload release wheels” 步骤从同时推送 PyPI+GitHub 改为仅推 PyPI，并使用新脚本 `upload‑release‑wheels‑pypi.sh`。  
2. `annotate-release.sh` 中更新 S3 路径、加入 CUDA‑13.0、CPU variant 的 wheel 下载指令，并同步改写 Docker 镜像的 pull、tag、push 与多架构 manifest（新增 `*-cu130` 系列）。  
3. `upload‑release‑wheels‑pypi.sh` 重构为 Bash‑style 条件 (`[[ … ]]`)，去除对 GH CLI 的下载与 GitHub Release 创建，仅保留 PyPI token 校验、twine 环境准备与默认 variant wheel 上传。  

**🎯 影响范围**  
- `.buildkite/` 下的 Release CI 配置与脚本；  
- 发行注释 UI（Buildkite annotation）；  
- 自动 Docker 镜像标签与多架构 manifest 生成；  
- 仅影响发布渠道，不会影响库的运行时代码。  

**💡 关注建议**  
- 确认 CI 环境已配置 `PYPI_TOKEN`（以及可选 `FORCE_RELEASE_IGNORE_VERSION_MISMATCH`），否则脚本会提前退出。  
- 本地或测试分支先跑一遍 `upload‑release‑wheels‑pypi.sh`，确保 `find … -not -name "*+*"` 能正确筛选出默认 variant。  
- 检查 S3 路径和 wheel 命名规则（`manylinux_2_31`、`manylinux_2_35`、`cu130`）与仓库已有 wheel 是否保持一致，防止用户下载失效。  
- 由于去掉了 GitHub Release 创建，如需在 GitHub 打 tag，请在其它流程中补充；否则发布日志会缺少对应的 Release 页面。  
- 关注 Docker manifest 创建的顺序和标签，尤其在 `*-cu130` 系列上线后，确保 `latest`、`v${RELEASE_VERSION}` 等标签指向预期镜像。  

整体来看，此次改动简化了发布流程、统一了 wheel 命名并扩展了 CUDA‑13.0/CPU variant，风险主要在 CI 环境变量与路径匹配上，建议在正式合并前做一次完整的发布跑通验证。

---

### Add Triton fused MoE config for B200 (Nemotron Nano) (#32804)
**SHA**: `8e2a469` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8e2a469b3b2f67bc900ed72724fe3f05e3564994)

**🎯 变更类型**：功能增强    
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `vllm/model_executor/layers/fused_moe/configs/` 目录下新增 `E=128,N=1856,device_name=NVIDIA_B200.json`，为 NVIDIA B200（Nemotron Nano）GPU 提供 Triton 3.5.1 的 fused‑MoE 参数配置。文件列举了不同 expert 数量下的 block‑size、warp、stage 等调优方案，便于在 B200 上自动选取最优 kernel。

**🎯 影响范围**  
- `vllm.model_executor.layers.fused_moe`：加载配置的路径与逻辑会新增对该 JSON 的识别。  
- Triton 编译/运行环境：要求装有 Triton 3.5.1，且对应的 CUDA 12.x 与 B200 驱动匹配。  
- 使用 fused‑MoE 的模型（如 `mixtral`, `gpt‑moe` 等）在 B200 硬件上将受益。  

**💡 关注建议**  
1. **硬件/环境**：在部署前确认机器使用的是 NVIDIA B200 且已安装 Triton 3.5.1；若仍使用旧版 Triton，配置将被忽略，回退至通用方案。  
2. **回归测试**：针对不同 expert 数目（如 1、8、128、1536）跑一遍 `model_executor` 的基准，用 `--device B200` 标记验证是否选用了新配置，检查吞吐和显存占用是否符合预期。  
3. **文档/提示**：在用户手册或 release notes 中加入 “B200 fused‑MoE config added (requires Triton 3.5.1)” 的说明，提醒用户更新 Triton。  
4. **兼容性**：若项目中有自定义 fused‑MoE 参数或手动覆盖 `fused_moe_config`，请确保不会因新增文件导致冲突；必要时在代码中加入 `if device_name == "NVIDIA_B200"` 的显式判断。  

总体来看，此配置提升了在 B200 GPU 上的 MoE 推理性能，风险主要集中在 Triton 版本不匹配或未正确识别设备名称时的回退行为，建议在 CI 中加入针对 B200 的单元/集成测试。

---

### [NVIDIA] [feat] Integrate flashinfer Trtllmgen bf16 moe (#32954)
**SHA**: `0493d89` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0493d897c4b7ed645b506bcd0452b7f3326f85f6)

**🎯 变更类型**：功能增强（在 unquantized MoE 路径中新增 FlashInfer‑TRTLLM bf16 实现）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `flashinfer_trtllm_moe.py` 中实现 bf16 版的 FlashInfer‑TRTLLM fused‑MoE kernel，并提供兼容性检查 `is_supported_config_trtllm_bf16`。  
2. 将该实现注册为自定义 Op `flashinfer_fused_moe_bf16`。  
3. `oracle/unquantized.py` 与 `unquantized_fused_moe_method.py` 增加对新后端 `FLASHINFER_TRTLLM` 的选择、权重重排与单体(monolithic)路径的调用。  
4. 新增 `convert_moe_weights_to_flashinfer_trtllm_block_layout` 用于 BF16 权重的块布局转换。  
5. `utils/flashinfer.py` 暴露 `flashinfer_trtllm_bf16_moe` 的懒加载入口。

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/*`（核心 MoE 调度与执行）  
- `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`（权重转换工具）  
- `vllm/utils/flashinfer.py`（动态导入层）  
- 环境变量 `VLLM_USE_FLASHINFER_MOE_FP16`（开启/关闭新后端）  

**💡 关注建议**  
1. **兼容性验证**：确保 BF16 权重在所有支持的 GPU（Hopper+）上通过 `has_flashinfer()` 检测后才会被选中；对不满足条件的设备保持原有路径。  
2. **权重转换成本**：`convert_moe_weights_to_flashinfer_trtllm_block_layout` 在模型加载时会复制并重新排列每个专家的权重，建议加入缓存或一次性持久化，以避免重复开销。  
3. **单体路径入口**：`forward_monolithic_cuda` 直接调用自定义 Op，需确认 `torch.ops.vllm.flashinfer_fused_moe_bf16` 已在运行时成功注册，否则会触发 fallback。  
4. **日志与提示**：已在 `select_unquantized_moe_backend` 中加入提示，建议在文档中补充如何通过 `VLLM_USE_FLASHINFER_MOE_FP16=1` 开启该特性，以及 BF16 精度的适用场景。  
5. **测试覆盖**：新增 BF16 路径后，需要在 CI 中加入：
   - FP16/FP32 与 BF16 结果数值一致性（在容忍误差范围内）。  
   - 多卡（dp、ep）组合下的性能基准。  
   - 失败回退到原有 CUTLASS 或 Triton 实现的路径。  

整体来看，此次提交为 vLLM 引入了更高效的 BF16 MoE 实现，涉及后端选择、权重布局转换和单体调用路径，需重点关注设备兼容、加载开销以及回退机制的可靠性。

---

#### 🟢 低重要度变更 (15)

### [BugFix][LoRA] TritonExperts is ModularMoEPath for FP8 models (#33393)
**SHA**: `74898a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/74898a70159f4217f1abf2a738c5cc6c4695cc44)

**🎯 变更类型**：代码重构/BugFix  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将对 fused_experts 类型的断言从 `(MarlinExperts, TritonExperts)` 简化为仅 `TritonExperts`，以兼容 FP8 模型的 ModularMoEPath，实现 LoRA 在 TritonExperts 上的正确注入。

---

### Fix `test_moe.py` for Transformers v5 (#33413)
**SHA**: `a11bc12` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a11bc12d53a55cfa395ace8af8c9956a045b1791)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/kernels/moe/test_moe.py` 中加入对 Transformers v5 的兼容处理，分别在 `ModuleList` 与新结构下加载权重并统一输出格式，确保 MoE 测试在不同版本中通过。

---

### [Doc] Enhance documentation around CPU container images (#32286)
**SHA**: `58cb55e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/58cb55e4def7ca45a1db52d967aee44fc28f959d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：完善 CPU 镜像文档，新增不同架构（x86_64、arm64）镜像拉取和运行示例，并在 Kubernetes 部署说明中加入镜像选择提示。

---

### [Misc] Clean up HIDDEN_DEPRECATED_METRICS after metric removal (#33323)
**SHA**: `cf896ae` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cf896ae0e3cde7670076fc7323a1afb71228697d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `tests/entrypoints/instrumentator/test_metrics.py` 中的 `HIDDEN_DEPRECATED_METRICS` 列表清空，移除已删除的度量项，以保持代码与实际可用指标一致。

---

### [Doc] [ROCm] Update Documentation to reflect v0.15.0 release (#33388)
**SHA**: `174f167` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/174f16700b5f5751ebfb51105e3fe130bd6c5545)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 安装指南中将示例命令和版本号从 **0.14.1** 更新为 **0.15.0**，同步最新发布的 vLLM 轮子地址。

---

### [BUGFIX] Pixtral cannot be loaded with --limit-mm-per-prompt 0 (#33406)
**SHA**: `8e2ad97` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8e2ad97ad0078f259a32e99902c90234d30c2ef4)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `_is_layer_none_or_staged` 辅助函数，并在 Pixtral 权重加载逻辑中以该函数判断 Vision 组件是否为 `None` 或 `StageMissingLayer`，解决在 `--limit-mm-per-prompt 0` 情况下模型加载失败的问题。

---

### Move decode context parallel validationn to `ParallelConfig` (#33239)
**SHA**: `d334dd2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d334dd26c4972312d9dc4f04dc7c93cdcacbd5a4)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将解码上下文并行 (DCP) 的 tp 与 dcp 大小可整除校验从 `engine/arg_utils.py` 移至 `config/parallel.py` 的 `ParallelConfig`，删除原断言，保持功能一致。

---

### [CI][AMD] Skip 4 GPUs testgroup ray tests (#33305)
**SHA**: `070c811` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/070c811d6f74c55302557878f5982411a3346b4d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 环境下检测到不支持 Ray，新增 `sys.exit` 跳过示例运行，并在分布式测试中加入平台检测，直接 `pytest.skip`，防止因 HIP_VISIBLE_DEVICES 引发错误。

---

### [BugFix] Disable async scheduling for Mamba prefix caching (#33352)
**SHA**: `ec51831` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ec51831a22cbb434646a5d8219c694ab15dbc4cb)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/config/vllm.py` 中加入判断，若启用 Mamba 前缀缓存（`mamba_cache_mode != "none"`），则强制关闭异步调度并抛出或记录警告，以避免两者不兼容导致错误。

---

### Fix `tie_word_embeddings` for multimodal models in Transformers v5 (#33359)
**SHA**: `80b918f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/80b918f2bdc627f9e4ed8c2684cbd40cc1a082ad)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Transformers v5 中多模态模型的 `tie_word_embeddings` 设置，将其从顶层配置同步到语言模型子配置，确保权重正确共享。

---

### [Docs] Adding links and intro to Speculators and LLM Compressor (#32849)
**SHA**: `1337657` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/133765760b21fde228bf1ca19fa4b35b5c206359)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修正特性表格链接指向 `spec_decode/README.md`，在量化文档加入 LLM Compressor 引导，并新增两篇文档分别介绍 LLM Compressor 与 Speculators，同时在 Speculative Decoding 页面添加相应提示链接。

---

### [Bugfix] Enable Triton MoE for FP8 per-tensor dynamic (#33300)
**SHA**: `bfb9bda` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bfb9bdaf3f44fdcd4c6ff66b5e8c84834b092534)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fused_batched_moe.py` 与 `fused_moe.py` 中新增对 `(kFp8StaticTensorSym, kFp8DynamicTokenSym)` 与 `(kFp8StaticTensorSym, kFp8DynamicTensorSym)` 的支持，修复 Triton MoE 在 FP8 per‑tensor 动态模式下的兼容性问题。

---

### [Bugfix][Kernel] Fix negative memory offset in GDN Triton kernel (#33326)
**SHA**: `23591e6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/23591e631e1982610731d3de1ed09cad16094d32)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fused_recurrent_gated_delta_rule_fwd_kernel` 中加入对 `PAD_SLOT_ID (-1)` 的检查，防止负偏移导致非法内存访问；相应地调整状态索引的加载与存储逻辑。

---

### [BUGFIX][XPU] fix memory check after XPU reuse GPU_worker  (#33358)
**SHA**: `8c8ebeb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8c8ebeb941d7bcc380b4aa685fd5201100c9eeca)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 GPU worker 中的内存检查从 “>” 改为 “≥”，防止在 XPU 重用后出现初始空闲内存等于当前空闲内存时的断言错误。

---

### [ez] Delete torch25_custom_graph_pass (#33287)
**SHA**: `5a66c9c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5a66c9cc760958036ee3f71a78f548b19e89b984)

**变更类型**：代码重构  
**重要程度**：🟢低  
**摘要**：删除 `vllm/compilation/torch25_custom_graph_pass.py`，移除针对 torch<2.6 的自定义图 pass 兼容实现，简化项目代码。

---

