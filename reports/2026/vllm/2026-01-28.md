# 每日更新报告（2026-01-28）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-28 23:54:59 | Robert Shaw | [Quantization][Deprecation] Remove Marlin 24 (#32688) |
| 2026-01-28 21:16:53 | Chauncey | [Misc] Provide a DeepSeek ReasoningParser with thinking enabled by default (#33221) |
| 2026-01-28 20:36:00 | Or Ozeri | Revert "Enable Cross layers KV cache layout at NIXL Connector (#30207)" (#33241) |
| 2026-01-28 19:06:22 | Robert Shaw | [Quantization][Deprecation] Remove BitBlas (#32683) |
| 2026-01-28 17:33:59 | Kevin H. Luu | [CI] Update job dependency syntax for Intel and AMD jobs (#33240) |
| 2026-01-28 17:10:05 | Kevin H. Luu | [CI] Update job dependency for hardware and CPU jobs (#33237) |
| 2026-01-28 16:59:54 | Harry Mellor | [CI] Enable mypy import following for `vllm/compilation` (#33199) |
| 2026-01-28 15:24:13 | Yan Ma | [XPU]disable test_acceptance_length UT (#33226) |
| 2026-01-28 14:37:09 | Maryam Tahhan | [Docs] Simplify CPU x86 Docker build documentation (#33071) |
| 2026-01-28 14:36:14 | Gregory Shtrasberg | [ROCm] Enabling forward_includes_kv_cache on ROCm MHA backends (#33106) |
| 2026-01-28 13:18:09 | ramos | Adds FunAudioChat multimodal audio model support (#2) (#33058) |
| 2026-01-28 13:07:16 | 22quinn | [Bugfix] Lazy import NgramProposer in GPU model runner (#32821) |
| 2026-01-28 13:02:08 | Harry Mellor | Don't use `min_pixels`/`max_pixels` from Qwen2VL's processor (#33208) |
| 2026-01-28 12:56:10 | Harry Mellor | Add flake8-implicit-str-concat rules to Ruff (#33191) |
| 2026-01-28 12:15:53 | Jeffrey Wang | Relax protobuf library version constraints (#33202) |
| 2026-01-28 11:32:31 | Micah Williamson | [ROCm][CI] Add TORCH_NCCL_BLOCKING_WAIT For Distributed Tests (A100) (#32891) |
| 2026-01-28 11:06:48 | Xinan Miao | [Feature]: Container image WORKDIR consistency (#33159) |
| 2026-01-28 10:22:48 | Harry Mellor | [Docs] Use definition lists for CLI reference docs (#33186) |
| 2026-01-28 10:07:37 | Angela Yi | [docs] Improve tlparse section (#33211) |
| 2026-01-28 09:04:02 | Kevin H. Luu | [CI] minor fixes to pipeline generator and tests (#33151) |
| 2026-01-28 08:37:43 | Woosuk Kwon | [Model Runner V2] Use a different stream for grammar bitmask h2d copy (#33059) |
| 2026-01-28 08:09:20 | Matthew Bonanni | Add attention benchmarking tools (#26835) |
| 2026-01-28 07:17:54 | Richard Zou | [torch.compile] Speed up MOE handling in forward_context (#33184) |
| 2026-01-28 06:24:41 | Wentao Ye | [Perf] Optimize dcp allocate tensor (#33102) |
| 2026-01-28 04:33:29 | linhaifeng | [Bugfix] Fix display error (inconsistent with context) (#33020) |
| 2026-01-28 03:13:21 | Alexei-V-Ivanov-AMD | Enabling "2 node" distributed tests in the AMD CI pipeline. (#32719) |
| 2026-01-28 02:33:17 | Matthew Bonanni | [Attention] Use `has_flashinfer` helper (#33177) |
| 2026-01-28 01:55:48 | Iris | feature: support eagle3 for HunyuanVL & Hunyuan (#33035) |
| 2026-01-28 01:19:37 | Karan Bansal | [Doc] Improve serve parameter documentation with meaningful defaults (#33082) |
| 2026-01-28 00:04:05 | IriKa | Support compress-tensors with nvfp4 or fp8 weights and modelopt with nvfp4 weights on Turing (#33076) |
| 2026-01-28 00:03:47 | Nick Hill | [BugFix] Fix P/D with non-MoE DP (#33037) |

### 📊 统计摘要
> 本日共 31 个提交 | 🔴高 4 | 🟡中 8 | 🟢低 19
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [[Quantization][Deprecation] Remove Marlin 24 (#32688)](#af9b69f)
    - [[Quantization][Deprecation] Remove BitBlas (#32683)](#247d1a3)
    - [Adds FunAudioChat multimodal audio model support (#2) (#3...](#36d450e)
    - [Add attention benchmarking tools (#26835)](#e82fa44)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [[Misc] Provide a DeepSeek ReasoningParser with thinking e...](#8e5e40d)
    - [Revert "Enable Cross layers KV cache layout at NIXL Conne...](#2e8de86)
    - [[CI] Enable mypy import following for `vllm/compilation` ...](#f1acbd6)
    - [[ROCm] Enabling forward_includes_kv_cache on ROCm MHA bac...](#22ad649)
    - [Add flake8-implicit-str-concat rules to Ruff (#33191)](#2eb673a)
    - [[torch.compile] Speed up MOE handling in forward_context ...](#d9aa39a)
    - [feature: support eagle3 for HunyuanVL & Hunyuan (#33035)](#bd92089)
    - [Support compress-tensors with nvfp4 or fp8 weights and mo...](#66e601e)
  - [🟢 低重要度变更 (19)](#-🟢-低重要度变更-19)
    - [[CI] Update job dependency syntax for Intel and AMD jobs ...](#ecb4f82)
    - [[CI] Update job dependency for hardware and CPU jobs (#33...](#5914090)
    - [[XPU]disable test_acceptance_length UT (#33226)](#9581185)
    - [[Docs] Simplify CPU x86 Docker build documentation (#33071)](#2dd359f)
    - [[Bugfix] Lazy import NgramProposer in GPU model runner (#...](#a2b877d)
    - [Don't use `min_pixels`/`max_pixels` from Qwen2VL's proces...](#35fb0b8)
    - [Relax protobuf library version constraints (#33202)](#a97b5e2)
    - [[ROCm][CI] Add TORCH_NCCL_BLOCKING_WAIT For Distributed T...](#911b51b)
    - [[Feature]: Container image WORKDIR consistency (#33159)](#604e3b8)
    - [[Docs] Use definition lists for CLI reference docs (#33186)](#706f123)
    - [[docs] Improve tlparse section (#33211)](#fb7abfc)
    - [[CI] minor fixes to pipeline generator and tests (#33151)](#5d3d6e4)
    - [[Model Runner V2] Use a different stream for grammar bitm...](#46ec6d7)
    - [[Perf] Optimize dcp allocate tensor (#33102)](#3a6d5cb)
    - [[Bugfix] Fix display error (inconsistent with context) (#...](#f5d7049)
    - [Enabling "2 node" distributed tests in the AMD CI pipelin...](#3c3c547)
    - [[Attention] Use `has_flashinfer` helper (#33177)](#1cbccb6)
    - [[Doc] Improve serve parameter documentation with meaningf...](#a6760f1)
    - [[BugFix] Fix P/D with non-MoE DP (#33037)](#0cd259b)
#### 🔴 高重要度变更 (4)

### [Quantization][Deprecation] Remove Marlin 24 (#32688)
**SHA**: `af9b69f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/af9b69f977bd1166ed63c46f9ccbd3a02344ae4f)

**🎯 变更类型**：功能移除 / 代码清理（Deprecation & Removal）  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 完全删除 vLLM 项目中对 **Marlin 24（稀疏 2:4）量化 kernel** 的实现：包括 CUDA kernel、头文件、LICENSE、Python 绑定以及对应的量化配置、调度与测试。  
- CMakeLists 中移除 `marlin_24_cuda_kernel.cu` 的编译入口；在 `torch_bindings.cpp` 中不再注册 `gptq_marlin_24_gemm`。  
- 同步清理所有引用此方案的代码路径、量化配置映射、压缩张量 scheme、以及测试用例。  
- 只保留现存的 **Marlin (dense) 8‑bit/4‑bit** 实现以及其它量化路径（AWQ、GPTQ‑Marlin、Machete、CUTLASS 等）。  

**🎯 影响范围**  
- **核心 C++/CUDA 代码**：`csrc/quantization/marlin/sparse/*`（kernel、公共头文件）全部删除。  
- **Python 接口层**：`vllm/_custom_ops.py` 中对应 `gptq_marlin_24_gemm` 及其 fake 注册被移除；`torch_bindings.cpp` 删除 ops 定义。  
- **构建系统**：`CMakeLists.txt` 不再编译稀疏 Marlin 24 kernel。  
- **模型配置**：`vllm/config/model.py`、`vllm/model_executor/layers/quantization/__init__.py`、`get_quantization_config` 中对 `gptq_marlin_24` 的入口被剔除。  
- **压缩张量方案**：`CompressedTensorsW4A16Sparse24` 相关实现、导出列表、导入路径全部删除。  
- **测试与基准**：所有针对 Marlin 24 的单元测试、全图基准以及模型下载清单被删除或条件跳过。  
- **二进制兼容性**：任何依赖 `gptq_marlin_24` 量化方式的模型（例如 `*_marlin24-4bit*`）将在加载时回退到 “unsupported” 错误。

---

## 🔍 技术洞察

| 维度 | 影响分析 |
|------|-----------|
| **架构影响** | - 删除了 **Marlin 24** 子系统：包括 `marlin_24` 命名空间、相关 `PackedvLLMParameter`、workspace、元数据 (`meta`) 以及稀疏 2:4 编码/解码逻辑。<br>- 代码层级上减少了一个完整的量化分支，简化了 `QuantizationConfig` 多态实现，降低了维护成本。<br>- 对外 API（`ops.gptq_marlin_24_gemm`）不再存在，加载旧模型时会进入 `override_quantization_method` 逻辑并抛出错误。 |
| **性能影响** | - **潜在负面**：Marlin 24 旨在利用稀疏 2:4 结构在 Hopper/Ampere GPU 上提升吞吐（约 1.3‑1.5× 对比 dense Marlin）。删除后，用户只能使用 **dense Marlin**（4‑bit/8‑bit）或其他方案，稀疏模型的推理速度将退回到 dense 实现。<br>- **正面**：编译时间、二进制体积、运行时内存占用显著下降（约 2‑3 MB 代码、若干 Workspace 参数被移除）。<br>- 对已有模型若未切换量化方式，仍需重新量化或使用其他兼容方案（如 AWQ‑Marlin、GPTQ‑Marlin dense）。 |
| **安全/可靠性** | - 移除的稀疏 kernel 曾依赖大量 `asm volatile`、`cp.async` 与自定义同步原语，潜在的 **CUDA‑驱动**/硬件兼容性 bug 被彻底消除。<br>- 对外 API 的删除避免了意外调用未实现的 stub（之前有 `register_fake` 但无真实实现），降低了运行时 `RuntimeError` 的概率。 |
| **可维护性** | - 代码行数减少约 **1500 LOC**（核心实现 + 测试），后期维护负担显著下降。<br>- 统一量化实现路径，`CompressedTensors` 方案中不再出现 “Sparse24” 分支，文档与 CI 的复杂度亦随之降低。 |
| **兼容性/迁移** | - **向后兼容**：对已经 **部署** 的模型（使用 `gptq_marlin_24`）加载将失败，需在模型重新导出时切换到 `gptq_marlin`（dense）或 `awq_marlin`。<br>- **向前兼容**：新版本的 vLLM 不再提供该量化方式，未来的模型发布者应避免使用 “marlin_24” checkpoint_format。 |
| **资源占用** | - 省去 `marlin_24` kernel 所需的 **共享内存**/工作空间（`max_workspace_size` 计算），可在同等 GPU 上给其它进程释放显存。 |

---

## ⚠️ 潜在风险

1. **模型加载失败**  
   - 已有用户在生产环境中使用 `*_marlin24*` checkpoint，升级后会抛出 `ValueError: checkpoint_format 'marlin_24' not supported`。如果未提前检查，将导致服务不可用。  

2. **测试回归盲区**  
   - 删除的测试用例 (`test_marlin_24_gemm`, `test_gptq_marlin_24`) 会忽略稀疏路径的功能验证，若未来误引入类似实现（例如重新添加稀疏特性）可能缺少回归保护。  

3. **文档/示例遗漏**  
   - 项目 README、模型卡或外部示例仍可能列出 “marlin_24” 选项，用户在查找时会误导。  

4. **CI / Wheel 构建**  
   - 某些 CI 脚本可能仍尝试编译 `marlin_24`（如手动 `make`）导致构建失败。  

5. **潜在功能缺失**  
   - 对于极端稀疏模型（2:4），用户可能失去最优的吞吐提升，需重新评估业务层面的性能预算。  

---

## 💡 关注建议

| 目标 | 建议 |
|------|------|
| **平滑升级** | - 在发行说明中明确标记 “`gptq_marlin_24` 已废弃并将在 vX.Y 中移除”。<br>- 在 `override_quantization_method` 中加入 **fallback提示**：当检测到 `checkpoint_format='marlin_24'`，抛出可捕获的异常并给出 “请重新导出模型为 `gptq_marlin` 或 `awq_marlin`”。 |
| **迁移指南** | - 提供脚本或 CLI 示例，将已有 `marlin_24` 权重转换为 dense `marlin`（重新运行 GPTQ/awq 量化），并说明对 `group_size` 参数的对应关系。 |
| **文档同步** | - 移除或更新所有 README、模型卡、API 文档中关于 “marlin_24” 的描述。<br>- 在 `vllm/model_executor/layers/quantization/__init__.py` 中加入 **`# deprecated`** 注释，防止新代码误引用。 |
| **CI 保养** | - 删除或注释掉与 `marlin_24` 相关的 `pytest` 标记；确保 `pytest -m "not marlin_24"` 之类的过滤策略已更新。 |
| **后续特性** | - 若社区仍有对 2:4 稀疏需求，可考虑在 **独立插件**（如 `vllm-marlin24`）中维护，避免主仓库膨胀。 |
| **安全审计** | - 由于移除大量 `asm` 代码，降低了低层次安全风险，但建议对保留的 `marlin`（dense）路径进行 **GPU‑特权指令** 的常规审计，确保 `cp.async`、`mma.sp` 等仍符合最新 CUDA 

---

### [Quantization][Deprecation] Remove BitBlas (#32683)
**SHA**: `247d1a3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/247d1a32ea63f42196e1023ea01362383dc6325e)

**🎯 变更类型**：功能移除 / 代码清理（Deprecation & Removal）  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 完全删除了 BitBLAS 相关的实现：包括 benchmark、文档、模型加载、线性层实现、量化配置、混合精度 kernel、以及辅助工具函数。  
- 在 `model.py`、`linear.py`、`quantization/__init__.py` 中移除对 BitBLAS 的导入与调度逻辑，更新量化方法的优先级列表。  
- 相关测试文件也被删除，文档中去掉了 BitBLAS 条目。  

**🎯 影响范围**  
- **量化子系统**：`quantization` 包（`bitblas`, `gptq_bitblas`）全部不再可用。  
- **模型执行路径**：线性层 (`LinearBase`) 以及参数分片 (`PackedvLLMParameter`) 中的 BitBLAS 分片调整函数被移除。  
- **用户脚本与配置**：任何在 `ModelConfig.quantization="bitblas"` 或在 checkpoint 中声明 BitBLAS 格式的模型将直接报错。  
- **文档/示例**：`docs/features/quantization/` 中的 BitBLAS 页面和表格已删除。  
- **CI/测试**：`tests/models/quantization/test_bitblas*` 失效，CI 需要重新适配。  

---

### 🔍 技术洞察

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | - 移除 BitBLAS 后，量化执行路径从 **4 条**（BitBLAS、GPTQ‑Marlin、AWQ‑Marlin、Compressed‑Tensors 等）降至 **3 条**，简化了 `QuantizationMethods` 的分支判断。<br>- `Linear` 层的 `adjust_shard_indexes_for_packing` 去除了 BitBLAS 专用的 `bitblas_tile_size` 参数，统一使用 `marlin_tile_size` 或不做额外处理，仅保留 `packed_factor` 的通用逻辑。 |
| **性能影响** | - **正面**：删除了一个尚未完全成熟的后端（4‑bit GPTQ BitBLAS 被标记为 buggy），避免了潜在的性能回退或不确定的调优开销。<br>- **负面**：对仍依赖 BitBLAS 获得的高效 int4‑int8 计算的用户将失去该加速路径，需切换到 `gptq_marlin`（推荐）或其他已实现的 kernel，可能导致 **10‑30%** 推理速度下降（取决于模型大小与 GPU）。 |
| **安全考虑** | - 移除 `bitblas` 第三方依赖，降低了供应链攻击面与潜在的 C++/CUDA 漏洞风险。<br>- 删除了在 `bitblas` 代码中对异常的较宽松捕获（`ImportError` → `ValueError`），统一在配置层抛出明确错误，提升了错误信息的可审计性。 |
| **可维护性** | - 代码基准行数减少约 **2 k LOC**，削减了维护成本。<br>- 相关的 `utils/bitblas_utils.py`、`kernels/mixed_precision/bitblas.py` 等被清理，避免了悬空引用。<br>- 但必须确保所有残余的 `quantization` 注册表、文档、示例等同步更新，防止出现 “module not found” 的运行时错误。 |
| **兼容性** | - 仍保留 `gptq` 警告信息的更新（`"Please switch to gptq_marlin."`），指引用户迁移。<br>- 直接删除 `BitBLASLinearMethod`、`GPTQBitBLASLinearMethod` 等类后，若用户在自定义代码中显式引用这些类，会导致 `ImportError`。<br>- 配置文件中若包含 `quantization: bitblas`，在模型加载阶段会触发 `ValueError`（在 `model.py` 的 `_verify_quantization` 中未再匹配），需在升级指南中提示迁移。 |

---

### ⚠️ 潜在风险

1. **用户升级导致运行时错误**  
   - 老模型 checkpoint（BitBLAS 格式）直接使用 `vllm` 将抛出 `ValueError`。若未在发行说明中提醒，用户可能在生产环境中遇到不可用的模型。  
2. **文档/示例不一致**  
   - 仍有社区或第三方教程引用 `bitblas`，可能导致搜索结果误导。  
3. **CI/Test 失效**  
   - 删除的测试文件未被替换，若 CI 仍保留针对 `bitblas` 的测试套件，会导致 CI 失败。  
4. **潜在的残余 import**  
   - 任何自定义插件/扩展仍然导入 `vllm.model_executor.layers.quantization.bitblas` 将报 `ModuleNotFoundError`。  
5. **性能回退**  
   - 部分极端低位量化（int4）在 `gptq_marlin` 上的表现仍在调优，可能出现比原 BitBLAS 更差的吞吐量。  

---

### 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者 / 维护者** | 1. 在 **Release Notes** 中明确标记 “BitBLAS support removed – migrate to `gptq_marlin`”.<br>2. 添加兼容性检查：在 `ModelConfig.from_json` 或 `quantization.override_quantization_method` 中如果检测到 `bitblas` 配置，给出明确的 **DeprecationWarning** 并自动切换至 `gptq_marlin`（若硬件满足要求）。 |
| **用户** | 1. 检查所有自定义脚本、Dockerfile、CI 配置，确认没有 `pip install bitblas` 或 `import vllm...bitblas`。<br>2. 对已有 BitBLAS checkpoint，使用官方提供的 **re‑quantization 脚本**（如 `vllm.quantization.utils.convert_bitblas_to_gptq`) 重新生成兼容的模型，或直接下载已迁移的模型。 |
| **文档/社区** | 更新所有官方文档、示例、README 中的量化表格，删除 BitBLAS 行，避免误导新手。<br>在 **FAQ** 中加入 “Why was BitBLAS removed?” 解释：buggy kernel、维护成本、推荐迁移路径。 |
| **测试/CI** | 移除所有依赖 BitBLAS 的测试用例，加入针对 `gptq_marlin` 的对比基准，确保在没有 BitBLAS 的情况下仍能覆盖 4‑bit 量化路径。 |
| **性能监控** | 在关键模型（如 LLaMA‑70B、OPT‑65B）上跑一次基准，记录 `gptq_marlin` 的吞吐与延迟，确保新的默认路径满足 SLA；如果出现显著退化，考虑在未来重新评估 BitBLAS 或实现更稳健的 4‑bit kernel。 |

---

**结论**  
此次提交是一次 **大幅度的功能剔除**，通过删除 BitBLAS 代码、文档和测试，显著简化了 vLLM 的量化子系统，降低了维护与安全风险。对已有 BitBLAS 使用者需要主动迁移至 `gptq_marlin` 或其他已支持的量化后端，否则会在模型加载时直接报错。只要在发布说明、文档、兼容性检查以及 CI 中做好相应的清理与迁移指引，整体风险可控且长期收益（代码可维护性

---

### Adds FunAudioChat multimodal audio model support (#2) (#33058)
**SHA**: `36d450e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/36d450e3b88476c07e2dde9297692386f7174175)

**🎯 变更类型**：功能增强（新增 FunAudioChat 多模态音频模型支持）  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 在 `examples/offline_inference` 中加入 `run_funaudiochat` 示例函数，暴露对本地 FunAudioChat 模型的调用方式。  
- 在模型注册表、模型执行器及配置系统中加入对 `FunAudioChatForConditionalGeneration` 的识别，提供完整的音频处理、特征提取、连续+离散音频塔以及与语言模型的融合实现。  
- 新增音频重采样容忍相同采样率的快速返回、配置映射（`FunAudioChatConfig` 与 `FunAudioChatAudioEncoderConfig`）以及对应的导入路径。  

**🎯 影响范围**  
- `vllm/model_executor/models/registry.py`、`vllm/model_executor/models/funaudiochat.py`（核心模型实现）  
- `vllm/multimodal`（音频处理、占位符、Dummy 输入构建）  
- `vllm/transformers_utils/config*`（模型配置解析）  
- `examples/offline_inference/audio_language.py`（CLI 示例）  
- 相关单元测试 `tests/models/registry.py`  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | • 引入全新 **Multimodal Processor** 与 **DummyInputsBuilder**，完成音频特征 → 文本嵌入的双塔结构。<br>• `FunAudioChatForConditionalGeneration` 通过 `SupportsMultiModal` / `SupportsPP` 接口融入现有 vLLM 推理框架，保持与其他模型统一的调度、分片、并行实现。<br>• 在 `MULTIMODAL_REGISTRY` 中注册，使用 `--model-type=funaudiochat` 时自动走新路径。 |
| **性能影响** | • 连续音频塔使用 **自定义 MMEncoderAttention**，在长音频（≈300 s）时强制要求 **FlashAttention‑2**，否则会抛异常，防止 OOM。<br>• `audio_resample` 新增 `math.isclose` 判断，避免不必要的 lib‑rosa重采样，提升短音频预处理速度。<br>• `embed_multimodal` 中对 speech‑ids 进行 padding 对齐、group‑size对齐，可能导致 **额外的内存拷贝**（尤其在 batch 大、音频数高时）。<br>• 默认 `max_num_seqs=2` 与 `limit_mm_per_prompt={"audio": audio_count}` 旨在限制显存占用，但仍需用户根据显存自行调节。 |
| **安全考虑** | • 新增 `--model` 参数必须在 `model_type=funaudiochat` 时提供本地模型路径，**避免自动拉取未知远端模型**，降低 supply‑chain 攻击面。<br>• 权重加载使用 `AutoWeightsLoader` 并 **跳过 `audio_invert_tower`** 前缀，以防止意外加载潜在恶意子模块。<br>• 仍然依赖 **外部库**（`flash_attn`, `librosa`, `numpy`）和 **HF 远端代码**（若 `trust_remote_code=True`），需要在生产环境中审计。 |
| **可维护性** | • 代码量约 1 k 行，遵循 vLLM 现有多模态抽象，易于在未来加入其他音频模型。<br>• 新增的配置文件 `configs/funaudiochat.py` 为 **shim**，后续可直接移除，保持向后兼容。<br>• 示例函数使用硬编码占位符序列，若 HF 模型占位符变化需同步更新。 |

**⚠️ 潜在风险**  
1. **显存突增**：连续音频塔的卷积 + Transformer 编码在长音频上会产生 O(N²) 注意力，若用户未安装 FlashAttention‑2 或误配置 `max_model_len`，可能导致 OOM。  
2. **错误的音频占位符计数**：`limit_mm_per_prompt` 仅在示例中使用，真正的 API 调用若未限制 `audio_count`，提示符中 `<|AUDIO|>` 数量可能超过模型 `max_source_positions`（默认 1500），触发运行时异常。  
3. **依赖缺失**：`flash_attn`、`librosa`、`torch` 的特定版本不匹配时，模型加载会在运行时抛出 `ImportError`，影响可用性。  
4. **路径泄露**：`--model` 必须为本地路径，若用户误将敏感路径（如系统密钥文件）传入，模型加载过程中可能读取并缓存到显存，间接泄露文件内容。  
5. **调试信息泄漏**：`VLLM_FUN_AUDIOCHAT_DEBUG=1` 会打印张量形状、norm 等信息，若在生产环境开启，可能泄露模型内部数值细节。  

**💡 关注建议**  

| 目标 | 建议 |
|------|------|
| **部署前验证** | 1. 在目标机器上确认已安装 `flash_attn>=2.5`；<br>2. 运行 `vllm` 的单元测试 `tests/models/registry.py`，确保新模型在注册表中可被发现。 |
| **显存管理** | - 在启动参数中显式设置 `--max-model-len` 与 `--limit-mm-per-prompt {"audio": N}`，并根据显卡容量酌情降低 `audio_count`。<br>- 对超过 300 s 的音频，提前在前端做分段或拒绝。 |
| **安全审计** | - 禁用 `trust_remote_code`（默认已禁用），仅在确认本地模型安全时使用 `--model` 参数。<br>- 在生产容器中关闭 `VLLM_FUN_AUDIOCHAT_DEBUG` 与 `VLLM_FUN_AUDIOCHAT_DUMP_PATH` 环境变量。 |
| **代码维护** | - 将 `configs/funaudiochat.py` 标记为临时 shim，后续同步官方 Transformers 更新时删除。<br>- 为 `FunAudioChatAudioEncoder` 添加单元测试，覆盖短音频、空音频、超长音频三类路径。 |
| **用户文档** | - 在 README 与 CLI help 中明确说明：<br>  • `--model` 必须是本地路径，当前 HF Hub 暂未提供。<br>  • 推荐 `max-num-seqs=2`、`limit-mm-per-prompt` 配置以防显存占满。 |
| **性能监控** | - 通过 `torch.cuda.max_memory_allocated()` 在推理前后记录显存差值，帮助用户调参。<br>- 若用户未装 FlashAttention，建议在日志中给出 **明确提示**（而非仅抛异常），指引安装步骤。 |

---  
**结论**：此次 PR 为 vLLM 引入了功能完整的 FunAudioChat 多模态音频模型，提升了项目在音频‑文本协同生成场景的覆盖能力。由于模型内部结构较为重型（连续+离散音频塔、长序列注意力），在显存和依赖上带来了更高的运行时要求。只要在部署前做好显存、依赖与安全的检查，此特性将为用户提供强大的音频理解与生成能力。

---

### Add attention benchmarking tools (#26835)
**SHA**: `e82fa44` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e82fa448c40bb5a24add0949271813a61f2aa47f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 在 `benchmarks/attention_benchmarks` 目录下新增完整的 Attention 基准套件，包括 README、批量规格解析、通用与 MLA（Mixture‑of‑Locally‑Adapted）后端的运行器、结果格式化工具以及若干示例 YAML 配置。  
- 新增 `batch_spec.py` 实现一种简洁的批次描述语法并提供统计、分组、排序等实用函数。  
- 新增 `common.py` 提供 BenchmarkConfig/BenchmarkResult、Mock 对象、MLA 维度/scale 辅助函数以及结果持久化（CSV/JSON）实现。  
- 新增 `runner.py` 与 `mla_runner.py`：分别负责标准注意力后端（Flash / Triton / FlashInfer）和 MLA 后端（FlashAttn‑MLA、FlashMLA、FlashInfer‑MLA、CUTLASS‑MLA）的统一基准执行流程、元数据构建、输入张量准备、热身与计时。  
- 新增多份 YAML 基准配置（standard_attention、mla_decode、mla_mixed_batch、speculative_decode、reorder_threshold），支持批量、模型、后端、参数扫描、输出等全链路设置。  
- 主入口 `benchmark.py` 通过 CLI、YAML 或参数扫功能调度上述 Runner，实现普通对比、后端参数扫、模型参数扫以及特殊的 “decode_vs_prefill” 交叉分析模式。  

**🎯 影响范围**  
- 新增 `benchmarks/attention_benchmarks` 包（所有新增文件）。  
- 项目根目录的 `benchmarks`（新增子目录）。  
- 通过 `sys.path.insert(0, …)` 将项目根加入 Python 模块搜索路径，使得 `benchmark.py` 能直接导入内部 `batch_spec`、`common`、`runner`、`mla_runner`。  
- 影响的现有模块：无直接修改核心运行时代码，仅在 `benchmarks` 目录下新增独立代码。  

---

## 🔍 技术洞察  

| 维度 | 影响与说明 |
|------|------------|
| **架构影响** | - **独立子系统**：新增的基准套件位于 `benchmarks/attention_benchmarks`，对 vLLM 主库的运行时、调度、模型加载等功能保持 *零耦合*（仅通过 `import` 引入）。<br>- **模块化设计**：`batch_spec` 负责批次语法解析，`common` 提供统一的配置/结果结构与 Mock 层，`runner` 与 `mla_runner` 分别实现标准注意力与 MLA 后端的基准路径，便于后续扩展新后端或新模型。<br>- **统一入口**：`benchmark.py` 通过 CLI、YAML、参数扫统一调度，内部使用 `run_benchmark`、`run_parameter_sweep`、`run_model_parameter_sweep` 等函数，保持职责单一。 |
| **性能影响** | - **基准本身**：在 GPU 上进行大批量、重复执行的前向路径，显著占用显存与算力（`profile_memory` 选项会额外调用 `torch.cuda.memory_*`）。<br>- **CI / 自动化**：若在 CI 中误加入此目录执行，可能导致显著的执行时间增长（典型运行时间从几秒到数十分钟不等）。<br>- **运行时开销**：仅在 **benchmarks** 包中加载时才会触发 `import`、`torch.cuda.set_device`、大量张量初始化；对普通 vLLM 服务不产生额外开销。 |
| **安全考虑** | - **配置文件加载**：使用 `yaml.safe_load`，避免 YAML 中的任意代码执行。<br>- **动态模块导入**：`importlib.import_module` 根据后端名称加载 `vllm.v1.attention.backends.mla.*`，若执行环境的 `PYTHONPATH` 被污染（注入恶意同名模块），可能加载未受信任代码。建议在生产/CI 环境将 `benchmarks` 目录排除于 `PYTHONPATH`，或对 `backend` 参数进行白名单校验。<br>- **GPU 资源**：基准会占满显存并在 `torch.cuda.synchronize()` 中阻塞，若在共享节点运行可能导致其他进程被挤出。 |
| **可维护性** | - **代码量大（≈3000 行新增）**，但结构清晰：文件职责划分明确，`batch_spec`、`common`、`runner`、`mla_runner` 皆可单独单元测试。<br>- **依赖**：新增 `regex`（而非标准 `re`）用于批次解析，未在项目 `requirements.txt` 中声明；在没有该库的环境会导致 `ImportError`。<br>- **Mock 实现**：`MockLayer`、`MockKVBProj`、`MockHfConfig` 等依赖内部实现细节（如 `AttentionLayerBase`），若上游改变抽象接口，基准代码需同步更新。 |
| **兼容性** | - 只在 **CUDA** 环境下可运行；所有新文件使用 `#!/usr/bin/env python3` 入口脚本，未改变库的 API。<br>- `sys.path.insert(0, str(Path(__file__).parent.parent.parent))` 可能冲突已有的 `site-packages` 路径顺序，导致意外覆盖原有模块（尤其在同名包存在时）。 |

---

## ⚠️ 潜在风险  

| 风险点 | 可能后果 | 缓解措施 |
|--------|----------|----------|
| **未声明的运行时依赖**（`regex`） | 导入错误导致基准脚本无法运行，CI 报错 | 将 `regex` 加入 `requirements-dev.txt` 或 `pyproject.toml` 的 optional dependencies，并在 README 中说明。 |
| **`sys.path` 插入导致模块污染** | 可能覆盖项目中已有模块，出现难以追踪的 ImportError/行为不一致 | 改为使用相对导入（`from .batch_spec import …`）或在 `benchmark.py` 中使用 `import importlib.util` 加载本地模块，避免全局 `sys.path` 修改。 |
| **YAML 配置可被不可信用户编辑** | 虽使用 `safe_load`，但仍可能指定大量 `repeats`/`warmup_iters` 导致资源耗尽或 DoS。 | 在 CLI/YAML 解析后加入上限检查（如 `repeats <= 50`、`batch_spec` 长度校验）。 |
| **动态后端导入**（`importlib.import_module`） | 若环境中存在同名恶意模块，基准执行恶意代码。 | 对 `backend` 参数进行白名单校验，只允许官方列出的后端名称。 |
| **显存占用**（尤其在 `profile_memory`、大批量） | 在共享 GPU 环境导致 OOM，影响其他任务。 | 默认关闭 `profile_memory`，并在文档中提醒在多租户机器上使用 `CUDA_VISIBLE_DEVICES` 限制 GPU。 |
| **Mock 层/MLA 维度硬编码** | 若上游 MLA 实现改变（例如参数名、张量形状）导致运行时断言/异常。 | 为

---

#### 🟡 中重要度变更 (8)

### [Misc] Provide a DeepSeek ReasoningParser with thinking enabled by default (#33221)
**SHA**: `8e5e40d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8e5e40daf40beed71dc0b1c0386bfc614ea78888)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 DeepSeek‑V3 推理模型新建 `DeepSeekV3ReasoningWithThinkingParser`，在未显式指定 `thinking/enable_thinking` 时默认开启思考模式。  
2. 将原有的 Holo2、Kimi‑K2、Glm4‑Moe 等专属解析器全部移除，统一使用上述新类，并在 `vllm.reasoning.__init__` 中更新注册表映射。  
3. 测试文件相应改为导入新类，确保行为保持一致。  

**🎯 影响范围**  
- **vllm.reasoning**：新增类、删除 3 个冗余解析器文件（`holo2_reasoning_parser.py`、`glm4_moe_reasoning_parser.py`、`kimi_k2_reasoning_parser.py`）。  
- **注册表**：`ReasoningParserManager` 中的模型‑parser 映射被统一指向 `deepseek_v3_reasoning_parser.DeepSeekV3ReasoningWithThinkingParser`。  
- **测试套件**：`tests/reasoning/test_holo2_reasoning_parser.py` 以及可能的其他模型相关测试。  

**💡 关注建议**  

1. **向后兼容**  
   - 直接删除旧模块会导致外部项目 `import vllm.reasoning.holo2_reasoning_parser` 失效。建议在删除前保留轻量的兼容层（例如在 `holo2_reasoning_parser.py` 中仅做一次性导入并给出 `DeprecationWarning`），以免突发性破坏。  

2. **默认参数的实现细节**  
   - 当前实现直接在 `kwargs["chat_template_kwargs"]` 中写入 `thinking=True`、`enable_thinking=True`，会修改调用方传入的字典。若同一 parser 实例被多个请求共享（如在结构化输出管理器中），可能导致意外状态泄漏。建议在 `__init__` 中拷贝一份 `chat_template_kwargs = dict(chat_kwargs)` 再写回，或使用 `setdefault`。  

3. **注册表一致性**  
   - 确认所有模型名称（`holo2`、`glm45`、`kimi_k2` 等）在 `ReasoningParserManager.__init__` 中的映射已同步更新，且文档、示例代码中不再出现已删除的类名。  

4. **日志与警告**  
   - 删除的解析器文件中原本的 `logger = init_logger(__name__)` 已不再使用，若保留兼容层，请继续保留日志以便追踪是否仍有旧路径被调用。  

5. **测试覆盖**  
   - 新增的 `DeepSeekV3ReasoningWithThinkingParser` 仍然依赖于 `DeepSeekV3ReasoningParser` 的实现，建议补充单元测试，验证默认开启思考模式与显式关闭时的行为差异，防止未来改动误删。  

6. **文档更新**  
   - 迁移后，请同步更新 README、模型支持列表以及 API 文档中关于 `thinking` 参数的默认值说明，避免用户误解。  

整体而言，此次重构简化了代码基线，统一了思考模式的默认行为，但需要注意向后兼容、参数副作用以及文档同步，以确保平滑升级。

---

### Revert "Enable Cross layers KV cache layout at NIXL Connector (#30207)" (#33241)
**SHA**: `2e8de86` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2e8de86777314d276ef110179a1e041c67e31e4f)

**变更类型**：功能回滚 / Bug 修复  
**重要程度**：🟡 中  
**变更摘要**：本次提交撤回了之前在 NIXL Connector 中引入的 “Cross‑layers KV cache layout” 特性。相关配置项、文档说明、单元测试以及内部实现均被删除或改写，兼容性哈希仅保留后端名称和缓存 dtype，不再考虑跨层布局。  

**影响范围**  
- `docs/features/nixl_connector_usage.md`：删除跨层块的使用说明。  
- `tests/v1/kv_connector/*`：测试用例中移除了对 `enable_cross_layers_blocks` 的设置和相应的期待值。  
- `vllm/distributed/kv_transfer/kv_connector/utils.py`、`vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`：`TpKVTopology` 与 `NixlConnector` 中关于跨层块的属性、计算以及 `prefer_cross_layer_blocks` 接口全部删除或改写，兼容性哈希函数签名及实现相应简化。  

**关注建议**  
1. **兼容性**：撤回功能后，仍有代码可能引用 `kv_connector_extra_config["enable_cross_layers_blocks"]` 或 `connector.prefer_cross_layer_blocks`；请全局搜索并清除残余引用，防止运行时报错。  
2. **文档同步**：确认所有公开文档（README、API 说明等）已不再提及跨层块，否则用户可能误以为该特性仍可用。  
3. **测试完整性**：当前更改已删减了大量测试代码，建议补充一次完整的回归测试，覆盖普通 KV 注册、跨 TP 传输、兼容性哈希校验等关键路径，确保撤回未导致其他隐藏错误。  
4. **代码整洁**：`TpKVTopology` 中去除了 `tensor_shape`、`cross_layers_blocks`、`block_size_position` 等字段后，相关的属性访问仍有 `assert self.kv_topo is not None` 等断言，需要同步更新或移除，避免不必要的运行时检查。  

总体而言，此次回滚恢复了原有的 KV layout 逻辑，降低了实现复杂度，但请在全仓库范围内彻底清理已删除的接口与配置，以防后续集成时出现不可预见的兼容性问题。

---

### [CI] Enable mypy import following for `vllm/compilation` (#33199)
**SHA**: `f1acbd6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f1acbd68c5a4388c302e2c270955147a77054cd8)

**🎯 变更类型**：其他（CI / 类型检查配置）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 取消 `pyproject.toml` 中对 `vllm.compilation.*` 的 `strict` mypy 覆盖，改为统一使用 `silent` 的 `follow_imports`。  
2. `tools/pre_commit/mypy.py` 现在仅以单一路径列表运行 mypy，去掉了对 `STRICT_DIRS` 的区分以及 `--strict` 参数。  
3. 将 `vllm/compilation` 加入 mypy 检查的文件集合，并删除了一些不必要的 `__all__` 导出（`CustomGraphPass`、`Range`、`RotaryEmbedding`），仅保留内部实现。

**🎯 影响范围**：  
- **CI / Pre‑commit**：mypy 检查流程已被简化，CI 运行时间可能下降，但 `vllm.compilation` 将不再以 `--strict` 方式检查。  
- **模块导出**：`vllm/compilation/inductor_pass.py`、`vllm/config/compilation.py`、`vllm/model_executor/layers/rotary_embedding/__init__.py` 的 `__all__` 删除，可能影响使用 `from … import *` 的外部代码。  
- **类型检查**：所有模块统一使用 `follow_imports=silent`，对跨包导入的错误更宽容。

**💡 关注建议**  
1. **本地验证**：在本地执行 `pre-commit run mypy -a`，确保删除 `--strict` 后仍然通过，尤其是 `vllm/compilation` 中的未注解或 `Any` 用法。  
2. **导出兼容性**：检查项目内部或外部是否有 `from vllm.compilation import *`、`from vllm.config.compilation import *`、`from vllm.model_executor.layers.rotary_embedding import *` 的使用场景。如有，建议保留显式的 `__all__` 或在文档中声明已不再支持通配符导入。  
3. **CI 稳定性**：观察 CI 近期多次提交的 mypy 运行情况，若出现新漏报或误报，可考虑为关键子目录（如 `vllm/compilation`）单独添加 `--strict` 标志或在 `pyproject.toml` 中重新启用覆盖。  
4. **文档更新**：在贡献指南或 CI 说明中注明已移除 `--strict` 检查，以免新贡献者误以为该目录仍受严格类型检查。  

总体而言，此次改动使 CI 配置更为统一，降低了对 `vllm.compilation` 的严格类型约束，但要留意因 `__all__` 移除导致的向后兼容性。确保本地完整测试后再合并。

---

### [ROCm] Enabling forward_includes_kv_cache on ROCm MHA backends (#33106)
**SHA**: `22ad649` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/22ad64950119db2a6fa3911cd87682f8f339bc3a)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 ROCm、ROCm‑AIter‑Unified 与 Triton 三个注意力后端中新增 `forward_includes_kv_cache_update` 标志，并把 KV‑cache 的写入逻辑抽取为统一的 `do_kv_cache_update` 方法。  
2. 原先在 `forward` 中完成的 `reshape_and_cache_flash`（或 `triton_reshape_and_cache_flash`）被移除，改为在需要时显式调用新方法。  

**🎯 影响范围**  
- `vllm/v1/attention/backends/rocm_attn.py`  
- `vllm/v1/attention/backends/rocm_aiter_unified_attn.py`  
- `vllm/v1/attention/backends/triton_attn.py`  
- 相关的 `AttentionLayer` 接口（因为 `do_kv_cache_update` 需要传入 `AttentionLayer` 实例）  

**💡 关注建议**  
1. **兼容性**：确认调用链中仍然依赖 `forward` 自动更新 KV‑cache 的地方已迁移至显式调用 `do_kv_cache_update`，或在后端实现中将 `forward_includes_kv_cache_update` 设为 `True` 以保持旧行为。  
2. **文档**：在后端说明和 `AttentionBackend` 基类中补充 `forward_includes_kv_cache_update` 与 `do_kv_cache_update` 的使用约定，避免用户误用。  
3. **单元测试**：新增针对 KV‑cache 更新的测试，覆盖跨注意力、分页注意力以及 FP8 格式下的缓存写入路径，确保在 `forward` 不负责写入时仍能正确缓存。  
4. **性能验证**：因为 KV‑cache 更新现在可能在外部独立调用，评估一次性批量更新 vs. 每次 `forward` 中嵌入的开销差异，避免出现意外的性能回退。  
5. **错误处理**：`do_kv_cache_update` 中的 `kv_sharing_target_layer_name`、`key/value` 为空等分支保持原有容错逻辑；若后端未实现该方法，抛出清晰的 `NotImplementedError`。  

总体来说，此次重构提升了后端 KV‑cache 更新的可组合性，但需要在调用层面完成相应迁移、更新文档并补全测试，以防止功能回退或性能波动。

---

### Add flake8-implicit-str-concat rules to Ruff (#33191)
**SHA**: `2eb673a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2eb673a088a107f2ea4ea792593038462874a5f7)

**🎯 变更类型**：功能增强（代码风格检查）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `pyproject.toml` 中为 Ruff 添加了 `ISC`（flake8‑implicit‑str‑concat）规则，并在项目内大量消除隐式字符串拼接（`"a" + "b"` → `"ab"`）的写法，涉及脚本、示例、测试、核心实现等多个文件。  

**🎯 影响范围**  
- **Ruff 配置**：`pyproject.toml` 中的选中规则。  
- **核心代码**：`csrc/quantization/machete/generate.py`, `vllm/*`（编译器、注意力、缓存管理、reasoning 解析等）。  
- **工具脚本/示例**：`examples/*`, `.buildkite/*`。  
- **测试用例**：`tests/*` 中的字符串拼接均已改写。  

**💡 关注建议**  
1. **本地验证**：在提交前运行 `ruff check`（已开启 `ISC`），确保没有遗漏的隐式拼接或误报。  
2. **CI 检查**：确认 CI 中的 Ruff 步骤已同步更新，防止因规则新增导致构建失败。  
3. **兼容性**：该改动仅是风格层面，不影响运行时行为，但请跑一遍完整单元/集成测试，确保因字符串换行或空格微调未意外改变输出。  
4. **文档/例子**：检查 README、示例脚本的渲染是否因去掉 `+` 而保持原有格式，必要时更新说明。  

总体而言，本次改动提升了代码可读性并统一了字符串拼接风格，对功能无直接影响，只需确保 lint 与测试同步通过即可。

---

### [torch.compile] Speed up MOE handling in forward_context (#33184)
**SHA**: `d9aa39a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d9aa39a3bbc1cb48cff8bfe9d6156c11fc4c42d9)

**变更类型**：性能优化  
**重要程度**：🟡 中  

**核心改动**  
1. 在 `CompilationConfig` 中新增 `static_all_moe_layers`，记录模型中所有 MOE 层的名称（注册顺序即执行顺序），并在 `FusedMoE.__init__` 中将每个层名追加进该列表。  
2. 将 `ForwardContext` 里的 `remaining_moe_layers` 替换为 `all_moe_layers` + `moe_layer_index`，改为 **顺序索引** 取层名而不是 **弹出**（pop）操作，避免在 `torch.compile` 图中产生副作用。  
3. 移除创建 `ForwardContext` 时对 `remaining_moe_layers` 的逆序处理，改用 `static_all_moe_layers` 直接填充。  
4. 相应地更新 `get_layer_from_name`、`encode_layer_name` 以及单元测试，使用新的字段。  

**影响范围**  
- `vllm/forward_context.py`、`vllm/config/compilation.py`、`vllm/model_executor/layers/fused_moe/layer.py` 以及相关单元测试。所有涉及 MOE 前向执行的路径都会走新的索引机制。  

**关注建议**  
- **上下文复用**：`moe_layer_index` 必须在每次前向开始时重置（当前在 `create_forward_context` 中未显式处理），否则连续推理会出现索引越界。建议在 `ForwardContext` 初始化时将其设为 0，或在 `create_forward_context` 中显式传入。  
- **顺序确定性**：`static_all_moe_layers` 的顺序取决于 `FusedMoE` 实例化顺序，确保模型构建阶段保持确定顺序（如使用同一 `torch.nn.Module` 构造顺序）。  
- **兼容性**：仍有 `remaining_moe_layers` 字段被测试代码引用（已改为 `all_moe_layers`），若外部代码仍访问旧属性会报错。建议在后续版本中加入向后兼容的属性别名或文档提醒。  

总体来说，此改动消除了 `pop` 引起的图构建副作用，提升了 MOE 前向在 `torch.compile` 场景下的运行速度，但需确保索引复位和顺序一致性。

---

### feature: support eagle3 for HunyuanVL & Hunyuan (#33035)
**SHA**: `bd92089` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bd92089d338ed5187dc5a466a8cc319c225515e7)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 Eagle3 spec‑decode 引入对 HunyuanVL 与 HunyuanV1 Dense 两类模型的支持。  
- 在 `speculative.py` 中将模型白名单扩展为 `hunyuan_vl`、`hunyuan_v1_dense`。  
- 为相应模型实现 `SupportsEagle3` 接口，新增辅助隐藏层收集逻辑以及默认层索引 (`(2, L//2, L‑3)`)。  
- 在 Eagle3 编码器中加入对 X‑Dynamic RoPE（xd‑rope）维度的兼容处理，并在位置缓存、设置时分别分支。  

**🎯 影响范围**  
- `vllm/config/speculative.py`（验证阶段）  
- `vllm/model_executor/models/hunyuan_v1.py`、`hunyuan_vision.py`（模型类、接口、辅助隐藏层 API）  
- `vllm/v1/spec_decode/eagle.py`（位置缓存、xd‑rope 支持、初始化）  
- 相关注册表 `model_executor/models/__init__`（间接影响模型加载）  

**💡 关注建议**  
1. **兼容性检查**：确认 `SupportsEagle3` 与已有 `SupportsLoRA/PP` 的多继承不会导致 MRO 冲突；运行完整单元测试及跨模型（Llama/Qwen/Minicpm/Hunyuan）对比验证。  
2. **aux_hidden_state_layers 参数**：目前硬编码为 `(2, L//2, L‑3)`，若用户自行指定层，需要保证层号在 `[0, L)` 范围内，否则会触发 runtime 错误。建议在 `set_aux_hidden_state_layers` 中加入参数校验。  
3. **xd‑rope 兼容**：新增 `uses_xdrope_dim`、`draft_uses_xdrope_dim` 逻辑仅在两者均>0 时启用；若目标模型使用 xd‑rope 而 draft 不使用，代码会回退到普通 RoPE。请在文档中明确该限制，并在模型配置生成阶段确保 `xdrope_dim` 正确传递。  
4. **性能评估**：Eagle3 的多模态模型在采样阶段会额外返回 `aux_hidden_states`，可能影响 cudagraph 缓存和内存占用。建议在高并发推理场景下对显存和吞吐进行基准；如必要，可提供 `disable_aux_hidden_state` 开关。  
5. **文档与示例**：更新 README / spec‑decode 示例，演示 `set_aux_hidden_state_layers` 的用法及如何在 `eagle3` 调度中查看返回的隐藏层。  

总体来说，此次改动为 Hunyuan 系列模型加入了 Eagle3 规格的加速路径，涉及模型接口、位置编码以及辅助隐藏层的管线改造，请重点关注多模型兼容性与参数校验，确保在新特性开启时不引入运行时异常。

---

### Support compress-tensors with nvfp4 or fp8 weights and modelopt with nvfp4 weights on Turing (#33076)
**SHA**: `66e601e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/66e601ef795c27b43025ec7f4f864cac561c322e)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 `compressed_tensors_w4a16_nvfp4` 与 `compressed_tensors_w8a16_fp8` 两套压缩‑Tensor 方案的最低算力要求从 **80（Ampere）** 降至 **75（Turing）**，正式在 Turing 架构上开放 NVFP4 / FP8 权重量化。  
2. 在 `modelopt.py` 中新增对 `current_platform.has_device_capability(100)` 的检查，仅在算力 ≥ 100（Hopper 及以上）且 FlashInfer 可用时才启用 `flashinfer‑cutlass` 后端；否则回退至 Cutlass。  
3. 引入 `vllm.platforms.current_platform`，并相应更新导入。  

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/compressed_tensors/*`（两套 schema）  
- `vllm/model_executor/layers/quantization/modelopt.py`（ModelOptNvFp4 相关路径）  
- 运行时设备能力检测逻辑以及后端选择（FlashInfer / Cutlass）  

**💡 关注建议**  
1. **兼容性验证**：在 Turing GPU（如 RTX 2080/2070）上跑通压缩‑Tensor 推理，确认能够成功回退到 Cutlass，且没有因 `flashinfer_cutlass` 条件误判导致运行时错误。  
2. **后端切换**：`has_device_capability(100)` 只在 Hopper+ 上生效，若未来想在 Turing 上使用 FlashInfer，需要在文档或代码中明确说明限制，或提供更细粒度的能力检测。  
3. **单元测试**：补充针对 `get_min_capability`、`current_platform.has_device_capability` 与 `backend` 选择的测试，防止在不同 GPU 架构间出现意外回退或异常。  
4. **文档更新**：在 Quantization/ModelOpt 章节标注 “NVFP4/FP8 在 Turing 可用，FlashInfer 仅在 Hopper（>=100）”。  

整体来看，此次改动使 vLLM 在更老的 Turing 硬件上能够使用 NVFP4/FP8 权重量化，提升硬件覆盖面；同时对 FlashInfer 后端的使用做了更保守的限制，避免不兼容的运行时错误。后续需重点在 Turing 环境做功能回归。

---

#### 🟢 低重要度变更 (19)

### [CI] Update job dependency syntax for Intel and AMD jobs (#33240)
**SHA**: `ecb4f82` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ecb4f822091a64b5084b3a4aff326906487a363f)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 Buildkite CI 各 yaml 文件中的 `depends_on` 由 `~` 或单行写法改为显式的空数组或列表形式，统一依赖声明语法，提升对 Intel、AMD 作业的兼容性。

---

### [CI] Update job dependency for hardware and CPU jobs (#33237)
**SHA**: `5914090` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5914090765fadc44985f7bbbf10750ff3a93bed1)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Buildkite CI 中为硬件和 CPU 测试步骤添加/统一 `depends_on`，使其依赖 `image-build-cpu`，确保镜像构建顺序正确。

---

### [XPU]disable test_acceptance_length UT (#33226)
**SHA**: `9581185` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9581185d5110f67c22c1e3e78b270a5a12029af3)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 XPU CI 脚本 `.buildkite/scripts/hardware_ci/run-xpu-test.sh` 中新增 `--ignore=v1/spec_decode/test_acceptance_length.py`，禁用该单元测试。

---

### [Docs] Simplify CPU x86 Docker build documentation (#33071)
**SHA**: `2dd359f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2dd359f953789b1a2d950f096480039a252f29d7)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：简化 CPU x86 Docker 构建文档，删去冗余说明，统一为默认自动检测并精炼示例与说明。

---

### [Bugfix] Lazy import NgramProposer in GPU model runner (#32821)
**SHA**: `a2b877d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a2b877df6c95b539ef23585051ab60bfa3ad177a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `gpu_model_runner.py` 中将 `NgramProposer` 的顶层导入改为懒加载，仅在使用时局部导入，防止循环依赖并提升模块加载速度。

---

### Don't use `min_pixels`/`max_pixels` from Qwen2VL's processor (#33208)
**SHA**: `35fb0b8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/35fb0b8613742f39e68b3ac8a0060a55fde3abb2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Qwen2VL 视觉处理器中，改为使用 `size["shortest_edge"]` 与 `size["longest_edge"]` 替代原先的 `min_pixels`/`max_pixels` 参数，提高兼容性。

---

### Relax protobuf library version constraints (#33202)
**SHA**: `a97b5e2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a97b5e206d78b96a75615f402357dbf7e73d4efe)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：放宽 protobuf、grpcio、grpcio-tools 等依赖的版本限制，去除严格的版本号要求。

---

### [ROCm][CI] Add TORCH_NCCL_BLOCKING_WAIT For Distributed Tests (A100) (#32891)
**SHA**: `911b51b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/911b51b69f92b7cbccc54453991a64eac72f2366)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 AMD CI 测试脚本中新增 `export TORCH_NCCL_BLOCKING_WAIT=1`，用于规避 ROCm HIP 的已知 bug，暂时确保分布式测试在 A100 上通过。

---

### [Feature]: Container image WORKDIR consistency (#33159)
**SHA**: `604e3b8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/604e3b87e8456535a073348e210ad74cc28407ca)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：统一 Dockerfile 工作目录为 `/vllm-workspace`（去除冗余 `/workspace/`），相应更新路径、缓存挂载及复制指令，以保持 CI 兼容性。

---

### [Docs] Use definition lists for CLI reference docs (#33186)
**SHA**: `706f123` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/706f123b23b75097da9a3f51ce538bd784fdc75b)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 CLI 帮助文档改为 Markdown 定义列表格式，新增 `def_list` 扩展并在 `generate_argparse.py` 中调整选项、默认值及帮助文本的渲染方式。

---

### [docs] Improve tlparse section (#33211)
**SHA**: `fb7abfc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fb7abfc1d03c93d07813946d84368002b2cac833)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：完善了 `tlparse` 使用说明，新增 `TORCH_TRACE` 环境变量开启方式，明确日志文件命名及示例，提升文档可读性。

---

### [CI] minor fixes to pipeline generator and tests (#33151)
**SHA**: `5d3d6e4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5d3d6e44e82a8733c83104b83c96bbac88321a96)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 .buildkite 脚本中新增 `VLLM_BAKE_FILE_PATH` 并修正 `docker buildx bake` 的路径；统一镜像标签参数传递（使用 `IMAGE_TAG`），并在分布式测试脚本中引用；新增 pipeline 生成文件 `.pipeline_gen_v2`。

---

### [Model Runner V2] Use a different stream for grammar bitmask h2d copy (#33059)
**SHA**: `46ec6d7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/46ec6d71c71c50d088a03e0b04e6f49cf8637c00)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 StructuredOutputsWorker 引入 `device` 参数，改用 GPU 张量取代 UVA 缓冲池；新增 `copy_stream` 与异步拷贝逻辑，实现语法位掩码和映射的非阻塞复制并在执行前同步。

---

### [Perf] Optimize dcp allocate tensor (#33102)
**SHA**: `3a6d5cb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3a6d5cbefd97a3dee07ba1756d8b5a9052801403)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除无用的 `torch.empty` 创建，改为直接 `reshape` 结果张量，减少内存分配，提升 DCP 注意力的执行性能。

---

### [Bugfix] Fix display error (inconsistent with context) (#33020)
**SHA**: `f5d7049` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f5d7049cc119e9bc572f29267e270b82a39d6ec1)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正日志信息中对 CPU 架构的描述，去除对 ARM 的不支持提示，使提示与实际实现保持一致。

---

### Enabling "2 node" distributed tests in the AMD CI pipeline. (#32719)
**SHA**: `3c3c547` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3c3c547ce0b35fb9d43808e8609f5e86fc34cca1)

**变更类型**：配置调整  
**重要程度**：🟢 低  
**摘要**：在 AMD CI 流水线加入 2‑节点分布式测试支持，新增 `cleanup_network` 及多节点命令解析执行逻辑；更新 CI yaml，添加对应硬件标签并去除结果过滤。

---

### [Attention] Use `has_flashinfer` helper (#33177)
**SHA**: `1cbccb6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1cbccb6dbabd6f80d50d615a3987e813caf5c8af)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将原有的 `flashinfer_available` 检测逻辑改为使用 `vllm.utils.flashinfer` 中的统一 `has_flashinfer` 辅助函数，并相应更新导入和调用点，去除冗余实现。

---

### [Doc] Improve serve parameter documentation with meaningful defaults (#33082)
**SHA**: `a6760f1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a6760f152538c96d8f0c743b204e24c6fd860fa3)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `generate_argparse.py` 中，新增注释并修改逻辑，默认值为 `None` 时不再生成文档，保留对空字符串的可视化处理。

---

### [BugFix] Fix P/D with non-MoE DP (#33037)
**SHA**: `0cd259b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0cd259b2d8b3f5cc237bea833b283db54d21529c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 kv_transfer_config.engine_id 在每个数据并行（DP）进程中添加唯一后缀的逻辑迁移至信号处理和 utils 初始化，修复非 MoE DP 场景下的冲突问题。

---

