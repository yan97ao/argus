# 每日更新报告（2026-01-22）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-22 23:52:57 | Maximilien de Bayser | Support bge-m3 sparse embeddings and colbert embeddings (#14526) |
| 2026-01-22 23:51:15 | Isotr0py | [Misc] Bump opencv-python dependecy version to 4.13 (#32668) |
| 2026-01-22 23:46:00 | Nick Hill | [Cleanup] Move scheduler `get_routed_experts` logic to separate method (#32706) |
| 2026-01-22 23:44:40 | Richard Zou | [torch.compile] Improve Cold Start for MoEs (#32805) |
| 2026-01-22 23:12:26 | Lucas Kabela | [Misc][BE] Turn on strict type coverage for vllm/compilation (#31756) |
| 2026-01-22 20:44:22 | Cyrus Leung | [Frontend] Introduce Renderer for processing chat messages (using `ModelConfig`) (#30200) |
| 2026-01-22 20:30:04 | Or Ozeri | OffloadingConnector: Support kernel_block_size != block_size (#30692) |
| 2026-01-22 19:34:14 | Chauncey | [Frontend] add prompt_cache_key for openresponses (#32824) |
| 2026-01-22 19:27:21 | Shengqi Chen | [CI] refactor release pipeline config into groups (#32833) |
| 2026-01-22 18:50:37 | Nicolò Lucchesi | [Bugfix] Fix Whisper/encoder-decoder GPU memory leak (#32789) |
| 2026-01-22 18:32:44 | wang.yuqi | [Frontend][2/n] Make pooling entrypoints request schema consensus \| ChatRequest (#32574) |
| 2026-01-22 18:12:58 | liranschour | Enable Cross layers KV cache layout at NIXL Connector (#30207) |
| 2026-01-22 18:03:15 | Nick Hill | [Benchmark] Don't default to `temperature==0` in `vllm bench serve` (#32723) |
| 2026-01-22 16:37:15 | Isotr0py | [Misc] Replace urllib's `urlparse` with urllib3's `parse_url` (#32746) |
| 2026-01-22 16:33:18 | Alex Sun | [AMD][ROCm] MoRI EP: a high-performance all2all backend (#28664) |
| 2026-01-22 16:20:27 | Cyrus Leung | [Model] Extend `collect_children` and `no_init_weights` contexts (#32757) |
| 2026-01-22 15:10:14 | Kebe | [bench] add start_times field to vllm bench serve json result (#32667) |
| 2026-01-22 14:11:09 | Andreas Karatzas | [ROCm][CI][Docs] Add comment explaining TRITON_ATTN fallback for ROCm (#32835) |
| 2026-01-22 13:55:25 | Andreas Karatzas | [ROCm][CI] Fix AITER test flakiness by using explicit attention backend (#32346) |
| 2026-01-22 13:47:33 | Micah Williamson | [ROCm][CI] Lower Acceptance Len Threshold For test_draft_model_quantization (#32731) |
| 2026-01-22 13:19:19 | Huy Do | Upgrade transformers-4.57.5 (#32287) |
| 2026-01-22 13:14:57 | Patrick von Platen | [Llama.py -> mistral.py] Extract mistral-only relevant code into separate file (#32780) |
| 2026-01-22 13:02:39 | Lucas Wilkinson | [FlashMLA] Update FlashMLA to expose new arguments (#32810) |
| 2026-01-22 12:27:47 | Divakar Verma | [ROCm][CI] fix get_valid_backends (#32787) |
| 2026-01-22 12:19:25 | Ifta khairul Alam Adil | [Docs] Remove outdated async_scheduling limitation with speculative decoding (#32775) |
| 2026-01-22 11:38:17 | Lucain | Cleanup some huggingface_hub-related stuff (#32788) |
| 2026-01-22 11:30:59 | knlnguyen1802 | [EC Connector] Optimize remote cache check in scheduler (#32585) |
| 2026-01-22 11:11:55 | Matt | [Bugfix] Fix potential EAGLE spec decode segfault during graph capture (#32818) |
| 2026-01-22 10:25:16 | Wentao Ye | [Deprecation] Remove deprecated environment variables (#32812) |
| 2026-01-22 09:02:48 | Woosuk Kwon | [Model Runner V2] Do not error on attention backends (#32820) |
| 2026-01-22 07:12:20 | Woosuk Kwon | [Model Runner V2] Refactor Prompt Logprobs (#32811) |
| 2026-01-22 06:49:51 | Xin Yang | [Kernel] Add topk_sigmoid kernel (#31246) |
| 2026-01-22 05:54:46 | Yanan Cao | [Misc] Add Helion version check to collect_env (#32797) |
| 2026-01-22 05:17:43 | Nick Hill | [ModelRunner V2] Don't pin reused flashinfer tensors (#32799) |
| 2026-01-22 02:41:11 | Divakar Verma | [ROCm] fix import for on_gfx9 (#32783) |
| 2026-01-22 02:30:10 | danisereb | Add missing import of fused_topk to benchmark_moe (#32784) |
| 2026-01-22 02:24:35 | Woosuk Kwon | [Model Runner V2] Minor refactor for `compute_slot_mappings` (#32794) |
| 2026-01-22 01:03:39 | Nick Hill | [Misc] Omit "disable NCCL for DP sync" startup log when not applicable (#32707) |
| 2026-01-22 00:49:50 | elvischenv | Bump Flashinfer to v0.6.1 (#30993) |
| 2026-01-22 00:38:04 | whx | [PluggableLayer][1/N] Define PluggableLayer (Fix ci) (#32744) |
| 2026-01-22 00:34:42 | Robert Shaw | [Quantization][Deprecation] Remove RTN (#32697) |

### 📊 统计摘要
> 本日共 41 个提交 | 🔴高 4 | 🟡中 20 | 🟢低 17
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [[Frontend] Introduce Renderer for processing chat message...](#d117a4d)
    - [[Frontend][2/n] Make pooling entrypoints request schema c...](#328cbb2)
    - [[Model] Extend `collect_children` and `no_init_weights` c...](#2b8a38b)
    - [[Kernel] Add topk_sigmoid kernel (#31246)](#63227ac)
  - [🟡 中重要度变更 (20)](#-🟡-中重要度变更-20)
    - [Support bge-m3 sparse embeddings and colbert embeddings (...](#ff365ee)
    - [[Cleanup] Move scheduler `get_routed_experts` logic to se...](#bc14663)
    - [[torch.compile] Improve Cold Start for MoEs (#32805)](#654a71f)
    - [[Misc][BE] Turn on strict type coverage for vllm/compilat...](#15e302d)
    - [OffloadingConnector: Support kernel_block_size != block_s...](#421012b)
    - [[CI] refactor release pipeline config into groups (#32833)](#1752262)
    - [[Bugfix] Fix Whisper/encoder-decoder GPU memory leak (#32...](#ea6102b)
    - [Enable Cross layers KV cache layout at NIXL Connector (#3...](#64e3d67)
    - [[Misc] Replace urllib's `urlparse` with urllib3's `parse_...](#8ebf271)
    - [[AMD][ROCm] MoRI EP: a high-performance all2all backend (...](#49a1262)
    - [[Llama.py -> mistral.py] Extract mistral-only relevant co...](#1579c9b)
    - [[FlashMLA] Update FlashMLA to expose new arguments (#32810)](#889722f)
    - [Cleanup some huggingface_hub-related stuff (#32788)](#24a163e)
    - [[EC Connector] Optimize remote cache check in scheduler (...](#378385b)
    - [[Deprecation] Remove deprecated environment variables (#3...](#6437ff1)
    - [[Model Runner V2] Refactor Prompt Logprobs (#32811)](#408195e)
    - [[Model Runner V2] Minor refactor for `compute_slot_mappin...](#e1da249)
    - [Bump Flashinfer to v0.6.1 (#30993)](#808d6fd)
    - [[PluggableLayer][1/N] Define PluggableLayer (Fix ci) (#32...](#1861ae8)
    - [[Quantization][Deprecation] Remove RTN (#32697)](#4e31b7f)
  - [🟢 低重要度变更 (17)](#-🟢-低重要度变更-17)
    - [[Misc] Bump opencv-python dependecy version to 4.13 (#32668)](#444e2e7)
    - [[Frontend] add prompt_cache_key for openresponses (#32824)](#841d53a)
    - [[Benchmark] Don't default to `temperature==0` in `vllm be...](#098b2d6)
    - [[bench] add start_times field to vllm bench serve json re...](#1bf1a34)
    - [[ROCm][CI][Docs] Add comment explaining TRITON_ATTN fallb...](#a810299)
    - [[ROCm][CI] Fix AITER test flakiness by using explicit att...](#eb1629d)
    - [[ROCm][CI] Lower Acceptance Len Threshold For test_draft_...](#019e2c3)
    - [Upgrade transformers-4.57.5 (#32287)](#f5fdec8)
    - [[ROCm][CI] fix get_valid_backends (#32787)](#49d9653)
    - [[Docs] Remove outdated async_scheduling limitation with s...](#a1d8246)
    - [[Bugfix] Fix potential EAGLE spec decode segfault during ...](#c5487e2)
    - [[Model Runner V2] Do not error on attention backends (#32...](#5e00b56)
    - [[Misc] Add Helion version check to collect_env (#32797)](#e675dda)
    - [[ModelRunner V2] Don't pin reused flashinfer tensors (#32...](#24dc30f)
    - [[ROCm] fix import for on_gfx9 (#32783)](#180fba6)
    - [Add missing import of fused_topk to benchmark_moe (#32784)](#f999539)
    - [[Misc] Omit "disable NCCL for DP sync" startup log when n...](#9b693d0)
#### 🔴 高重要度变更 (4)

### [Frontend] Introduce Renderer for processing chat messages (using `ModelConfig`) (#30200)
**SHA**: `d117a4d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d117a4d1a915c7acf124563f9b337de5c0aa2c2f)

**🎯 变更类型**：功能增强 / 架构变更 / 重构  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：  
- 在 `vllm` 中引入 **Renderer** 抽象层，用于统一处理聊天消息的渲染（chat template、工具调用、图片/音频等多模态输入）以及生成的 Prompt（文本或 Token ID）。  
- 将原本散落在 `vllm.entrypoints.chat_utils` 的模板解析、参数过滤、ChatTemplateResolution 等逻辑迁至 **renderer** 实现，并提供 `HfRenderer、MistralRenderer、DeepseekV32Renderer、Grok2Renderer、TerratorchRenderer` 等专属实现。  
- 新增 `RendererRegistry` 与 `renderer_from_config`，根据模型配置自动选取对应 renderer。  
- 大量调用链改为通过 `engine_client.renderer`（或 `input_processor.renderer`）获取渲染器，而不再直接使用 `engine_client.get_tokenizer()`。  
- 移除/简化 `process_chat_template`、`apply_hf_chat_template`、`apply_mistral_chat_template` 等老旧函数；新增 `safe_apply_chat_template`（统一异常包装）。  
- 相关测试、CI 配置、文档均已同步更新。

---

## 🔍 技术洞察

### 1. 架构影响
| 影响点 | 说明 |
|--------|------|
| **渲染抽象层（Renderer）** | 将“聊天消息 → Prompt” 的整个过程拆分为 `render_messages` / `render_messages_async` 两个接口，统一返回 `(conversation, Prompt)`，显式携带多模态数据结构。 |
| **RendererRegistry** | 通过 `renderer_mode`（从 `tokenizer_mode` 或 `model_impl` 中派生）动态加载对应渲染实现，提供插件式扩展点。 |
| **InputProcessor → Renderer** | `InputProcessor` 现在持有 `Renderer` 实例 (`self.renderer`) 而不是直接持有 `Tokenizer`，从而在渲染阶段能够自行决定同步/异步 tokenization。 |
| **EngineClient / Engine / LLMEngine** | 添加 `renderer` 抽象属性，所有上层 endpoint（ChatCompletion、Completion、Embedding、Pooling、Score、Serve/Tokenize 等）均改为使用 `renderer`。 |
| **Tokenizer 初始化懒加载** | 当 `skip_tokenizer_init=True`，renderer 仍能返回 **placeholder** Prompt（例如 `"placeholder"`），随后在 `AsyncLLM` 中通过 `_tokenize_prompt_input_async` 完成真正的 token 化。 |
| **ChatTemplate 解析迁移** | 原本在 `chat_utils` 中的 Jinja‑AST 分析、`_detect_content_format`、`resolve_chat_template_kwargs` 等全部迁入 `vllm.renderers.hf`，并统一通过 `safe_apply_chat_template` 调用。 |
| **多模态处理** | 渲染器仍负责返回 `multi_modal_data` 与 `multi_modal_uuids`，保持原有多模态管线不变，只是将解析搬到了渲染器内部。 |
| **插件兼容** | 新增 `RendererLike` 协议，保证第三方插件可实现自定义渲染器而无需改动核心代码。 |

**整体结论**：渲染层的抽象极大提升了代码可维护性和可扩展性（不同模型/Tokenizer 的特殊行为可以在各自渲染器中封装），但也引入了新的模块间耦合点（RendererRegistry、配置映射），需要确保向后兼容性和正确的模式选择。

---

### 2. 性能影响
| 场景 | 预期变化 | 备注 |
|------|----------|------|
| **同步渲染**（大多数模型） | `renderer.render_messages` 仍然是同步返回 `Prompt`（字符串或 token list），与原来的 `apply_hf_chat_template` 基本等价。 | **无显著回归**。 |
| **Mistral 渲染** | Mistral 的 `apply_chat_template` 可能会阻塞 IO；现在通过 `ThreadPoolExecutor + make_async` 在 `render_messages_async` 中异步执行，避免 event‑loop 被卡住。 | **显著提升**，尤其在 `skip_tokenizer_init=False` 场景下的单请求 latency 会下降。 |
| **`skip_tokenizer_init=True`** | 渲染阶段返回占位文本，真正的 tokenization 延迟到 `AsyncLLM._tokenize_prompt_input_async`（使用 `AsyncMicrobatchTokenizer`），保持原有的 **懒加载** 策略。 | **性能保持**，且代码路径更清晰。 |
| **Renderer Registry 动态加载** | 首次渲染时加载相应模块（import），后续使用 LRU 缓存的类实例。开销极小（一次性的 import）。 | **可接受**。 |
| **多模态** | 仍然通过 `InputPreprocessor` 的 MM 处理器进行缓存/预处理，渲染器只负责把解析结果包装进 Prompt 中。 | **无额外负担**。 |
| **整体吞吐** | 代码路径更长（渲染 → tokenizer → post‑process），但大部分对 CPU 的实际消耗仍在 tokenizer 与模型推理上，渲染层的额外开销在 **微秒级**。 | 预计 **对吞吐无负面影响**。 |

---

### 3. 安全考虑
| 项目 | 说明 |
|------|------|
| **Jinja 沙箱** | `vllm.renderers.hf` 使用 `jinja2.sandbox.ImmutableSandboxedEnvironment`（与 HF 原实现相同），防止模板注入执行任意 Python 代码。 |
| **ChatTemplate 参数过滤** | `resolve_chat_template_kwargs` 只保留 tokenizer 支持的关键字、HF 基础参数以及模板实际使用的变量，抛出 `ValueError` 对于 `chat_template` / `tokenize` 等不允许的字段。 |
| **异常包装** | `safe_apply_chat_template` 捕获并记录 `transformers`、`mistral_common` 抛出的异常，统一转为 `ValueError`，防止异常泄漏内部实现细节。 |
| **Renderer 加载** | `RendererRegistry.load_renderer_cls` 通过 `resolve_obj_by_qualname` 动态 import，若恶意配置指向非法模块会在启动阶段抛异常并记录日志，避免运行时加载不受信任代码。 |
| **skip_tokenizer_init** | 当禁用 tokenizer 初始化时，渲染器仍返回 placeholder，所有后续的 tokenizer 调用（包括安全检查）都会在明确的 `get_tokenizer()` 时触发，从而防止在未准备好的情况下调用外部库。 |

**总体安全**：保持与原实现相同或更严格的安全检查，且通过统一的渲染入口可以在未来进一步加入安全审计（如模板内容白名单）。

---

## ⚠️ 潜在风险

| 风险类别 | 具体描述 | 可能的后果 | 缓解措施 |
|----------|----------|------------|----------|
| **错误的 Renderer 选择** | `renderer_from_config` 根据 `tokenizer_mode`/`model_impl` 选取 renderer。如果自定义模型使用非标准 `tokenizer_mode`（如 `auto`）且实现不在 `_VLLM_RENDERERS`，会抛 `ValueError`。 | 启动失败、请求 5xx。 | - 为自定义模型添加对应 renderer 条目或使用 `--renderer-mode` 配置（若添加新 flag）。<br>- 单元测试覆盖常见模型组合。 |
| **兼容性回归** | 老代码仍可能直接调用 `apply_hf_chat_template`（已在 `chat_utils` 中使用 `__getattr__` 发出弃用警告），但某些插件或第三方可能依赖它。 | 运行时异常或性能下降。 | - 在生产环境前彻底

---

### [Frontend][2/n] Make pooling entrypoints request schema consensus | ChatRequest (#32574)
**SHA**: `328cbb2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/328cbb2773d93d45d18dfd383d631de4fa276e69)

**🎯 变更类型**：功能增强 / 重构 / 文档更新  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：  
本次提交为 Pooling 系列（`/pooling`、`/embed`、`/classify`）统一了聊天请求的参数定义，引入 `ChatRequestMixin`，集中管理 `messages、add_generation_prompt、continue_final_message、add_special_tokens、chat_template、chat_template_kwargs` 等字段并加入互斥校验。文档、示例脚本以及大幅度的单元/集成测试同步更新，以验证新参数在不同入口（online/offline、embedding、classification）下的行为一致性。

**🎯 影响范围**：  
- `vllm/entrypoints/pooling/base/protocol.py`（新增 `ChatRequestMixin`）  
- `vllm/entrypoints/pooling/classify/protocol.py`、`vllm/entrypoints/pooling/embed/protocol.py`（混入 `ChatRequestMixin`）  
- `vllm/entrypoints/pooling/classify/serving.py`、`vllm/entrypoints/pooling/pooling/serving.py`（使用新字段传递给 tokenizer）  
- 文档 `docs/**`、示例脚本 `examples/pooling/**`、以及测试 `tests/entrypoints/pooling/**`。  

---

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **统一抽象**：`ChatRequestMixin` 把所有 Pooling 类接口的聊天相关字段抽离为可复用的基类，消除了 `classify`、`embed`、`pooling` 三套实现中重复的字段和校验逻辑。<br>- **模型验证**：通过 Pydantic `model_validator` 实现 `continue_final_message` 与 `add_generation_prompt` 的互斥检查，提升了输入合法性保障，防止后台生成异常。<br>- **向后兼容**：所有新增字段默认值为 `False`/`None`，即现有请求（仅 `model`、`input`）行为不变。<br>- **文档/示例同步**：统一了示例入口函数名和路径，降低了用户使用门槛。 |
| **性能影响** | - **额外字段传递**：仅是结构体字段的增添，对序列化/反序列化开销可忽略（相对于模型推理毫秒级时延）。<br>- **Tokenizer 调用**：`add_generation_prompt`、`continue_final_message`、`add_special_tokens` 在 `ChatTemplateContentFormatOption` 中被转发给 tokenizer，可能导致 **一次额外的模板渲染**，在极大量并发请求下会有细微的 CPU 增量（≈5‑10%），但仍在可接受范围。<br>- **验证阶段**：新增的互斥检查是 O(1) 操作，不产生性能瓶颈。 |
| **安全考虑** | - **输入校验**：通过 Pydantic 校验提升了对恶意/错误请求的防御，避免因两种互斥参数同时开启导致异常的内部状态泄漏。<br>- **无新增外部依赖**：仅引入 `Any` 类型注解，不涉及外部库或网络行为，安全面无新增风险。 |
| **可维护性** | - **代码复用**：多处协议类合并至 `ChatRequestMixin`，后续若需新增聊天相关选项，只需在该 mixin 中统一维护。<br>- **测试覆盖**：新增 100+ 条针对聊天参数的正负例，显著提升了回归安全网。 |
| **兼容性** | - 对已有客户端 **不强制改动**，但若客户端仍旧同时传 `add_generation_prompt=true` 与 `continue_final_message=true`，将从原本的“后者覆盖前者”变为 **抛异常**（HTTP 400），需要在升级文档中提醒用户。 |

---

### ⚠️ 潜在风险  

1. **向后兼容性误报**  
   - 旧版客户端如果误传了两者均为 `true`（虽然之前被静默接受），升级后会返回 400 错误，可能导致短暂的服务中断。  
2. **Chat Template 依赖**  
   - `chat_template` 参数默认 `None`，但在某些 tokenizer 中若未配置默认模板且用户未显式提供，渲染阶段会抛 `ValueError`。需要确保文档说明「在 tokenizer 没有默认模板时必须提供 `chat_template`」。  
3. **OpenAPI/Swagger 文档同步**  
   - 新增字段后相应的 OpenAPI schema 需要同步更新，否则客户端生成的 SDK 仍缺失这些字段。  
4. **测试环境差异**  
   - 部分测试使用 `RemoteOpenAIServer` 直接发送 HTTP 请求，若后端服务未更新到最新代码（例如在 CI 中使用旧镜像），会出现 **字段不识别** 的错误。保证 CI 镜像版本统一至最新提交。  

---

### 💡 关注建议  

| 对象 | 建议 |
|------|------|
| **开发者** | - 在发布前 **更新 OpenAPI schema**（`vllm/entrypoints/openai/openapi_schema.json`）并生成对应的 SDK。<br>- 为 `add_generation_prompt` 与 `continue_final_message` 的互斥情况在**用户文档**中加显著提示（示例代码中添加 `assert`），避免用户在升级后遭遇 400。<br>- 将 `ChatRequestMixin` 迁移至 `vllm/entrypoints/pooling/base/__init__.py`，并在 `__all__` 中显式导出，以便外部插件复用。 |
| **维护者** | - 在 CI 中加入 **兼容性回归测试**：使用老版请求体（不带新字段）和新版冲突请求体（两者均为 true），确认行为分别为“保持兼容”和“抛错”。<br>- 对使用 `chat_template` 的模型，提供 **fallback**：若 `chat_template` 为 `None` 且 tokenizer 没有默认模板，自动使用 `self.chat_template`（全局配置）或抛出友好的错误信息。 |
| **用户** | - 在升级至本次版本后，检查自己的调用代码是否 **误同时开启** `add_generation_prompt` 与 `continue_final_message`。<br>- 对于 **自定义 tokenizer**（无默认聊天模板），确保在请求体中显式传递 `chat_template` 或使用 `chat_template_kwargs`。 |
| **安全审计** | - 该修改仅涉及 **输入合法性检查**，未引入外部网络或文件系统访问，现有安全审计规则仍然适用。可将 `model_validator` 的异常信息列入安全日志以便追踪潜在恶意构造请求。 |

---

**总结**：此提交通过抽象 `ChatRequestMixin` 实现了 Pooling 相关入口的请求参数统一，提升了代码可维护性和输入校验强度，文档与示例同步更新，测试覆盖显著扩大。只要注意向后兼容的错误提示、OpenAPI 文档同步以及对缺失 chat template 的模型提供明确指引，即可安全发布并让用户受益于更一致的 API 行为。

---

### [Model] Extend `collect_children` and `no_init_weights` contexts (#32757)
**SHA**: `2b8a38b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2b8a38b6d6be4f6e09cc20381c7027e7c35cb6c9)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. **集合工具**：在 `vllm.utils.collection_utils` 中新增 `common_prefix`（支持字符串与任意序列），并补充对应单元测试。  
2. **缺失层占位实现**：将原来的 `LMMissingLayer` / `TowerMissingLayer` 替换为统一的 `StageMissingLayer`，并支持在 **meta‑device** 环境下避免真实内存分配。  
3. **上下文管理**：引入 `collect_children` 与 `no_init_weights` 两个通用上下文，用于**收集子模块名称**和**在构造阶段阻止权重初始化**，并支持 `targets` 参数以处理非直接子模块的情况。  
4. **模型适配层**：`vllm/model_executor/models/adapters.py` 以及大量具体模型实现改为使用新的 `_create_pooling_model_cls`、`_mark_language_model`、`_mark_tower_model`、`_mark_composite_model`，实现更细粒度的语言模型/多模态塔模型标记。  
5. **缓存语言模型**：在 `interfaces.py` 中为 `SupportsMultiModal.get_language_model` 添加前缀公共前缀解析与缓存，提升检索效率。  
6. **其他模型文件**：大幅改写各模型的 `__init__`，统一使用 `no_init_weights` 与 `StageMissingLayer`，并删除对已废弃占位类的引用。  

---

### 🎯 影响范围
- **核心模块**：`vllm/model_executor/models/interfaces.py`、`vllm/model_executor/models/utils.py`、`vllm/model_executor/models/adapters.py`。  
- **所有多模态模型实现**：`bagel.py`, `chameleon.py`, `gemma3_mm.py`, `glm4v.py`, `idefics3.py`, `internlm2.py`, `minicpmv.py`, `mllama4.py`, `nano_nemotron_vl.py`, `paddleocr_vl.py`, `paligemma.py`, `qwen3_vl_moe.py`, `qwen_vl.py`, `whisper.py` 等。  
- **工具函数**：`vllm/utils/collection_utils.py`（新增 `common_prefix`）。  
- **测试**：`tests/utils_/test_collection_utils.py`（覆盖新函数）。  

---

### 🔍 技术洞察

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | • **统一占位层**：`StageMissingLayer` 取代 `LMMissingLayer/TowerMissingLayer`，通过 **meta‑device** + 动态占位实现，减少层级判断分支，简化 `interfaces` 中的缺失层检测逻辑。<br>• **可组合标记上下文**：`_mark_composite_model` 将语言模型与塔模型的标记统一在一个 `ExitStack` 中完成，降低模型实现的重复代码，提升可维护性。<br>• **子模块收集**：`collect_children` 与 `no_init_weights` 通过注册模块注册钩子 (`register_module_module_registration_hook`) 实现，避免在模型构造阶段对实际子模块进行实例化，提升模型装配的灵活性。 |
| **性能影响** | • **内存占用**：使用 `torch.device("meta")` + `StageMissingLayer` 在模型初始化时不分配真实显存，显著降低多模态模型在 **CPU‑only** 或 **GPU‑less** 环境下的启动成本。<br>• **加载速度**：`no_init_weights` 通过一次性拦截子模块创建，避免重复对象创建与删减属性的开销；`common_prefix` 的实现 O(N·L)（N 为序列数，L 为最短长度），对整体运行影响可忽略。<br>• **运行时开销**：在常规推理路径中，`StageMissingLayer` 不会被调用，仅在 **mm‑encoder‑only** 场景下被避免；因此运行时性能几乎不受影响。 |
| **安全考虑** | • 新增的上下文管理器和占位层均不执行任何计算，仅在属性访问时转发到真实模块，未引入外部依赖或可执行代码，安全风险极低。<br>• `common_prefix` 只进行序列切片与比较，也不存在安全问题。 |
| **可维护性** | • 大幅减少 **重复的 `with _no_init_weights(...):`** 代码块，统一为 `no_init_weights`，提升可读性。<br>• `StageMissingLayer` 统一追踪缺失阶段 (`stage_name`) 便于调试，日志信息更明确。<br>• 对 `SupportsMultiModal.get_language_model` 加入缓存与公共前缀求解，降低未来模型层级变化导致的定位错误。 |

---

### ⚠️ 潜在风险
1. **向后兼容性**  
   - 直接引用旧类 `LMMissingLayer` / `TowerMissingLayer` 的第三方插件或用户代码将导致 `ImportError`。推荐在发行说明中提供兼容别名（如 `LMMissingLayer = StageMissingLayer`）或保持旧类的轻量包装。  
2. **模块注册钩子行为**  
   - `register_module_module_registration_hook` 在复杂的 `torch.nn.Module` 子类（尤其是自定义 `nn.ModuleList`、`nn.ParameterDict`）上可能产生意外的子模块捕获，需要在 CI 中加入覆盖这类结构的单元测试。  
3. **`targets` 参数的误用**  
   - 若 `targets` 设定不当，可能导致 **非目标子模块** 被误标记为 `StageMissingLayer`，进而在推理时抛出 RuntimeError。确保模型实现方使用明确的类/tuple。  
4. **`common_prefix` 的返回类型**  
   - 对于空序列返回 `[]`（列表）而非空的相同类型序列，可能在使用方做类型判断时产生不一致。建议在文档明确说明或统一返回 `type(items[0])()`。  

---

### 💡 关注建议
- **文档与迁移指南**  
  - 在官方文档中说明 `StageMissingLayer`、`collect_children`、`no_init_weights` 的使用方法与示例，特别标注已废弃的 `LMMissingLayer` / `TowerMissingLayer`。  
- **兼容层**  
  - 在 `vllm/model_executor/models/interfaces.py` 里添加：`LMMissingLayer = StageMissingLayer`、`TowerMissingLayer = StageMissingLayer`（仅作为别名），避免外部代码立即破裂。  
- **单元/集成测试**  
  - 增加针对 `no_init_weights` 与 `collect_children` 的负载测试，验证在 `--mm-encoder-only` 与 `--limit-mm-per-prompt=0` 两种模式下，模型能够成功创建且不产生额外显存占用。  
- **性能基准**  
  - 在典型多模态模型（如 `qwen_vl`、`whisper`）上对比 **模型加载时间** 与 **显存峰值**（使用 `torch.cuda.memory_allocated`），确保新实现带来预期的资源节省。  
- **审计 Hook 使用**  
  - 考虑在 `register_module_module_registration_hook` 前后加入调试日志（可通过环境变量开启），帮助定位在复杂模型层级中未被捕获的子模块。  

--- 

**结论**：此轮提交通过统一缺失层占位、引入通用的子模块收集与权重初始化拦截机制，显著提升了 vLLM 对多模态模型的装配灵活性与显存效率，同时在代码结构上实现了大量去重和可维护性的提升。只要在升级路径上提供向后兼容别名并完善相应测试，风险可控，建议尽快

---

### [Kernel] Add topk_sigmoid kernel (#31246)
**SHA**: `63227ac` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/63227accf5af941215e5a7fd48c305a15e2c8814)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 在 MoE（Mixture‑of‑Experts）路由层新增 *topk_sigmoid* 核心实现，支持 **sigmoid** 计分函数以及 **bias‑correction**。  
2. 将原有 *topk_softmax* 接口扩展为可选 `bias` 参数，保持向后兼容。  
3. 完成 CUDA kernel、C++ header、Python binding、调度层以及路由工厂的全链路改造，新增 `scoring_func` 参数（默认 “softmax”，可选 “sigmoid”）。  
4. 添加 **benchmark** 与 **单元测试**（包括 bias 与 sigmoid 两种路径），确保功能、精度与性能回归。  

**🎯 影响范围**  
- `csrc/moe/*`（kernel 实现、头文件）  
- `vllm/_custom_ops.py`、`vllm/_aiter_ops.py`（自定义算子注册）  
- `vllm/model_executor/layers/fused_moe/router/*`（路由器实现、调度函数）  
- `vllm/model_executor/layers/fused_moe/config.py`（路由方式枚举）  
- `vllm/model_executor/models/minimax_m2.py`（模型层调用）  
- 新增 benchmark 与 test 文件 `benchmarks/kernels/benchmark_fused_topk.py`、`tests/kernels/moe/test_fused_topk.py`  

---

### 🔍 技术洞察  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • 引入 `ScoringFunc` 枚举，使 kernel 能在同一个实现中切换 **softmax** 与 **sigmoid**，避免代码重复。<br>• `topk_softmax` 与 `topk_sigmoid` 通过统一的 C++/CUDA 接口暴露，Python 端统一走 `torch.ops._moe_C.topk_*`，保持 API 整洁。<br>• 路由工厂 `create_fused_moe_router` 取消了对非‑softmax 的硬性检查，允许在非分组 Top‑K 场景直接使用 sigmoid，提升模型配置的灵活性。 |
| **性能影响** | • Sigmoid 计算只需一次 `exp`（`1/(1+exp(-x))`），相较于 Softmax 的 `exp` → `sum` → `div`，在 **large‑expert** 场景可显著降低算子 FLOPs。<br>• 在加入 bias 时，使用 **单次读取 + 加法** 的方式完成纠偏，仍在同一 kernel 中完成 Top‑K 选取，避免额外的显存拷贝。<br>• 通过 `benchmark_fused_topk.py` 可量化两者在不同 `num_experts`、`topk`、`num_tokens` 组合下的耗时，预计在 256‑512 experts 区间的加速幅度在 **1.2×‑1.8×** 之间（具体取决于显卡和 dtype）。 |
| **安全考虑** | • 代码仅在显存内部进行数值运算，无外部 I/O，未引入新的安全威胁。<br>• `bias` 参数通过 `std::optional<torch::Tensor>` 传递，内部强制检查 `dtype == float32`、`dim == 1`、`size == num_experts`，防止不匹配导致内存越界。 |
| **可维护性** | • kernel 中的 `if constexpr (SF == ...)` 分支清晰，新增 scoring 只需在枚举中加入相应实现，代码路径不重复。<br>• Python 调度层通过 `dispatch_topk_softmax_func` / `dispatch_topk_sigmoid_func` 隐蔽了 ROCm‑AITer 与原生实现的差异，后续若添加更多平台，只需在 `_aiter_ops` 中补全对应实现即可。 |
| **兼容性** | • `topk_softmax` 与 `topk_sigmoid` 的 Python 调用保持旧签名（`bias` 为可选），对已有代码无破坏性。<br>• `FusedTopKRouter` 默认 `scoring_func="softmax"`，除非显式配置，旧模型行为不变。<br>• 但对 **ROCm** 平台，仅实现了 `topk_softmax` 的 `fake`，`topk_sigmoid` 仍为占位实现，使用 ROCm 时会触发 `NotImplementedError`（需后续补全）。 |

---

### ⚠️ 潜在风险  

1. **ROCm 支持不全**：当前提交仅为 `topk_sigmoid` 提供 “fake” 实现，若用户在 ROCm 环境下打开 `use_rocm_aiter=True`，会收到空实现且不报错，可能导致隐蔽的性能或正确性问题。  
2. **Bias 参数错误**：如果外部传入的 bias 长度不等于 `num_experts` 或 dtype 非 float32，虽然在 C++ 已有检查，但在 Python 层未明确报错信息，可能导致 *CUDA illegal memory access*。  
3. **数值溢出**：Sigmoid 在极端负数上会产生非常小的值，后续 `topk` 选取时可能出现全部值接近 0 的异常行为（尤其在 `renormalize=False` 时）。  
4. **分支路径未完全覆盖**：`topk_sigmoid` 在 `dispatch_topk_sigmoid_func` 中默认走 `vllm_topk_sigmoid`，但在某些老版本的 `torch.ops._moe_C` 仍未注册对应 kernel，可能出现 `RuntimeError: no kernel named topk_sigmoid`。  
5. **工作空间分配**：当 `num_experts` 不是 2 的幂且大于 256 时，需要额外 `workspace`；若显存紧张，可能触发 OOM。  

---

### 💡 关注建议  

| 对象 | 建议 |
|------|------|
| **开发者** | 1. 为 ROCm 平台实现完整的 `topk_sigmoid` kernel，或者在 `dispatch_topk_sigmoid_func` 中检测平台并回退到 CUDA 实现。<br>2. 在 Python API（`topk_softmax` / `topk_sigmoid`）加入显式的参数校验与友好的异常信息（如 “bias 必须是 1‑D float32，长度须等于 num_experts”。）<br>3. 在 `vllm/_custom_ops.py` 为 `topk_sigmoid` 添加文档字符串，说明 `bias` 的作用与使用场景。 |
| **测试/CI** | 1. 将 **ROCm**

---

#### 🟡 中重要度变更 (20)

### Support bge-m3 sparse embeddings and colbert embeddings (#14526)
**SHA**: `ff365ee` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ff365eea94add487cf91316087bcfa8f2b39c2b8)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 `BAAI/bge-m3` 模型加入稀疏（BM25）和 ColBERT 多向量嵌入的完整支持。新增 `BgeM3EmbeddingModel`，在 `roberta.py` 中实现二次权重加载（`sparse_linear.pt`、`colbert_linear.pt`），并通过 `BOSEOSFilter` 过滤 BOS/EOS。文档、模型注册、测试全部同步更新，提供示例 API 调用方式。  

**🎯 影响范围**  
- `vllm/model_executor/models/roberta.py`（核心模型实现、二次权重加载）  
- `vllm/model_executor/layers/pooler/*`（新增 `BOSEOSFilter`、token‑wise pooler 参数）  
- `vllm/model_executor/models/registry.py`（模型映射）  
- 文档 `docs/models/pooling_models.md`、示例 `examples/...`、测试 `tests/models/...`  

**💡 关注建议**  
1. **权重加载**：二次权重文件名硬编码为 `sparse_linear.pt`、`colbert_linear.pt`，确保模型仓库始终包含这两个文件；若缺失会导致 `load_weights` 报错，建议在代码中加入更友好的错误提示或回退路径。  
2. **HF‑overrides**：由于原 `config.json` 中 `architectures` 错误，用户必须手动使用 `--hf-overrides '{"architectures":["BgeM3EmbeddingModel"]}'`。可考虑在模型注册时自动检测并补全，以降低使用门槛。  
3. **BOSEOSFilter**：当前仅在 `token_embed` 过滤 BOS，`token_classify` 同时过滤 BOS/EOS。若后续模型的 BOS/EOS 处理方式变化，需要相应更新此类。建议在 `__all__` 中注明适用任务。  
4. **兼容性**：新模型继承自 `RobertaEmbeddingModel`，保持原有 `embed` 接口不变，旧模型不受影响。但 `pooler_for_token_embed` 参数新增 `projector`，确保内部调用更新为 `pooler_for_token_embed(pooler_config, self.colbert_linear)`。  
5. **测试覆盖**：已加入稠密、稀疏、ColBERT 三套校验，建议再加入异常路径（如缺失二次权重、错误的 `hf-overrides`）的单元测试，提升鲁棒性。  

总体来看，此次改动为 vLLM 引入了对 BGE‑M3 多模态检索的完整支持，功能提升明显，影响局部且可通过文档与测试快速验证。开发者在使用前注意 `hf-overrides` 与权重文件完整性即可。

---

### [Cleanup] Move scheduler `get_routed_experts` logic to separate method (#32706)
**SHA**: `bc14663` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bc14663e6a058722542789a044f70863edcb5a6a)

**🎯 变更类型**：重构（代码清理）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原本在 `update_from_output` 中为 stopped 请求计算 `routed_experts` 的逻辑抽取到新私有方法 `_get_routed_experts`。  
- 在生成 `EngineCoreOutput` 时的条件中加入 `stopped`，确保在请求被 stop 后仍能返回 `routed_experts`（当配置开启时）。  
- 新方法在配置关闭时直接返回 `None`，并保持原有的 slot‑mapping 计算流程不变。

**🎯 影响范围**  
- `vllm/v1/core/sched/scheduler.py`（调度器核心逻辑）。  
- 可能波及到使用 `engine_core_outputs` 的上层调用，如 `Engine`、`AsyncEngine`，以及所有依赖 `enable_return_routed_experts` 的测试/示例。

**💡 关注建议**  

1. **功能等价性**：抽取后逻辑保持不变，但在 `update_from_output` 的 `if` 条件里加入 `stopped`，必须确认原先在 `stopped` 且 `new_token_ids/pooler_output/kv_transfer_params` 均为空的情况不会产生额外的 `EngineCoreOutput`（只会带 `routed_experts`）。建议在单元测试中覆盖 “stop‑only、无新 token” 场景。  

2. **边界检查**：`num_tokens = request.num_tokens - 1` 在 `request.num_tokens == 0` 时会得到 `-1`，导致 `slot_mapping[:num_tokens]` 产生空数组。虽然大多数请求至少有一个 token，但最好在方法入口加 `if request.num_tokens <= 1: return None` 防止负数切片引发意外。  

3. **性能影响**：原逻辑仅在 `stopped` 时执行一次计算，新方法保持相同调用频率，不会显著增加开销。但 `np.arange`、`reshape` 等操作仍在每次 stop 时进行，若频繁 stop 可能成为热点。可考虑缓存 `block_offsets`（`np.arange(self.block_size)`）为实例属性以避免重复创建。  

4. **代码可读性**：抽取后代码结构更清晰，符合单一职责原则。建议在类的私有方法区域统一放置所有 `_` 前缀的方法，或在文件顶部添加简要注释说明该方法的用途。  

5. **兼容性**：新增方法使用了 `np.ndarray | None` 返回类型，确保项目的 `typing` 配置已允许 PEP‑604 联合类型（Python 3.10+），否则可能触发类型检查警告。  

6. **测试覆盖**：当前 PR 未携带新增测试，建议补充：  
   - `enable_return_routed_experts=True/False` 两种配置均能正确返回/返回 `None`。  
   - `request.num_tokens` 为 1、2、>block_size 等不同情况的 slot‑mapping 正确性。  
   - `stopped` 但无新 token 的情况下仍产生包含 `routed_experts` 的 `EngineCoreOutput`。  

总体来看，此次抽取提升了代码可维护性，风险主要在边界条件和潜在的轻微性能回退上。按以上建议补全测试并加入少量防御性检查后即可安全合并。

---

### [torch.compile] Improve Cold Start for MoEs (#32805)
**SHA**: `654a71f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/654a71fc3c6d0665b8b4805b0219e35dc485416e)

**🎯 变更类型**：功能增强（torch.compile 冷启动优化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `ForwardContext` 中新增 `remaining_moe_layers` 用于在编译图时动态提供 MoE 层名称，避免将字符串写入 Torch‑compile 静态图。  
2. `create_forward_context` 现在会把所有 `FusedMoE` 层的名字倒序放入 `remaining_moe_layers`。  
3. `FusedMoE` 通过 `encode_layer_name` 与新工具函数 `get_layer_from_name`，在调用自定义算子 `vllm.moe_forward/_shared` 时弹出对应层名或使用占位 `"from_forward_context"`。  
4. 单元测试中加入 `get_forward_context().remaining_moe_layers = None` 以兼容原有行为。  

**🎯 影响范围**  
- `vllm/forward_context.py`（上下文结构与创建）  
- `vllm/model_executor/layers/fused_moe/layer.py`（MoE 前向调度与自定义算子入口）  
- `tests/kernels/moe/test_moe.py`（单元测试适配）  

**💡 关注建议**  
- 确认在所有使用 `torch.compile` 的入口都会通过 `create_forward_context` 构造上下文，否则 `remaining_moe_layers` 可能为 `None` 导致硬编码。  
- 防止 `remaining_moe_layers` 与实际 MoE 调用次数不匹配；若出现异常，提示信息已给出，但应在 CI 中加入对应检查。  
- 文档中补充 “torch.compile 冷启动” 的使用说明以及对 `static_forward_context` 配置的影响。  
- 考虑在未来移除字符串硬编码的方案时，保持向后兼容（例如保留 `layer_name` 参数的回退路径）。  

这些改动提升了编译期的可复用性，但同时引入了对 ForwardContext 生命周期的依赖，务必在模型部署脚本和测试中确保上下文正确初始化。

---

### [Misc][BE] Turn on strict type coverage for vllm/compilation (#31756)
**SHA**: `15e302d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/15e302dfced9e0fc31e4b6c2e8138b6957ed4740)

**🎯 变更类型**：功能增强（类型安全）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交为 `vllm/compilation` 包开启了 MyPy **strict** 检查，并在代码中补全、收紧了大量类型注解（函数签名、变量、返回值、内部缓存结构等），同时对 CI‑pre‑commit 的 mypy 脚本做了改造，使 `vllm/compilation` 目录在 CI 中以 `--strict` 运行。为适配新规则，对少数不符合 strict 的调用（如在 `tests/compile/test_pass_manager.py` 中的 `pass_manager.add`）加入了 `# type: ignore[arg-type]`。

**🎯 影响范围**  
- 核心模块：`vllm/compilation/*`（后端、缓存、装饰器、matcher、piecewise_backend、sequence_parallelism 等）  
- 测试用例：`tests/compile/test_pass_manager.py`  
- CI 脚本：`tools/pre_commit/mypy.py` 与 `pyproject.toml` 的 mypy 配置  

**💡 关注建议**  
1. **保持类型完整性**：后续新功能若涉及 `vllm.compilation`，必须遵循当前的严格类型约束，避免出现 `# type: ignore`；若必须忽略，请在注释中说明原因。  
2. **CI 兼容性**：检查 CI 环境的 MyPy 版本是否满足 `--strict` 所需的选项（如 `warn-return-any`），防止 CI 失效。  
3. **文档同步**：在项目文档或贡献指南中加入 “编译模块必须通过 MyPy strict” 的说明，帮助外部贡献者提前发现类型错误。  
4. **回归测试**：确保所有已有单元测试仍然通过，尤其是涉及动态张量形状或 flashinfer 的路径，因为严格模式引入了 `assert flashinfer_comm is not None` 检查。  
5. **性能考量**：类型检查本身不会影响运行时，但在极端情况下增加 CI 时长，可考虑对非关键子模块使用 `ignore_missing_imports` 或局部 `# type: ignore` 以平衡检查粒度。  

总体来看，此次改动提升了 **vllm.compilation** 的代码可维护性和安全性，风险有限，只需在后续开发中遵守新的类型规范即可。

---

### OffloadingConnector: Support kernel_block_size != block_size (#30692)
**SHA**: `421012b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/421012b63ae2e3f26bba0506e9f3951e9dc53ded)

**变更类型**：功能增强  
**重要程度**：🟡 中  

**核心改动**  
1. **swap_blocks 接口**  
   - 在 C++/CUDA 实现 `swap_blocks` 新增 `int64_t block_size_in_bytes` 参数，去掉内部通过 stride 推算的逻辑。  
   - Python 绑定 (`torch_bindings.cpp`) 与高层包装 `vllm._custom_ops.swap_blocks` 同步添加该参数并补充使用说明。  

2. **测试更新**  
   - 所有涉及块拷贝的单元测试改为显式传入 `block_size_in_bytes`，并加入对 `kernel_block_size` 与 `logical_block_size` 的组合测试。  

3. **kv_offload 逻辑重构**  
   - 移除 `kv_dim_before_num_blocks` 旧标识，改为统一使用 **kernel‑block‑size** 与 **logical‑block‑size** 的比例。  
   - 在 `SingleDirectionOffloadingHandler` 中计算每个张量的 `block_size_in_bytes`，并在 `transfer_async` 中统一调用新版 `swap_blocks`。  
   - 在 `CpuGpuOffloadingHandlers` 构造时解析 GPU 张量是否为 “split K/V” 结构，创建对应的 CPU 张量并统一使用 kernel‑block‑size 计算块因子。  

4. **其他细节**  
   - 新增 `LOGICAL_BLOCK_SIZES`、`LOGICAL_BLOCKS_PER_CPU_BLOCK` 参数，支持 logical‑block‑size 与 kernel‑block‑size 不等的场景。  
   - 移除原先对 `kv_dim_before_num_blocks` 的特殊处理，简化块拷贝路径。  

**影响范围**  
- `csrc/cache*`、`vllm/_custom_ops.py`、`vllm/v1/kv_offload/*` 以及所有调用 `swap_blocks` 的位置。  
- 任何直接使用旧签名 `swap_blocks(src, dst, mapping)` 的第三方代码将失效，需要补充 `block_size_in_bytes` 参数。  

**建议**  
- **文档/示例**：在 API 文档和使用示例中明确 `block_size_in_bytes` 的来源（`tensor.element_size()*tensor.stride(0)`），并说明仅在 GPU ↔ CPU、GPU ↔ GPU 场景下使用。  
- **兼容层**：考虑在 Python 包中提供包装函数 `swap_blocks_legacy`，内部自动计算 `block_size_in_bytes`，减小升级冲击。  
- **安全检查**：在 C++ 端加固对 `block_size_in_bytes` 与张量实际布局的一致性校验，防止 stride 变化导致拷贝越界。  
- **性能验证**：跑一次完整的 benchmark，确保在不同 `logical_block_size` / `kernel_block_size` 组合下拷贝开销没有回退。  

总体而言，此次改动解耦了块大小的推导逻辑，提升了对多粒度块布局的支持，但也引入了向后兼容风险，需在文档和兼容层上做好补偿。

---

### [CI] refactor release pipeline config into groups (#32833)
**SHA**: `1752262` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1752262e96f94d3cf4d236d8d7f3dada2cdb927e)

**🎯 变更类型**：其他（CI 配置重构）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 `.buildkite/release-pipeline.yaml` 中的所有步骤按照功能划分为 **四个 group**（“Build Python wheels”“Build release Docker images”“Publish release images”“Publish wheels”），并把原有的 `depends_on`、`block`、`input` 等依赖关系迁移到对应的 group 名称。步骤本身的 ID 与命令基本保持不变，只是层级结构由平铺改为嵌套。  

**🎯 影响范围**  
- CI/CD：Buildkite Release Pipeline（所有构建、镜像打包、发布、上传 wheel 的步骤）  
- 触发方式：原有手动 `block`、`input` 仍在，只是依赖改为 `build-wheels`（group）等  
- 可能受影响的工具：Buildkite UI（分组展示）、任何自动化脚本依赖 step‑ID 的外部系统  

**💡 关注建议**  

1. **依赖完整性检查**：group 本身并不影响执行顺序，但 `depends_on` 中引用的名称必须是 **step ID**，而不是 group 名。当前 `block-upload-release-wheels` 改为 `depends_on: - input-release-version - build-wheels`，确认 Buildkite 能正确解析 `build-wheels` 为 group 内所有步骤的聚合。若不行，需要改为显式列出每个 wheel‑build 步骤的 ID。  

2. **运行验证**：在测试分支触发一次完整的 release 流程，观察是否所有构建镜像、上传 wheel、创建 Manifest 步骤都按预期执行，尤其是手动 `block` 前后是否仍会等待。  

3. **文档同步**：更新项目 README 或内部 CI 文档，说明新 pipeline 已分组，若有外部系统通过 step ID 自动触发，需要使用原有 ID（如 `build-wheel-x86-cuda-12-9`）而非 group。  

4. **回滚准备**：保留原始 `release-pipeline.yaml`（或在同一 PR 中保留注释版），以便在 Buildkite 解析出现异常时快速恢复。  

整体来看，此次改动仅是 **可读性与维护性提升**，不影响实际构建逻辑。只要在首次运行后确认依赖解析正常，即可安全合并。

---

### [Bugfix] Fix Whisper/encoder-decoder GPU memory leak (#32789)
**SHA**: `ea6102b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ea6102b85da808b23055912391977f43fbe3f227)

**🎯 变更类型**：Bug 修复（Whisper 编码‑解码模型的 GPU 内存泄漏）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `vllm/v1/core/encoder_cache_manager.py` 中引入 `allocated` 与 `to_free` 两个列表，改写了 `allocate`、`free` 与 `get_freed_mm_hashes` 的行为，使得在 Encoder‑Decoder 场景下，缓存条目只在模型实际执行完毕后才被释放，避免了“分配‑释放在同一次调度步中完成”导致的泄漏。  
- 新增回归测试 `test_encoder_cache_cleanup`，在单进程模式下循环提交 Whisper‑large‑v3‑turbo 的音频请求，检查请求结束后缓存条目数是否为 0。  

**🎯 影响范围**  
- `vllm/v1/core/encoder_cache_manager.py`（缓存管理核心）  
- Whisper 及其他 Encoder‑Decoder 多模态模型的 `model_runner`（通过 `engine_core.model_executor.driver_worker.worker.model_runner.encoder_cache` 访问）  
- 测试套件 `tests/models/multimodal/generation/test_whisper.py`（新增）  

**💡 关注建议**  
1. **多进程兼容性**：当前实现依赖单进程（`VLLM_ENABLE_V1_MULTIPROCESSING=0`）进行状态检查，建议在后续版本中确认 `to_free` 在多进程调度下仍能正确同步，防止跨进程缓存不一致。  
2. **接口保持**：`check_and_update_cache` 仍返回 `False`，若后续引入更细粒度的缓存失效检测，请务必保持与 `EncoderCacheManager` 的统一语义，以免出现隐藏的逻辑冲突。  
3. **性能回归**：虽然延迟释放不会影响正确性，但可能在高并发请求下导致短暂的缓存占用峰值。建议在压力测试中监控 `allocated` 大小，必要时调节 `cache_size`。  
4. **文档与用户提示**：在使用 Whisper 或其他 Encoder‑Decoder 模型时，提醒用户启用单进程模式或确保环境变量配置与新缓存策略兼容，以免出现意外的内存占用。  

总体来说，此次修改通过“分配 → 使用 → 延后释放”模型解决了泄漏问题，并提供了可验证的回归测试，影响范围局限在 Encoder‑Decoder 路径，风险相对可控。后续关注多进程环境下的同步一致性即可。

---

### Enable Cross layers KV cache layout at NIXL Connector (#30207)
**SHA**: `64e3d67` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/64e3d67ac01a279b67ef1b67df16882acd74f8fb)

**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交为 NIXL KV‑Connector 添加了 *cross‑layers blocks*（跨层块）特性。开启后，同一物理内存块会连续存放多个逻辑层的 KV 缓存，减少跨层传输的 buffer 数量。实现包括：  
1. 文档说明与脚本参数 `enable_cross_layers_blocks`。  
2. `KVTransferConfig` 支持 `kv_connector_extra_config.enable_cross_layers_blocks`。  
3. `TpKVTopology` 增加 `cross_layers_blocks`、`block_size_position` 等属性，并在构造时根据实际 tensor shape 判断是否采用跨层布局。  
4. `NixlConnector` 新增属性 `prefer_cross_layer_blocks`，在注册 KV 缓存时根据后端兼容性与配置决定是否使用。  
5. `register_kv_caches` 中首次依据实际张量 shape 创建 `TpKVTopology`，并重新计算兼容性 hash。  
6. 相关单元测试扩展，验证普通布局与跨层布局下的 block 划分、hash 检查以及错误处理。

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/utils.py`（TpKVTopology）  
- `vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py`（NixlConnector）  
- 配置/文档 `docs/features/nixl_connector_usage.md`  
- 测试目录 `tests/v1/kv_connector/*`  
- 相关工具 `KVConnectorModelRunnerMixin.allocate_uniform_kv_caches` 的 API 调用路径。

**💡 关注建议**  
1. **后端兼容**：跨层块仅在 FlashAttention / FlashInfer 上有效，其他后端会自动回退。请在部署时检查 `attention_backend`。  
2. **配置一致性**：`kv_transfer_config.kv_connector_extra_config.enable_cross_layers_blocks` 必须为字符串 `"True"`（兼容旧解析），否则保持默认。  
3. **性能验证**：开启后可显著降低跨层传输次数，建议在大模型（层数≥32）且每层 KV 大小相同的场景做基准测试，以评估实际收益。  
4. **兼容性 hash**：hash 计算已加入 `cross_layers_blocks` 标记，升级时需确保两端配置一致，否则会触发 handshake 失败。  
5. **代码维护**：`TpKVTopology` 中的属性依赖 `tensor_shape`，请避免在未调用 `register_kv_caches` 前访问 `kv_topo`，否则会抛 AssertionError。  

总体而言，改动集中在 KV‑Connector 的拓扑建模与 handshake 流程，对已有功能影响有限，只要在支持的后端上打开配置即可安全使用。

---

### [Misc] Replace urllib's `urlparse` with urllib3's `parse_url` (#32746)
**SHA**: `8ebf271` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8ebf271bb6d1e7e9b1a55be73d755ef1a57dbbe5)

**🎯 变更类型**：功能增强（统一 URL 解析库，提升兼容性）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交把项目中散落的 `urllib.parse.urlparse` 替换为 `urllib3.util.parse_url`，并相应地对返回对象的属性访问做了适配。涉及 `connections.py、envs.py、multimodal/utils.py、utils/network_utils.py` 四个模块，统一使用 urllib3 的 `Url` 类型，增强了对 IPv6、空路径、文件 URL 等 edge‑case 的处理。

**🎯 影响范围**  
- **网络连接层** (`vllm.connections`、`vllm.utils.network_utils`)：校验 HTTP/HTTPS URL、解析 ZMQ 路径。  
- **环境变量解析** (`vllm.envs`)：`VLLM_PORT` 检查。  
- **多模态文件/数据 URL 加载** (`vllm.multimodal.utils`)：数据 URL、file URL、域名白名单判断。  

**💡 关注建议**  

1. **兼容性测试**  
   - 添加/扩展单元测试，覆盖 IPv6 (`tcp://[::1]:1234`)、无 scheme 本地路径 (`/tmp/file.jpg`) 以及 `data:` URL。  
   - 确认 `parse_url` 对非法 URL（如缺少 scheme）仍抛 `ValueError`，与旧实现行为保持一致。  

2. **属性访问安全**  
   - 当前代码对 `url_spec.path`、`url_spec.netloc` 做了 `or ""` 兜底，建议在所有使用处统一使用 `getattr(url_spec, "path", "")` 以防 urllib3 未来更改属性命名。  

3. **依赖版本**  
   - `urllib3` 已是项目依赖，但若出现极端环境（如自定义 `urllib3` 轻量版）可能缺少 `parse_url`。可以在导入时提供回退或在 `setup.cfg` 中锁定最低版本。  

4. **文档和类型提示**  
   - 更新项目文档中关于 URL 解析的说明，明确使用 `urllib3.util.Url`。  
   - 代码已引入 `Url` 类型标注，建议在 `mypy` 配置中加入 `urllib3` 的 stub（`types-urllib3`），防止类型检查报错。  

5. **性能与行为**  
   - `parse_url` 与标准库实现差异极小，性能影响可忽略。  
   - 注意 `parse_url` 会自动去除 IPv6 地址两端的方括号，代码已作相应处理，但仍需确认在生成 ZMQ `tcp://[::1]:5555` 之类的完整 URL 时不会出现遗漏。  

总体来看，此次改动对项目的功能没有明显破坏，反而提升了 URL 解析的鲁棒性。只要补足上述测试与文档，风险可以控制在最低水平。

---

### [AMD][ROCm] MoRI EP: a high-performance all2all backend (#28664)
**SHA**: `49a1262` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/49a12622678906727dd9d1cb157d5e14ab24d91b)

**🎯 变更类型**：功能增强（新增 MoRI EP All‑2‑All 后端）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 为 ROCm 环境引入 MoRI（ROCm Multi‑Operator‑Reduction‑Instancing）作为 “mori” All‑2‑All 后端，支持高性能的 expert‑parallel 调度。  
- 新增 `has_aiter()`、`has_mori()` 检测函数，扩展 `ParallelConfig.all2all_backend` 与环境变量 `VLLM_ALL2ALL_BACKEND` 的可选值。  
- 在 `fused_moe` 模块中加入对 MoRI 的 `PrepareAndFinalize` 实现、在 `mk_objects` 中注册对应 expert 类型，并在 `all2all_utils` 中完成句柄创建与调用。  
- 更新 AITer‑Fused‑MoE 接口以接受 `num_local_tokens`、`output_dtype` 等参数，使其能够与 MoRI 联动。  
- 相应的测试、CLI 参数、导入路径均作了兼容性调整。

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/*`（新增 `mori_prepare_finalize.py`、改动 `rocm_aiter_fused_moe.py`、`layer.py`、`config.py`）  
- `vllm/distributed/device_communicators/*`（All2All 管理器新增 `MoriAll2AllManager`，`cuda_communicator` 根据 backend 选择）  
- `vllm/utils/import_utils.py`（新增可选依赖检测）  
- `vllm/config/parallel.py`、`vllm/envs.py`（后端枚举与默认值）  
- `vllm/platforms/rocm.py`（新增 `on_gfx942` 判断）  
- 测试目录 `tests/kernels/moe/...`（CLI 参数与可用性检查）  

**💡 关注建议**  

1. **依赖准备**：MoRI 与 AITer 均为可选的第三方库。部署前请确保 `pip install mori aiter`（或通过系统包）并满足 ROCm gfx942 / gfx950 硬件要求，否则 `VLLM_ALL2ALL_BACKEND=mori` 会在运行时抛出断言。  

2. **环境变量**：若不想使用 MoRI，保持默认 `VLLM_ALL2ALL_BACKEND=allgather_reducescatter` 即可；若启用请在节点间统一设置 `VLLM_ALL2ALL_BACKEND=mori`，并检查 `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS=0`（MoRI 暂不支持共享专家）。  

3. **兼容性**：代码在没有 MoRI/​AITer 时仍会回退到 DeepEP、PPLX 等路径，确保现有测试不受影响。建议在 CI 中加入 “mori‑available” 的矩阵测试，验证 `has_mori()` 为 true 时的路径。  

4. **性能调优**：MoRI 支持 intra‑node 与 inter‑node 两种 kernel 类型，自动根据 `self.internode` 选择。根据实际集群规模，可通过 `num_ep_ranks` / `gpu_per_node` 参数调节，以避免 RDMA‑block 配置不匹配导致性能下降。  

5. **调试日志**：`MoriAll2AllManager.get_handle` 已加入 `logger.debug` 输出完整参数，遇到不匹配错误时可通过 `VLLM_LOG_LEVEL=DEBUG` 快速定位。  

6. **文档**：请在项目的 ROCm/​MoRI 使用指南中补充安装、硬件兼容、环境变量说明，避免用户因缺少依赖或不匹配的 GPU 架构导致启动失败。  

总体而言，此次提交为 vLLM 在 ROCm 环境下的 expert‑parallel 提供了更高效的 All‑2‑All 实现，只要满足依赖与硬件前提，能够显著降低跨节点通信开销。开发者在后续迭代中可继续完善异构调度与量化路径的兼容性。

---

### [Llama.py -> mistral.py] Extract mistral-only relevant code into separate file (#32780)
**SHA**: `1579c9b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1579c9b5fd0f6f213be3bbc55c88b1d215245ed6)

**🎯 变更类型**：重构 / 功能抽离  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原本在 `vllm/model_executor/models/llama.py` 中仅与 Mistral 相关的实现（如 Llama 4 scaling、Mistral 权重映射等）抽取到新文件 `mistral.py`，形成独立的 `MistralAttention、MistralDecoderLayer、MistralModel、MistralForCausalLM` 四个类。  
- `LlamaAttention` 与 `LlamaForCausalLM` 中的 Mistral 代码被删除，保持纯 LLaMA 逻辑。  
- 在模型注册表 `registry.py` 中将 `"MistralForCausalLM"` 的实现从 `llama.LlamaForCausalLM` 改为 `mistral.MistralForCausalLM`。  

**🎯 影响范围**  
- `vllm/model_executor/models/llama.py`（核心注意力/权重加载逻辑）  
- 新增 `vllm/model_executor/models/mistral.py`（专属 Mistral 实现）  
- `vllm/model_executor/models/registry.py`（模型映射）  

**💡 关注建议**  
1. **兼容性测试**：确认使用 `--load-format mistral` 加载的 Consolidated‑safetensors 仍能成功映射权重；尤其要验证 `permute` 逻辑在不同量化设置下的行为。  
2. **Llama‑4‑Scaling**：该特性已迁移至 `MistralAttention`，如有纯 LLaMA‑4 模型仍需使用，需在后续补充对应实现或保持向后兼容。  
3. **文档与示例**：更新模型列表说明，明确 Mistral 系列模型现在通过 `mistral` 包加载，同时保留原 LLaMA 接口不受影响。  
4. **导入路径检查**：确认项目其他位置（如自定义 LoRA、插件等）没有硬编码 `llama.LlamaAttention` 来获取 Mistral‑specific属性，必要时改为 `MistralAttention` 或使用抽象基类。  
5. **运行基准**：在不同并行度（tp、pp）下跑一次全流程推理，确保新层的 `rotary_emb`、`cache_config` 与原实现一致，避免因 `head_dim` 计算变化导致精度回退。  

总体而言，此次抽离提升了代码组织度，使 LLaMA 与 Mistral 的实现分离，后续维护和特性扩展将更清晰。建议在 CI 中加入针对 Mistral 权重映射的专门测试，以防止回归。

---

### [FlashMLA] Update FlashMLA to expose new arguments (#32810)
**SHA**: `889722f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/889722f3bfdacf54cf2016b94631690b327b9076)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 FlashMLA 集成到 vLLM，新增对 FP8 KV‑cache 的支持并暴露 `flash_mla_with_kvcache_fp8`、`get_mla_metadata_dense_fp8`等接口；在 CMake 中 vendoring 代码并对 wheel 打包做适配；相应地改写 `vllm/v1/attention/backends/mla/*` 以使用新元数据结构 `FlashMLASchedMeta`。  

**🎯 影响范围**：  
- `vllm/third_party/flashmla`（源码拷贝、接口包装）  
- `vllm/v1/attention/backends/mla/flashmla*.py`（解码/稀疏实现）  
- `vllm/v1/attention/ops/flashmla.py`（条件导入、API 替换）  
- CMake、`setup.py`（外部项目获取、安装路径、可选编译）  

**💡 关注建议**：  
1. 确认运行环境 CUDA ≥12.9、GPU 为 Hopper（sm90a）才能启用 FlashMLA；否则需确保 `torch.ops._flashmla_*` 不会被意外调用。  
2. FP8 路径依赖 `flash_mla_with_kvcache_fp8`，建议在 CI 中加入 FP8‑KV‑cache 的单元/性能测试，防止遗漏异常。  
3. 代码中对 `FlashMLASchedMeta` 的复制逻辑已简化，注意保持 cudagraph 场景下的持久缓冲区行为；如有需求再恢复相应检查。  
4. `setup.py` 新增的 `flashmla_regex` 与 `install` 任务确保打包时含 `flash_mla_interface.py`，请在发布前验证 wheel 中该文件的存在。  
5. 文档与示例应同步更新，说明如何通过 `VLLM_USE_PRECOMPILED` 或 `CUDA_HOME` 启用 FlashMLA，并提供 FP8‑KV‑cache 的使用说明。  

整体实现清晰，但请在多卡/不同 CUDA 版本环境下做回归，以防出现隐藏的导入或运行时错误。

---

### Cleanup some huggingface_hub-related stuff (#32788)
**SHA**: `24a163e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/24a163ed77a4dba8f46221c945943f8a3ff7ee27)

**🎯 变更类型**：重构 / 代码清理  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：一次性剔除了自实现的 HuggingFace token 读取 (`_get_hf_token`) 与部分异常捕获，统一使用 `huggingface_hub` 的默认行为；精简了 `huggingface_hub.utils` 的导入；在 `file_exists` 中加入缓存化 `list_repo_files` 的使用注释，以提升多文件判定的效率。  

**🎯 影响范围**  
- `vllm/lora/utils.py` – 下载 LoRA 权重时不再捕获 `RepositoryNotFoundError`、`EntryNotFoundError`，仅保留 `HfHubHTTPError`、`HFValidationError`。  
- `vllm/transformers_utils/config.py` – 所有涉及模型/Tokenizer 加载的函数去掉了 `token=_get_hf_token()` 参数，改为直接调用对应的 huggingface_hub 接口。  
- `vllm/transformers_utils/repo_utils.py` – 删除了 `_get_hf_token` 实现，`file_or_path_exists`、`get_hf_file_bytes` 等函数不再显式传递 token；在 `file_exists` 加入注释说明使用缓存的 `list_repo_files` 更高效。  

**💡 关注建议**  
1. **私有仓库兼容性**：`huggingface_hub` 在未提供 token 时会尝试匿名访问，私有模型将直接报 403。若项目仍需透明地支持 `HF_TOKEN`，建议在这些入口保留可选 `token` 参数或在内部 fallback 到环境变量。  
2. **异常捕获**：`EntryNotFoundError`、`RepositoryNotFoundError` 被移除，若上游 `huggingface_hub` 在未来把这些异常重新包装进 `HfHubHTTPError`，代码仍可正常工作；但在当前版本中，这两类错误会直接冒泡，可能导致未预期的崩溃。建议在关键下载路径加一层通用 `Exception` 捕获或记录日志。  
3. **缓存行为**：`list_repo_files` 已经用了 `lru_cache`，但在离线或网络不稳定时仍会重试。确保 `file_exists` 的调用频率不会导致缓存失效或内存膨胀。可考虑在文档中说明 “大量不同 repo_id 的检查会占用缓存”。  
4. **测试覆盖**：添加或更新以下测试场景：  
   - 私有模型/Tokenizer 在有/无 `HF_TOKEN` 环境变量下的行为；  
   - 多文件 existence 检查（如 `config.json`, `model.safetensors`）是否受缓存提升；  
   - 对不存在的公开仓库的下载错误仍能被捕获并记录。  

综上，此次提交主要是代码整洁与轻量化，对公开模型影响不大，但私有模型的使用路径需要额外关注 token 的传递与异常处理，以免产生隐藏的授权错误。

---

### [EC Connector] Optimize remote cache check in scheduler (#32585)
**SHA**: `378385b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/378385b90cddbe8cbc6e51d4ed59ce83e499530a)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将 **ECConnector** 的缓存检查 API 从 `has_caches(request) → List[bool]` 重构为 `has_cache_item(identifier) → bool`，并相应更新了 `ECExampleConnector`、`Scheduler._try_schedule_encoder_inputs` 以及全部关联单元测试。调度器现在在遍历每个 `mm_feature` 时逐条查询远程缓存，而不再一次性返回列表。

**🎯 影响范围**  
- `vllm/distributed/ec_transfer/ec_connector/base.py`（抽象接口）  
- `vllm/distributed/ec_transfer/ec_connector/example_connector.py`（实现）  
- `vllm/v1/core/sched/scheduler.py`（调度逻辑）  
- 相关测试文件 `tests/v1/core/test_scheduler.py`、`tests/v1/ec_connector/unit/test_ec_example_connector.py`  

**💡 关注建议**  
1. **兼容性**：如果项目中仍有旧代码调用 `has_caches`，会导致运行时错误。建议在 `base.py` 中保留一个向后兼容的包装实现（例如 `def has_caches(self, request): return [self.has_cache_item(f.identifier) for f in request.mm_features]`），并在文档中标记已废弃。  
2. **性能评估**：逐条远程查询可能比一次性批量检查增加网络往返次数。考虑在连接器实现层面做批量查询的内部优化，或在调度器中聚合同一轮需要检查的 `identifier` 列表后一次性调用。  
3. **错误处理**：`has_cache_item` 现在返回单个 `bool`，若底层实现抛异常需要在调度器侧捕获并回退为 “未命中”。当前代码未显式处理异常，建议增加 `try/except` 包装。  
4. **文档更新**：同步更新 `ECConnector` 接口文档、使用示例以及 changelog，明确新旧 API 的迁移路径。  
5. **测试覆盖**：新加入的 `side_effect` 测试已经覆盖了部分场景，建议再补充多请求并行检查的压力测试，确保在高并发情况下仍保持正确性和性能。  

总体而言，此次改动使缓存检查的粒度更细，代码可读性提升，但应注意向后兼容和潜在的网络开销。及时补齐文档和兼容层，可降低迁移风险。

---

### [Deprecation] Remove deprecated environment variables (#32812)
**SHA**: `6437ff1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6437ff1fb9dd64ca4637007a9659d866d93ddae4)

**🎯 变更类型**：功能增强 / 其他（废弃旧的环境变量）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 “vllm/config/attention.py”、`vllm/envs.py`、`vllm/platforms/rocm.py` 等核心文件中彻底移除一批已废弃的环境变量（如 `VLLM_ATTENTION_BACKEND`、`VLLM_FLASH_ATTN_VERSION`、`VLLM_USE_CUDNN_PREFILL` 等），并将相应的默认值、校验逻辑迁移到 `AttentionConfig` 参数与命令行 `--attention-config`。测试脚本和 CI 配置也改为通过 `--attention-backend` 参数显式传递后端。  

**🎯 影响范围**  
- **配置系统**：所有依赖旧环境变量的代码被改为使用 `AttentionConfig`，包括 `vllm/config/attention.py`、`vllm/platforms/rocm.py`、`vllm/usage/usage_lib.py`。  
- **CI / Test 脚本**：`.buildkite/test-amd.yaml`、`tests/v1/spec_decode/test_acceptance_length.py` 等已改为使用 CLI 参数。  
- **用户代码**：任何仍通过环境变量控制注意力后端、Flash‑Attn 版本、prefill‑decode 开关等的部署方式将在 v0.14.0 / v1.0.0 前报错或失效。  

**💡 关注建议**  
1. **迁移配置**：尽快在启动脚本、容器镜像、K8s/Helm 等部署方式中，将 `VLLM_*` 环境变量替换为 `--attention-config.<field>=<value>` 或在 Python 中通过 `AttentionConfig` 实例传入。  
2. **兼容性检查**：在本地或 CI 中运行一次完整的单元/集成测试，确认未残留对已删除 envvar 的读取（尤其是自定义脚本或第三方插件）。  
3. **文档更新**：保证 README、部署指南以及任何 `docker run -e VLLM_…` 示例同步修改为新参数。  
4. **警惕旧镜像**：使用旧镜像的用户在升级 vllm 版本后若仍保留环境变量，会收到警告并可能导致默认后端回退，建议重新构建镜像。  

总体而言，此次变更提升了配置统一性和类型安全，但需要用户在升级前完成一次配置迁移，以免在 v0.14.0 之后出现启动失败。

---

### [Model Runner V2] Refactor Prompt Logprobs (#32811)
**SHA**: `408195e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/408195ec59e5631c834c8ce5e8ecf5f8f1c7b47a)

**🎯 变更类型**：功能增强（Model Runner V2 – Prompt Logprobs 重构）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将原本在 `ModelRunner` 中完成的 **prompt‑logprobs** 计算抽离为独立的 `PromptLogprobsWorker` 类，实现“请求‑级”管理并用 Triton 加速 token‑id 采集。  
2. 删除 `states.py` 中的 `needs_prompt_logprobs` 与 `in_progress_prompt_logprobs`，改由 `PromptLogprobsWorker` 维护。  
3. 去除 `logprob.py` 中的 `compute_prompt_logprobs` 实现，迁移到新模块 `prompt_logprob.py`（含分块计算、GPU kernel）。  
4. `ModelRunner.sample_tokens` 调用新 worker 的 `compute_prompt_logprobs`，其参数已显式化（logits_fn、prefill_token_ids 等）。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`（核心调度/采样路径）  
- `vllm/v1/worker/gpu/sample/prompt_logprob.py`（新增）  
- `vllm/v1/worker/gpu/sample/logprob.py`（删减）  
- `vllm/v1/worker/gpu/states.py`（移除旧字段）  
- 相关 import 与调度清理（如 `LogprobsTensors` 的字段位置变化）  

**💡 关注建议**  
1. **兼容性**：确认 `SamplingParams.prompt_logprobs` 为 `None` 时仍保持旧行为，防止意外触发 `PromptLogprobsWorker`。  
2. **并发安全**：`PromptLogprobsWorker.in_progress_prompt_logprobs` 使用 `req_id` 作为键，确保在 `remove_request` 时彻底清理，避免内存泄漏。  
3. **性能验证**：新 Triton kernel ` _prompt_logprobs_token_ids_kernel` 依赖 `BLOCK_SIZE=1024`，在不同显卡上应跑基准，确保没有因 kernel 启动开销导致吞吐下降。  
4. **单元测试**：新增对 `PromptLogprobsWorker.compute_prompt_logprobs` 的覆盖，包括（a）完整 prompt、（b）分块 prompt、（c）被 preempt/resumed 场景。  
5. **文档更新**：在 sampling 参数说明中加入 `prompt_logprobs` 使用方式，以及在 `ModelRunner` 流程图中标注新 worker 的位置。  

总体来看，此次重构解耦了 prompt‑logprob 逻辑，提升可维护性与潜在加速空间，但需注意对旧接口的回退兼容及对 Triton kernel 的跨硬件鲁棒性。

---

### [Model Runner V2] Minor refactor for `compute_slot_mappings` (#32794)
**SHA**: `e1da249` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e1da249c931d65b0e9434028a136b52860384735)

**🎯 变更类型**：功能增强（Model Runner V2‑阶段）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. `BlockTable.compute_slot_mappings` 增加 `idx_mapping` 参数，用于把请求索引映射到当前状态索引。  
2. Triton kernel 参数及内部变量同步改名（`cu_num_tokens → idx_mapping`、`page_sizes → block_sizes`、`BLOCK_SIZE → TRITON_BLOCK_SIZE`），并在循环中改为使用 `idx_mapping` 与 `query_start_loc` 计算起止位置。  
3. 上层调用（`model_runner.py`、`eagle.py`）相应加入 `idx_mapping`，并在 spec‑decode 场景下统一使用同一映射张量。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/block_table.py`（核心 KV‑cache slot 计算）  
- `vllm/v1/worker/gpu/model_runner.py`（prepare_inputs）  
- `vllm/v1/worker/gpu/spec_decode/eagle.py`（草稿‑目标同步、slot mapping 调用）  

**💡 关注建议**  

1. **正确性**：`idx_mapping` 必须在所有路径上保持与 `query_start_loc` 同步；如果某些请求在 batch‑split、re‑ordering（如 `swap_in` / `swap_out`）后未及时更新，可能导致 slot 计算错位。建议在每次调度后对 `idx_mapping` 与 `query_start_loc` 进行一致性检查（如断言 `idx_mapping.shape[0] == query_start_loc.shape[0]-1`）。  

2. **性能回归**：kernel launch 维度改为 `(num_groups, num_reqs + 1)`，但内部循环改用 `TRITON_BLOCK_SIZE`。请在不同 batch 大小、kv‑group 数量下跑基准，确保吞吐量未下降。若出现回退，可考虑把 `TRITON_BLOCK_SIZE` 调整为 512/2048 进行调优。  

3. **兼容性**：该改动属于 **非向后兼容**（原 `compute_slot_mappings` 不再接受旧签名）。若库还有旧版调用（如外部插件或自定义模型），需要提供包装函数或保持旧 API（可在 `BlockTable` 中实现 `compute_slot_mappings_legacy` 兼容层）。  

4. **测试覆盖**：  
   - 单元测试：对 `BlockTable._compute_slot_mappings_kernel` 的输入/输出进行显式校验，尤其是 `idx_mapping` 变换后的 slot id 是否等于 `block_number * block_size + position % block_size`。  
   - 集成测试：在 `eagle` 的 speculative decode 场景下跑完整的文本生成，验证生成序列与未改动前一致（使用随机种子锁定）。  
   - CUDA‑graph 场景：确保 pad‑slot 仍然在最末的 batch 里填充 `-1`，防止图重用错误。  

5. **代码风格**：kernel 参数注释已改为 `TRITON_BLOCK_SIZE`，但仍使用 `# type: ignore`，建议在 `pyproject.toml` 中显式声明 `TRITON_BLOCK_SIZE` 为 `int` 常量，避免类型检查警告。  

总体来看，此次重构提升了 `compute_slot_mappings` 对多‑state（如 speculative decode）请求的可追溯性，但需重点验证映射一致性与性能回归，且提供向后兼容层以防外部依赖破坏。

---

### Bump Flashinfer to v0.6.1 (#30993)
**SHA**: `808d6fd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/808d6fd7b97f71f64a14ad4eecb9afd7b4d9dcf8)

**🎯 变更类型**：功能增强 / 依赖升级  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 将 FlashInfer 依赖从 **0.5.3** 更新至 **0.6.1**，同步修改 Dockerfile、`versions.json`、`requirements/cuda.txt` 等构建配置。  
- 移除 `next_positive_power_of_2` 与 `calculate_tile_tokens_dim` 的实现，相关调用在 MoE、TRT‑LLM、FP4 等路径全部改为 **`None`**（或直接省略），并在部分测试中删除对该函数的依赖。  
- 为 `vllm/v1/attention/backends/flashinfer.py` 增加 `o_data_type` 参数，以兼容新版本的输出 dtype 接口，并对 `plan` 参数构造做了条件分支（FA2 后端需要额外参数）。  
- 由于新版 FlashInfer 对 tile‑size 的实现已有内部约束，原有的手工计算逻辑被删除；对应的单元测试被标记为 **skip**。

**🎯 影响范围**  
- **Docker 镜像**：构建时会拉取新的 flashinfer‑cubin 与 jit‑cache 包，镜像体积略增。  
- **模型执行层**：`flashinfer_trtllm_moe.py`、`trtllm_moe.py`、`mxfp4.py`、`flashinfer_utils.py` 等文件的 MoE 调度参数被简化，可能影响性能调优（tile‑size 固定为内部默认）。  
- **注意力后端**：`vllm/v1/attention/backends/flashinfer.py` 增加 `o_data_type`，涉及调度器、解码计划的调用签名变更。  
- **测试**：`tests/kernels/moe/test_ocp_mx_moe.py` 大量删减，`test_flashinfer_sampler` 暂时被跳过，CI 通过依赖于新实现的行为。

**💡 关注建议**  

| 对象 | 建议 |
|------|------|
| **开发者** | 1. 在本地完整编译 Dockerfile（尤其是 `nightly_torch`）确认 flashinfer 源码能够在目标 CUDA 版本下成功构建。<br>2. 检查所有调用 `flashinfer` 的地方是否仍需要 `tile_tokens_dim` 参数；若有自定义调度逻辑，请根据新版文档确认默认 tile‑size 是否满足需求。<br>3. 将 `test_flashinfer_sampler` 标记为 skip 的原因记录在 Issue 中，后续在 flashinfer 0.6.1 修复后恢复。<br>4. 更新所有 `plan` 调用的签名，确保在非 FA2 后端仍保持 19 参数不变。 |
| **用户** | 1. 使用官方提供的 Docker 镜像（或自行构建）时请拉取最新的 `vllm` 镜像，旧镜像仍会使用 flashinfer 0.5.3，可能出现不兼容错误。<br>2. 若自行编译，请在 `requirements/cuda.txt` 中同步更新 `flashinfer-python==0.6.1`，并确保 `uv` 包管理器缓存已清理。<br>3. 注意模型配置中的 `dtype` 与 `o_data_type` 保持一致，避免因类型不匹配导致推理异常。 |

总体来看，此次升级为后续的性能改进与新特性铺平道路，但需要确认 **MoE tile‑size** 与 **plan 参数** 的兼容性，建议在生产环境部署前完成一次端到端的回归测试。

---

### [PluggableLayer][1/N] Define PluggableLayer (Fix ci) (#32744)
**SHA**: `1861ae8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1861ae8aae186d2796bb1deee3b8fc800b846bd0)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm/model_executor/custom_op.py` 中新增 `PluggableLayer` 抽象，并把原先 `CustomOp.op_registry`、`op_registry_oot` 提升为模块级变量 `op_registry`、`op_registry_oot`，统一管理 CustomOp 与 PluggableLayer。  
2. 所有原来通过 `CustomOp.op_registry[...]` 访问的代码改为 `op_registry[...]`，包括基准、单元测、配置检查等。  
3. 将 MLA 层从 `CustomOp` 注册改为 `PluggableLayer.register`，并简化 forward 实现。  
4. 文档、CI、测试相应更新，去掉对 `CustomOp.op_registry` 的直接引用。

**🎯 影响范围**  
- `vllm/model_executor/custom_op.py`（核心注册机制）  
- 依赖注册表的所有模块：`benchmarks/kernels/*`、`tests/*`、`vllm/config/compilation.py`、`vllm/model_executor/layers/mla.py`  
- 文档 `docs/design/custom_op.md`  

**💡 关注建议**  
- **兼容性**：对外仍可能有旧代码直接引用 `CustomOp.op_registry`，建议在 `custom_op.py` 中保留一个兼容别名或在文档中明确迁移路径。  
- **注册冲突**：`op_registry` 现在同时存放 `CustomOp` 与 `PluggableLayer`，需要确保名称唯一；可在 `register`/`register_oot` 中加入类型校验，防止同名的 Op 与 Layer 混用。  
- **测试覆盖**：新增 `PluggableLayer` 的实例化与 OOT 替换逻辑应补充专门单元测试，确保 `__new__` 的分支在未注册和已注册情况下均表现正确。  
- **性能**：`PluggableLayer` 的实例化在 `__new__` 里做一次 OOT 查找，开销极小，但建议在热点路径（如 MLA 前向）确认没有额外的 Python 调用导致编译器无法内联。  
- **文档更新**：`custom_op.md` 已删除旧的类属性示例，需补充 `PluggableLayer` 的使用说明及两者的区别，以帮助开发者快速上手。  

总体而言，此次改动为 vLLM 引入更灵活的层级插件机制，并统一注册表，提升可扩展性。但请注意向后兼容、名称冲突以及测试覆盖，以免在现有插件生态中产生意外破坏。

---

### [Quantization][Deprecation] Remove RTN (#32697)
**SHA**: `4e31b7f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4e31b7f2284b9c5b88e8504895794b9511980345)

**🎯 变更类型**：功能增强 / 已弃用 (Removal of RTN Quantization)  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 删除了 RTN（Round‑to‑Nearest）量化实现，包括 `vllm/model_executor/layers/quantization/rtn.py`（626 行）以及对应的测试 `tests/quantization/test_rtn.py`。  
- 在 `quantization/__init__.py` 中去掉了 `rtn` 条目和 `RTNConfig` 的映射。  
- 同时在 `utils/marlin_utils.py` 中删除了 `apply_rtn_marlin_linear` 辅助函数。  

**🎯 影响范围**  
- **模型启动 / 推理**：任何仍在代码或脚本中显式指定 `quantization="rtn"` 的调用将立刻失效，`allow_deprecated_quantization` 再也不能恢复。  
- **配置与 CLI**：`QuantizationMethods` 列表、`get_quantization_config`、`QuantizationConfig` 注册表均不再包含 `rtn`，因此在 `vllm run … --quantization=rtn` 时会抛出非法参数错误。  
- **文档与示例**：旧文档、README 中关于 RTN 的章节、示例脚本以及第三方教程需要同步更新。  
- **内部依赖**：`apply_rtn_marlin_linear` 已被删除，若还有残余调用（如自定义插件或旧的单元测试）会导致 `ImportError`。  

**💡 关注建议**  

1. **错误提示**  
   - 在 `vllm/model_executor/layers/quantization/__init__.py` 里加入对未知量化方法的统一报错（如 `raise ValueError(f"Quantization method '{quantization}' is no longer supported")`），以避免出现 “找不到属性” 的低层错误。  

2. **文档同步**  
   - 移除或标记所有关于 RTN 的章节，更新 `QUANTIZATION.md`、`README.md`、API 文档以及 CI 示例。  
   - 在迁移指南中说明 “若您正在使用 RTN，请改用 INT8/4‑bit (GPTQ/INC) 或自行实现自定义量化”。  

3. **兼容性检查**  
   - 运行全库搜索（`grep -R "rtn"`）确认没有遗漏的硬编码字符串或旧的单元测试。  
   - 对外部插件/用户脚本在升级后会报错的情况给出明确的迁移提示（可在 `vllm.exceptions` 中定义 `DeprecatedQuantizationError` 并在入口处捕获）。  

4. **版本号与发布说明**  
   - 该改动属于 **破坏性变更**（Breaking Change），建议在 `CHANGELOG.md` 中标记为 `vX.Y.Z` 的 **major**/**minor** 更新（取决于项目的语义化版本策略），并在 Release Note 中明确说明 “RTN 量化已被移除”。  

5. **回退路径（可选）**  
   - 如果社区仍有少量用户依赖 RTN，可考虑保留一个轻量的 “stub” 实现，仅抛出 `DeprecationWarning`，并在文档中标明即将在下一个主要版本彻底删除。  

**结论**  
本次提交通过删除已不再维护的 RTN 量化实现，减小了代码基量和维护负担，但也引入了潜在的向后不兼容风险。只要在入口处提供明确的错误信息、更新文档并确保没有残余引用，影响应可控制在可接受范围。建议在正式发布前完成上述兼容性检查与文档同步。

---

#### 🟢 低重要度变更 (17)

### [Misc] Bump opencv-python dependecy version to 4.13 (#32668)
**SHA**: `444e2e7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/444e2e7e1f79f3c8fdf57c362272127df55e4847)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `opencv-python-headless` 依赖升级至 4.13 版，更新相关 `requirements` 文件的版本约束；同步提升 `genai_perf`、`tritonclient`、`numpy`、`optuna` 等依赖版本；在翻译测试中加入 `repetition_penalty` 参数并添加模型长度注释。全局影响有限，主要是兼容性和测试增强。

---

### [Frontend] add prompt_cache_key for openresponses (#32824)
**SHA**: `841d53a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/841d53aaa8d674f2c9f72503e77f75e5ffa79c71)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `ResponsesRequest` 中新增 `prompt_cache_key` 字段，记录用于读取/写入 prompt 缓存的键（当前未实现， vLLM 会忽略）。

---

### [Benchmark] Don't default to `temperature==0` in `vllm bench serve` (#32723)
**SHA**: `098b2d6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/098b2d66fe310197cf7d70b7f2d43612b59f20c9)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：移除 `vllm bench serve` 默认将 `temperature` 设为 0 的行为，删除请求体中的硬编码温度，并在未指定时给出警告，默认温度由服务器决定。

---

### [bench] add start_times field to vllm bench serve json result (#32667)
**SHA**: `1bf1a34` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1bf1a34b1903189fd5da2f41313d3b8085c8ce48)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/benchmarks/serve.py` 中为 benchmark 结果 JSON 新增 `start_times` 字段，并在统计和返回数据时同步收集、输出该信息。

---

### [ROCm][CI][Docs] Add comment explaining TRITON_ATTN fallback for ROCm (#32835)
**SHA**: `a810299` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a810299838739b32fea9ecdb6eb32bc845aca6c5)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `tests/v1/spec_decode/test_acceptance_length.py` 中为 ROCm 平台添加注释，说明 ROCm 默认使用 Triton 注意力后端（因不支持 Flash Attention），提升代码可读性。

---

### [ROCm][CI] Fix AITER test flakiness by using explicit attention backend (#32346)
**SHA**: `eb1629d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/eb1629da2496fe351251abe1905f88021fbc3509)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 AMD CI 中新增 `amdproduction` 镜像硬件；针对 ROCm 环境将生成测试的 `max_num_seqs` 限为 1 并加入编译配置，以避免批次波动；同步更新 AMD 专用的 fused‑moe JSON 配置参数。

---

### [ROCm][CI] Lower Acceptance Len Threshold For test_draft_model_quantization (#32731)
**SHA**: `019e2c3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/019e2c3b7ca799e4497a67eb7e49edea361cb90e)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/v1/e2e/test_spec_decode.py` 中，将 `expected_acceptance_len` 的阈值从 `2.90 + 1` 调整为 `2.8 + 1`，降低了对接受长度的期望，以匹配 ROCm CI 环境下的实际表现。

---

### Upgrade transformers-4.57.5 (#32287)
**SHA**: `f5fdec8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f5fdec8ce21ae97d1140613b7a265a1d6af844f0)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将项目依赖的 transformers 升级至 4.57.5，并在 tests/models/multimodal/pooling/test_jinavl_reranker.py 中为该版本增加 skipif 标记，规避已知的 transformers#43295 问题。

---

### [ROCm][CI] fix get_valid_backends (#32787)
**SHA**: `49d9653` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/49d9653852c49893efabb5c2bff5309268517a41)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `get_available_attention_backends` 中改为从平台类获取 `get_valid_backends`，防止属性不存在时报错；若未实现且为 ROCm 环境，返回 `TRITON_ATTN`，否则返回 `FLASH_ATTN`。

---

### [Docs] Remove outdated async_scheduling limitation with speculative decoding (#32775)
**SHA**: `a1d8246` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a1d82466ea2b174768e20874d9eb1a689a0e675d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：从 `SchedulerConfig` 的 `async_scheduling` 字段说明中删除了已不再适用的 “不支持 speculative decoding 与 pipeline parallelism 时会自动禁用” 的限制描述，更新文档以反映当前实现。

---

### [Bugfix] Fix potential EAGLE spec decode segfault during graph capture (#32818)
**SHA**: `c5487e2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c5487e2b96b2627e8c2def872449e3e43246f6a8)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `eagle.py` 中新增对 `use_cudagraphs` 的判断，未使用 CUDA Graph 时显式设为 `CUDAGraphMode.NONE` 并使用原始 token 数，防止在图捕获期间出现解码段错误。

---

### [Model Runner V2] Do not error on attention backends (#32820)
**SHA**: `5e00b56` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5e00b561cddd2cecf8be7a341cb49d446613d6ef)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `vllm/v1/worker/gpu/model_runner.py` 中移除对非支持的 attention backend 抛出 `NotImplementedError` 的检查，避免在使用未知 backend 时直接报错。

---

### [Misc] Add Helion version check to collect_env (#32797)
**SHA**: `e675dda` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e675dda67be278d21c4ec177b2baa8b7a0550920)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/collect_env.py` 中将 `"helion"` 加入待检测的 pip 包列表，实现 Helion 版本检查。

---

### [ModelRunner V2] Don't pin reused flashinfer tensors (#32799)
**SHA**: `24dc30f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/24dc30f7ff4b304088297ffa2b34ab9aba07bea8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 FlashInfer 后端实现中，针对 ModelRunner V2 移除对复用 CPU 缓冲区的 pin_memory，以避免异步拷贝与下一步更新的竞争条件，仅在非 V2 模式下才启用 pin。文件改动 6 行新增，1 行删除。

---

### [ROCm] fix import for on_gfx9 (#32783)
**SHA**: `180fba6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/180fba653ead2274bfcd3951a275f4d6cf9ade04)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 环境下改为运行时导入 `on_gfx9`，并使用 `is_rocm_on_gfx9` 判断是否支持 FP8，防止直接访问 `p.rocm.on_gfx9()` 导致的导入错误。

---

### Add missing import of fused_topk to benchmark_moe (#32784)
**SHA**: `f999539` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f99953986945be69b20cc395fc5b74a5b78d4585)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `benchmark_moe.py` 中新增对 `fused_topk` 的导入，并将 DeepGemm 专家模块的创建改为条件性初始化，以避免未使用时的导入错误。

---

### [Misc] Omit "disable NCCL for DP sync" startup log when not applicable (#32707)
**SHA**: `9b693d0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9b693d023cf595e60b5346fdeeb41cf2a6eda838)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/config/vllm.py` 中增加对 `data_parallel_size` 与模型是否为 MoE 的检查，仅在 `data_parallel_size > 1 且 (model_config 为 None 或是 MoE)` 时输出 “Disabling NCCL for DP synchronization when using async scheduling.” 的启动日志，避免不适用场景下的日志。

---

