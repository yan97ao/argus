# 每日更新报告（2026-01-06）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-06 23:47:04 | Robert Shaw | [MoE Refactor][14/N] Clean Up FI Quant Config Smuggling (#31593) |
| 2026-01-06 23:34:17 | Robert Shaw | [MoE Refactor] Add Temporary Integration Tests - H100/B200 (#31759) |
| 2026-01-06 23:15:56 | Isotr0py | [Bugfix]: Fix cross attention backend selection for Turing GPU (#31806) |
| 2026-01-06 23:12:25 | Jee Jee Li | [LoRA]Disable linear LoRA  kernel PDL (#31777) |
| 2026-01-06 22:16:04 | wang.yuqi | [Model] rename use_pad_token to use_sep_token (#31784) |
| 2026-01-06 21:53:21 | Chauncey | [Frontend] Support GLM-4.5 / GLM-4.7 with enable_thinking: false (#31788) |
| 2026-01-06 20:59:17 | Jzz1943 | [Bugfix]: avoid overriding audio/text kwargs (Qwen3-Omni) (#31790) |
| 2026-01-06 20:08:22 | Cyrus Leung | [Misc] Implement `TokenizerLike.convert_tokens_to_ids` (#31796) |
| 2026-01-06 20:06:20 | kzwrime | [Bugfix] Fix torch.compile error for DP + MoE on CPU Backend (#31650) |
| 2026-01-06 20:05:17 | Lucas Wilkinson | [Attention][1/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31773) |
| 2026-01-06 19:55:59 | Cyrus Leung | [Chore] Cleanup `mem_utils.py` (#31793) |
| 2026-01-06 19:30:24 | BlankR | [Doc] Fix format of multimodal_inputs.md (#31800) |
| 2026-01-06 19:30:05 | wang.yuqi | [CI] Increase the MTEB_EMBED_TOL threshold to 5e-4. (#31797) |
| 2026-01-06 19:29:55 | Cyrus Leung | [Misc] Use `deprecated` for `seed_everything` (#31780) |
| 2026-01-06 18:57:57 | Fadi Arafeh | [cpu][bench] Add CPU paged attention benchmarks (#31720) |
| 2026-01-06 18:25:14 | Cyrus Leung | [Chore] Remove more V0 dead code from `sequence.py` (#31783) |
| 2026-01-06 16:44:22 | Isotr0py | [Bugfix][CI/Build] Fix failing pooling models test due to Triton kernel accuracy diff (#31776) |
| 2026-01-06 16:00:25 | Isotr0py | [Models]: Use `MMEncoderAttention` for MoonViT (#31738) |
| 2026-01-06 15:53:22 | vllmellm | [Bugfix][ROCm] Fix Unsupported attention metadata type for speculative decoding in `eagle.py` (#31714) |
| 2026-01-06 15:27:19 | Cyrus Leung | [Doc] Show that `use_audio_in_video` is supported in docs (#30837) |
| 2026-01-06 14:10:59 | Kevin McKay | [Bugfix][Hardware][AMD] Fix exception types in AITER MLA FP8 check (#31177) |
| 2026-01-06 12:22:18 | Robert Shaw | [CI] Fix CPU MM PRocessor Test (#31764) |
| 2026-01-06 11:14:33 | Michael Goin | [Bugfix] Add init_workspace_manager to moe kernel benchmarks (#31042) |
| 2026-01-06 11:13:36 | Michael Goin | [UX] Add `-ep` shorthand for `--enable-expert-parallel` (#30890) |
| 2026-01-06 10:09:18 | maang | [Cleanup] Remove redundant `decoder_layer_type` assignment in `Qwen2` (#31760) |
| 2026-01-06 10:01:13 | Wentao Ye | [Perf] Optimize additional `fill(0)` in cutlass moe, 2.9% E2E throughput improvement, 10.8% TTFT improvement (#31754) |
| 2026-01-06 09:51:54 | maang | [Docs] Improve malformed exception caused by backslash line continuations (#31694) |
| 2026-01-06 09:26:33 | Michael Goin | Revert "[CI Failure] Disable B200 tests while runner is broken" (#31750) |
| 2026-01-06 08:23:00 | John Calderon | [Bugfix] vLLM produces invalid UTF-8 tokens and “�” (#28874) |
| 2026-01-06 07:56:08 | Seiji Eicher | [CI/Build] Allow user to configure NVSHMEM version via ENV or command line (#30732) |
| 2026-01-06 07:20:46 | Michael Goin | [Bugfix] Properly apply v_scale for mimo_v2_flash (#31175) |
| 2026-01-06 07:18:38 | Robert Shaw | [Bugfix] Fix Broken ModelOpt NVFP4 MoE (#31742) |
| 2026-01-06 06:52:59 | Yongye Zhu | [MoE Refactor] Aiter Experts for BF16 MoE (#31542) |
| 2026-01-06 06:31:21 | Wentao Ye | [Bug] Revert torch warning fix (#31585) |
| 2026-01-06 06:17:59 | Matthew Bonanni | [CI][DeepSeek] Add nightly DeepSeek R1 `lm_eval` tests on H200 (#30356) |
| 2026-01-06 05:07:14 | Nick Hill | [Cleanup] Remove deprecated fields from CachedRequestData class (#31734) |
| 2026-01-06 05:00:14 | amitz-nv | [Model] Nemotron Parse 1.1 Support (#30864) |
| 2026-01-06 04:50:39 | Qidong Su | [docker] install cuda13 version of lmcache and nixl (#30913) |
| 2026-01-06 04:15:40 | gnovack | pin lora_b moe weights on cpu (#31317) |
| 2026-01-06 04:11:35 | Roberto L. Castro | [BugFix] Fix architecture flags to prevent issues on SM103 (#31150) |
| 2026-01-06 04:03:18 | Wang Kunpeng | [Misc][Model][Refactor] Pass the prefix into Linear layers (#31669) |
| 2026-01-06 03:32:43 | baonudesifeizhai | Fix GLM-4.6v flash tool calling in transformers 5.x (#31622) |
| 2026-01-06 03:31:49 | Isotr0py | [Misc] Enable Paligemma's PrefixLM attention mask computation (#31725) |
| 2026-01-06 03:29:16 | Or Ozeri | Triton Attention: Support cross-layers blocks (#30687) |
| 2026-01-06 01:26:09 | kzwrime | [Bugfix] Add missing extra_tensors arg to DeviceCommunicatorBase.disp… (#31644) |
| 2026-01-06 01:25:38 | RickyChen / 陳昭儒 | [Bugfix][CPU] Fix RotaryEmbedding fallback causing gibberish with --enforce-eager (#31643) |
| 2026-01-06 00:50:51 | Michael Goin | [CI Failure] Disable B200 tests while runner is broken (#31732) |
| 2026-01-06 00:34:35 | Kevin Šuc | [Frontend] [Doc] Exclude log deltas feature (#30322) |
| 2026-01-06 00:00:23 | Isotr0py | [v1] Add encoder-only/cross attention support to Triton Attention backend (#31406) |

## vllm-project/vllm 的LLM分析结果

### af8fd730512341773cb43c6f1763f14e120a781d
https://github.com/vllm-project/vllm/commit/af8fd730512341773cb43c6f1763f14e120a781d
[MoE Refactor][14/N] Clean Up FI Quant Config Smuggling (#31593)

Signed-off-by: Robert Shaw <robshaw@redhat.com>
Co-authored-by: Robert Shaw <robshaw@redhat.com>
**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将原先在 `register_moe_scaling_factors` 中的隐藏式量化因子注入逻辑拆分为显式的 `make_fp8_moe_alpha_scales_for_fi` 与 `register_scales_for_trtllm_fp8_per_tensor_moe` 两个工具函数。  
2. 统一 FlashInfer（CUTLASS、TRTLLM）FP8 MoE 的尺度计算方式，新增 `a1_gscale`、`a2_gscale` 的解释并在配置构造函数中使用统一的 per‑tensor / per‑channel 表达。  
3. 更新单元测试 `tests/kernels/moe/test_flashinfer.py` 参数签名，改用 `is_trtllm` 标识并在测试中调用新注册函数。  
4. 相应地在 `fp8.py`、`modelopt.py`、`flashinfer_utils.py` 等文件中调整权重量化、权重格式转换以及量化配置获取的实现细节。  

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/fp8.py`  
- `vllm/model_executor/layers/quantization/modelopt.py`  
- `vllm/model_executor/layers/fused_moe/*`（CUTLASS、TRTLLM 相关实现）  
- `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py`  
- 测试文件 `tests/kernels/moe/test_flashinfer.py`  

**🔍 技术洞察**  
- **架构影响**：  
  - 明确了 FlashInfer 量化尺度的来源，避免了对模型层属性的隐式修改（“smuggling”），提升代码可维护性与可读性。  
  - 对 TRTLLM 后端不再走模块化 kernel 抽象路径，专门在 `register_scales_for_trtllm_fp8_per_tensor_moe` 中注册所需属性，降低了层间耦合。  
- **性能影响**：  
  - 计算路径未变，时间/空间复杂度保持不变。  
  - 通过统一 `g1_alphas/g2_alphas` 计算，略微减少了中间张量的拷贝，潜在微小的启动开销下降。  
- **安全考虑**：  
  - 只涉及张量尺度的数值处理，无新增安全风险。  
  - 仍需确保新注册的属性在所有后端分支均已正确初始化，否则可能触发未捕获的 `AttributeError`。  

**⚠️ 潜在风险**  
1. **兼容性破坏**：外部代码可能仍依赖旧的 `register_moe_scaling_factors` 副作用，升级后会出现属性缺失错误。  
2. **数值回归**：若 `make_fp8_moe_alpha_scales_for_fi` 的实现细节（如 squeeze、广播）与原始实现不完全一致，可能导致 FP8 量化精度轻微下降。  
3. **测试覆盖不足**：仅更新了 `test_flashinfer.py`，其他使用 FlashInfer 的模型（如 Llama3、Mistral）若未同步调用新函数，可能在运行时崩溃。  

**💡 关注建议**  
- **兼容层**：可在 `vllm/model_executor/layers/quantization/utils/flashinfer_utils.py` 中保留一个轻量包装的 `register_moe_scaling_factors`（内部调用新函数），以平滑过渡。  
- **回归测试**：加入对 `g1_alphas、g2_alphas、a1_gscale、a2_gscale` 的数值校验，对比旧版与新版在相同随机权重下的输出误差。  
- **文档更新**：在 Quantization 章节明确指出 FP8‑FlashInfer 后端的配置参数变化及 `is_trtllm` 标记的意义。  
- **升级指南**：在发布说明中提示用户如果自行实现了自定义 MoE 层，需要调用 `register_scales_for_trtllm_fp8_per_tensor_moe` 来完成尺度注入。  

### d3e477c01342f800790b170b8fa799b467621358
https://github.com/vllm-project/vllm/commit/d3e477c01342f800790b170b8fa799b467621358
[MoE Refactor] Add Temporary Integration Tests - H100/B200 (#31759)

Signed-off-by: Robert Shaw <robshaw@redhat.com>
Co-authored-by: Robert Shaw <robshaw@redhat.com>
**🎯 变更类型**：测试/CI 配置、临时集成测试  
**⚡ 重要程度**：🟢 低  
**📋 变更摘要**：  
- 在 Buildkite CI 流水线中新增两条可选的 MoE 重构集成测试步骤，分别针对 H100 与 B200 GPU。  
- 为 GSM8K 评估任务加入大量 MoE 相关的模型/后端配置文件（FlashInfer、DeepGEMM、Marlin、FP4/FP8 等），并提供针对不同硬件的配置清单 (`config-h100.txt`、`config-b200.txt`)。  
- 这些改动旨在在 CI 环境中快速验证 MoE 重构的正确性与性能基准。

**🎯 影响范围**：  
- `.buildkite/test-pipeline.yaml`（CI 流水线）  
- `tests/evals/gsm8k/configs/moe-refactor/` 目录下的所有新配置文件  
- CI 运行时的 Python 测试套件 `pytest`（`evals/gsm8k/test_gsm8k_correctness.py`）

**🔍 技术洞察**  

- **架构影响**：无对核心业务代码的结构性修改，仅在 CI 层面扩展测试矩阵。通过 `optional: true` 标记，保持对现有流水线的向后兼容。  
- **性能影响**：新增的测试会在 CI 中触发实际模型加载与推理，可能显著增加 CI 运行时长，尤其是多 GPU（2 张）环境下的 FP8/FlashInfer 等高性能后端。对生产环境无直接影响。  
- **安全考虑**：仅涉及环境变量 (`VLLM_*`)，均为内部实验性开关，未引入外部依赖或网络交互，安全风险极低。  

**⚠️ 潜在风险**  

1. **CI 时长激增**：大量模型配置及多 GPU 运行可能导致流水线超时或资源占用过高。  
2. **配置错误**：新加入的 YAML 配置若拼写或参数错误（如环境变量名写错），会导致测试失败，误报回归。  
3. **资源竞争**：可选步骤若被误设为必选，可能在 GPU 资源紧张时阻塞其他构建。  
4. **依赖漂移**：部分配置依赖实验性特性（如 `VLLM_USE_DEEP_GEMM`、`VLLM_FLASHINFER_MOE_BACKEND`），若底层实现变更可能导致测试失效。  

**💡 关注建议**  

- **验证 CI 资源**：在推送前本地或在预发布分支上跑一次完整的 H100/B200 测试，确保 GPU 资源足够且配置文件可被正确解析。  
- **监控运行时长**：在 CI 仪表盘中观察新增测试的平均耗时，必要时对低优先级配置使用 `optional: true` 或拆分为独立的 nightly job。  
- **配置审查**：对新增的 `env:` 项做一次统一的 lint 检查，避免拼写错误；可考虑将共用环境变量抽取到全局变量文件中。  
- **回退机制**：若临时测试导致流水线不稳定，利用 Buildkite 的 `optional` 标记快速关闭该步骤，或在 `.buildkite/pipeline.yml` 中添加 `if: false` 暂时禁用。  
- **文档同步**：在项目的 CI/测试文档中加入 “MoE Refactor 临时集成测试” 章节，说明使用方法、支持的 GPU 类型以及如何在本地复现。

### 02809af1e7cdf63bcf0d73b63a35ad3ea5c6fb81
https://github.com/vllm-project/vllm/commit/02809af1e7cdf63bcf0d73b63a35ad3ea5c6fb81
[Bugfix]: Fix cross attention backend selection for Turing GPU (#31806)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：修复了在 Turing GPU 上选择跨注意力（Cross Attention）后端时的错误。通过在调用 `get_attn_backend` 时显式传入 `attn_type=AttentionType.ENCODER_DECODER`，确保能够正确获取适配跨注意力场景的后端实现，从而避免因后端选择不当导致的计算错误或性能异常。  

**🎯 影响范围**：`vllm/attention/layers/cross_attention.py`（跨注意力层），以及间接依赖该层的模型推理路径（Encoder‑Decoder 模型、Chat 模型等）。  

**🔍 技术洞察**：  
- **架构影响**：无结构性改动，仅在跨注意力层的初始化阶段调整了后端获取方式，保持模块划分和接口不变。  
- **性能影响**：通过正确的后端选择，Turing GPU 将使用专门为 Encoder‑Decoder 场景优化的 kernel，提升跨注意力计算的吞吐和延迟；在其他 GPU 上行为保持不变。  
- **安全考虑**：不涉及安全功能或数据处理，风险为 “无”。  

**⚠️ 潜在风险**：  
- 如果 `get_attn_backend` 在未来的实现中对 `attn_type` 参数的处理不兼容，可能导致后端选择回退到不适配的实现。  
- 受影响的代码路径在非 Turing GPU 上仍会执行同样的调用，若该路径本来不需要显式 `attn_type`，可能出现不必要的分支检查（影响可忽略）。  

**💡 关注建议**：  
- 在多种 GPU（包括 Ampere、Ada 等）上跑完整的单元/集成测试，确保跨注意力功能在所有硬件上均保持正确。  
- 验证性能基准（如吞吐、延迟）在 Turing GPU 上是否得到提升，防止出现回退到通用后端的情况。  
- 关注后续 `get_attn_backend` 的签名或实现变化，必要时更新调用处的参数保持兼容。  

### cbd4690a03a458580370660dc443ee9de59416cd
https://github.com/vllm-project/vllm/commit/cbd4690a03a458580370660dc443ee9de59416cd
[LoRA]Disable linear LoRA  kernel PDL (#31777)

Signed-off-by: Jee Jee Li <pandaleefree@gmail.com>
**🎯 变更类型**：性能优化 / Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在 LoRA 相关的 Triton kernel 中关闭了 PDL（Persistent Thread Block Launch）特性，并在内部函数签名中新增 `use_gdc` 参数以显式控制该特性。文档同步更新，提供了当前多模态模型 LoRA 支持状态的查询链接。此举避免了因 LoRA kernel 非连续调用导致的 PDL 无效问题，从而提升整体执行的可靠性。  

**🎯 影响范围**：  
- `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`（核心 Fusion MoE LoRA 实现）  
- `vllm/lora/ops/triton_ops/lora_expand_op.py`、`vllm/lora/ops/triton_ops/lora_shrink_op.py`（LoRA Expand / Shrink Triton kernels）  
- `docs/features/lora.md`（文档）  

**🔍 技术洞察**：  
- **架构影响**：无大幅架构改动，仅在 LoRA Triton 实现层新增 `use_gdc` 参数并默认设为 `False`，对上层调用保持兼容。  
- **性能影响**：  
  - 正面：避免了 PDL 在非 back‑to‑back 调用场景下产生的无效启动，防止潜在的性能抖动和错误。  
  - 负面：在支持 PDL 的 GPU 上（如 Nvidia Hopper+）若 LoRA kernel 能够满足 back‑to‑back 条件，关闭 PDL 可能导致稍微的性能下降。总体影响预计在 **10% 以内**，且对大多数用户为正向提升。  
- **安全考虑**：无涉及安全敏感的改动，关闭 PDL 只影响执行调度，不会引入新的攻击面。  

**⚠️ 潜在风险**：  
1. **性能回归**：在少数能够真正利用 PDL 的硬件上，默认关闭可能使 LoRA 推理略慢。  
2. **兼容性**：新增的 `use_gdc` 参数若在未来的上层调用中未显式传递，可能导致行为与预期不一致（虽然当前已统一设为 `False`）。  
3. **回滚成本**：若后续需要重新开启 PDL，需恢复 `supports_pdl` 调用并确保 kernel 调度满足 back‑to‑back 条件。  

**💡 关注建议**：  
- **回归测试**：在支持 PDL 的 GPU（如 A100、H100）上对比开启/关闭 `use_gdc` 的吞吐量与延迟，确保性能下降在可接受范围。  
- **基准监控**：发布后监控关键模型（尤其是多模态 MoE）在生产环境的推理时延，以捕捉潜在的性能异常。  
- **文档同步**：继续在 `lora.md` 中维护“当前模型支持状态”链接，帮助用户快速了解哪些模型已实现相应的 token helper。  
- **代码注释**：保留 `# supports_pdl(inputs.device)` 形式的注释，以便后续评估在特定场景下重新打开 PDL。  

---

### 96860af655688163d0228e128a06dd6c462be271
https://github.com/vllm-project/vllm/commit/96860af655688163d0228e128a06dd6c462be271
[Model] rename use_pad_token to use_sep_token (#31784)

Signed-off-by: wang.yuqi <yuqi.wang@daocloud.io>
**🎯 变更类型**：重构 / 依赖更新（属性重命名 & 旧属性兼容）

**⚡ 重要程度**：🟡 中

**📋 变更摘要**  
将模型配置中原先的 `use_pad_token` 参数统一改为 `use_sep_token`，并在 `ModelConfig` 中提供向后兼容的警告与读取旧属性的逻辑。所有涉及该标识的示例、单元测试、以及运行时路径均同步更新，以统一使用 “separating token” 的概念。

**🎯 影响范围**  
- `examples/pooling/score/convert_model_to_seq_cls.py`（模型转换脚本）  
- `tests/entrypoints/pooling/score/test_utils.py`（相关单元测试）  
- `vllm/config/model.py`（配置模型核心实现）  
- `vllm/entrypoints/score_utils.py`（生成评分 Prompt 的工具）  
- `vllm/model_executor/models/adapters.py`（模型适配器配置更新）  

**🔍 技术洞察**  

- **架构影响**：  
  - 通过在 `ModelConfig.use_sep_token` 中加入对旧属性 `use_pad_token` 的读取与一次性警告，实现了向后兼容，避免因直接访问旧属性导致的 `AttributeError`。  
  - 其他模块（如 `score_utils`、`adapters`）均改为依据 `use_sep_token` 决定是否在 Prompt 中插入分隔符，从而统一了跨模型（cross‑encoder 与 llm‑as‑reranker）的分隔符策略。  

- **性能影响**：  
  - 仅是属性名称的切换和一次属性读取的额外判断，时间与空间复杂度均不变，对运行时性能影响可以忽略不计。  

- **安全考虑**：  
  - 无直接安全风险。唯一需要注意的是 **警告信息**（`logger.warning_once`）在生产环境可能被静默，若用户未留意新属性文档，可能在后续升级时出现行为不一致的情况。  

**⚠️ 潜在风险**  

1. **向后兼容失效**：外部工程直接使用 `model_config.use_pad_token`（而非通过 `ModelConfig` 提供的属性访问）将不再获得预期值，可能导致 Prompt 生成逻辑错误。  
2. **文档/示例遗漏**：若项目文档未同步更新 “use_sep_token” 的说明，使用者可能继续阅读旧示例，引起混淆。  
3. **测试覆盖不足**：当前仓库的单元测试已更新，但对第三方用户自定义的适配器、插件等未覆盖，升级后可能出现隐蔽错误。  

**💡 关注建议**  

- **迁移指南**：在发布说明中明确 `use_pad_token` 已被废弃，推荐将所有代码、配置文件、模型微调脚本统一为 `use_sep_token`。  
- **兼容性检查**：在升级前使用搜索工具定位项目中仍调用 `use_pad_token` 的位置，并补全迁移。  
- **持续监控**：在 CI 中加入对 `logger.warning_once` 的捕获校验，确保旧属性的使用能够触发一次性警告，帮助用户尽早发现潜在风险。  
- **回归测试**：在升级后执行完整的 `pytest -m "pooling"`（或全套）以确认 Prompt 生成逻辑在 `use_sep_token=True/False` 两种情形下与预期一致。  
- **文档同步**：更新 `README`、API 文档、以及示例脚本中的注释，说明分隔符的概念及默认行为（cross‑encoder 默认使用，LLM‑as‑reranker 默认不使用）。  

通过上述措施，可在保持功能不变的情况下平滑完成属性重命名，降低因老旧代码导致的运行时错误风险。

### 0202971a4842e129027b0931870112e45ee9ad65
https://github.com/vllm-project/vllm/commit/0202971a4842e129027b0931870112e45ee9ad65
[Frontend] Support GLM-4.5 / GLM-4.7 with enable_thinking: false (#31788)

Signed-off-by: chaunceyjiang <chaunceyjiang@gmail.com>
**🎯 变更类型**：功能增强 / 重构  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `DeepSeekV3ReasoningParser` 中改为安全读取 `chat_template_kwargs`，并显式支持 `enable_thinking` 参数。  
- `Glm4MoeModelReasoningParser` 由继承 `DeepSeekR1ReasoningParser` 改为继承 `Holo2ReasoningParser`，以适配 GLM‑4.5/4.7。  
- `Holo2ReasoningParser` 现在同时考虑 `thinking` 与 `enable_thinking`，默认均为 `True`，并在两者均为 `True` 时启用思考模式（使用 `DeepSeekR1ReasoningParser`），否则使用 `IdentityReasoningParser`。  

**🎯 影响范围**  
- `vllm/reasoning/*` 包下的解析器实现：`deepseek_v3_reasoning_parser.py`、`glm4_moe_reasoning_parser.py`、`holo2_reasoning_parser.py`。  
- 受影响的模型：GLM‑4.5、GLM‑4.7 以及使用 `Holo2ReasoningParser`（间接影响 DeepSeek 系列模型）。  

**🔍 技术洞察**  

- **架构影响**：  
  - 将 `Glm4MoeModelReasoningParser` 的基类切换到 `Holo2ReasoningParser`，统一了思考模式的实现路径，提升了代码复用。  
  - `Holo2ReasoningParser` 现在通过 `thinking` 与 `enable_thinking` 双重开关控制，提供更细粒度的配置能力，符合新版 GLM 的需求。  

- **性能影响**：  
  - 读取 `kwargs` 改为 `get` 而非 `pop`，避免对原始字典的副作用，性能变化可忽略（O(1) 读取）。  
  - 逻辑分支略有增加（两层布尔与），对整体推理吞吐量影响微乎其微。  

- **安全考虑**：  
  - 更改为 `kwargs.get` 保持原始字典不被修改，降低意外泄露或误删参数的风险。  
  - 新增的 `enable_thinking` 默认 `True`，若下游系统误将其设为 `False` 可能导致思考模式被关闭，需在文档中明确说明默认行为。  

**⚠️ 潜在风险**  

1. **向后兼容性**  
   - 之前代码可能依赖 `pop` 语义（即在解析器内部移除 `thinking`/`enable_thinking`），现在这些键仍保留在 `kwargs` 中，若后续其他组件再次读取同一字典，可能得到与预期不一致的值。  
2. **默认行为变更**  
   - `enable_thinking` 的默认值从 `False`（在 `DeepSeekV3` 中）变为 `True`（在 `Holo2` 中），可能影响未显式指定该参数的模型行为。  
3. **组合逻辑误用**  
   - 新的 `thinking = thinking and enable_thinking` 逻辑要求两者均为 `True` 才会开启思考模式，用户若只设置其中一个可能误以为已开启。  

**💡 关注建议**  

- **回归测试**：  
  - 对所有已有模型（包括 DeepSeek、Holo2、GLM‑4 系列）执行完整的推理、结构化输出和思考模式测试，确保 `thinking` / `enable_thinking` 的不同组合行为符合预期。  
- **文档更新**：  
  - 在 `chat_template_kwargs` 与模型配置文档中明确说明 `thinking` 与 `enable_thinking` 的默认值、交互关系以及何时需要显式关闭思考模式。  
- **参数审计**：  
  - 检查项目中是否有其他解析器或组件在调用前后直接修改 `kwargs`，防止出现隐藏的副作用。  
- **兼容层**：  
  - 若担心旧代码仍依赖 `pop`，可在解析器入口处添加兼容性包装：在读取后 `pop` 掉已有键，保持旧行为不变。  

通过上述措施，可平滑支持 GLM‑4.5/4.7 的新特性，同时降低对现有模型的破坏风险。

### 2c1a4f2488da2bb3c60dcbbb5df61a370e39ee27
https://github.com/vllm-project/vllm/commit/2c1a4f2488da2bb3c60dcbbb5df61a370e39ee27
[Bugfix]: avoid overriding audio/text kwargs (Qwen3-Omni) (#31790)

Signed-off-by: Zhongze Jiang <jiangzhongze.jzz@ant-intl.com>
**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/model_executor/models/qwen3_omni_moe_thinker.py` 中修复了在处理音频/文本参数时会意外覆盖已有 `audio_kwargs`、`text_kwargs` 的问题。通过在使用前显式创建这两个字典并采用 `setdefault` 合并 `truncation` 参数，保证原有自定义参数不被擦除。  

**🎯 影响范围**：  
- `qwen3_omni_moe_thinker` 模型执行路径（仅该文件相关的代码）  
- 与 Transformers 版本 < 4.58.0 兼容的音频特征提取/文本 tokenization 逻辑  

**🔍 技术洞察**：  
- **架构影响**：无全局架构变更，仅局部函数内部的参数组织方式调整，保持原有模块耦合不变。  
- **性能影响**：新增两次 `dict()` 构造和 `setdefault` 调用，开销极小，对整体吞吐量和 latency 基本可以忽略。  
- **安全考虑**：未涉及安全相关逻辑，改动仅为防止参数意外覆盖，不会引入新的安全风险。  

**⚠️ 潜在风险**：  
- 对于极端情况下 `mm_kwargs` 本来已经包含 `audio_kwargs`/`text_kwargs` 且其中有 `None` 值的场景，`dict(None)` 会抛异常。当前实现使用 `or {}` 可避免此问题，但仍需确认上游代码不会传递非映射对象。  
- 仅在 Transformers 版本 `< 4.58.0` 时生效，若未来升级到更高版本且仍保留此分支，可能出现冗余或不一致的 `truncation` 处理。  

**💡 关注建议**：  
1. **回归测试**：加入包含自定义 `audio_kwargs`、`text_kwargs`（尤其是已经设置 `truncation`）的单元测试，确保这些键在不同 Transformers 版本下均能保留。  
2. **升级提示**：在发布说明中注明此修复针对 Transformers `< 4.58.0` 的兼容性，建议用户在升级到 >= 4.58.0 时验证 `truncation` 行为是否仍符合预期。  
3. **代码审查**：后续若对 `mm_kwargs`/`tok_kwargs` 做进一步结构化改动，继续使用 `setdefault` 或更显式的合并策略，避免类似覆盖问题重复出现。  

### 6444824873711eeeb68ded97cad79394bd051ed1
https://github.com/vllm-project/vllm/commit/6444824873711eeeb68ded97cad79394bd051ed1
[Misc] Implement `TokenizerLike.convert_tokens_to_ids` (#31796)

Signed-off-by: DarkLight1337 <tlleungac@connect.ust.hk>
**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 `vllm/tokenizers` 模块中为 `TokenizerLike` 接口以及其具体实现（DeepSeek‑V32、Mistral）新增 `convert_tokens_to_ids` 方法的重载声明与实现。该方法接受单个 token（`str`）或 token 列表（`list[str]`），并分别返回对应的 id（`int`）或 id 列表（`list[int]`），实现上直接委托给底层 HuggingFace `tokenizer`。

**🎯 影响范围**  
- `vllm/tokenizers/deepseek_v32.py`  
- `vllm/tokenizers/mistral.py`  
- `vllm/tokenizers/protocol.py`（`TokenizerLike` 抽象协议）  

**🔍 技术洞察**  

- **架构影响**：  
  - 为统一的 `TokenizerLike` 接口补全了 token‑to‑id 的转换能力，提升了抽象层的一致性。  
  - 通过 `overload` 提供更细致的类型签名，改善了 IDE 与 `mypy` 等静态检查工具的可用性。  
  - 对现有类的实现仅增加了薄包装层，无需修改调用方的业务逻辑。  

- **性能影响**：  
  - 方法仅是一次函数调用的转发，时间复杂度与底层 `transformers` 的实现保持一致（`O(1)` 对单 token，`O(n)` 对 token 列表）。  
  - 由于未引入额外计算或缓存，空间开销几乎为零。  

- **安全考虑**：  
  - 无新增安全相关逻辑，调用底层 tokenizer 的转换函数本身已在项目中使用多年。  
  - 仅需确保传入的 token 类型符合期望，否则底层库会抛出 `KeyError` / `ValueError`，这属于预期行为。  

**⚠️ 潜在风险**  

1. **兼容性**  
   - 新增的抽象方法在 `TokenizerLike` 协议中标记为必实现，但在协议本身仍保持 `raise NotImplementedError`，因此旧的自定义 tokenizer 只要实现 `convert_tokens_to_ids` 即可通过类型检查。若有第三方实现未实现该方法，可能在运行时触发 `AttributeError`。  

2. **类型检查冲突**  
   - `overload` 必须与实现签名严格对应，若未来对 `tokenizer.convert_tokens_to_ids` 的返回类型做出改动（如返回 `torch.Tensor`），需要同步更新重载声明。  

3. **误用**  
   - 调用者可能误以为传入 `list[str]` 会返回单一 `int`（或相反），需要明确文档说明。  

**💡 关注建议**  

- **单元测试**：为每个 tokenizer 添加测试，覆盖  
  - 单 token → 单 id  
  - 多 token 列表 → id 列表  
  - 非法 token（如空字符串、未收录的 token）是否抛出合适异常。  

- **文档更新**：在 `vLLM` 的 tokenizer 使用手册中说明新方法的用途、参数与返回值。  

- **向后兼容检查**：如果项目中有自定义实现 `TokenizerLike`（非 HuggingFace），请确保它们实现 `convert_tokens_to_ids`，否则在使用统一接口时会出现运行时错误。  

- **升级注意事项**：此变更为向后兼容的增量功能，升级 vLLM 版本不需要额外迁移步骤；仅在使用自定义 tokenizer 时需实现新方法。  

### bf0f3a4638869e3fdb2df38b61f41557acc69dfe
https://github.com/vllm-project/vllm/commit/bf0f3a4638869e3fdb2df38b61f41557acc69dfe
[Bugfix] Fix torch.compile error for DP + MoE on CPU Backend (#31650)

Signed-off-by: kunzh <zhikun.wu@outlook.com>
**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：修复了在 CPU 后端使用数据并行 (DP) + Mixture‑of‑Experts (MoE) 时，`torch.compile` 报错的问题。核心改动是调整 `vllm/model_executor/layers/fused_moe/layer.py` 中 `forward_impl` 对 `post_quant_allgather` 条件的判断顺序，使得对 `has_flashinfer_trtllm_fused_moe()` 的调用在确认量化方法非空且类型匹配后才执行，从而避免在不满足前置条件的情况下触发潜在的编译错误。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/fused_moe/` 目录下的 `layer.py`（MoE 相关前向实现）  
- 依赖该层的模型执行路径，尤其是 **CPU** 环境下开启 **DP**（数据并行）并使用 **MoE** 的场景  

**🔍 技术洞察**：  
- **架构影响**：  
  - 仅涉及局部条件判断顺序的修改，无结构性变化，保持原有模块划分和接口不变。  
  - 通过延后 `has_flashinfer_trtllm_fused_moe()` 的调用，避免在未满足量化前置条件时触发可能依赖 GPU/特定库的检查，从而兼容 CPU 后端。  

- **性能影响**：  
  - 条件判断顺序改变后，**cpu** 上的分支路径更短（只有在量化方法匹配时才调用 `has_flashinfer_trtllm_fused_moe()`），理论上略微降低了分支预测失误率和函数调用开销。  
  - 对整体吞吐量或延迟影响可以忽略不计。  

- **安全考虑**：  
  - 无安全相关改动。该修改仅影响逻辑分支，不会引入新的外部依赖或数据处理路径。  

**⚠️ 潜在风险**：  
1. **兼容性回归**：如果在未来的版本中 `has_flashinfer_trtllm_fused_moe()` 的实现发生变化（比如不再安全地在未满足量化前置条件时调用），仍可能触发错误。  
2. **未覆盖的测试场景**：该分支主要在 **CPU + DP + MoE + 量化** 的组合下触发，若缺乏相应的单元/集成测试，可能出现未捕获的分支错误。  

**💡 关注建议**：  
- **测试**：在 CI 中加入 CPU 后端下开启 DP、MoE 并使用 `ModelOptNvFp4FusedMoE` 量化的矩阵乘法/推理路径测试，确保 `torch.compile` 不再抛异常。  
- **文档**：更新 Release Note 或 README，说明在 CPU 环境下已修复 DP+MoE 的 compile 错误，用户可以放心使用。  
- **代码审核**：后续若对 `has_flashinfer_trtllm_fused_moe` 进行改动，请保持其“安全”特性，即在不满足前置条件时应返回 `False` 而不触发任何底层库调用。  
- **监控**：部署后关注错误日志中是否仍出现与 `torch.compile` 相关的报错，及时回滚或补丁。  

### e0327c9db20652b3cef6ffc29adb3a2e1cc3fba1
https://github.com/vllm-project/vllm/commit/e0327c9db20652b3cef6ffc29adb3a2e1cc3fba1
[Attention][1/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31773)

Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com>
**🎯 变更类型**：重构 / 性能优化 / 依赖更新  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 移除了 `CommonAttentionMetadata` 中已废弃的属性 `seq_lens_cpu` 与 `num_computed_tokens_cpu` 的直接使用，统一改为在需要时通过 `seq_lens.cpu()` 或 `compute_num_computed_tokens()` 访问。  
- 为 `CommonAttentionMetadata` 新增 `compute_num_computed_tokens()` 方法并加入内部缓存，以在设备上一次性计算并复用 `num_computed_tokens`。  
- 相应地更新了多个 attention backend 实现以及单元测试，确保代码路径都走新接口。

**🎯 影响范围**  
- `vllm/v1/attention/backends/*`（flashinfer、flex_attention、mla、flashmla_sparse、triton_attn）  
- `vllm/v1/attention/backends/utils.py`（`CommonAttentionMetadata`）  
- 测试文件 `tests/v1/attention/*`  

**🔍 技术洞察**  
- **架构影响**：  
  - 统一了对序列长度与已计算 token 数的访问方式，降低了不同 backend 之间的实现差异。  
  - 新增的缓存字段 `_num_computed_tokens_cache` 将元数据对象的状态从“纯只读”轻微转为“可变”，但仅在第一次计算后保持不变，符合目前的使用场景（`CommonAttentionMetadata` 在一次推理请求生命周期内不再修改）。  
- **性能影响**：  
  - 通过 `seq_lens.cpu()` 的显式调用，避免了旧实现中隐式、可能重复的 CPU 数据拉取。  
  - `compute_num_computed_tokens()` 在设备上一次性完成计算，并在后续使用时直接复用缓存，减少了 CPU‑GPU 同步和额外的张量创建，预期在大批量 decode/prefill 场景下可降低显式同步开销。  
- **安全考虑**：  
  - 无直接安全风险。唯一需要关注的是缓存的正确性：若在同一 `CommonAttentionMetadata` 实例生命周期内 `seq_lens` 或 `query_start_loc` 被意外修改，缓存会变得不一致。当前代码路径未修改这些张量，风险可接受。  

**⚠️ 潜在风险**  
1. **缓存失效风险**：若未来引入对 `seq_lens`、`query_start_loc` 的原地修改（例如在增量推理中复用同一对象），需在修改后显式清除 `_num_computed_tokens_cache`，否则会得到错误的 `num_computed_tokens`。  
2. **兼容性**：外部代码仍可能直接访问已废弃的属性 `seq_lens_cpu`、`num_computed_tokens_cpu`（虽然已标记为 `@deprecated`），在未升级的用户环境中会触发属性访问错误。  
3. **异步模式下的 CPU 拉取**：在 `flashinfer` 中仍保留 `needs_seq_lens_cpu` 判断，确保仅在必要时才调用 `.cpu()`，但若误将 `needs_seq_lens_cpu` 设为 `True`，仍会产生一次 CPU 同步。  

**💡 关注建议**  
- **回归测试**：在多卡、异步推理、混合 decode‑prefill 场景下运行完整的注意力后端测试，确保没有因为缓存或 `.cpu()` 调用导致的顺序错误或性能回退。  
- **文档与发布说明**：在 Release Notes 中明确标记 `seq_lens_cpu`、`num_computed_tokens_cpu` 已废弃，并推荐使用 `seq_lens` (或 `seq_lens.cpu()`) 与 `compute_num_computed_tokens()`。  
- **防御性实现**：如有计划在未来允许 `CommonAttentionMetadata` 可变，考虑在 `compute_num_computed_tokens()` 前加入 `if self._num_computed_tokens_cache is None or self._seq_lens_changed:` 的检查，或者提供 `invalidate_cache()` 接口。  
- **监控指标**：在高吞吐量部署中监控 `torch.cuda.synchronize` 的调用次数或 CPU‑GPU 数据传输时间，验证新实现带来的性能提升是否符合预期。

### 14df02b4e15891c00a24b7fcf7d62076079c38d5
https://github.com/vllm-project/vllm/commit/14df02b4e15891c00a24b7fcf7d62076079c38d5
[Chore] Cleanup `mem_utils.py` (#31793)

Signed-off-by: DarkLight1337 <tlleungac@connect.ust.hk>
**🎯 变更类型**：重构 / 代码清理  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 修正了 `mem_utils.py` 中的拼写错误和注释格式，使错误信息更准确；  
- 将 `MemoryProfilingResult.weights_memory` 从 `float` 改为 `int`，并在 `__post_init__` 中自动创建 `before_profile` 与 `after_profile` 快照；  
- 调整 `memory_profiling` 上下文管理器的签名、实现及文档，使其更易使用并兼容默认参数。  

**🎯 影响范围**  
- `vllm/utils/mem_utils.py`（核心工具模块）  
- 依赖 `MemoryProfilingResult` 与 `memory_profiling` 的所有调用方（模型加载、内存分析脚本、单元测试）  

**🔍 技术洞察**  
- **架构影响**：  
  - `MemoryProfilingResult` 现在在实例化后即拥有 `before_profile` 与 `after_profile` 两个 `MemorySnapshot`，提升了对象的自洽性，减少了外部手动赋值的需求。  
  - `memory_profiling` 采用了带默认值的 `weights_memory` 参数，向后兼容性提升，调用方可以省略该参数。  
- **性能影响**：  
  - 代码改动仅涉及对象初始化和字符串拼接，无额外计算开销；  
  - 使用 `torch.cuda.reset_peak_memory_stats(device)`（如果 PyTorch 支持）可避免全局重置，提高在多 GPU 环境下的准确性。  
- **安全考虑**：  
  - 无安全相关改动。  
  - 断言信息更清晰，对异常排查有帮助。  

**⚠️ 潜在风险**  
1. **PyTorch 兼容性**：`torch.cuda.reset_peak_memory_stats(device)` 仅在较新版本的 PyTorch 中接受 `device` 参数，旧版可能抛出 `TypeError`，导致运行时错误。  
2. **类型变更**：`weights_memory` 从 `float` 改为 `int`，若外部代码依赖其为浮点数进行除法或格式化，可能出现 `TypeError` 或精度差异。  
3. **`__post_init__` 使用 `before_create.device_`**：若实例化时未提供 `before_create`（仍使用默认工厂），`device_` 仍可获取；但若后续手动修改 `before_create`，`before_profile`/`after_profile` 已在对象创建时固定，可能与后期更改不一致。  

**💡 关注建议**  
- **兼容性测试**：在 CI 中加入对多个 PyTorch 版本（尤其是 1.13 以下）执行 `memory_profiling` 的回归测试，确保 `reset_peak_memory_stats` 的调用方式不会导致异常。  
- **类型校验**：检查所有调用 `memory_profiling(..., weights_memory=…)` 的代码路径，确认传入的值为整数或在需要浮点的场景中自行转换。  
- **文档更新**：在项目文档或函数注释中明确 `weights_memory` 参数的类型与默认值，防止误用。  
- **额外测试场景**：  
  - 多 GPU 环境下分别调用 `memory_profiling`，验证 `device_` 参数正确传递且统计数据准确。  
  - 在使用自定义 `MemorySnapshot` 的情况下，确认 `before_profile` 与 `after_profile` 能正确捕获快照而不受 `__post_init__` 影响。  

通过上述验证与文档补充，可将本次重构的潜在风险降至最低，并确保在现有以及未来的 vLLM 版本中保持稳定可靠。

### 6ebb66ccea611fa8bd8718074836917c6ad9f5ef
https://github.com/vllm-project/vllm/commit/6ebb66ccea611fa8bd8718074836917c6ad9f5ef
[Doc] Fix format of multimodal_inputs.md (#31800)

Signed-off-by: BlankR <hjyblanche@gmail.com>
**🎯 变更类型**：文档  

**⚡ 重要程度**：🟢低  

**📋 变更摘要**：  
- 对 `docs/features/multimodal_inputs.md` 中的多模态示例代码块格式进行修正，使代码块能够正确渲染并保持可执行示例的完整性。  
- 补全了缺失的代码块标记（```python）、调整了缩进和注释位置，确保文档在 GitHub/阅读器中呈现为可复制的 Python 示例。  

**🎯 影响范围**：  
- `docs/features/multimodal_inputs.md`（文档页面）  
- 可能间接影响依赖该文档的自动化文档生成或 CI 检查流程  

**🔍 技术洞察**：  
- **架构影响**：无（仅文档层面）  
- **性能影响**：无（不涉及运行时代码）  
- **安全考虑**：无（不涉及代码执行或安全敏感信息）  

**⚠️ 潜在风险**：  
- 若在修复过程中不慎引入拼写错误或破坏现有链接，可能导致阅读者误解或示例无法运行。  
- 文档渲染工具（如 Sphinx、MkDocs）在解析异常的代码块时可能触发警告，需要确认 CI 文档构建通过。  

**💡 关注建议**：  
- 本地预览或 CI 中重新构建文档，确认所有代码块均能正确高亮并可复制运行。  
- 在发布新版本之前，执行一次文档链接完整性检查，确保 `examples/offline_inference/vision_language_multi_image.py` 等引用仍然有效。  
- 若项目使用自动化文档生成（如 `mkdocs`），建议在合并后运行一次完整的构建，以捕捉潜在的 Markdown 语法错误。  

### 43d384bab4b86e494cb1c3d04b14e13512b999d1
https://github.com/vllm-project/vllm/commit/43d384bab4b86e494cb1c3d04b14e13512b999d1
[CI] Increase the MTEB_EMBED_TOL threshold to 5e-4. (#31797)

Signed-off-by: wang.yuqi <yuqi.wang@daocloud.io>
**🎯 变更类型**：测试 / 配置  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 将 `MTEB_EMBED_TOL` 阈值从 `1e-4` 调高至 `5e-4`，放宽对模型在 MTEB 嵌入任务上的误差容忍度。  
2. 在多个 MTEB 相关测试文件中删除显式的 `dtype="float32"` 参数，改为使用默认或自动推断的 dtype。  

**🎯 影响范围**：  
- `tests/models/language/pooling_mteb_test/` 目录下的所有测试文件（`mteb_embed_utils.py`、`test_baai.py`、`test_gte.py`、`test_jina.py`、`test_st_projector.py`）。  
- 与 MTEB 嵌入评估相关的 CI 流程。  

**🔍 技术洞察**：  
- **架构影响**：无。此改动仅涉及测试代码，未触及生产业务逻辑或模块接口。  
- **性能影响**：无直接性能影响。放宽容差可能导致部分回归检测被忽略。  
- **安全考虑**：无安全风险。  

**⚠️ 潜在风险**：  
1. **回归检测失效**：阈值提升 5 倍后，细微的模型质量下降可能不再被测试捕获，增加质量回退的隐蔽性。  
2. **dtype 隐式变化**：删除 `dtype="float32"` 依赖默认 dtype，若底层模型默认切换到 `bfloat16`、`float16` 等低精度格式，可能导致测试结果波动或不一致。  
3. **CI 可靠性**：若 CI 环境的默认 dtype 与本地开发环境不一致，可能出现 “本地通过 / CI 失败” 的情况。  

**💡 关注建议**：  
- 在 CI 中显式打印或记录实际使用的 dtype，确保所有环境使用统一的数据类型。  
- 若计划长期使用更宽松的容差，建议在提交说明中明确该阈值的依据，并在关键评估指标（如对外发布的模型基准）保持更严格的阈值。  
- 添加补充测试：  
  - 验证不同 dtype（`float32`、`bfloat16`）下的 MTEB 分数波动范围是否仍在可接受范围内。  
  - 在关键模型（如 `Qwen3-Embedding` 系列）上进行对比测试，确保提升容差后仍能检测到显著性能退化。  
- 若未来恢复显式 dtype 参数，请在对应测试文件中添加注释说明原因，防止因隐式 dtype 变化导致误判。  

### db318326a5d23893386143ece6be40d48b081f80
https://github.com/vllm-project/vllm/commit/db318326a5d23893386143ece6be40d48b081f80
[Misc] Use `deprecated` for `seed_everything` (#31780)

Signed-off-by: DarkLight1337 <tlleungac@connect.ust.hk>
**🎯 变更类型**：重构 / 兼容性改动（废弃 `seed_everything`，改用 `set_random_seed`）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在所有 benchmark 相关脚本中，将原先通过 `current_platform.seed_everything` 设置随机种子的调用替换为 `vllm.utils.torch_utils.set_random_seed`。  
2. 在平台抽象 `vllm/platforms/interface.py` 中为 `seed_everything` 添加 `@deprecated` 装饰，明确其将在 v0.15.0 后被移除，同时更新文档对应的废弃提示。  
3. 对文档和少量 import 进行相应调整，确保代码库在未来版本中统一使用 `set_random_seed`。

**🎯 影响范围**  
- `benchmarks/kernels/` 下的所有基准测试脚本（activation、layernorm、moe、moe_permute_unpermute、mrope、paged_attention、quant、reshape_and_cache、reshape_and_cache_flash、silu_mul_fp8_quant）  
- 平台抽象层 `vllm/platforms/interface.py`  
- 文档 `docs/design/plugin_system.md`  
- 依赖 `vllm.utils.torch_utils` 中的 `set_random_seed`（已被广泛引入）  

**🔍 技术洞察**  
- **架构影响**：  
  - 将随机数种子设置从平台层抽象（`current_platform`）下沉到通用工具层，降低平台接口耦合度。  
  - 通过 `@deprecated` 明确废弃路径，保持向后兼容的同时为后续清理提供清晰指引。  
- **性能影响**：  
  - `set_random_seed` 的实现本质上与原 `seed_everything` 相同，仅是调用路径的变化，时间/空间复杂度不变。  
- **安全考虑**：  
  - 无新增安全风险。统一的种子设置有助于可复现性，避免因平台不同实现产生的微小差异。  

**⚠️ 潜在风险**  
1. **破坏性升级风险**：在 v0.15.0 之前仍然使用 `current_platform.seed_everything` 的外部项目会在升级后遇到 `AttributeError`（方法已被移除）。  
2. **警告噪声**：现阶段仍保留 `seed_everything` 方法，会在运行时产生 deprecation 警告，可能影响 CI/测试日志的干净度。  
3. **遗漏改动**：如果还有未在搜索范围内的代码（如自定义插件或内部脚本）仍调用 `seed_everything`，升级后会失效。  

**💡 关注建议**  
- **开发者**：尽快在所有自研代码、脚本、插件中改用 `vllm.utils.torch_utils.set_random_seed`，并在本地 CI 中确认不再出现废弃警告。  
- **用户**：在升级到 v0.15.0 前，留意升级日志，确保已完成上述替换；必要时在 `setup.cfg`/`pytest.ini` 中屏蔽 `DeprecationWarning`，但建议直接迁移。  
- **测试**：对现有 benchmark 测试套跑一遍，验证随机种子设置前后得到的可复现性一致，防止因导入顺序或函数实现细节差异导致结果偏差。  
- **文档**：在下个版本发布说明中明确 `seed_everything` 将在 v0.15.0 完全移除，提供迁移示例。  

### 799b5721f6bb89ad03e47ba79131e0ce65023635
https://github.com/vllm-project/vllm/commit/799b5721f6bb89ad03e47ba79131e0ce65023635
[cpu][bench] Add CPU paged attention benchmarks (#31720)

Signed-off-by: Fadi Arafeh <fadi.arafeh@arm.com>
**🎯 变更类型**：性能优化 / 基准测试  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 新增 `benchmarks/kernels/cpu/benchmark_cpu_attn.py` 脚本，用于对 vLLM CPU paged attention 核心实现进行可配置的微基准测试。  
- 脚本封装了随机张量缓存、CPU ISA（neon / amx / vec）自动检测、以及 KV‑cache 打包与调度元数据生成，输出 min/mean/max/std 等统计信息。  
- 为后续性能调优、平台对齐（ARM vs x86）及 ISA 选型提供了可重复、可参数化的测量手段。

**🎯 影响范围**：  
- `benchmarks/kernels/cpu/benchmark_cpu_attn.py`（新增）  
- 间接依赖：`vllm._custom_ops`、`vllm.platforms`、`vllm.v1.attention.backends.cpu_attn`、`vllm.utils.argparse_utils`、`vllm.utils.torch_utils`  

**🔍 技术洞察**：  
- **架构影响**：无业务层代码改动，仅在 *benchmarks* 目录新增独立脚本。通过 `current_platform.get_cpu_architecture()` 与 `torch._C._cpu._is_amx_tile_supported()` 动态选择 ISA，保持与 CPUAttentionBackend 的实现一致，未破坏模块边界。  
- **性能影响**：脚本本身是测量工具，不会影响库运行时性能。使用 `functools.lru_cache` 缓存随机张量，以削减基准热身阶段的生成开销；基准循环只计时 `cpu_attention_with_kv_cache`，提供准确的毫秒级统计。  
- **安全考虑**：仅涉及读取 CPU 信息、随机数生成和内存分配，无网络或文件写入操作，未引入新的安全风险。  
- **可维护性**：代码结构清晰，参数通过 `FlexibleArgumentParser` 管理，支持多平台、不同 head‑size、block‑size、KV‑split、滑动窗口等组合。文档注释较少，可在后续加入使用示例或 CI 指南。  

**⚠️ 潜在风险**：  
- **依赖加载**：脚本在导入阶段会拉入 `vllm._custom_ops`（可能包含 C++/CUDA 扩展），在不具备相应二进制的环境中运行会报错。  
- **CI 影响**：若项目的 CI 自动执行 `benchmarks/` 目录下的 *.py*，可能导致不必要的长时间运行或资源占用。  
- **平台差异**：ISA 自动检测基于当前 PyTorch 编译特性，某些自定义构建的 PyTorch 可能缺少 `torch._C._cpu._is_amx_tile_supported()`，导致回退至不匹配的实现。  

**💡 关注建议**：  
1. **文档 & 示例**：在项目 README 或 `benchmarks/README.md` 中说明脚本的使用场景、必备依赖（CPU ISA 支持的 PyTorch 构建）以及推荐的运行参数。  
2. **CI 配置**：将该基准脚本排除在常规单元测试/CI 流程之外，或仅在专门的性能回归流水线中执行。  
3. **兼容性检测**：在脚本入口加入对 `torch._C._cpu._is_amx_tile_supported` 的存在性检查，提供友好的回退提示。  
4. **随机性控制**：当前使用 `torch.randn` 再缓存，建议在随机种子变化后显式清空 `tensor_cache`（或在 `tensor_cache` 中加入 `seed` 参数），确保不同 seed 能得到不同数据，防止误认为结果相同。  
5. **扩展性**：若后续加入新的 ISA（如 AVX‑512）或新的注意力实现，确保 `get_attn_isa` 与 `cpu_attention_with_kv_cache` 接口保持同步，避免「ISA 与实现不匹配」的隐性错误。  

---  

*以上分析聚焦于此次提交对项目整体架构、性能测评流程及潜在风险的影响，供开发者在合并前参考。*

### 97ca4c3b60fee59850f5a0d83fc8d899ae4dd0e6
https://github.com/vllm-project/vllm/commit/97ca4c3b60fee59850f5a0d83fc8d899ae4dd0e6
[Chore] Remove more V0 dead code from `sequence.py` (#31783)

Signed-off-by: DarkLight1337 <tlleungac@connect.ust.hk>
**🎯 变更类型**：重构 / 代码清理  

**⚡ 重要程度**：🟢低  

**📋 变更摘要**：  
- 从 `vllm/sequence.py` 中删除了已失效的 `RequestMetrics` 数据类及其相关常量，彻底清理了 V0 代码痕迹。  
- 相应地，在 `vllm/outputs.py` 中将 `CompletionOutput` 构造函数的 `metrics` 参数类型从 `RequestMetrics | RequestStateStats | None` 调整为仅 `RequestStateStats | None`，保持接口一致性。  

**🎯 影响范围**：  
- `vllm/sequence.py`（模块内已移除约 34 行代码）  
- `vllm/outputs.py`（构造函数签名变化）  
- 可能间接影响任何直接或间接导入 `vllm.sequence.RequestMetrics` 的外部代码或插件。  

**🔍 技术洞察**：  
- **架构影响**：`RequestMetrics` 已不再是公开 API，模块间的耦合度进一步降低，代码基线更趋向单一的 V1 统计结构 (`RequestStateStats`)。  
- **性能影响**：删除的类仅是数据容器，未参与运行时逻辑，移除后对启动时间、内存占用均可忽略不计。  
- **安全考虑**：无新增安全风险；相反，移除未使用的结构可降低误用导致的信息泄露或错误统计的潜在风险。  

**⚠️ 潜在风险**：  
- 任何仍在使用 `vllm.sequence.RequestMetrics`（如自定义监控插件、旧版脚本）将在导入时抛出 `ImportError`，导致运行时失败。  
- 文档或示例中若仍提及 `RequestMetrics`，可能导致用户混淆。  

**💡 关注建议**：  
1. **兼容性检查**：在项目内部或 CI 中搜索对 `RequestMetrics` 的引用，确保全部迁移至 `RequestStateStats` 或已删除。  
2. **文档更新**：同步更新 API 文档、示例代码及 release notes，明确 `RequestMetrics` 已被移除。  
3. **回归测试**：运行完整的单元/集成测试套件，特别是涉及输出统计的路径，以确认 `outputs.CompletionOutput` 在无 `RequestMetrics` 参数的情况下仍能正常实例化。  
4. **用户提示**：若对外发布该版本，建议在升级指南中加入 “已移除 `RequestMetrics`，请改用 `RequestStateStats`” 的说明，帮助使用者平滑迁移。  

### ee2e69d6cda898736cc2987cb8cdfaa2601c1375
https://github.com/vllm-project/vllm/commit/ee2e69d6cda898736cc2987cb8cdfaa2601c1375
[Bugfix][CI/Build] Fix failing pooling models test due to Triton kernel accuracy diff (#31776)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🟡中  

**📋 变更摘要**：  
将 `tests/models/language/pooling/test_token_classification.py` 中的数值相等断言从 `torch.allclose` 换为 `torch.testing.assert_close`，并放宽绝对容差至 `1.2e-2`、新增相对容差 `1e-3`。此改动解决了因 Triton kernel 精度差异导致的模型池化测试失败，使 CI 构建能够通过。  

**🎯 影响范围**：  
- `tests/models/language/pooling` 目录下的 token classification 测试  
- CI 流水线中的模型池化单元测试  

**🔍 技术洞察**：  
- **架构影响**：无，修改仅限于测试代码，不触及库的核心实现或模块依赖。  
- **性能影响**：无，`torch.testing.assert_close` 与 `torch.allclose` 在测试环境下的执行成本相当。  
- **安全考虑**：无安全相关改动，亦未引入新的安全风险。  

**⚠️ 潜在风险**：  
- 调宽容差可能掩盖实际数值偏差过大的回归问题，导致模型质量下降时未被及时发现。  
- 若后续对模型精度有更严格的要求，现有容差阈值可能需要重新评估。  

**💡 关注建议**：  
- 在提交说明或测试注释中记录该容差调整的原因（Triton kernel 精度差异），便于后续审查。  
- 定期回顾该测试的容差阈值，在模型或底层算子升级后确认是否仍然合适。  
- 若团队对数值精度有更高要求，考虑在 CI 中加入额外的数值回归检测或对比基准。

### 7101e0851f73b8157145cf04835a87dc0837d450
https://github.com/vllm-project/vllm/commit/7101e0851f73b8157145cf04835a87dc0837d450
[Models]: Use `MMEncoderAttention` for MoonViT (#31738)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
Signed-off-by: Roger Wang <hey@rogerw.io>
Signed-off-by: h100 <h100@inferact.ai>
Co-authored-by: Roger Wang <hey@rogerw.io>
Co-authored-by: gemini-code-assist[bot] <176961590+gemini-code-assist[bot]@users.noreply.github.com>
Co-authored-by: h100 <h100@inferact.ai>
**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将 MoonViT 的视觉编码器从自定义 Flash‑Attention / SDPA 实现迁移至统一的 `MMEncoderAttention`，并引入 `MultiModalConfig` 以统一管理多模态并行模式。  
- 替换原始的 `ReplicatedLinear` 为支持张量并行的 `ColumnParallelLinear / RowParallelLinear / QKVParallelLinear`，并相应调整 `use_data_parallel` 与 TP（Tensor Parallel）逻辑。  
- `kimi_vl.py` 调用构造函数时改为传入 `multimodal_config`，以保持统一配置路径。

**🎯 影响范围**：  
- `vllm/model_executor/models/moonvit.py`（核心模型实现）  
- `vllm/model_executor/models/kimi_vl.py`（使用 MoonViT 的上层模型）  
- 相关的配置结构 `vllm/config.py`（`MultiModalConfig`）  
- 依赖的注意力实现 `vllm/attention/layers/mm_encoder_attention.py`  

**🔍 技术洞察**：

- **架构影响**：  
  - 引入 `MMEncoderAttention` 统一了视觉编码器的注意力实现，消除了对平台特定 Flash‑Attention 2 的硬编码分支。  
  - 通过 `MultiModalConfig` 将数据并行 / 张量并行模式抽象为配置项，降低了模型层级对并行策略的耦合度。  
  - 替换线性层为并行版后，模型在 TP 环境下能够自动切分权重，提升大模型部署的可扩展性。

- **性能影响**：  
  - **正向**：在有 TP 环境（模型并行）时，`ColumnParallelLinear`、`RowParallelLinear`、`QKVParallelLinear` 能显著降低单卡显存占用和通信开销。  
  - **负向**：当 `use_data_parallel=True`（即仍走数据并行）且未启用 TP 时，`disable_tp` 会关闭切分，理论上性能保持不变，但额外的 `MMEncoderAttention` 包装层可能带来微小的函数调用开销。  
  - 移除 `flash_attn_varlen_func` 的分支后，若运行环境仍可使用 Flash‑Attention 2，则 `MMEncoderAttention` 会内部决定是否调用高效实现；否则回退到普通 SDPA，保持功能完整。

- **安全考虑**：  
  - 变更主要是内部算子替换，无新增外部依赖或 I/O，未引入明显的安全风险。  
  - 需确保 `MultiModalConfig` 的来源可信，防止配置注入导致 TP 参数异常。

**⚠️ 潜在风险**：

1. **兼容性**：  
   - 旧的序列化模型（checkpoint）可能仍保存 `use_data_parallel` 标记，而新代码期望 `multimodal_config`。若未迁移配置，加载会失败。  
2. **并行策略冲突**：  
   - `multimodal_config.mm_encoder_tp_mode` 设置错误（如同时开启 data‑parallel 与 tensor‑parallel）可能导致张量切分不一致，触发维度不匹配错误。  
3. **性能回退**：  
   - 在不支持 Flash‑Attention 的平台上，若 `MMEncoderAttention` 未正确回退到有效实现，可能出现显著的性能下降。  
4. **代码路径削减**：  
   - 移除的自定义 `multihead_attention`、`sdpa_attention` 以及对应的注册表 `VL_VISION_ATTENTION_FUNCTIONS`，意味着若外部代码仍引用这些名称将抛出 `KeyError`。

**💡 关注建议**：

- **迁移配置**：在升级前，确保所有模型实例（包括旧的 checkpoint）都附带或生成 `MultiModalConfig`，并显式设置 `mm_encoder_tp_mode`（`"data"` 或 `"tensor"`）。  
- **回归测试**：  
  - 对比 `flash_attn`、`sdpa` 与 `MMEncoderAttention` 在相同输入下的数值误差，确保精度保持在可接受范围（< 1e‑5）。  
  - 在单卡、Data‑Parallel、Tensor‑Parallel 三种部署模式下跑完整的推理基准，验证显存占用、吞吐量与延迟的预期变化。  
- **兼容性检查**：搜索代码库是否仍有 `VL_VISION_ATTENTION_FUNCTIONS` 或直接调用 `multihead_attention`/`sdpa_attention`，并补充迁移或提供兼容包装。  
- **日志与监控**：在 `MMEncoderAttention` 初始化时加入配置打印（debug 级别），帮助定位并行模式是否按预期生效。  
- **文档更新**：在模型配置文档中说明 `multimodal_config` 的新字段含义及取值，提醒用户在使用 `kimi_vl` 等上层模型时同步传递该配置。  

通过上述验证与迁移步骤，可在保持模型功能正确性的前提下，享受更灵活的并行能力和潜在的性能提升。

### e9717801bdc5f8dec45ee1d1a8cebed9f4a13544
https://github.com/vllm-project/vllm/commit/e9717801bdc5f8dec45ee1d1a8cebed9f4a13544
[Bugfix][ROCm] Fix Unsupported attention metadata type for speculative decoding in `eagle.py` (#31714)

Signed-off-by: vllmellm <vllm.ellm@embeddedllm.com>
**🎯 变更类型**：Bug修复 / 依赖更新  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
- 在 `vllm/v1/spec_decode/eagle.py` 中，针对 ROCm 平台的投机解码（speculative decoding）路径，将原本硬编码的 `FlashAttentionMetadata` 替换为 ROCm 专用的 `RocmAttentionMetadata`。  
- 通过动态导入并在 `allowed_attn_types` 中加入 `RocmAttentionMetadata`，解决了在 ROCm 环境下出现 “Unsupported attention metadata type” 错误的问题。  

**🎯 影响范围**：  
- `vllm/v1/spec_decode/eagle.py`（投机解码实现）  
- 受影响的注意力后端：ROCm 平台的 `RocmAttentionMetadata`（新增依赖 `vllm.v1.attention.backends.rocm_attn`）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 采用了基于平台条件的依赖注入（在 `if current_platform.is_rocm():` 中局部导入），保持了现有注意力抽象层的统一接口 (`allowed_attn_types`)。  
  - 移除对 `FlashAttentionMetadata` 的硬依赖，降低了在 ROCm 环境中不兼容的风险。  

- **性能影响**：  
  - 使用 `RocmAttentionMetadata` 可以让 ROCm 平台使用专为其优化的注意力实现（如 ROCm AITer FA），在投机解码场景下预计会提升吞吐量与延迟。  
  - 代码改动仅涉及元数据类型的选择，对运行时抽象层开销几乎为零。  

- **安全考虑**：  
  - 无直接涉及安全或权限的改动。  
  - 新增的动态 import 在异常路径下会抛出 `ImportError`，但仅在 ROCm 平台且缺少相应模块时触发，属于可预期的错误，风险可控。  

**⚠️ 潜在风险**：  
- 若 `vllm.v1.attention.backends.rocm_attn` 模块在某些 ROCm 环境下缺失或未正确编译，`import` 将失败，导致投机解码初始化错误。  
- 代码中仍保留了对 `FlashAttentionMetadata` 的导入（已删除），若其他文件仍假设其可用，可能出现遗漏的兼容性问题。  
- 对 `allowed_attn_types` 的动态修改在多线程初始化时需确保线程安全，当前实现为实例属性，风险较低。  

**💡 关注建议**：  
1. **兼容性测试**：在 CI 中加入 ROCm 平台的单元/集成测试，验证在没有 `rocm_attn` 模块的环境下能够优雅回退（如使用 `TritonAttentionMetadata`）。  
2. **文档更新**：在项目的 ROCm 支持说明中补充“投机解码需要 `RocmAttentionMetadata`”的前置条件。  
3. **异常处理**：可考虑捕获 `ImportError` 并给出明确的错误提示，防止用户在缺少 ROCm 注意力后端时遇到模糊的初始化异常。  
4. **回归监控**：监控投机解码的性能指标（tokens/s、latency）在 ROCm 与非 ROCm 环境的差异，确保新元数据类型未引入性能回退。  

---  

### da71d444107dbec33b597faebece6b816fab64eb
https://github.com/vllm-project/vllm/commit/da71d444107dbec33b597faebece6b816fab64eb
[Doc] Show that `use_audio_in_video` is supported in docs (#30837)

Signed-off-by: DarkLight1337 <tlleungac@connect.ust.hk>
**🎯 变更类型**：文档 / 说明  
**⚡ 重要程度**：🟢 低  
**📋 变更摘要**：  
- 移除了在 `docs/models/supported_models.md`、示例 README 以及代码注释中关于 `use_audio_in_video` 尚未支持的说明。  
- 表明在当前版本中，`use_audio_in_video` 已被视为可用功能（对应 Qwen2.5‑Omni 与 Qwen3‑Omni）。  

**🎯 影响范围**：  
- `docs/models/supported_models.md`（模型支持表）  
- `examples/offline_inference/qwen2_5_omni/README.md`（离线推理示例）  
- 相关模型实现文件的注释 (`qwen2_5_omni_thinker.py`, `qwen3_omni_moe_thinker.py`)  

**🔍 技术洞察**：  
- **架构影响**：无代码实现层面的变更，仅删除了提示“V1 引擎不支持交叉模态”的 TODO 注释。若底层仍未完成重叠模态嵌入的实现，文档可能与实际行为不一致。  
- **性能影响**：不涉及任何运行时代码，更不会产生性能回归。  
- **安全考虑**：无安全相关改动。  

**⚠️ 潜在风险**：  
1. **文档误导**：如果底层仍缺少 `use_audio_in_video` 对应的实现（如 V1 引擎未完成交叉模态嵌入），用户依据文档直接使用该参数可能导致运行时错误或异常。  
2. **兼容性隐患**：部分旧版或自定义部署的 V1 引擎可能仍不支持此功能，升级文档而未同步实际二进制/依赖，可能引发兼容性问题。  

**💡 关注建议**：  
- **验证实现**：在发布前确认主分支的 V2（或更高）引擎已经实现 `use_audio_in_video` 的完整路径，尤其是交叉模态嵌入的逻辑。若仍在 V1 上未实现，请在文档中保留相应提示或提供版本限制说明。  
- **发布说明**：在 Release Notes 中明确标记 “`use_audio_in_video` 已在 X 版本支持”，并说明是否仅在新引擎（V2+）可用，以免用户在旧环境中遇到错误。  
- **回归测试**：添加针对 `--mm-processor-kwargs '{"use_audio_in_video": true}'` 的集成测试，确保在不同模型（Qwen2.5‑Omni、Qwen3‑Omni）上能够正确读取视频中的音频流。  
- **文档同步**：定期检查文档与代码实现的一致性，防止类似的“TODO → 已支持”忘记同步的情况。  

---  
*该变更主要是文档层面的信息更新，风险集中在可能的文档与实现不匹配。确保实现已经落地并在 CI 中加入相应测试，可将风险降至最低。*

### 1fb0209bbc476df3ddbe1dbbaf22fa57248cc8b1
https://github.com/vllm-project/vllm/commit/1fb0209bbc476df3ddbe1dbbaf22fa57248cc8b1
[Bugfix][Hardware][AMD] Fix exception types in AITER MLA FP8 check (#31177)

Signed-off-by: c0de128 <kevin.mckay@outlook.com>
Co-authored-by: Claude Opus 4.5 <noreply@anthropic.com>
**🎯 变更类型**：Bug修复 / 测试  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将 `_check_aiter_mla_fp8_support()` 中的宽泛 `except Exception` 替换为对 `ImportError、ModuleNotFoundError、AttributeError、ValueError、TypeError` 的显式捕获，防止误吞其他异常并保证在这些已知异常情况下返回 `False`。  
- 新增 `tests/rocm/aiter/test_mla_fp8_support_check.py` 单元测试，覆盖上述异常路径以及缓存行为，确保函数在错误条件下不会崩溃并正确记录结果。

**🎯 影响范围**：  
- `vllm/_aiter_ops.py`（核心检测逻辑）  
- `tests/rocm/aiter/test_mla_fp8_support_check.py`（新增测试）  
- 任何依赖 AITER MLA FP8 支持检测的模块，尤其是针对 AMD GPU 的路径。

**🔍 技术洞察**：  
- **架构影响**：仅限于内部实现细节，无公共 API 变更；保持模块间耦合不变，提升错误隔离程度。  
- **性能影响**：捕获特定异常相比捕获所有异常略有额外判断成本，且函数结果在首次检查后会被缓存 (`_AITER_MLA_SUPPORTS_FP8`) ，对运行时性能影响可忽略不计。  
- **安全考虑**：无安全风险；相反，避免因未知异常被吞掉而隐藏潜在问题，提升代码可观测性。

**⚠️ 潜在风险**：  
- 若未来出现未列入的异常类型（如 `RuntimeError` 或自定义异常），这些异常将不再被捕获，可能导致函数抛出并导致上层调用崩溃。  
- 单元测试使用 `patch` 替换 `inspect.signature`，在极端情况下可能与其他测试产生冲突，需要确保测试执行顺序或使用更细粒度的 mock。

**💡 关注建议**：  
- 在 CI 中加入对 AMD/ROCm 环境的完整回归测试，确保在真实硬件上仍能正确检测 FP8 支持。  
- 若遇到新异常导致检测失败，考虑在捕获块中加入日志记录并在必要时扩展捕获的异常列表。  
- 在发布新版本前，运行全部单元测试，特别是 `tests/rocm/aiter/` 目录下的新增测试，以验证缓存逻辑和异常处理的正确性。

### 81323ea2215fdcd236ff0281e72c9820a2b32ce2
https://github.com/vllm-project/vllm/commit/81323ea2215fdcd236ff0281e72c9820a2b32ce2
[CI] Fix CPU MM PRocessor Test (#31764)

Signed-off-by: Robert Shaw <robshaw@redhat.com>
Co-authored-by: Robert Shaw <robshaw@redhat.com>
**🎯 变更类型**：依赖更新 / 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将测试依赖文件 `requirements/test.in` 中的 `timm` 版本约束从“>=1.0.17”放宽为固定的 `1.0.17`，以解决 CI 环境下 CPU/MM Processor 测试失败的问题。此更改确保使用的 `timm` 版本与项目的其它依赖（尤其是 `internvl` 与 `gemma3n-mm`）保持兼容，提升 CI 稳定性。  

**🎯 影响范围**：  
- 测试环境依赖解析模块  
- `internvl`、`gemma3n-mm` 相关的测试用例  
- CI/CD 流水线（尤其是使用 `requirements/test.in` 进行依赖安装的阶段）  

**🔍 技术洞察**：  
- **架构影响**：无直接业务代码改动，仅影响构建/测试层面的依赖解析，属于横切关注点。  
- **性能影响**：不涉及运行时代码，故对性能无直接影响；但锁定版本可避免因新版 `timm` 引入的潜在性能回退或不兼容导致的测试波动。  
- **安全考虑**：锁定到已知的 `1.0.17` 版本可防止意外拉取包含安全漏洞的未来版本，属于安全的保守做法。  

**⚠️ 潜在风险**：  
1. **依赖冲突**：如果项目其他模块或上游库对 `timm` 有更高的最低版本要求，固定在 `1.0.17` 可能导致冲突或无法安装。  
2. **功能缺失**：后续 `timm` 的新特性或 bug 修复将不被当前 CI 环境使用，可能需要手动升级并重新验证测试。  
3. **环境差异**：本地开发者若使用 `pip install -r requirements/test.in`，将被强制使用 `1.0.17`，如果他们已有更高版本的 `timm`，可能出现降级提示或破坏已有实验环境。  

**💡 关注建议**：  
- **依赖检查**：在 CI 前加入 `pip check` 或类似工具，确保 `timm==1.0.17` 与其他依赖不会产生冲突。  
- **版本升级流程**：制定明确的升级计划：当需要使用 `timm` 的新特性时，先在分支中提升版本并运行完整测试套件，确保不回归。  
- **文档更新**：在项目的贡献指南或 CI 文档中注明 `timm` 版本已被锁定，提醒开发者在本地环境中保持一致。  
- **回滚策略**：若后续发现 `1.0.17` 与某些新模型不兼容，准备好快速回滚到之前的 `>=1.0.17` 方案，并在 CI 中加入对应的兼容性测试。  

### e1cd7a5faffd188cd204f7b54eea6cb35f787ee9
https://github.com/vllm-project/vllm/commit/e1cd7a5faffd188cd204f7b54eea6cb35f787ee9
[Bugfix] Add init_workspace_manager to moe kernel benchmarks (#31042)

Signed-off-by: mgoin <mgoin64@gmail.com>
**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：在三个 CUTLASS MoE 相关的 benchmark 脚本中新增 `init_workspace_manager` 调用，以在 CUDA 设备上初始化工作空间管理器。此改动解决了之前运行这些基准时因缺少工作空间初始化而导致的错误或异常，确保 CUTLASS MoE 核心能够正确分配临时缓冲区。  

**🎯 影响范围**：  
- `benchmarks/kernels/benchmark_cutlass_moe_fp8.py`  
- `benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py`  
- `benchmarks/kernels/benchmark_grouped_gemm_cutlass.py`  

**🔍 技术洞察**：  
- **架构影响**：无全局架构变更，仅在 benchmark 入口处加入对 `vllm.v1.worker.workspace` 模块的调用，遵循现有的初始化模式。  
- **性能影响**：初始化工作空间管理器会在首次调用时进行一次 GPU 端的缓冲区分配，开销极小（一次性 O(1)），对后续 benchmark 执行的时间和吞吐量基本没有负面影响。相反，避免了因缺少工作空间导致的潜在回退或错误路径。  
- **安全考虑**：`init_workspace_manager` 只涉及内部 GPU 缓冲区管理，无网络、文件系统或权限操作，不会引入新的安全风险。  

**⚠️ 潜在风险**：  
- 如果用户在非 CUDA 环境（如 CPU-only）运行这些 benchmarks，显式的 `torch.device("cuda:0")` 可能触发 `RuntimeError`。需要在文档或脚本中明确要求 CUDA 环境。  
- 该初始化函数在未来若更改签名或行为（例如需要额外的配置），可能导致这些 benchmark 脚本失效，需要同步更新。  

**💡 关注建议**：  
- 在运行上述 benchmark 前，确保机器安装并可用的 CUDA 设备（至少 `cuda:0`）。  
- 为提升兼容性，可在脚本中加入 `if torch.cuda.is_available():` 判断，若不可用则给出友好的提示或跳过初始化。  
- 在 CI 或自动化测试中加入对这些 benchmark 的回归测试，确保后续改动不会破坏工作空间初始化流程。  
- 更新项目文档，注明这些 benchmark 现在依赖 `vllm.v1.worker.workspace.init_workspace_manager`，并提供简要说明。  

### a68e703c32dc42e10d8887b1452311cc4966adae
https://github.com/vllm-project/vllm/commit/a68e703c32dc42e10d8887b1452311cc4966adae
[UX] Add `-ep` shorthand for `--enable-expert-parallel` (#30890)

Signed-off-by: mgoin <mgoin64@gmail.com>
**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟢低  
**📋 变更摘要**：在 `vllm/engine/arg_utils.py` 中为 `--enable-expert-parallel` 参数新增 `-ep` 简写形式，提升命令行使用体验。  
**🎯 影响范围**：`vllm/engine` 模块的 CLI 参数解析；涉及的子系统包括启动脚本、分布式并行调度器以及相关文档/示例。  

**🔍 技术洞察**  
- **架构影响**：无结构性改动，仅在 `FlexibleArgumentParser` 中注册额外的短选项。不会改变内部模块关系或设计模式。  
- **性能影响**：解析参数的时间复杂度保持不变，额外的短选项字典条目对运行时性能影响可以忽略。  
- **安全考虑**：不涉及安全敏感逻辑，新增的短选项与现有选项不存在冲突（`-ep` 未被其他参数占用），风险极低。  

**⚠️ 潜在风险**  
1. **命名冲突**：若未来新增其他 `-e*` 短选项，需确认不会与 `-ep` 冲突。  
2. **文档/示例不同步**：使用旧文档的用户可能不知道该简写，导致使用不一致。  
3. **测试覆盖不足**：当前变更仅修改了代码，若没有相应的 CLI 测试用例，可能遗漏对 `-ep` 的回归测试。  

**💡 关注建议**  
- **文档同步**：在项目文档、README 以及 `--help` 输出中添加 `-ep` 的说明。  
- **测试补全**：在 CI 中加入针对 `-ep` 的单元测试或集成测试，验证等价于 `--enable-expert-parallel`。  
- **冲突检查**：在 `arg_utils.py` 中对已有短选项做一次唯一性校验，防止未来冲突。  
- **发布说明**：在 CHANGELOG 中标记此 UX 改进，提醒下游用户更新脚本或 alias。  

总体而言，此次改动风险极低，属于轻量级的可用性提升，建议快速合并并同步文档与测试。

### cd1245a1848d866fa640cb1051c6bbb5b1c68f4a
https://github.com/vllm-project/vllm/commit/cd1245a1848d866fa640cb1051c6bbb5b1c68f4a
[Cleanup] Remove redundant `decoder_layer_type` assignment in `Qwen2` (#31760)

Signed-off-by: maang <maang_h@163.com>
**🎯 变更类型**：重构 / 清理  
**⚡ 重要程度**：🟢低  

**📋 变更摘要**  
- 在 `vllm/model_executor/models/qwen2.py` 中删除了对 `decoder_layer_type` 的冗余赋值逻辑。  
- 直接使用函数参数 `decoder_layer_type`（其默认值已在签名中设为 `Qwen2DecoderLayer`），保持层创建流程不变。  
- 目的在于简化代码，去除不必要的本地变量，提升可读性。

**🎯 影响范围**  
- `Qwen2` 模型的构造函数 (`Qwen2.__init__`)。  
- 受影响的仅是该文件，没有波及其他模块或外部接口。

**🔍 技术洞察**  
- **架构影响**：无。层的创建仍通过 `make_layers` 调用 `decoder_layer_type`，行为保持一致。  
- **性能影响**：无。删除两行局部赋值对运行时开销可忽略不计。  
- **安全考虑**：无。变更不涉及权限、数据处理或外部输入。  

**⚠️ 潜在风险**  
- 如果 `__init__` 的函数签名默认值意外地不是 `Qwen2DecoderLayer`（例如在未来修改时忘记同步），则在未显式传参的情况下会出现 `NameError`。当前代码库的默认值已设为 `Qwen2DecoderLayer`，风险极低。  

**💡 关注建议**  
- 确认 `__init__` 参数列表中 `decoder_layer_type` 的默认值仍然是 `Qwen2DecoderLayer`。  
- 在进行下一次大幅重构或升级时，留意该参数的签名变更，以防止类似的未定义引用。  
- 该提交不影响现有功能，无需额外的回归测试，只需在常规的模型加载测试中确认模型仍能正常实例化即可。

### ffec8154225f00a41447c6cd0617d5245972f0d5
https://github.com/vllm-project/vllm/commit/ffec8154225f00a41447c6cd0617d5245972f0d5
[Perf] Optimize additional `fill(0)` in cutlass moe, 2.9% E2E throughput improvement, 10.8% TTFT improvement (#31754)

Signed-off-by: yewentao256 <zhyanwentao@126.com>
**🎯 变更类型**：性能优化 / 重构  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `cutlass_moe.py` 中去除了对 `mm2_out` 的冗余 `fill_(0)` 操作，并将 `expert_offsets` 的来源由原来的 `expert_offsets` 改为 `expert_first_token_offset`（并在后续调用中使用切片 `[:-1]`）。  
- 同时在 `moe_permute` 返回值中加入 `expert_first_token_offset` 参数，并在后续的 `cutlass_moe_mm` / `cutlass_w4a8_moe_mm` 调用中显式传递该偏移信息。  
- 这些改动在端到端吞吐量上提升约 2.9%，首次响应时间（TTFT）提升约 10.8%。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/cutlass_moe.py`（FP8 与 W4A8 两套实现）  
- 依赖 `moe_permute`、`cutlass_moe_mm`、`cutlass_w4a8_moe_mm` 的所有模型推理路径（尤其是使用 MoE 的大模型）  

**🔍 技术洞察**  

- **架构影响**：  
  - 仅在 **Cutlass MoE** 子模块内部做了接口调整，没有改变整体调用链或模块边界。  
  - 新增 `expert_first_token_offset` 参数的显式传递，使得上层对专家 token 排序的依赖更明确，代码可读性略有提升。  

- **性能影响**：  
  - 移除 `mm2_out.fill_(0)` 消除了一次大规模的 GPU 内存写入（O(N)），该写入在之前的实现中是冗余的，因为随后会被 `cutlass_*_mm` 完全覆盖。  
  - 通过直接使用 `expert_first_token_offset` 替代 `expert_offsets`，避免了额外的拷贝或切片操作，进一步减少了 GPU 内存访问。  
  - 预计时间复杂度保持不变（仍是 O(N) 主要由 GEMM 主导），但常数项降低，特别在大 batch / 多专家场景下收益显著。  

- **安全考虑**：  
  - **无**：变更仅涉及内部数据流和内存填充，不涉及外部输入校验、加密或权限控制。  

**⚠️ 潜在风险**  

1. **接口兼容性**：  
   - `moe_permute` 的返回值顺序改变，外部如果有直接依赖（比如自定义层或插件）可能会出现解包错误。  
   - 解决办法：确保所有调用点已同步更新，或在 `moe_permute` 中保留向后兼容的旧返回字段（如通过 `namedtuple`）。  

2. **未初始化的 `mm2_out`**：  
   - 移除 `fill_(0)` 后，如果后续的 `cutlass_*_mm` 在某些边缘分支下未覆盖全部输出（例如 expert_map 为 `None` 时的快速路径），可能出现残留旧值。  
   - 需要确认 `cutlass_*_mm` 在所有路径上都会写入完整 `mm2_out`。  

3. **测试覆盖**：  
   - 该改动涉及底层数值计算，若有稀疏或异常的 `expert_map`（如空张量）可能触发未定义行为，需要在单元测试中覆盖。  

**💡 关注建议**  

- **回归测试**：在 CI 中加入对 `cutlass_moe_fp8` 与 `cutlass_moe_w4a8_fp8` 两条路径的数值对比测试，确保在 `expert_map == None`、单 Expert、以及多 Expert 场景下输出与之前版本完全一致。  

- **性能基准**：建议在不同 batch size、token 长度、expert 数量的组合上跑一次微基准（throughput、TTFT），确认提升在预期范围内，并记录最大回退情况。  

- **文档/注释**：在 `moe_permute` 与 `cutlass_*_mm` 的函数签名处加入 `expert_first_token_offset` 参数的说明，帮助后续维护者理解该参数的语义。  

- **向后兼容**：如果项目对外暴露 `moe_permute`（例如在插件或第三方库中直接调用），考虑提供包装函数或保持旧返回顺序的兼容层。  

---  

**结论**：本次提交通过去除冗余的内存填充和明确数据流，带来了可观的推理性能提升，风险主要集中在接口兼容性和未覆盖写入的极端路径。只要在升级前完成上述回归测试和兼容检查，即可安全推广到生产环境。祝顺利！

### d386ab141273041693455d33651a3cba6a12e8d1
https://github.com/vllm-project/vllm/commit/d386ab141273041693455d33651a3cba6a12e8d1
[Docs] Improve malformed exception caused by backslash line continuations (#31694)

Signed-off-by: maang <maang_h@163.com>
Signed-off-by: maang <55082429+maang-h@users.noreply.github.com>
Co-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>
Co-authored-by: Wentao Ye <44945378+yewentao256@users.noreply.github.com>
**🎯 变更类型**：文档/代码风格（改进异常信息的字符串拼接）  
**⚡ 重要程度**：🟢 低  
**📋 变更摘要**：  
本次提交将多个使用反斜杠 (`\`) 进行换行的异常/错误提示字符串改写为显式的多行字符串拼接，消除因行续写导致的异常信息格式异常。目标是提升错误信息的可读性和可靠性，避免出现 “malformed exception” 的情况。  

**🎯 影响范围**：  
- `vllm/config/compilation.py`  
- `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`  
- `vllm/model_executor/model_loader/weight_utils.py`  
- `vllm/model_executor/models/llama.py`  
- `vllm/model_executor/models/phi4mm.py`  
- `vllm/multimodal/registry.py`  
- `vllm/utils/argparse_utils.py`  

**🔍 技术洞察**  
- **架构影响**：无。仅涉及异常/错误提示字符串的写法，不改变模块间的调用关系或设计模式。  
- **性能影响**：无。字符串拼接的微小差异对运行时性能可以忽略不计。  
- **安全考虑**：无。改动不涉及权限、数据泄露或加密等安全相关逻辑。  

**⚠️ 潜在风险**  
- **测试回归**：若项目或下游库中存在对异常文本的精确匹配（例如 `assertRaisesRegex`），字符串内容的微调可能导致测试失效。  
- **用户脚本依赖**：极少数用户可能在捕获异常后解析错误信息字符串（例如自动化监控），字符串变动会导致解析失败。  

**💡 关注建议**  
1. **回归测试**：执行完整的单元测试和集成测试，重点关注包含 `raise ValueError`、`raise NotImplementedError` 等的路径，确认测试仍然通过。  
2. **文档同步**：如项目文档中列举了异常信息示例，及时更新对应章节，以免出现文档与实际行为不一致。  
3. **兼容性声明**：在 changelog 中注明此为 “错误信息文本微调”，提醒依赖文本匹配的用户做好适配。  
4. **代码审查**：后续新增异常提示时，统一采用显式多行拼接或使用 `textwrap.dedent`/三引号的方式，避免再次出现反斜杠换行导致的格式问题。  

### ccb309a9640f90ca288ecea817fff7cc9474dbf5
https://github.com/vllm-project/vllm/commit/ccb309a9640f90ca288ecea817fff7cc9474dbf5
Revert "[CI Failure] Disable B200 tests while runner is broken" (#31750)

Signed-off-by: Michael Goin <mgoin64@gmail.com>
**🎯 变更类型**：配置 / CI 调整  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交撤销了此前为 B200 GPU 测试步骤添加的 `optional: true` 标记，恢复了这些步骤在 Buildkite CI 中的必执行属性。目标是重新启用 B200 相关的全套测试，以确保代码在该硬件上得到完整验证。预期效果是提升对 B200 GPU 的回归覆盖，防止因测试被跳过而潜在引入缺陷。  

**🎯 影响范围**：  
- `.buildkite/test-pipeline.yaml` 中的多个 B200 GPU 测试步骤（V1 Attention、量化相关测试、DeepSeek V2 等）  
- 受影响的 CI 运行环境：Buildkite pipeline、使用 B200 GPU 的测试节点  

**🔍 技术洞察**：  
- **架构影响**：仅涉及 CI 流水线配置修改，不影响业务代码或系统架构。恢复必执行后，CI 对 B200 GPU 的依赖更为显式。  
- **性能影响**：测试执行时间将增加（原本可选的步骤现在必须运行），可能导致整体 CI 时长延长约 10‑30% 视具体步骤而定。  
- **安全考虑**：无直接安全影响，属于构建配置调整。  

**⚠️ 潜在风险**：  
1. **CI 失效风险**：如果 B200 GPU runner 仍未修复，必执行的步骤将导致 pipeline 直接报错，阻塞合并流程。  
2. **构建时长增长**：更多必跑步骤可能使 CI 队列排队时间变长，影响开发者的快速反馈体验。  
3. **误判风险**：暂时性硬件故障被误认为代码回归，引发不必要的回滚或调查。  

**💡 关注建议**：  
- 在合并前确认 B200 GPU runner 已恢复正常，或临时提供备用 runner，以防止 CI 持续阻塞。  
- 监控本次撤销后的 CI 失败率，若出现异常波动，快速定位是硬件问题还是代码回归。  
- 考虑在 CI 中加入 runner 健康检查步骤，若检测到 runner 异常，可自动将对应步骤标记为 `optional`，避免手动回退。  
- 如业务对 CI 时长敏感，可评估是否仅对关键 B200 测试保持必执行，其他较低风险的步骤仍设为可选。

### 2f4e6548efec402b913ffddc8726230d9311948d
https://github.com/vllm-project/vllm/commit/2f4e6548efec402b913ffddc8726230d9311948d
[Bugfix] vLLM produces invalid UTF-8 tokens and “�” (#28874)

Signed-off-by: John Calderon <jcalderon@nvidia.com>
Co-authored-by: Benjamin Chislett <bchislett@nvidia.com>
**🎯 变更类型**：Bug修复 / 新增测试 / 功能增强  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 修复 vLLM 在使用 **byte‑fallback 分词** 时产生的非法 UTF‑8 token（结尾为 Unicode 替换字符 “�”）的问题。  
- 新增 `_correct_decoded_token` 与 `_verify_tokens` 两个内部方法，对出现 “�” 的 token 进行上下文合并解码或回退为空字符串的纠正逻辑。  
- 为此实现编写了 **全面的单元与集成测试**（包括真实模型 `facebook/opt-125m` 的端到端验证），确保纠正效果和不再出现 “�”。  

**🎯 影响范围**  
- `vllm/v1/engine/logprobs.py` （核心日志概率处理流程）  
- 测试目录 `tests/v1/engine/`, `tests/v1/sample/`（新增大量针对 UTF‑8 修正的测试）  
- 依赖 `vllm/v1/engine/output_processor` 中的断言也同步更新，以适配纠正后 token 的比较方式  

**🔍 技术洞察**  
- **架构影响**：  
  - 在 **LogprobsProcessor** 中加入了 **后处理层**（_verify_tokens → _correct_decoded_token），保持对外 API 不变，仅在内部解码阶段做额外校正。  
  - 新增 `Iterable` 类型注解，兼容 `None` (即 `NONES`) 与实际 token 列表的统一返回类型，提升代码可读性。  
  - 测试层面引入了多个 `pytest.fixture`，模拟不同 token 历史（有/无前置 logprob）以及多字节 UTF‑8 场景，提升对边界条件的覆盖率。  

- **性能影响**：  
  - 每个 token 仅在检测到 “�” 时才触发额外的两次 `tokenizer.decode`（合并前后 token 或前置 logprob），在典型生成任务中出现率极低，整体时间开销可忽略（< 0.5 %）。  
  - 额外的 list 切片与字典映射在 O(N) 的 `decoded_tokens_list` 迭代中完成，空间占用仅多出少量临时字符串。  

- **安全考虑**：  
  - 仅涉及字符解码路径，无外部输入写入或权限提升。  
  - 通过显式断言确保 `self.tokenizer` 必定非空，防止因意外 `None` 引发的 `AttributeError`。  
  - 未引入新的依赖或网络调用，安全风险极低。  

**⚠️ 潜在风险**  
1. **回退行为变化**：原来在出现 “�” 时直接返回原字符串，现改为空字符串或纠正后字符串，可能导致下游业务在统计 token 长度或字符计数时出现轻微差异。  
2. **日志/监控兼容性**：如果外部系统依赖 “�” 出现作为错误标记，需更新监控阈值。  
3. **极端多字节序列**：在极少数极端的多字节拆分（> 2 个 token）场景下，当前纠正逻辑仍会回退为空字符串，理论上可能漏掉可恢复的字符。  

**💡 关注建议**  
- **回归测试**：在升级后运行全套 `vllm` 测试，特别是自定义的 `logprobs` 统计脚本，确认统计结果与预期一致。  
- **监控更新**：若已有监控关注 “�” 出现频率，需把阈值调低或直接关闭相关告警。  
- **文档说明**：在 Release Notes 中注明 **LogprobsProcessor** 现在会自动纠正 UTF‑8 破碎 token，避免用户对空字符串产生误解。  
- **性能观察**：在大批量生成（> 10k tokens）场景下，可开启采样日志，确保额外 decode 调用未导致显著延迟。  
- **后续迭代**：考虑对 “多于两 token 的连续拆分” 实现更深层次的回溯合并，以进一步提升纠正率（可在下一个 minor 版本中加入）。  

### 3c98c2d21b38dc2037dd39b77d8f4d52eda572cf
https://github.com/vllm-project/vllm/commit/3c98c2d21b38dc2037dd39b77d8f4d52eda572cf
[CI/Build] Allow user to configure NVSHMEM version via ENV or command line (#30732)

Signed-off-by: Seiji Eicher <seiji@anyscale.com>
Co-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>
**🎯 变更类型**：功能增强 / 配置 / 安全  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
- 在 `docker/Dockerfile` 中新增 `ARG NVSHMEM_VER`，并在调用 `install_python_libraries.sh` 时传递 `--nvshmem-ver` 参数，使构建镜像时可通过构建参数或环境变量自定义 NVSHMEM 版本。  
- `tools/ep_kernels/install_python_libraries.sh` 增加对应的命令行选项 `--nvshmem-ver`，并对输入进行完整性校验（非空、无斜杠、仅允许字母数字、点、短横线），默认值仍为 `3.3.24`。  

**🎯 影响范围**：  
- Docker 镜像构建流程 (`docker/Dockerfile`)  
- EP Kernels 安装脚本 (`tools/ep_kernels/install_python_libraries.sh`)  
- 任何依赖该脚本的 CI/CD 流水线或本地构建环境  

**🔍 技术洞察**：  
- **架构影响**：  
  - 仅在构建阶段引入可配置参数，对运行时组件（库加载、API）没有结构性改动。  
  - 脚本新增参数解析逻辑，保持向后兼容；未修改其它模块的接口。  

- **性能影响**：  
  - 参数传递本身开销可忽略。  
  - 若用户自行指定不兼容的 NVSHMEM 版本，可能导致后续编译或运行时出现错误，但不会导致性能回退。  

- **安全考虑**：  
  - 新增了对 `NVSHMEM_VER` 的正则校验，防止路径遍历、命令注入等攻击面。  
  - 仍然依赖外部下载（未在 diff 中展示），若指定恶意版本，仍可能引入供应链风险；建议在 CI 中对下载的 URL 进行校验或签名验证。  

**⚠️ 潜在风险**：  
1. **兼容性风险**：用户自行指定的 NVSHMEM 版本可能与当前 CUDA 版本或其它库（如 NCCL）不兼容，导致构建失败或运行时错误。  
2. **CI 配置漂移**：已有 CI 流水线未显式设置 `NVSHMEM_VER`，在新版 Dockerfile 中默认仍为 `3.3.24`，但如果后续默认值改变，可能导致意外的构建差异。  
3. **文档同步**：目前 README/文档未说明该新参数的使用方式，可能导致使用者不知如何覆盖默认值。  

**💡 关注建议**：  
- **测试**：在 CI 中加入矩阵测试，分别使用默认版本 (`3.3.24`) 与几个历史版本（如 `3.2.0`）进行全链路编译和基本功能验证，确保版本兼容性。  
- **文档**：在项目的构建指南或 Dockerfile 注释中补充 `NVSHMEM_VER` 参数的说明，包括支持的 CUDA 版本范围、推荐的版本号等。  
- **安全**：若脚本后续进行网络下载，考虑对下载的 tarball 进行 SHA256 校验，以抵御供应链篡改。  
- **回滚**：保留 `ARG NVSHMEM_VER` 的默认值，避免因意外删除导致已有镜像构建失败。若需要在未来删除此特性，务必提供清晰的迁移指南。  

### 95130298981428053a23376e89a771afc7fc37af
https://github.com/vllm-project/vllm/commit/95130298981428053a23376e89a771afc7fc37af
[Bugfix] Properly apply v_scale for mimo_v2_flash (#31175)

Signed-off-by: mgoin <mgoin64@gmail.com>
**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
在 `MiMoV2Attention`（flash 实现）中新增 `v_scale` 参数，并在 forward 时对 value 张量 `v` 进行显式缩放，取代之前在权重加载阶段的缩放逻辑。这样能够确保在使用 `attention_value_scale` 配置时，缩放在正确的时机（注意力计算前）生效，修复了原实现可能导致的数值偏差或未生效的 bug。

**🎯 影响范围**：  
- `vllm/model_executor/models/mimo_v2_flash.py`（MiMoV2 flash attention 实现）  
- 相关的模型初始化路径（`MiMoV2Attention` 的创建）  
- 权重加载逻辑（`load_weights` 中对 `v_scale` 的旧处理被移除）  

**🔍 技术洞察**：

- **架构影响**：  
  - 新增 `v_scale` 作为构造函数参数并保存为实例属性，保持向后兼容（默认 `None`）。  
  - 通过 `VllmConfig.attention_value_scale` 将配置向下传递至 attention 层，遵循现有配置注入模式。  
  - 移除 `load_weights` 中对 `v_scale` 的处理，防止在权重加载阶段就进行数值变换，简化了权重加载路径。

- **性能影响**：  
  - 前向计算中增加一次标量乘法（`v = v * self.v_scale`），对整体吞吐几乎没有影响。  
  - 删除权重加载时的乘法逻辑，略微加快模型初始化过程。  

- **安全考虑**：  
  - 无新增安全风险。唯一需要注意的是 `v_scale` 的数值范围，若设置为异常大的/小的浮点数可能导致数值溢出或梯度消失。

**⚠️ 潜在风险**：

1. **向后兼容性**：若已有模型检查点在加载时曾依赖 `load_weights` 中的 `v_scale` 乘法（如手动在 checkpoint 中未预乘），移除该逻辑后数值会改变。  
2. **配置遗漏**：在未显式设置 `attention_value_scale` 时仍会走默认路径（`None`），不影响原有行为；但若误将 `v_scale` 设置为不合适的数值，可能导致模型质量下降。  
3. **并行/分片场景**：`v_scale` 在每个分片上均会被乘一次，确保所有分片使用相同的标量值，否则会出现不一致的注意力输出。

**💡 关注建议**：

- **回归测试**：对使用 `MiMoV2Attention`（尤其是 `mimo_v2_flash`）的模型进行前向推理比对，确认在有/无 `attention_value_scale` 时输出一致性符合预期。  
- **检查点迁移**：如果已有 checkpoints 依赖于旧的权重加载缩放，建议在迁移到新版本前手动在 checkpoint 中预乘 `v_scale`，或在加载后再次校准。  
- **配置验证**：在启动时加入对 `attention_value_scale` 范围的校验（如 `0 < v_scale < 10`），避免极端数值导致数值不稳定。  
- **文档更新**：在配置说明中补充 `attention_value_scale` 的含义及使用场景，提醒用户该参数仅在 MiMoV2 flash attention 中生效。  

---

### f6c0009afa366e4238cb118882415cd6c448c16b
https://github.com/vllm-project/vllm/commit/f6c0009afa366e4238cb118882415cd6c448c16b
[Bugfix] Fix Broken ModelOpt NVFP4 MoE (#31742)

Signed-off-by: Robert Shaw <robshaw@redhat.com>
Co-authored-by: Robert Shaw <robshaw@redhat.com>
**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**：  
本次提交修复了 ModelOpt NVFP4 MoE 在使用 FlashInfer‑Cutlass 后端时出现的崩溃问题。主要通过去除错误的导入、在 `all2all_utils.py` 中加入断言并明确禁止在该路径创建 FlashInfer‑Cutlass 实例，同时在 `fp8.py`、`modelopt.py` 与 `flashinfer_cutlass_moe.py` 中补充对 `build_flashinfer_fp8_cutlass_moe_prepare_finalize` 的调用，使 NVFP4 与 FP8 两条路径能够统一使用正确的 `prepare_finalize` 实例。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/fused_moe/all2all_utils.py`  
- `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`  
- `vllm/model_executor/layers/quantization/fp8.py`  
- `vllm/model_executor/layers/quantization/modelopt.py`  

**🔍 技术洞察**：  
- **架构影响**：  
  - 把原本在 `all2all_utils.py` 中隐式创建 FlashInfer‑Cutlass `prepare_finalize` 的逻辑抽离，统一交由 `fp8.py` / `modelopt.py` 中的显式构造函数 `build_flashinfer_fp8_cutlass_moe_prepare_finalize` 负责。  
  - 新增的断言 `assert not moe.use_flashinfer_cutlass_kernels` 强制该路径只能在 `modelopt.py` 或 `fp8.py` 中创建，防止模块之间出现循环依赖或重复实例化，提升代码的职责分离度。  
- **性能影响**：  
  - 代码路径未引入额外计算，仅在初始化阶段增加一次断言检查，开销可以忽略不计。  
  - 正确的 `prepare_finalize` 实例化恢复了 NVFP4 MoE 的高效 All‑to‑All 通信路径，整体推理吞吐量应回到预期水平。  
- **安全考虑**：  
  - 无涉及安全关键逻辑的变更。断言仅在开发 / 测试环境触发，不会泄露信息。  

**⚠️ 潜在风险**：  
1. **断言触发导致启动失败**：如果还有遗留代码在 `all2all_utils.py` 使用 `moe.use_flashinfer_cutlass_kernels=True`，运行时会抛出 AssertionError，导致服务不可用。  
2. **TP（Tensor Parallel）分支未覆盖**：在 `modelopt.py` 与 `fp8.py` 中对 `dp_size == 1` 的特殊处理返回 `None`，若后续出现其他 TP 配置，可能出现未实现的路径或功能缺失。  
3. **兼容性回归**：对 `FlashinferMoeBackend.CUTLASS` 与 `FlashinferMoeBackend.TENSORRT_LLM` 的分支进行了微调，需确认旧版模型在这些后端仍能正常工作。  

**💡 关注建议**：  
- **回归测试**：在多种硬件 (SM80、SM90、SM100) 与不同的 `FlashinferMoeBackend`（CUTLASS、TRT LLM、LATENCY）组合下执行完整的 MoE 推理基准，确保 NVFP4、FP8 两条路径均无回归。  
- **TP/DP 场景验证**：特别验证 `dp_size > 1` 与 `dp_size == 1` 两种情况的行为，确认 `prepare_finalize` 为 `None` 时不会导致后续执行错误。  
- **监控断言**：在生产环境中可以将断言改为日志警告（或使用 `if` 检查并抛出可捕获的异常），避免因异常导致服务崩溃。  
- **文档同步**：更新 `ModelOpt` 与 `FP8` 使用手册，明确在何处应创建 `prepare_finalize`，以及已经不再支持在 `all2all_utils.py` 中直接创建的限制。  
- **代码审查**：后续的 MoE 重构应继续把 “创建‑准备‑结束” 的职责集中在专用模块（如 `modelopt.py`、`fp8.py`）中，避免类似的跨文件隐式依赖。  

### 776ca1e18784707a8e94f62086d2cbd7f42a20c5
https://github.com/vllm-project/vllm/commit/776ca1e18784707a8e94f62086d2cbd7f42a20c5
[MoE Refactor] Aiter Experts for BF16 MoE (#31542)

Signed-off-by: Yongye Zhu <zyy1102000@gmail.com>
Co-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>
**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交对 `unquantized_fused_moe_method.py` 进行结构性重构，引入 `AiterExperts` 以支持 BF16 MoE（ROCm）实现。去除了旧的 `rocm_aiter_fused_experts` 路径和显式权重量化/洗牌逻辑，将其统一包装进 `mk.FusedMoEModularKernel`，实现统一的 kernel 调用路径。

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py`（核心 MoE 执行层）  
- `vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py`（新增/使用的 `AiterExperts` 实现）  
- 相关单元测试与 CI 在 ROCm 环境下的 MoE 路径  

**🔍 技术洞察**  

- **架构影响**  
  - 将原本分支式的 `rocm_aiter_fused_experts` 与 `flashinfer_cutlass` 两套实现合并到统一的 `mk.FusedMoEModularKernel` 中，提升代码可维护性。  
  - 引入 `AiterExperts` 作为 `MoEPrepareAndFinalizeNoEP` 的子模块，遵循“模块化 kernel → 具体专家实现” 的设计模式。  
  - 移除 `self.rocm_aiter_fused_experts` 成员，简化对象状态，减少条件分支。

- **性能影响**  
  - 通过在 `process_weights_after_loading` 中一次性完成权重洗牌并使用 `replace_parameter`，避免在推理时再次复制或重排权重，理论上降低内存访问开销。  
  - `AiterExperts` 直接集成进 `mk.FusedMoEModularKernel`，减少一次函数调用层次，预期在 ROCm（尤其是 BF16 支持的 GPU）上提升吞吐量。  
  - 对非 ROCm 环境仍保持原有 `flashinfer_cutlass` 路径，不会产生性能回退。

- **安全考虑**  
  - 无安全敏感操作（如外部输入解析、加密等），风险主要在于代码逻辑错误导致模型推理结果不一致。  

**⚠️ 潜在风险**  

1. **行为差异**：权重洗牌的实现方式改动后，若 `rocm_aiter_ops.shuffle_weights` 与 `AiterExperts` 的内部假设不完全匹配，可能导致专家分配错误或数值偏差。  
2. **兼容性**：`self.rocm_aiter_moe_enabled` 为 `False` 时仍走原有路径，但旧的 `self.rocm_aiter_fused_experts` 被移除，若有第三方代码仍引用该属性会报 AttributeError。  
3. **平台依赖**：新实现依赖 `mk.FusedMoEModularKernel` 与 `AiterExperts` 在 ROCm 环境下的正确编译与运行，未在 AMD GPU 上充分验证可能导致运行时崩溃或性能回退。  
4. **回退路径**：若 `rocm_aiter_moe_enabled` 为 `True` 且 `AiterExperts` 初始化失败，当前代码未提供显式的回退到 `flashinfer_cutlass`，可能导致整体 MoE 功能失效。  

**💡 关注建议**  

- **单元/集成测试**：在多种 ROCm 环境（不同驱动、不同 GPU 架构）下运行完整的 MoE 推理基准，验证输出一致性（尤其是 BF16 精度）和性能提升幅度。  
- **回退机制**：考虑在 `AiterExperts` 初始化异常时，自动切换到 `flashinfer_cutlass` 或 CPU fallback，以提升鲁棒性。  
- **文档更新**：在 README/CHANGELOG 中注明 “ROCm Aiter MoE 仅在 BF16 支持的 GPU 上可用”，并提供开启/关闭开关的配置说明。  
- **属性清理**：如果没有外部依赖，完全删除 `self.rocm_aiter_fused_experts` 相关代码，并在 `__init__` 中仅保留 `self.rocm_aiter_moe_enabled` 标志，防止意外属性访问。  
- **持续监控**：在 CI 中加入 ROCm 构建与运行的健康检查，确保新 kernel 随依赖库升级仍保持兼容。  

### af9a7ec2558293981fc984d064dfca4887c8033c
https://github.com/vllm-project/vllm/commit/af9a7ec2558293981fc984d064dfca4887c8033c
[Bug] Revert torch warning fix (#31585)

Signed-off-by: yewentao256 <zhyanwentao@126.com>
**🎯 变更类型**：Bug 修复 / 重构（撤回先前的警告抑制实现）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将 vLLM 对 float32 矩阵乘法精度的配置方式由直接写入 `torch.backends.cuda.matmul.fp32_precision`（会触发警告）恢复为官方推荐的 `torch.set_float32_matmul_precision`。同时把环境变量 `VLLM_FLOAT32_MATMAT_PRECISION` 的可选取值从 `["ieee", "tf32"]` 改为 `["highest", "high", "medium"]`，并将默认值改为 `"highest"`，对应 PyTorch 的默认行为。相应单元测试也已随之更新。

**🎯 影响范围**  
- `vllm/envs.py`（环境变量解析与默认值）  
- `vllm/v1/worker/gpu_worker.py`（工作进程初始化时的精度设置）  
- `tests/v1/e2e/test_async_scheduling.py`（测试环境变量使用）  

**🔍 技术洞察**  

- **架构影响**：  
  - 仅涉及配置层面的改动，未改变模块间调用关系或核心业务逻辑。  
  - 通过统一使用 `torch.set_float32_matmul_precision`，提升了代码的可读性和对上游库的兼容性。  

- **性能影响**：  
  - 默认精度从 `"ieee"`（强制全精度）切换为 `"highest"`，在多数 GPU 上会使用 TensorFloat32（TF32）加速，可能提升 `matmul` 的吞吐量。  
  - 对于极端需要 IEEE 完全精度的场景，用户必须显式把 env 设回 `"highest"` 对应的 `torch.set_float32_matmul_precision("high")` 或者自行在代码里覆盖。  

- **安全考虑**：  
  - 无直接安全风险，变更仅涉及数值精度。  
  - 需要注意数值误差在下游任务（如金融、科学计算）中的潜在影响。  

**⚠️ 潜在风险**  

1. **向后兼容性**  
   - 旧版部署如果仍使用 `VLLM_FLOAT32_MATMUL_PRECISION=ieee` 或 `tf32`，会因为 `env_with_choices` 验证失败而在启动时抛出异常。  
   - 运行环境的 PyTorch 版本低于 1.13（`torch.set_float32_matmul_precision` 尚未实现）会导致 `AttributeError`，导致服务启动失败。  

2. **数值行为变更**  
   - 默认从强制 IEEE 精度切换为 “最高” （即可能使用 TF32），可能导致模型输出细微数值差异，进而影响推理结果的可重复性或对精度敏感的基准。  

3. **测试覆盖**  
   - 仅修改了一个 e2e 测试用例，未覆盖所有使用该 env 的路径（如多卡训练、不同模型类型），可能遗漏回归。  

**💡 关注建议**  

- **兼容性**：在发布说明中明确列出已废弃的取值 `ieee`、`tf32`，并提供迁移指南（例如 `ieee` → `high`，`tf32` → `medium`）。  
- **最低 PyTorch 版本**：在项目 `README` 或 `setup.cfg` 中声明最低支持的 PyTorch 版本（例如 `>=1.13`），或在运行时加入回退逻辑：若 `torch.set_float32_matmul_precision` 不存在，则继续使用旧的 `torch.backends.cuda.matmul.fp32_precision`。  
- **回归测试**：补充单元/集成测试，覆盖不同取值组合、单卡/多卡、以及对数值敏感的模型（如 BFloat16 转换路径）。  
- **监控数值差异**：建议在关键模型的 CI 中加入对比前后精度输出的差异阈值检测，防止意外的数值回退。  
- **文档更新**：同步更新 `vllm/envs.py` 中的环境变量文档，说明每个选项对应的实际行为（`highest` → IEEE, `high` → TF32, `medium` → 可能的混合模式），帮助用户根据需求自行配置。  

### 276e03b92c168f9457f922d8d957900dd30e5d89
https://github.com/vllm-project/vllm/commit/276e03b92c168f9457f922d8d957900dd30e5d89
[CI][DeepSeek] Add nightly DeepSeek R1 `lm_eval` tests on H200 (#30356)

Signed-off-by: Matthew Bonanni <mbonanni@redhat.com>
Co-authored-by: Michael Goin <mgoin64@gmail.com>
**🎯 变更类型**：测试 / CI 配置  
**⚡ 重要程度**：🟡中  
**📋 变更摘要**：在 Buildkite CI 流水线中新增一个针对 H200 GPU（8 卡）的 DeepSeek‑R1 大模型 LM‑Eval 测试步骤，并补充对应的模型配置文件（DP/TP 两种并行方式）及配置列表。同步更新测试代码以支持可配置的启动等待时间。  

**🎯 影响范围**：  
- `.buildkite/test-pipeline.yaml`（新增 H200 测试步骤）  
- `tests/evals/gsm8k/configs/`（新增 DeepSeek‑R1‑DP/TP 配置）  
- `tests/evals/gsm8k/configs/models-h200.txt`（列出新增的两份配置）  
- `tests/evals/gsm8k/test_gsm8k_correctness.py`（使用配置中的 `startup_max_wait_seconds`）  

**🔍 技术洞察**：  
- **架构影响**：仅在 CI 环境中引入新的测试阶段，属于外围设施的扩展，对产品代码和运行时架构无影响。  
- **性能影响**：该步骤使用 8 块 H200 GPU，超时时间 60 分钟，可能显著增加 CI 总时长和资源占用；但对业务服务性能无直接影响。  
- **安全考虑**：未涉及代码执行路径的安全改动；仅在受控 CI 环境中运行，风险可控。  

**⚠️ 潜在风险**：  
1. **资源竞争**：若 H200 GPU 实例不足或被其他流水线占用，新增步骤将频繁被标记为 “optional” 而被跳过，导致测试覆盖率下降。  
2. **CI 时长激增**：8‑GPU 大模型启动与评估可能接近 60 分钟上限，容易导致流水线阻塞。  
3. **配置错误**：新增的 YAML 配置文件若语法或参数拼写错误（如 `tensor-parallel-size` vs `data-parallel-size`），会导致测试启动失败。  
4. **兼容性**：`--speculative-config` 参数在部分旧版本 VLLM 中可能不被识别，需确保 CI 镜像已更新。  

**💡 关注建议**：  
- 在新增 H200 资源前，确认 Buildkite 代理机器已装配相应 GPU 并能正常分配 8 卡。  
- 将此测试标记为 `optional` 已是防护措施，建议监控其成功率，若频繁跳过，考虑补充 H200 资源或调低并行卡数。  
- 在本地或预生产环境先跑一次完整的 `pytest -s -v evals/gsm8k/test_gsm8k_correctness.py --config-list-file=configs/models-h200.txt`，确保配置文件可被解析，`startup_max_wait_seconds` 参数生效。  
- 若 CI 时长持续接近上限，可考虑分拆 DP/TP 两种并行方式为独立步骤或调低 `timeout_in_minutes`。  
- 定期审查 `configs/models-h200.txt` 与实际可用模型列表保持同步，防止因模型名称变更导致 CI 误报。

### 32f4e4db00f79188e2c4b5986391a38d0bac0558
https://github.com/vllm-project/vllm/commit/32f4e4db00f79188e2c4b5986391a38d0bac0558
[Cleanup] Remove deprecated fields from CachedRequestData class (#31734)

Signed-off-by: njhill <nickhill123@gmail.com>
**🎯 变更类型**：重构 / 清理  
**⚡ 重要程度**：🟢 低  
**📋 变更摘要**：本次提交在 `vllm/v1/core/sched/output.py` 中删除了 `CachedRequestData` 类的已废弃属性 `resumed_from_preemption` 与 `resumed_req_token_ids`，并同步更新了对应单元测试以使用新的 `resumed_req_ids` 与 `all_token_ids` 接口。  

**🎯 影响范围**：  
- `vllm/v1/core/sched/output.py` 中的 `CachedRequestData` 类  
- `tests/v1/core/test_scheduler.py`（与调度器返回数据结构的断言）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 仅涉及内部数据结构的 API 清理，不改变模块间的调用关系。  
  - 移除的属性曾标记为 `@deprecated`，其存在主要是向后兼容，删除后可简化类实现，降低维护成本。  

- **性能影响**：  
  - 删除两个 `@cached_property`（分别计算布尔列表和 token‑id 列表）后，实例化 `CachedRequestData` 时不再执行额外的惰性计算，微小的 CPU 与内存开销削减。  
  - 对整体调度性能影响可忽略不计。  

- **安全考虑**：  
  - 无直接安全相关改动。  
  - 唯一潜在风险是外部代码（如自定义插件或旧版示例）仍引用已删除属性，会抛出 `AttributeError`，导致服务异常。  

**⚠️ 潜在风险**：  
1. **向后兼容性**：如果项目的下游用户或内部工具在 v0.13 之前的版本中依赖 `resumed_from_preemption` / `resumed_req_token_ids`，升级后将出现属性缺失错误。  
2. **测试覆盖**：仅更新了核心测试文件，若还有未覆盖的代码路径（例如自定义调度器实现）使用这些属性，可能在运行时触发错误。  

**💡 关注建议**：  
- **升级指引**：在发布说明中明确标注 “在 v0.14 中移除 `resumed_from_preemption` 与 `resumed_req_token_ids`，请改用 `resumed_req_ids` 与 `all_token_ids`”。建议使用者在升级前搜索代码库中对这两个属性的引用并进行相应改写。  
- **回归测试**：在 CI 中加入对旧属性访问的负向测试，确保在新版本中抛出预期异常，防止意外的成功调用掩盖兼容性问题。  
- **文档更新**：同步更新 `CachedRequestData` 的文档注释，删除已废弃属性的说明，加入新属性的使用示例。  
- **兼容层（可选）**：若担心破坏性升级影响广泛用户，可在下一次大版本中提供轻量的兼容 shim（如在 `__getattr__` 中捕获旧属性名字并返回等价数据），并在文档中标明该 shim 将在 v0.14 后彻底删除。  

总体而言，此次清理对功能没有实质影响，收益在于代码整洁度和轻微的性能提升；只需在升级指南中做好向后兼容提示即可。

### ee212918250ab5aa33a2968c027d71ffd6048ae4
https://github.com/vllm-project/vllm/commit/ee212918250ab5aa33a2968c027d71ffd6048ae4
[Model] Nemotron Parse 1.1 Support (#30864)

Signed-off-by: amitz-nv <203509407+amitz-nv@users.noreply.github.com>
Signed-off-by: Michael Goin <mgoin64@gmail.com>
Co-authored-by: Michael Goin <mgoin64@gmail.com>
**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
为 vLLM 引入 NVIDIA Nemotron‑Parse‑v1.1 多模态模型的完整支持，新增模型实现 `NemotronParseForConditionalGeneration`、对应的多模态处理器、权重加载逻辑以及示例测试。同步更新了依赖（albumentations）、registry、配置结构以及若干相关模块（radio、image asset、tests）以兼容新模型的特性（图像预处理、CLS‑token 机制、教师模型等）。

**🎯 影响范围**  
- `vllm/model_executor/models/nemotron_parse.py`（新模型主实现）  
- `vllm/model_executor/models/radio.py`（CLS‑token 与 teacher 逻辑调整）  
- `vllm/transformers_utils/configs/radio.py`（新增配置字段 `cpe_max_size`, `register_multiple`, `teachers`, `cls_token_per_teacher`）  
- `vllm/model_executor/models/registry.py`、`vllm/model_executor/models/registry.py`（注册新模型）  
- `vllm/assets/image.py`（提供 `pil_image_ext` 接口）  
- `tests/.../test_nemotron_parse.py`（新增端到端对比测试）  
- 依赖文件 `requirements/test.in`、`requirements/test.txt`（加入 `albumentations`）  
- 相关多模态测试与工具文件的微调（`tests/conftest.py`、`tests/models/multimodal/...` 等）。

**🔍 技术洞察**  

- **架构影响**  
  - 新增 **Encoder‑Decoder** 结构：图像通过 `RadioWithNeck`（基于 Radio 视觉模型）抽取特征，decoder 采用改写的 `MBartDecoderNoPos`。  
  - 引入 **多教师（teachers）** 概念及可选 **每教师独立 CLS‑token**，通过 `RadioConfig` 传递至 `RadioWithNeck`。  
  - 新的 **MultimodalProcessor** 完成图像‑文本拼接、占位符处理，注册至全局 `MULTIMODAL_REGISTRY`。  
  - 通过 `NemotronParseMultiModalProcessor` 实现 **PromptReplacement**，把图像占位符展开为固定数量的图像 token。  
  - 修改 `Radio` 的 patch‑generator 与 `ViTPatchGenerator` 参数，使得 `register_multiple` 与 `num_cls_tokens` 与新配置保持一致。  

- **性能影响**  
  - 视觉前端使用 **Radio + 自定义 neck**，在 `forward` 中进行两次卷积+reshape，额外的 `summary` 抽取会略增显存占用（约 +0.5~1 GB 取决于 `image_size` 与 `patch_size`）。  
  - 采用 **ColumnParallelLinear / RowParallelLinear** 仍保持跨卡并行，整体吞吐基本保持与原 Radio 模型相当。  
  - `tests/nemotron_parse` 中对比 HF 与 vLLM，使用 `use_cache=False` 避免 HF 端崩溃，说明在缓存路径上已有轻微差异，实际推理时仍建议开启 KV‑cache 以获得最佳性能。  

- **安全考虑**  
  - 新增 **albumentations** 作为图像预处理依赖。该库在过去的版本中曾出现 CVE（如 < 2023‑xxx），需在 CI 中锁定安全的版本（当前 `1.4.6` 已无已知高危漏洞）。  
  - 没有涉及代码执行或外部网络调用，仅在本地对图像做预处理，风险可控。  

**⚠️ 潜在风险**  

1. **向后兼容性**  
   - `RadioConfig` 新增字段，旧模型的配置文件若缺少这些字段，`from_pretrained` 仍会采用默认值；但若在自定义 config 中手动覆盖 `max_img_size` 等旧字段，可能导致 **属性名称不匹配**（如 `max_img_size` → `cpe_max_size`），需要在迁移脚本或文档中说明。  

2. **权重加载**  
   - `RadioWithNeck.load_weights` 对 encoder 部分做了权重拆分，若权重命名与预期不完全一致（例如某些模型使用 `cls_token` 而非 `register_multiple`），会产生 **“Found unexpected weight”** 警告或加载失败。  

3. **依赖冲突**  
   - `albumentations` 与项目中已有的 `torchvision`、`opencv` 可能出现版本冲突，尤其在 CI 环境下需确保 `requirements/test.txt` 中的锁版本一致。  

4. **内存/显存峰值**  
   - 新模型默认 `max_num_seqs=64`、`limit_mm_per_prompt={"image": 1}`，在单卡显存 < 16 GB 的机器上运行大型 batch（如 64 seq）可能 OOM；需要在文档中提供显存推荐配置。  

5. **测试依赖**  
   - `test_nemotron_parse.py` 依赖 `ImageAsset` 与 `albumentations`，若本地缺少对应图片资源或网络下载失败，CI 会报错。  

**💡 关注建议**  

- **升级指导**：在升级到包含 Nemotron‑Parse 的 vLLM 版本时，请同时升级 `albumentations>=1.4.6`，并检查 `requirements/test.in` 是否已同步。  
- **兼容性检查**：对现有使用 Radio 模型的项目运行完整回归测试，尤其关注 `config.max_img_size` → `config.cpe_max_size` 的迁移；可在自定义 config 中加入兼容层（如在加载前手动映射字段）。  
- **显存评估**：在 GPU 环境下使用 `vllm run` 的 `--limit-mm-per-prompt '{"image":1}'` 及适当的 `--max-num-seqs`，确保不超过显存上限。建议提供 `--tensor-parallel-size` 与 `--pipeline-parallel-size` 的组合示例。  
- **权重验证**：在首次加载 Nemotron‑Parse 权重后，运行 `test_nemotron_parse` 完整对比 HF 与 vLLM 的 log‑probs，确认 `use_cache=False` 在 HF 侧的兼容性；若在生产中使用 KV‑cache，建议额外做一次端到端的功能验证。  
- **安全审计**：定期检查 `albumentations` 的安全公告，必要时在 CI 中加入 `pip audit` 步骤。  

总体而言，此次提交为 vLLM 引入了重要的多模态模型支持，提升了项目的功能覆盖度，但也带来了配置兼容、显存需求和依赖安全的新增关注点。建议在正式发布前完成上述风险验证，并在发布说明中明确迁移步骤与硬件要求。

### af1b07b0c57fab2b65f55a0c14ced3186c23a63f
https://github.com/vllm-project/vllm/commit/af1b07b0c57fab2b65f55a0c14ced3186c23a63f
[docker] install cuda13 version of lmcache and nixl (#30913)

Signed-off-by: Qidong Su <soodoshll@gmail.com>
**🎯 变更类型**：功能增强 / 配置 / 依赖更新  
**⚡ 重要程度**：🟡中  
**📋 变更摘要**：在 `docker/Dockerfile` 中新增 `CUDA_VERSION` 参数，并根据 CUDA 主版本号是否 ≥13，自动安装针对 CUDA 13 的 `nixl-cu13` 包以及对应的 `libcusparse-dev`、`libcublas-dev`、`libcusolver-dev` 开发库。若默认安装失败，则回退到源码编译并在完成后清理开发包，提升对 CUDA 13 环境的兼容性。  

**🎯 影响范围**：  
- `docker/Dockerfile`（vllm‑openai‑base 镜像构建）  
- KV‑Connectors 安装流程  
- 运行时依赖 `lmcache` / `nixl` 的容器镜像  

**🔍 技术洞察**：  
- **架构影响**：仅在 Docker 构建阶段引入新的构建路径，不影响运行时代码结构。通过 `ARG CUDA_VERSION` 把 CUDA 版本信息显式传递给后续的 apt 与 pip 安装步骤，提升镜像对不同 CUDA 版本的适配能力。  
- **性能影响**：新增的 apt 安装仅在构建阶段执行，且在成功后会 `apt-get purge` 开发包并删除 apt 缓存，对最终镜像大小影响有限（约几 MB）。运行时不引入额外的性能开销。  
- **安全考虑**：使用 `apt-get install --no-install-recommends` 限制额外依赖，构建完成后立即清理 dev 包，降低攻击面。唯一需要关注的是 `ARG CUDA_VERSION` 的值来源，确保不被未受信任的 CI 参数篡改导致意外安装错误的系统库。  

**⚠️ 潜在风险**：  
1. **CUDA_VERSION 未定义或格式异常** → `CUDA_MAJOR` 解析失败，导致条件判断错误或构建中断。  
2. **APT 包名称不存在**（如 `libcusparse-dev-13-0` 在某些 Ubuntu repo 中缺失） → 镜像构建失败。  
3. **fallback 编译路径依赖大量编译时间和网络**，在 CI 环境可能导致超时。  
4. **对旧版 CUDA 环境的兼容性**：若 `CUDA_VERSION` 仍为 12.x，`if [ "$CUDA_MAJOR" -ge 13 ]` 分支不会执行，但后续仍会尝试安装 dev 包，需确保这些包对应的版本可在系统仓库中获取。  

**💡 关注建议**：  
- 在 CI/CD 流程中显式传入 `CUDA_VERSION`（如 `13.0`），并对其格式做基本校验。  
- 为常见的 CUDA 12 与 CUDA 13 场景分别做完整的镜像构建测试，验证 `apt-get install` 的包名是否准确。  
- 若某些 dev 包在官方 repo 中不可得，考虑在 Dockerfile 中加入对应的 apt source 或使用 `conda`/`pip` 替代方案。  
- 监控构建日志中 `uv pip install --system nixl-cu13` 的返回码，确保在 CUDA 13 环境下成功拉取预编译 wheel。  
- 在镜像发布前执行一次容器启动测试，确认 `lmcache` 能正常加载对应的 CUDA 库。  
- 文档中补充说明 `CUDA_VERSION` 参数的使用方式及支持的版本列表，帮助用户避免因参数遗漏导致构建失败。

### c77a993cc297f23ae6f0c3ff1efb2d6d022bdfd2
https://github.com/vllm-project/vllm/commit/c77a993cc297f23ae6f0c3ff1efb2d6d022bdfd2
pin lora_b moe weights on cpu (#31317)

Signed-off-by: gnovack <gnovack@amazon.com>
**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
本次提交将 MOE（Mixture‑of‑Experts）LoRA 权重的切分与重排逻辑从 `activate_adapter` 时迁移至模型合并阶段，并在合并后对权重进行 **CPU pinned‑memory** 处理。为此新增 `_stack_moe_lora_weights` 方法，在 `LoRAModel` 合并时统一完成权重的 reshape、permute 与列表包装；同时删除了原先在 `FusedMoE3DWithLoRA.set_lora` 中的临时 reshape 步骤。

**🎯 影响范围**：  
- `vllm/lora/layers/fused_moe.py`（`FusedMoE3DWithLoRA.set_lora`）  
- `vllm/lora/model_manager.py`（`activate_adapter`、`_create_merged_loras_inplace`、新增 `_stack_moe_lora_weights`）  

**🔍 技术洞察**  

- **架构影响**：  
  - LoRA 权重的切分与排列从 “运行时激活” 移到 “模型加载/合并” 阶段，职责更清晰，`ModelManager` 负责一次性完成所有权重的准备工作。  
  - 新增的 `_stack_moe_lora_weights` 只在检测到 `FusedMoE3DWithLoRA` 实例时执行，保持了对其他普通 LoRA 层的兼容。  

- **性能影响**：  
  - **加载阶段**：多次 `reshape`、`permute`、`contiguous()` 会产生一次性额外的内存拷贝，但只在模型合并时进行，对推理时的吞吐量几乎没有影响。  
  - **推理阶段**：权重被 `pin_memory()` 到 CPU 后，可在后续 `to(device)` 时利用更快的 DMA 传输，略微提升跨设备切换或 CPU‑GPU 同步的效率。  
  - **空间复杂度**：在合并阶段会临时保持两套张量（原始 + 重排后），峰值内存略升高，尤其在 **3D MoE** 场景下（每个专家的权重都要复制一次）。  

- **安全考虑**：  
  - 仅涉及张量布局与内存固定，不引入网络、文件或权限相关代码，暂无新增安全风险。  

**⚠️ 潜在风险**  

1. **兼容性**  
   - 依赖 `module.w13_lora_a_stacked[0].shape[1]` 计算 `num_experts`；若旧模型未生成 `w13_lora_a_stacked` 或形状不符合预期，会抛出 `AttributeError`/`IndexError`。  
   - 对不具备 `is_3d_moe_weight` 标记的模型仍走 fallback 分块路径，逻辑较为复杂，可能在极端形状下产生切分错误。  

2. **内存峰值**  
   - 合并时会创建 reshape 后的临时张量以及 `contiguous` 副本， 对显存/CPU 内存限制较紧的部署（例如多卡共享同一主机）可能出现 OOM。  

3. **适配器切换**  
   - 原先在 `activate_adapter` 中即可完成切分，现在必须先完成 `_create_merged_loras_inplace`（即合并）才能使用 LoRA。若业务在运行时频繁加载/卸载适配器，可能会失去原有的“即时激活”能力。  

**💡 关注建议**  

- **回归测试**：  
  - 分别在 **3D MoE** 与 **传统 MoE**（未标记 `is_3d_moe_weight`）模型上加载带 LoRA 的 checkpoint，确认 `num_experts` 计算、权重切分、`set_lora` 调用均正常。  
  - 对比激活前后模型输出，确保数值一致性（尤其是 `gate_up_proj` 与 `down_proj` 的顺序未被意外调换）。  

- **内存监控**：  
  - 在大规模专家（如 64、128）模型上监控合并阶段的峰值内存，必要时在部署脚本中提前 `torch.cuda.empty_cache()` 或使用 `torch.utils.checkpoint` 分段加载。  

- **兼容性保护**：  
  - 若历史模型缺失 `w13_lora_a_stacked`，可在 `_stack_moe_lora_weights` 中加入回退逻辑（例如根据 `module_lora.lora_a.shape[0] // module_lora.rank` 推算 `num_experts`），并记录日志提示用户。  

- **文档更新**：  
  - 在 LoRA 使用手册中注明：**MOE LoRA 权重在加载阶段会被预先切分并 pin 到 CPU**，并建议在部署前完成 `model_manager.merge_and_pin()`（或相应的 API）后再进行推理。  

- **未来改进**：  
  - 考虑把权重切分逻辑抽象为独立的 “MoE LoRA 工具类”，供其他组件复用，降低 `ModelManager` 的复杂度。  
  - 若有需求在推理时动态切换 LoRA，保留一种 “lazy split” 方案，仅在首次使用时执行切分，以兼顾内存与灵活性。

### fdcc5176beb70e801d9baae14f992819b1ffcffb
https://github.com/vllm-project/vllm/commit/fdcc5176beb70e801d9baae14f992819b1ffcffb
[BugFix] Fix architecture flags to prevent issues on SM103 (#31150)

Signed-off-by: LopezCastroRoberto <robertol.c510@gmail.com>
**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
- 修正了 QUTLASS 与 FlashInfer 在不同 CUDA 版本下的架构标记逻辑，避免在 CUDA 12.8 以上或 CUDA 13.0+ 时出现编译错误。  
- 新增对 SM10.0 (10.0f) 与 SM10.3、SM12.0 等新架构的兼容性，统一在 CMake 与 Shell 脚本中进行合理的版本分支判断。  

**🎯 影响范围**：  
- `cmake/external_projects/qutlass.cmake`（QUTLASS 的 CUDA 架构检测）  
- `tools/flashinfer-build.sh`（FlashInfer AOT 编译脚本的 CUDA 架构列表）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 将原先硬编码的 `12.0a;10.0a` 等列表改为基于 `CMAKE_CUDA_COMPILER_VERSION` 的条件分支，明确区分 `<13.0`、`>=13.0` 与 `>=12.8` 的不同需求。  
  - 引入对 `10.0f`（SM10.0）以及 `10.3a`（SM10.3）的匹配，使得在新的 CUDA 12.8+、13.0+ 环境下能够正确选择 `QUTLASS_TARGET_CC`。  
- **性能影响**：  
  - 此更改仅影响编译阶段的 **架构选择**，不改变运行时代码路径或算法实现，理论上不产生性能回归。  
  - 正确的架构标记可避免因编译了不支持的 SM 导致的运行时错误，从间接上提升可靠性。  
- **安全考虑**：  
  - 无直接安全相关改动。  
  - 需确保新增的架构列表仅包含实际受支持的 SM，以免引入潜在的 **未验证二进制**（例如针对不存在的 GPU 编译的代码）导致异常行为。  

**⚠️ 潜在风险**：  
1. **兼容性风险**：若用户使用的 CUDA 编译器版本在 12.8–12.9 之间但未更新到对应的驱动，`10.0a/10.3a` 可能仍不可用，导致链接错误。  
2. **架构误匹配**：`10.0f` 只在 CUDA 13.0+ 受支持，若误在 12.x 环境下触发，可能产生编译失败。  
3. **脚本同步问题**：两个文件分别维护了类似的版本分支，如果将来 CUDA 版本继续演进，需同步更新两处逻辑，防止出现不一致的架构列表。  

**💡 关注建议**：  
- **测试覆盖**：在 CI 中加入以下矩阵测试：  
  - CUDA 12.7、12.8、12.9、13.0+ 各对应的 `ARCH_LIST` 是否生成正确。  
  - 真实硬件上（如 A100、H100、RTX 4090）编译并运行一次端到端的 FlashInfer 示例，确保生成的二进制能在对应 GPU 上正常执行。  
- **文档更新**：在项目的构建说明中注明：  
  - 对于 CUDA 12.8–12.9 需要使用 `10.0a 10.3a 12.0`；  
  - 对于 CUDA 13.0+ 使用 `10.0f` 而非 `10.0a`。  
- **未来维护**：建议抽象出 “CUDA version → supported SM 列表” 的映射表（如 JSON/YAML），统一在 CMake 与 Shell 脚本中读取，降低分散硬编码的维护成本。  

---  

### 5708297e4e7f27c4d06d6e4f62d75c3d2d1fc674
https://github.com/vllm-project/vllm/commit/5708297e4e7f27c4d06d6e4f62d75c3d2d1fc674
[Misc][Model][Refactor] Pass the prefix into Linear layers (#31669)

Signed-off-by: Wang Kunpeng <1289706727@qq.com>
**🎯 变更类型**：重构（Refactor）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 vllm 项目中，向大量 Linear（ColumnParallelLinear、RowParallelLinear、MergedColumnParallelLinear、ReplicatedLinear 等）以及相关子模块的构造函数中新增 `prefix` 参数，并在实例化时传入基于父层前缀生成的名称。目的是在模型并行/张量并行场景下为每个子层提供唯一的命名空间，以便权重加载、日志、调试以及分布式切片更可追踪。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/mamba/*`（Mamba Mixer）  
- 多个模型实现：`aria.py`, `gpt_neox.py`, `hunyuan_v1.py`, `jamba.py`, `jina_vl.py`, `minicpm*.py`, `mistral_large_3_eagle.py`, `modernbert.py`, `molmo.py`, `olmoe.py`, `phimoe.py`, `qwen*.py`, `qwen_vl.py`, `zamba2.py` 等。  
- 直接受影响的核心组件为 **并行线性层**（Column/Row/Merged/Replicated）以及包装这些层的 MLP、注意力、投影等模块。  

---

## 🔍 技术洞察

### 架构影响
- **模块命名层级化**：通过 `prefix` 让每个子层在分布式图中拥有可辨识的路径（如 `layer7.attn.Wqkv`），提升了 **可观测性** 与 **调试便利性**，对模型并行框架（Tensor Parallel / Pipeline Parallel）更友好。
- **向后兼容**：多数新增 `prefix` 参数默认值为空字符串，保持旧代码运行不变。但若调用方显式传入 `prefix`，则必须保证所有子层都同步使用 `maybe_prefix`（或手动拼接）来避免 **命名不一致**。
- **依赖统一化**：新增 `prefix` 参数后，权重加载逻辑（`weight_loader`、`maybe_prefix`）需要在所有模型构造路径中保持一致，否则可能导致权重映射错误。

### 性能影响
- **运行时开销**：仅是字符串拼接与参数传递，对前向/反向计算无实质影响，**时间/空间复杂度保持不变**。
- **内存占用**：多出少量用于保存前缀的 `str` 对象，几乎可以忽略。

### 安全考虑
- **无直接安全风险**：改动不涉及网络、文件系统或权限逻辑。唯一需关注的是 **权重加载错误** 可能导致模型行为异常，进而影响下游业务（比如错误的生成结果），需在测试阶段验证。

---

## ⚠️ 潜在风险
| 风险点 | 说明 | 严重度 |
|--------|------|--------|
| **权重加载兼容性** | 老的 checkpoint 文件使用原始层名前缀（无 `prefix`），若在新代码中强制使用前缀会导致 `KeyError` 或错误映射。 | 中 |
| **前缀拼写错误** | 手动拼接 `prefix` 时容易出现漏写、重复点号等错误，导致同层出现多个实例或权重冲突。 | 低 |
| **缺失 `maybe_prefix`** | 某些模型在子模块初始化时未使用 `maybe_prefix`（如 `MolmoVisionBackbone` 等），在多层前缀传递链路中可能出现 **层级断层**。 | 中 |
| **文档/示例同步** | 新增参数未在官方文档、示例或 API 说明中更新，使用者可能不知需要传递前缀，导致调试困难。 | 低 |
| **序列化/保存** | `torch.save` 中保存模型时会把 `prefix` 作为属性保存，若后续加载不提供相同前缀，可能出现 **不匹配**（但权重本身不依赖前缀）。 | 低 |

---

## 💡 关注建议
1. **回归测试**  
   - 用已有的公开 checkpoint（包括旧版本的）分别在 **不传 `prefix`** 与 **传递空前缀** 的两种模式下加载，确保兼容。  
   - 对比前向输出（比如 logits）确保数值一致。

2. **统一前缀生成**  
   - 推荐在所有模型入口（如 `ModelExecutor`）统一使用 `maybe_prefix(prefix, sub_name)`，避免手动拼接错误。  
   - 若某些子模块暂未使用 `maybe_prefix`，尽快补全。

3. **文档 & 示例更新**  
   - 在 `README`、`api.md`、以及模型构造示例中说明 `prefix` 参数的意义、默认行为与推荐用法。  
   - 提供 **迁移指南**：如何在已有代码中加入 `prefix`，以及如何加载老 checkpoint。

4. **日志/调试辅助**  
   - 在关键层初始化处（如 `ColumnParallelLinear.__init__`）加入 `logger.debug(f"Layer prefix: {prefix}")`，帮助使用者快速定位层级结构。

5. **CI 检查**  
   - 添加 CI 步骤，确保所有模型的 `__init__` 参数签名包含 `prefix: str = ""`（可通过静态分析或自定义脚本）。  
   - 检查 `maybe_prefix` 是否在所有子模块中被使用。

6. **权重转换脚本**  
   - 如有需要提供 **checkpoint 转换工具**，将旧 checkpoint 中的权重键名映射到新前缀格式（比如 `layer.0.attn.Wqkv.weight` → `layer.0.attn.Wqkv.weight`，前缀保持不变即可）。

---

**结论**：此轮改动主要是为了在大规模并行训练/推理场景下提供更清晰的层级命名，属于**低风险的结构性重构**。只要在权重加载兼容性、前缀统一使用以及文档同步方面做好验证，即可安全上线，且对后续模型可观测性、调试效率提升显著。

### 02dbb933cb28eb2b0f05dd491f20273d2f84e517
https://github.com/vllm-project/vllm/commit/02dbb933cb28eb2b0f05dd491f20273d2f84e517
Fix GLM-4.6v flash tool calling in transformers 5.x (#31622)

Signed-off-by: baonudesifeizhai <baonudesifeizhai@gmail.com>
**🎯 变更类型**：Bug修复 / 功能增强  

**⚡ 重要程度**：🟡中  

**📋 变更摘要**：  
1. 新增 `examples/tool_chat_template_glm4.jinja`，为 GLM‑4.6v 提供完整的工具调用 chat 模板，明确了 `<tool_call>` 标签的使用规范。  
2. 在 `vllm/tool_parsers/glm4_moe_tool_parser.py` 中实现 `adjust_request`，在使用工具调用且 `tool_choice != "none"` 时强制 `skip_special_tokens=False`，解决 Transformers 5.x 版本中对特殊 token 解码行为的回退问题。

**🎯 影响范围**：  
- `vllm` 的工具解析模块（`glm4_moe_tool_parser`）  
- 使用 GLM‑4.6v（或基于同类模型）进行工具调用的下游应用  
- 示例目录 `examples/`（仅文档层面，不影响运行时）

**🔍 技术洞察**：  
- **架构影响**：  
  - 新增的 `adjust_request` 方法在 `ToolParser` 层拦截并修改 `ChatCompletionRequest`，属于轻量级的扩展，不改变类层次结构或依赖注入方式。  
  - 引入的 Jinja 模板只用于示例和 Prompt 生成，对核心库代码无耦合影响。  

- **性能影响**：  
  - `adjust_request` 只在请求初始化阶段执行一次，额外的属性赋值开销极低，几乎不影响整体吞吐或响应时延。  
  - 新增模板文件体积极小，对加载时间影响可以忽略不计。  

- **安全考虑**：  
  - 无新增外部依赖或执行代码，未引入安全风险。  
  - `skip_special_tokens=False` 可能导致模型输出中的特殊 token 被原样返回，需要上层调用方确保不直接将这些 token 传递给不可信的 downstream 系统。  

**⚠️ 潜在风险**：  
1. **兼容性**：在使用旧版（<5.0）Transformers 时，`skip_special_tokens=False` 仍会生效，可能导致与旧版默认行为不一致，进而出现额外的 `<tool_call>` 标记被显式输出。  
2. **功能回归**：如果上层代码依赖于默认的 `skip_special_tokens=True`（例如在不使用工具调用时想要过滤所有特殊 token），该修改在 `tools` 与 `tool_choice` 为 `none` 时不会生效，但在某些边界条件（如 `tools` 列表为空但仍设置 `tool_choice`）可能意外触发。  
3. **模板使用错误**：示例模板中强制使用 `<tool_call>` 包裹函数名和参数，若用户自行修改模板而未遵守标签格式，可能导致解析失败或产生错误的 tool calls。  

**💡 关注建议**：  
- **回归测试**：在不同的 Transformers 版本（4.x、5.x）下跑全套 `vllm` 单元测试，特别是包含工具调用的 `ChatCompletion` 场景，确认 `skip_special_tokens` 行为符合预期。  
- **向后兼容**：可在 `adjust_request` 中加入版本检测（如 `if transformers.__version__.startswith("5.")`），仅在需要时修改 `skip_special_tokens`，以避免对旧版产生不必要影响。  
- **文档提示**：在 README 或模板文件注释中说明 `<tool_call>` 标签的必要性及 `skip_special_tokens=False` 的意义，提醒使用者在自定义 Prompt 时保持一致。  
- **监控与日志**：在实际部署环境中捕获并日志记录 `request.skip_special_tokens` 的最终值，便于定位因配置误差导致的异常行为。  

---  

### 51e38a8e3010ef20c6cb2f33f27fff0a2753aef4
https://github.com/vllm-project/vllm/commit/51e38a8e3010ef20c6cb2f33f27fff0a2753aef4
[Misc] Enable Paligemma's PrefixLM attention mask computation (#31725)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
**🎯 变更类型**：Bug修复 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm/config/model.py` 中将 `paligemma` 恢复至多模态 PrefixLM 模型列表，使其能够使用 PrefixLM attention mask；相应地在测试 `tests/models/multimodal/generation/test_common.py` 中移除对该模型的跳过标记，使其纳入 CI 测试。  

**🎯 影响范围**：  
- `vllm/config/model.py`（模型配置检测逻辑）  
- `tests/models/multimodal/generation/test_common.py`（多模态生成测试）  

**🔍 技术洞察**：  
- **架构影响**：`is_mm_prefix_lm` 方法会在检测到 `model_type == "paligemma"` 时返回 `True`，从而在后续的模型构建阶段走 “PrefixLM” 注意力路径，涉及 `FlexAttention` 的软上限注意力实现。  
- **性能影响**：开启 PrefixLM 注意力 mask 将导致额外的掩码计算和可能的双向注意力开销，理论上会略微增加显存占用和计算时间，但对整体吞吐影响有限。  
- **安全考虑**：无直接安全相关改动；唯一需要关注的是若注意力掩码计算错误，可能导致模型生成不符合预期，进而影响业务逻辑的正确性。  

**⚠️ 潜在风险**：  
1. 当前 `FlexAttention` 对软上限注意力的支持仍在实验阶段，若该特性未完整实现，运行 `paligemma` 可能抛出异常或产生错误的注意力掩码。  
2. 移除测试的 `skip` 标记后，CI 环境如果缺少相应的依赖或硬件（如不支持 `bfloat16`），可能导致测试失败。  
3. 对已有的模型配置逻辑产生不兼容的行为，尤其是用户自行覆盖 `MM_PREFIX_LM_MODELS` 时可能产生意外结果。  

**💡 关注建议**：  
- **测试**：在本地和 CI 中完整运行包含 `paligemma` 的多模态生成测试，重点验证注意力掩码的形状、数值以及生成文本的正确性。  
- **回滚策略**：若在生产环境出现异常，建议在 `vllm/config/model.py` 中暂时注释掉 `"paligemma"`，或通过环境变量/配置显式禁用 PrefixLM 注意力。  
- **文档**：更新模型兼容性列表，说明 `paligemma` 需要 `FlexAttention` 的软上限注意力支持。  
- **监控**：上线后监控显存使用和推理时延，确保新增的注意力计算不会导致资源异常。

### d8e38d493907882eb247a2ba4ced32610020ac27
https://github.com/vllm-project/vllm/commit/d8e38d493907882eb247a2ba4ced32610020ac27
Triton Attention: Support cross-layers blocks (#30687)

Signed-off-by: Or Ozeri <oro@il.ibm.com>
**🎯 变更类型**：功能增强 / 测试  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 CPU 测试中加入 `TRITON_ATTN` 作为可选注意力后端，扩大覆盖范围。  
2. 为 Triton 注意力后端实现 `get_kv_cache_stride_order` 静态方法，支持跨层（跨层数）KV 缓存布局的 stride 排序，以便在包含 `num_layers` 维度时正确映射内存布局。  

**🎯 影响范围**：  
- `vllm/v1/attention/backends/triton_attn.py`（Triton 注意力实现）  
- `tests/v1/kv_offload/test_cpu_offloading.py`（CPU offloading 测试）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 新增的 `get_kv_cache_stride_order` 方法为 Triton 后端提供了统一的 stride 排序查询接口，与其他后端（如 FlashAttention）保持 API 对齐。  
  - 通过 `include_num_layers_dimension` 参数，支持在多层模型中将层维度加入 stride 计算，提升跨层 KV 缓存的可复用性。  
- **性能影响**：  
  - 方法本身仅返回硬编码的元组，没有运行时计算开销，几乎不影响性能。  
  - 通过统一 stride 排序，可避免在后续代码中进行额外的维度转换，潜在地提升内存访问效率。  
- **安全考虑**：  
  - 无涉及外部输入或安全敏感操作，风险极低。  

**⚠️ 潜在风险**：  
- **API 兼容性**：如果已有代码对 `TritonAttentionBackend` 实例调用了不存在的 `get_kv_cache_stride_order`（如旧版本）或对返回值形状有假设，可能导致运行时 `AttributeError` 或维度不匹配。  
- **平台差异**：在 ROCm 环境下，原来的代码曾显式设定 `ATTN_BACKENDS = ["TRITON_ATTN"]`，但此条件被注释掉，可能导致在 ROCm 上缺少对应后端测试覆盖。  
- **测试覆盖不足**：仅在 CPU 测试中加入 `TRITON_ATTN`，但缺少对应的实际 Triton 后端运行时验证，可能隐藏实现细节缺陷。  

**💡 关注建议**：  
1. **新增单元测试**：为 `TritonAttentionBackend.get_kv_cache_stride_order` 编写专门的单元测试，覆盖 `include_num_layers_dimension=True/False` 两种路径，并验证与 `get_kv_cache_shape` 的对应关系。  
2. **兼容性检查**：在项目的公共接口文档中记录该方法，并在 CI 中加入对旧版调用的兼容性检查（如使用 `hasattr` 或 `try/except`）。  
3. **ROCm 支持**：确认在 ROCm 平台上 `TRITON_ATTN` 是否可用，如不可用则在测试中条件性排除，或恢复原有的 `ATTN_BACKENDS` 设置。  
4. **回归测试**：在包含跨层 KV 缓存的模型（例如多层 Decoder）上运行完整的性能/正确性回归，以确保新 stride 排序不会导致内存布局错误。  

通过上述措施可降低潜在风险，确保新功能在不同硬件平台和模型配置下平稳运行。

### 21156ff1999f180c95176d4be9b9efb16dcb8473
https://github.com/vllm-project/vllm/commit/21156ff1999f180c95176d4be9b9efb16dcb8473
[Bugfix] Add missing extra_tensors arg to DeviceCommunicatorBase.disp… (#31644)

Signed-off-by: kunzh <zhikun.wu@outlook.com>
**🎯 变更类型**：Bug修复 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
在 `DeviceCommunicatorBase.dispatch` 方法以及 XPU 具体实现中新增了 `extra_tensors` 参数，以兼容调用方在分发阶段需要额外同步的张量。并在 XPU 的 all2all 后端判断处加入 `type: ignore` 注解，防止类型检查误报。此改动修复了因缺少该参数导致的运行时错误（#31644）。

**🎯 影响范围**：  
- `vllm/distributed/device_communicators/base_device_communicator.py`（基类）  
- `vllm/distributed/device_communicators/xpu_communicator.py`（XPU 实现）  
- 任何直接或间接调用 `DeviceCommunicatorBase.dispatch` 的子类或上层调度逻辑。

**🔍 技术洞察**：  
- **架构影响**：  
  - 基类签名变更属于向后兼容的 API 扩展，所有子类需要保持同样的签名。XPU 实现已同步更新，其他通信器（如 NCCL、CPU）若未同步修改，可能在运行时出现 `TypeError`。  
  - `extra_tensors` 为可选列表，默认 `None`，不影响已有调用路径，保持现有模块耦合度。  

- **性能影响**：  
  - 仅在调用 `all2all_manager.dispatch` 时多传递一个引用，算法复杂度、内存占用基本不变。若实际使用 `extra_tensors`，会产生额外的通信开销，需根据业务场景评估。  

- **安全考虑**：  
  - 无直接安全风险。`extra_tensors` 可能包含敏感信息，若在跨设备通信中未加密/校验，仍遵循原有 all‑to‑all 通道的安全模型。  

**⚠️ 潜在风险**：  
1. **子类未同步签名**：其他 `DeviceCommunicator` 实现如果未添加 `extra_tensors` 参数，调用基类方法时会抛出 `TypeError`。  
2. **调用方遗漏参数**：上层调用如果显式传递 `extra_tensors`（如 `list[torch.Tensor]`），但实际实现仍使用旧签名，可能导致参数被忽略或错误。  
3. **类型忽略导致误用**：`type: ignore[has-type]` 与 `type: ignore[call-arg]` 隐藏了潜在的类型不匹配问题，若后续更改 all2all_manager 接口，可能不易被检测到。  

**💡 关注建议**：  
- **全面回归测试**：对所有 `DeviceCommunicator` 子类执行单元/集成测试，确保 `dispatch` 调用签名一致且不抛异常。  
- **文档更新**：在 `DeviceCommunicatorBase` 的文档或代码注释中说明新增的 `extra_tensors` 参数及其默认行为。  
- **上线注意**：在生产环境升级时，验证使用 XPU 通信的模型是否仍能正常运行，特别是涉及序列并行（sequence‑parallel）和自定义 extra tensors 的场景。  
- **代码审查**：后续若新增其他通信后端（如 GPU、CPU），务必同步实现 `extra_tensors` 参数，避免出现类似破坏性 API 不匹配的问题。  

### c455b771fdfd9185ca5c20d315c07dd5390b5459
https://github.com/vllm-project/vllm/commit/c455b771fdfd9185ca5c20d315c07dd5390b5459
[Bugfix][CPU] Fix RotaryEmbedding fallback causing gibberish with --enforce-eager (#31643)

Signed-off-by: rickychen-infinirc <ricky.chen@infinirc.com>
**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**：  
- 修复了在 CPU 环境下强制使用 eager 模式 (`--enforce-eager`) 时，RotaryEmbedding 回退导致的输出乱码问题。  
- 为 CPU 路径实现了专门的 `forward_cpu` 逻辑，直接调用自定义的 `rotary_embedding` 原位算子，避免错误地复用 CUDA 实现。  

**🎯 影响范围**：  
- `vllm/model_executor/custom_op.py`（CPU 路径的默认实现从 `forward_cuda` 改为 `forward_native`）  
- `vllm/model_executor/layers/rotary_embedding/base.py`（新增 `forward_cpu` 实现，使用自定义算子）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 为 CPU 执行分支提供了明确实现，避免了之前对 CUDA 实现的错误依赖，提升了平台抽象的一致性。  
  - 通过 `vllm._custom_ops` 在 CPU 上使用同一套原位算子，保持了代码路径的统一性。  

- **性能影响**：  
  - `ops.rotary_embedding` 为原位操作，内存开销与原生实现相当，且保持了高效的向量化计算。  
  - 与之前误用 `forward_cuda` 产生的潜在数据拷贝或不兼容行为相比，性能更稳定。  

- **安全考虑**：  
  - 无新增安全风险。唯一需要注意的是自定义算子在不同硬件/OS 环境的兼容性。  

**⚠️ 潜在风险**：  
- 如果自定义算子在目标机器上未编译或加载失败，CPU 路径将缺失实现，可能导致运行时 `ImportError` 或崩溃。  
- 现有的单元测试若未覆盖 CPU `forward_cpu` 分支，回归风险仍在。  

**💡 关注建议**：  
- 在 CI 中加入 CPU‑only（无 CUDA）且开启 `--enforce-eager` 的集成测试，验证输出正确性。  
- 为未构建自定义算子的环境提供明确的错误提示或回退到 PyTorch 原生实现的机制。  
- 更新文档，说明在 CPU 环境下需要确保 `vllm._custom_ops` 已正确编译并可用。  
- 对外发布时，可在发行说明中标明此修复，以提醒使用 CPU 预测的用户升级。  

### eefa713a6605624c3cb2c538982bd37f3d56b187
https://github.com/vllm-project/vllm/commit/eefa713a6605624c3cb2c538982bd37f3d56b187
[CI Failure] Disable B200 tests while runner is broken (#31732)

Signed-off-by: mgoin <mgoin64@gmail.com>
**🎯 变更类型**：配置  
**⚡ 重要程度**：🟢低  
**📋 变更摘要**：在 `.buildkite/test-pipeline.yaml` 中将多处使用 B200 GPU 的 CI 步骤标记为 `optional: true`，以绕过当前无法使用的 GPU runner。目的是防止 CI 因 runner 故障全部失败，仍保留这些步骤供后续恢复时手动开启。  

**🎯 影响范围**：  
- CI/CD 流水线（Buildkite）  
- 受影响的测试步骤包括：V1 Attention、Quantization FP4、FlashInfer utils、DeepSeek V2 等涉及 B200 GPU 的任务  

**🔍 技术洞察**：  
- **架构影响**：无。仅修改 CI 配置文件，不影响项目代码或运行时架构。  
- **性能影响**：无。对实际业务运行或模型推理性能没有直接影响。  
- **安全考虑**：无。此更改不涉及代码或依赖的安全风险。  

**⚠️ 潜在风险**：  
- **测试覆盖下降**：被标记为 `optional` 的步骤在 runner 故障时会被跳过，可能导致相关功能的回归缺失，增加潜在缺陷未被及时发现的风险。  
- **误解风险**：如果未在提交说明或 CI 报告中突出标记为“可选”，团队成员可能误以为这些测试已成功通过。  

**💡 关注建议**：  
1. **监控 runner 状态**：在 runner 修复后，及时移除 `optional: true` 并恢复这些步骤为必选，确保完整的回归测试。  
2. **CI 报告清晰化**：在 Buildkite UI 或 PR 注释中明确标注哪些步骤被临时设为可选，以防团队误解。  
3. **补充本地或其他环境的测试**：在本地或使用可用的 GPU（如 A100）手动跑一次被禁用的 B200 测试，确保关键功能未出现回归。  
4. **定期审计**：在每次 CI 运行后检查是否仍有 `optional` 标记遗留，防止长时间忽视。  

总体而言，此次更改仅为临时的 CI 维护措施，风险可控，但务必在 runner 修复后及时恢复完整测试，以确保代码质量不受影响。

### 79ed460dd52acc38c70e046eef56ed826712a688
https://github.com/vllm-project/vllm/commit/79ed460dd52acc38c70e046eef56ed826712a688
[Frontend] [Doc] Exclude log deltas feature (#30322)

Signed-off-by: Catacomba <kevinsuc16@gmail.com>
Signed-off-by: Kevin Šuc <kevinsuc16@gmail.com>
Co-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>
**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🟢低  

**📋 变更摘要**  
新增 `--exclude-log-deltas` 参数，用于在开启日志输出的情况下，仅在完整流式响应结束后记录模型输出，而不记录每个增量（delta）。实现包括 CLI 参数、参数校验、API Server 初始化以及聊天流式生成器的日志分支控制。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/api_server.py`（初始化 `OpenAIServingChat`）  
- `vllm/entrypoints/openai/cli_args.py`（新增 CLI 参数与文档）  
- `vllm/entrypoints/openai/serving_chat.py`（构造函数、流式日志分支）  

**🔍 技术洞察**  

- **架构影响**：  
  - 在 `OpenAIServingChat` 中加入 `exclude_log_deltas` 成员，保持向后兼容（默认 `False`），不影响原有模块交互。  
  - 仅修改聊天流（`chat_completion_stream_generator`）的日志路径，未触及其他服务（如 completions、embeddings），因此模块耦合度保持低。  

- **性能影响**：  
  - 当 `exclude_log_deltas=True` 时，减少每个增量的日志写入，能够显著降低 I/O 与日志聚合系统的负载，特别是在高并发流式请求场景下。  
  - 代码路径改变极小，时间/空间复杂度基本不变。  

- **安全考虑**：  
  - 通过减少日志量，间接降低了潜在的敏感信息泄露风险（比如中间 token、prompt 细节）。  
  - 未引入新的安全漏洞；新增参数的校验 (`--exclude-log-deltas` 必须配合 `--enable-log-outputs`) 防止误用。  

**⚠️ 潜在风险**  

1. **日志下游依赖缺失**：某些监控或审计系统可能依赖增量日志进行实时分析，开启 `exclude_log_deltas` 可能导致这些系统失效或误报。  
2. **不一致的行为**：目前仅在聊天流的日志分支加入排除逻辑，其他流式接口（如 `completion`）仍会记录增量，可能导致文档或使用者对 “exclude‑log‑deltas” 的期望不一致。  
3. **参数组合错误**：如果用户未显式开启 `--enable-log-outputs`，`--exclude-log-deltas` 会在校验阶段抛异常，升级脚本或自动化部署需要同步更新参数。  

**💡 关注建议**  

- **测试覆盖**：  
  - 在开启 `--enable-log-outputs` + `--exclude-log-deltas` 的组合下，验证日志仅包含最终完整输出且不包含增量。  
  - 同时执行不带 `--exclude-log-deltas` 的对照测试，确保原始日志行为未被意外改变。  

- **文档同步**：  
  - 更新官方文档、帮助信息以及示例脚本，明确 `--exclude-log-deltas` 的使用前提和效果。  

- **统一实现**：  
  - 如项目中还有其他流式生成器（如 `completion`），考虑同步加入相同的 `exclude_log_deltas` 检查，保持行为一致。  

- **升级提示**：  
  - 在发布说明中提醒用户：若已使用 `--enable-log-outputs` 并依赖增量日志，请确认是否需要切换到 `--exclude-log-deltas`，以及相应的下游系统兼容性。  

- **监控调整**：  
  - 若使用日志聚合/实时分析平台，请根据新参数调整过滤规则，以免误判为日志缺失。  

### 6aa5b18e1dfd9c678fcdd12ca8b954343eb373eb
https://github.com/vllm-project/vllm/commit/6aa5b18e1dfd9c678fcdd12ca8b954343eb373eb
[v1] Add encoder-only/cross attention support to Triton Attention backend (#31406)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
**🎯 变更类型**：功能增强、测试、后端实现扩展、平台选择调整  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 新增 **triton_prefill_attention** 实现，加入对 **滑动窗口 (sliding‑window) 与 encoder‑only / cross‑attention** 的支持，并在 Triton kernel 中实现因果及双向滑动窗口掩码。  
2. 拓展后端抽象 `AttentionType`，在 TritonAttention 实现中加入 `supports_attn_type` 与 `Encoder/Encoder‑Only` 的处理路径，允许在 encoder 阶段直接使用 QKV 而不走 KV‑Cache。  
3. 增加多组 **单元测试**：  
   - `tests/kernels/attention/test_triton_prefill_attention.py` 验证前向结果（包括滑动窗口）与 PyTorch SDPA 基准的数值误差。  
   - `tests/v1/attention/test_attention_backends.py` 添加 encoder‑only prefill 场景以及滑动窗口的后端正确性验证。  
   - 调整 Whisper 多模态测试，加入 `float` dtype 参数。  
4. 在 ROCm 平台选择逻辑中删除对 `Encoder‑Only` 强制使用 FlexAttention 的硬编码优先级，恢复统一使用 Triton（已支持此类型）。  

**🎯 影响范围**  
- `vllm.attention.ops.triton_prefill_attention`（新文件）  
- `vllm/v1/attention/backends/triton_attn.py`（后端实现）  
- `vllm/platforms/rocm.py`（平台后端选择）  
- `tests/kernels/attention/*`、`tests/v1/attention/*`、`tests/models/multimodal/generation/test_whisper.py`（测试套件）  

**🔍 技术洞察**  

- **架构影响**  
  - 引入 `AttentionType.ENCODER` 与 `ENCODER_ONLY` 的显式支持，后端实现不再假设只能是 Decoder 或 Encoder‑Decoder。  
  - `TritonAttention` 现在提供统一的 `supports_attn_type` 方法，后端注册机制可以根据模型属性自动匹配，无需在平台层硬编码。  
  - 编码器路径通过 `context_attention_fwd` 直接对 Q/K/V 完成 FlashAttention，省去 KV‑Cache 的创建与管理，降低内存占用。  

- **性能影响**  
  - **正面**：消除 encoder 时的 KV‑Cache 写入/读取，显著降低显存占用与内存带宽消耗，尤其在大批量、长序列的 encoder‑only 模型（如 embedding‑type）上。  
  - **滑动窗口**：在 kernel 中仅在必要的块上计算 `qk`，并通过 mask 直接在 Triton 中裁剪，避免额外的 mask 张量创建，理论上与原生 `F.scaled_dot_product_attention` 能保持相似的 FLOPs 与内存访问模式。  
  - **潜在负面**：新增的分支以及 `SLIDING_WINDOW_Q/K` 参数在不使用滑动窗口时仍会传递 0，导致 kernel 中的条件分支略有开销，但影响可忽略。  

- **安全考虑**  
  - 代码仅涉及数值计算与内存访问，没有引入外部依赖或系统调用，安全风险极低。  
  - `context_attention_fwd` 对输入尺寸进行边界检查（via mask），防止越界读取。  

**⚠️ 潜在风险**  

| 风险点 | 描述 | 影响程度 |
|--------|------|----------|
| **数值误差回归** | Triton 实现与 PyTorch SDPA 在极端 `dtype=bfloat16`、极大序列长度（> 2k）下可能出现略大误差。 | 中 |
| **Encoder‑Only 分支未覆盖的异常路径** | `kv_cache_dtype` 为 `fp8*` 时直接抛 `NotImplementedError`，若用户误将 encoder 模型配置为 fp8 量化会导致运行时错误。 | 低 |
| **平台选择回退** | ROCm 仍未实现 FlexAttention 的 encoder‑only 路径，若 Triton 在特定 AMD GPU 上不支持（如旧显卡），会回退至默认 Triton，实现可能不兼容。 | 低 |
| **测试依赖 CUDA** | 新增的 Triton 测试直接使用 `device="cuda"`，在 CI 中若缺少 CUDA 环境会导致测试失败。 | 中 |
| **滑动窗口参数计算** | `sliding_window_q/k` 为 `None` 时在 kernel 中被强制置 0，若用户误传负数可能导致不期望的行为。 | 低 |

**💡 关注建议**  

1. **回归测试**：在 CI 中加入 **非 CUDA**（如 ROCm）以及 **fp16/bfloat16** 双精度的完整回归，以捕获潜在数值偏差。  
2. **文档更新**：在模型配置文档中明确说明：  
   - Encoder‑only 与 Encoder‑Decoder 均可使用 Triton 后端。  
   - `sliding_window` 参数在 encoder 场景下会自动转换为 `(window‑1, window‑1)`；若不需要滑动窗口请保持 `None`。  
3. **参数校验**：在 `TritonAttention.__init__` 中加入对 `sliding_window` 为负数的断言，防止意外传入非法值。  
4. **兼容性提示**：在 `vllm/platforms/rocm.py` 中保留注释，提醒用户在不支持 Triton 的 AMD GPU 上可自行实现 FlexAttention（若将来添加）。  
5. **异常处理**：对 encoder‑only 使用 fp8 量化的情况给出更友好的错误信息，或在未来实现对应的量化路径。  
6. **性能监控**：建议在实际生产环境中监控 encoder‑only 模型的显存占用与吞吐量，以验证“省去 KV‑Cache”带来的预期收益。  

---  
**结论**：本次提交为 vLLM 引入了多模态的 **encoder‑only / cross‑attention** 支持，核心通过 Triton‑based 前向 kernel 实现了高效且内存友好的滑动窗口注意力。除少量数值误差与平台兼容性需进一步验证外，整体提升了框架的适用范围与性能，建议尽快合并并在 CI 中覆盖全部平台。  

