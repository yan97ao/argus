# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-02-05ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-02-05 20:20:25 | NicolÃ² Lucchesi | [Docs] Add bart-plugin to docs (#33905) |
| 2026-02-05 18:51:22 | wang.yuqi | [Bugfix] Fix corner case of sparse embedding  (#33886) |
| 2026-02-05 18:43:42 | Cyrus Leung | [Refactor] Clean up input preprocessing (#33687) |
| 2026-02-05 18:29:54 | Isotr0py | [Bugfix] Fix Kimi-K2.5 NVFP4 checkpoints weight loading (#33876) |
| 2026-02-05 18:23:11 | jiahanc | [perf] Integrate flashinfer concat_mla_k (#31171) |
| 2026-02-05 18:17:02 | liranschour | Enable Cross layers KV cache layout at NIXL Connector V2 (#33339) |
| 2026-02-05 18:01:23 | Andreas Karatzas | [ROCm][Bugfix][CI] Fix hybrid models and their tests (Mamba/Jamba/Bamba) (#32710) |
| 2026-02-05 17:33:11 | Cyrus Leung | [Refactor] Move `task` outside of `PoolingParams.verify` (#33796) |
| 2026-02-05 17:32:10 | Pavani Majety | [Bugfix] Kimi-K2 grouped_topk usage for Flashinfer monolithic kernels. (#33858) |
| 2026-02-05 15:57:27 | Mark McLoughlin | [KV Connector][Metrics] Do not count local prefix cache hits in connector queries (#30522) |
| 2026-02-05 15:45:29 | Chauncey | [Perf] Optimize the performance of structured output + reasoning (#33557) |
| 2026-02-05 15:07:14 | Li, Jiang | [CI/Build] Fix CPU CI test case title (#33870) |
| 2026-02-05 14:26:09 | Fadi Arafeh | [CPU][BugFix] Allow w8a8 oneDNN quantized matmul to support 3D inputs (#33727) |
| 2026-02-05 14:17:00 | Andreas Karatzas | [Bugfix] Fix ScoreMultiModalParam multi-document scoring returning single result (#33837) |
| 2026-02-05 13:53:48 | Li, Jiang | [CI/Build] Parallelize CPU CI tests (#33778) |
| 2026-02-05 13:46:15 | Andrew Xia | [2/N] move responses/serving _make_response_output_items logic to parser (#33281) |
| 2026-02-05 13:05:48 | rasmith | [CI][AMD][BugFix] Ensure VLLM_ROCM_USE_AITER is set so test_rocm_aiter_topk.py can run correctly (#33840) |
| 2026-02-05 12:50:59 | rinbaro | [docs] fix unintentional misspellings (#33863) |
| 2026-02-05 12:38:20 | Nick Hill | [Minor] Include `StreamingInput` in inputs package (#33856) |
| 2026-02-05 11:54:27 | Luka GovediÄ | Revert "[Attention][FA3] Update FA3 to include new swizzle optimization" (#33841) |
| 2026-02-05 11:14:06 | Andreas Karatzas | [CI][Bugfix]: return McpCall for built-in MCP tools in non-streaming mode (#32762) |
| 2026-02-05 10:07:35 | Kevin H. Luu | [release] Minor fixes to release annotation (#33849) |
| 2026-02-05 09:28:36 | Chauncey | [Bugfix] fix DeepSeek R1 with CUTLASS MLA Broken on B200 (#33637) |
| 2026-02-05 08:49:18 | zhanqiuhu | [Bugfix] Disable TRTLLM attention when KV transfer is enabled (#33192) |
| 2026-02-05 08:09:03 | Luka GovediÄ | [CI][torch.compile] Reduce e2e fusion test time (#33293) |
| 2026-02-05 08:05:13 | Ilya Boytsov | feat: Add ColBERT late interaction model support (#33686) |
| 2026-02-05 07:40:22 | Nick Hill | [Core] Don't schedule spec tokens with prefill chunks (#33652) |
| 2026-02-05 06:02:46 | Sage Moore | Change the type signature of MixtureOfExperts.expert_weights to MutableSequence[Sequence[Tensor]] (#33573) |
| 2026-02-05 05:59:59 | Richard Zou | Revert "[torch.compile] Significantly speed up cold start times" (#33820) |
| 2026-02-05 05:17:47 | Muhammad Hashmi | [Model] Add transcription support for Qwen3-Omni (#29828) |
| 2026-02-05 04:17:41 | Simon Danielsson | [Bugfix] Support `RotaryEmbedding` CustomOp for gpt-oss (#33800) |
| 2026-02-05 04:11:39 | Taeksang Kim | Implement zero-copy GQA for multimodal and CPU (#33732) |
| 2026-02-05 02:09:14 | kourosh hakhamaneshi | [rocm][ray] Fix: Unify Ray device visibility handling across CUDA and ROCm (#33308) |
| 2026-02-05 01:54:45 | Isotr0py | [Bugfix] Fix interns1-pro initialization and PP (#33793) |
| 2026-02-05 00:41:57 | Lucas Wilkinson | [Misc] Delay deprecation of CommonAttentionMetadata properties (#33801) |
| 2026-02-05 00:41:45 | jiangkuaixue123 | [Bugfix] Fix ubatch wrapper num_tokens calculate (#33694) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 36 ä¸ªæäº¤ | ğŸ”´é«˜ 2 | ğŸŸ¡ä¸­ 19 | ğŸŸ¢ä½ 15
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (2)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-2)
    - [[CI][torch.compile] Reduce e2e fusion test time (#33293)](#4d95135)
    - [feat: Add ColBERT late interaction model support (#33686)](#439afa4)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (19)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-19)
    - [[Refactor] Clean up input preprocessing (#33687)](#7bd42e6)
    - [[perf] Integrate flashinfer concat_mla_k (#31171)](#59a5cb3)
    - [Enable Cross layers KV cache layout at NIXL Connector V2 ...](#8322d4e)
    - [[Refactor] Move `task` outside of `PoolingParams.verify` ...](#038914b)
    - [[KV Connector][Metrics] Do not count local prefix cache h...](#2abd975)
    - [[Perf] Optimize the performance of structured output + re...](#6abb045)
    - [[Bugfix] Fix ScoreMultiModalParam multi-document scoring ...](#1f70313)
    - [[CI/Build] Parallelize CPU CI tests (#33778)](#07daee1)
    - [[2/N] move responses/serving _make_response_output_items ...](#9595afd)
    - [[Minor] Include `StreamingInput` in inputs package (#33856)](#add9f1f)
    - [Revert "[Attention][FA3] Update FA3 to include new swizzl...](#e3bf79f)
    - [[CI][Bugfix]: return McpCall for built-in MCP tools in no...](#fb1270f)
    - [[Core] Don't schedule spec tokens with prefill chunks (#3...](#fa4e0fb)
    - [Revert "[torch.compile] Significantly speed up cold start...](#9f14c92)
    - [[Model] Add transcription support for Qwen3-Omni (#29828)](#535de06)
    - [[Bugfix] Support `RotaryEmbedding` CustomOp for gpt-oss (...](#4292c90)
    - [Implement zero-copy GQA for multimodal and CPU (#33732)](#6e98f6d)
    - [[rocm][ray] Fix: Unify Ray device visibility handling acr...](#2f6d17c)
    - [[Bugfix] Fix interns1-pro initialization and PP (#33793)](#192ad46)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (15)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-15)
    - [[Docs] Add bart-plugin to docs (#33905)](#81a90e5)
    - [[Bugfix] Fix corner case of sparse embedding  (#33886)](#1c3a221)
    - [[Bugfix] Fix Kimi-K2.5 NVFP4 checkpoints weight loading (...](#a252283)
    - [[ROCm][Bugfix][CI] Fix hybrid models and their tests (Mam...](#3e472e8)
    - [[Bugfix] Kimi-K2 grouped_topk usage for Flashinfer monoli...](#d2f4a71)
    - [[CI/Build] Fix CPU CI test case title (#33870)](#db6f71d)
    - [[CPU][BugFix] Allow w8a8 oneDNN quantized matmul to suppo...](#fd03538)
    - [[CI][AMD][BugFix] Ensure VLLM_ROCM_USE_AITER is set so te...](#c1395f7)
    - [[docs] fix unintentional misspellings (#33863)](#007b183)
    - [[release] Minor fixes to release annotation (#33849)](#72bb24e)
    - [[Bugfix] fix DeepSeek R1 with CUTLASS MLA Broken on B200 ...](#a7be77b)
    - [[Bugfix] Disable TRTLLM attention when KV transfer is ena...](#bbe0574)
    - [Change the type signature of MixtureOfExperts.expert_weig...](#ce498a6)
    - [[Misc] Delay deprecation of CommonAttentionMetadata prope...](#0e92298)
    - [[Bugfix] Fix ubatch wrapper num_tokens calculate (#33694)](#87d9a26)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (2)

### [CI][torch.compile] Reduce e2e fusion test time (#33293)
**SHA**: `4d95135` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4d9513537d00a9b6678a2b1ed3c3566a81f7dd77)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„ / CI/æµ‹è¯•æ”¹è¿›  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´ é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- å°†åŸæœ‰çš„ **`tests/compile/distributed/test_fusions_e2e.py`**ï¼ˆ321 è¡Œï¼‰ä»¥åŠå…¶å…¬å…±å·¥å…· **`tests/compile/fusion_test_utils.py** å®Œå…¨åˆ é™¤ã€‚  
- æ–°å¢ **`tests/compile/fusions_e2e/`** æµ‹è¯•å­ç›®å½•ï¼Œæä¾› **TPâ€‘1ï¼ˆå•æœºå•å¡ï¼‰** ä¸ **TPâ€‘2ï¼ˆå¤šå¡ï¼‰** åœºæ™¯ä¸‹çš„ **é‡åŒ–ã€RMSâ€‘Normã€åºåˆ—å¹¶è¡Œã€Asyncâ€‘TPã€ARï¼‹RMS** ç­‰èåˆï¼ˆfusionï¼‰æ£€æŸ¥ã€‚  
- CI é…ç½®ï¼ˆ`.buildkite/*.yaml`ï¼‰åŒæ­¥æ›´æ–°ï¼š  
  - åˆ é™¤æ—§çš„ â€œFusion E2Eâ€ æ­¥éª¤ï¼Œæ”¹ä¸º **â€œFusion E2E Quick / Config Sweepâ€**ï¼ˆH100/B200ï¼‰ä»¥åŠ **â€œTPâ€‘2 Quick / Asyncâ€‘TPâ€** æ­¥éª¤ã€‚  
  - å¤§å¹…å‰Šå‡è¿è¡Œæ—¶é—´ï¼ˆåŸ 30â€‘40â€¯min â†’ 10â€‘20â€¯minï¼‰ï¼Œå¹¶ç›¸åº”è°ƒæ•´è¶…æ—¶é˜ˆå€¼ã€‚  
- `vllm/config/vllm.py` ä¸­ **åºåˆ—å¹¶è¡Œï¼ˆSPï¼‰** çš„é»˜è®¤å¼€å¯é€»è¾‘åŠ å…¥äº†å®‰å…¨æ£€æŸ¥ï¼šåœ¨ **tensorâ€‘parallelâ€¯=â€¯1** æ—¶è‡ªåŠ¨å…³é—­ SP å¹¶å…³é—­ `fuse_gemm_comms`ï¼Œå¹¶åœ¨ `-rms_norm` è¢«ç¦ç”¨æ—¶ç»™å‡ºè­¦å‘Šã€‚  
- `tests/test_config.py` å°å¹…æ”¹åŠ¨ï¼Œä½¿ç”¨ **`enable_qk_norm_rope_fusion`** æ›¿ä»£åŸæ¥å¯¹ `fuse_gemm_comms` çš„éªŒè¯ï¼Œä¿æŒ API å¯¹é½ã€‚

---

## ğŸ” æŠ€æœ¯æ´å¯Ÿ  

| ç»´åº¦ | å½±å“æè¿° |
|------|----------|
| **æ¶æ„å½±å“** | - ä»…æ¶‰åŠ **æµ‹è¯•å±‚** ä¸ **CI æµæ°´çº¿**ï¼Œä¸å½±å“è¿è¡Œæ—¶åº“çš„æ ¸å¿ƒæ¶æ„ã€‚<br>- æ–°å¢çš„ `tests/compile/fusions_e2e/` é€šè¿‡ç»Ÿä¸€çš„ **`common.py`ã€`conftest.py`ã€`models.py`** æŠ½è±¡å‡ºæ¨¡å‹ã€åç«¯ã€åŒ¹é…è§„åˆ™ï¼Œç»“æ„æ›´æ¸…æ™°ã€æ˜“äºæ‰©å±•ã€‚ |
| **æ€§èƒ½å½±å“** | - åˆ é™¤äº† 321 è¡Œæ—§ E2E èåˆæµ‹è¯•ï¼ˆå¤šæ•°åœ¨ B200/H100 ä¸Šè·‘ **å®Œæ•´æ¨¡å‹**ï¼‰ï¼Œæ˜¾è‘—é™ä½ CI æ€»æ—¶é•¿ï¼ˆä» ~2â€¯h é™è‡³ <â€¯1â€¯hï¼‰ï¼Œæå‡ PR åé¦ˆé€Ÿåº¦ã€‚<br>- æ–°çš„ â€œQuickâ€ ä¸ â€œConfig Sweepâ€ æµ‹è¯•åªè·‘ **å°æ¨¡å‹ (dummy format)**ã€åªéªŒè¯ **æ—¥å¿—åŒ¹é…**ï¼Œå¯¹å®é™…æ¨ç†æ€§èƒ½æ— å½±å“ã€‚ |
| **å®‰å…¨è€ƒè™‘** | - ä»£ç å˜æ›´å‡åœ¨ **æµ‹è¯•è·¯å¾„**ï¼Œä¸å½±å“ç”Ÿäº§äºŒè¿›åˆ¶ã€‚<br>- CI ä¸­å¯¹ **GPU èƒ½åŠ›**ï¼ˆSM90/SM100ï¼‰å’Œ **torch ç‰ˆæœ¬**ï¼ˆâ‰¥â€¯2.9ï¼‰åšäº†æ˜¾å¼ `skipif`ï¼Œé˜²æ­¢åœ¨ä¸æ”¯æŒçš„ç¡¬ä»¶/ç¯å¢ƒä¸‹æ‰§è¡Œå¯¼è‡´é”™è¯¯æ³„éœ²ä¿¡æ¯ã€‚<br>- `vllm/config/vllm.py` ä¸­æ–°å¢çš„ **æ—¥å¿—è­¦å‘Š**ï¼ˆSeqâ€‘Parallel éœ€è¦ TP>1ï¼‰é˜²æ­¢ç”¨æˆ·è¯¯é…å¯¼è‡´å¼‚å¸¸å´©æºƒã€‚ |
| **å¯ç»´æŠ¤æ€§** | - æ—§çš„ `test_fusions_e2e.py` ä¸é…å¥— utilities è¢«ç»Ÿä¸€è¿ç§»è‡³ `fusions_e2e/`ï¼Œæ–‡ä»¶ç»“æ„æ›´åŠ æ¨¡å—åŒ–ã€‚<br>- ä½¿ç”¨ **parameterized fixtures** (`run_e2e_fusion_test`) æŠŠæ—¥å¿—æ•è·ã€åŒ¹é…ã€æ–­è¨€å°è£…ï¼Œåç»­æ–°å¢æ¨¡å‹æˆ–åç«¯ä»…éœ€åœ¨ `models.py` æ·»åŠ ä¸€è¡Œé…ç½®ã€‚ |
| **å…¼å®¹æ€§** | - ä»ç„¶ä¿ç•™ **åŸæœ‰çš„å• GPUã€å…¨å›¾ï¼ˆfullgraphï¼‰** æµ‹è¯•ï¼Œåªæ˜¯ç¼©çŸ­è¶…æ—¶ã€‚<br>- å¯¹å¤– API æœªå˜æ›´ï¼Œå”¯ä¸€å¯è§çš„è¡Œä¸ºå˜æ›´æ˜¯ **`VLLM_DISABLE_COMPILE_CACHE`**ã€**`VLLM_WORKER_MULTIPROC_METHOD`** è‡ªåŠ¨åœ¨ CI ä¸­è®¾å®šã€‚ |
| **ä¾èµ–å˜åŒ–** | - æ–°å¢å¯¹ `regex`ï¼ˆå·²åœ¨é¡¹ç›®ä¸­ï¼‰ä¸ `torch._inductor.utils.CUDAGraphWrapperMetadata` çš„ `skipif` æ£€æŸ¥ã€‚<br>- éœ€è¦ **torch â‰¥â€¯2.9** æ‰èƒ½è·‘ `inductor_graph_partition=True` çš„åˆ†æ”¯ï¼›åœ¨ CI ä¸­å·²é€šè¿‡ `has_cuda_graph_wrapper_metadata` åˆ¤æ–­ã€‚ |

---

## âš ï¸ æ½œåœ¨é£é™©  

| é£é™©ç‚¹ | è¯´æ˜ | ä¸¥é‡åº¦ | ç¼“è§£æªæ–½ |
|--------|------|--------|----------|
| **æµ‹è¯•è¦†ç›–å›é€€** | åˆ é™¤çš„æ—§ E2E èåˆæµ‹è¯•è¦†ç›–äº†æ›´å¤§æ¨¡å‹ä¸å®Œæ•´æµæ°´çº¿ï¼Œè‹¥æ–° â€œQuickâ€ æµ‹è¯•æœªè¦†ç›–åˆ°æŸäº›è¾¹ç¼˜è·¯å¾„ï¼ˆä¾‹å¦‚æç«¯è‡ªå®šä¹‰ op ç»„åˆï¼‰ï¼Œå¯èƒ½å¯¼è‡´å›å½’æœªè¢«æ•æ‰ã€‚ | ä¸­ | - åœ¨æœ¬åœ°æ‰‹åŠ¨è¿è¡Œå®Œæ•´çš„æ—§ `test_fusions_e2e.py`ï¼ˆä»…ä¸€æ¬¡æ€§ï¼‰ï¼Œç¡®è®¤æ–°æµ‹è¯•çš„ç­‰ä»·æ€§ã€‚<br>- CI ä¸­ä¿ç•™ **æ ‡è®°ä¸º `optional: true`** çš„æ—§æ­¥éª¤ï¼ˆå·²è¢«æ³¨é‡Šï¼‰ä»¥é˜²ç´§æ€¥å›æ»šã€‚ |
| **ç¡¬ä»¶/é©±åŠ¨ä¾èµ–** | éƒ¨åˆ†æµ‹è¯•ä»…åœ¨ **Blackwell (SM100)** æˆ– **H100 (SM90)** ä¸Šå¯è¿è¡Œï¼ŒCI æœºå™¨è‹¥æ›´æ¢ä¸ºä½é˜¶ GPU å°†å¯¼è‡´å¤§é‡ `skip`ï¼Œå®é™…èåˆåŠŸèƒ½åœ¨ç”¨æˆ·ä½ç«¯ GPU ä¸Šä»å¯èƒ½å¤±æ•ˆã€‚ | ä½ | - CI é…ç½®å·²ä½¿ç”¨ `skipif` æ˜ç¡®æŠ¥é”™åŸå› ï¼Œæ–‡æ¡£ä¸­æ³¨æ˜ â€œFusion E2E ä»…åœ¨ Blackwell/H100 æœºå™¨ä¸ŠéªŒè¯â€ã€‚ |
| **Torch ç‰ˆæœ¬ä¸å…¼å®¹** | `inductor_graph_partition` éœ€è¦ `torch >= 2.9`; è‹¥ CI é•œåƒå‡çº§åˆ° 2.8 æˆ–æ›´ä½ï¼Œç›¸å…³æµ‹è¯•ä¼šå…¨éƒ¨ `skip`ï¼Œå¯¼è‡´è¯¯åˆ¤ã€‚ | ä¸­ | - CI è„šæœ¬ä¸­å·²æ£€æŸ¥ `has_cuda_graph_wrapper_metadata`ï¼Œè‹¥ç¼ºå¤±ä¼šè‡ªåŠ¨ skipã€‚å»ºè®®åœ¨ CI é•œåƒå‡çº§å‰æ›´æ–°æµ‹è¯•çš„ `skipif` æ¡ä»¶æˆ–é”å®š torch ç‰ˆæœ¬ã€‚ |
| **é…ç½®è‡ªåŠ¨å…³é—­ SP** | æ–°å¢çš„ `if self.parallel_config.tensor_parallel_size == 1: disable SP` é€»è¾‘ä¼šåœ¨ç”¨æˆ·æ˜¾å¼æ‰“å¼€ `enable_sp`ï¼ˆæ¯”å¦‚è°ƒè¯•ï¼‰æ—¶ silentlyæŠŠå®ƒå…³æ‰ï¼Œä»…åœ¨æ—¥å¿—é‡Œç»™å‡º warningï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·è¯¯è®¤ä¸ºé…ç½®ç”Ÿæ•ˆã€‚ | ä½ | - åœ¨æ–‡æ¡£ä¸­æ˜ç¡®è¯´æ˜ â€œTP=1 æ—¶ SP è‡ªåŠ¨ç¦ç”¨â€ã€‚<br>- è‹¥ä¸šåŠ¡åœºæ™¯éœ€è¦åœ¨å•å¡ä¸‹å¼ºåˆ¶å¼€å¯ SPï¼Œæä¾›æ˜¾å¼ç¯å¢ƒå˜é‡æˆ– API è¦†ç›–ï¼ˆä¾‹å¦‚ `VLLM_FORCE_ENABLE_SP=1`ï¼‰ã€‚ |
| **ç¯å¢ƒå˜é‡å†²çª** | CI ä¸­å¼ºåˆ¶è®¾ `VLLM_WORKER_MULTIPROC_METHOD=spawn`ï¼Œå¦‚æœç”¨æˆ·åœ¨æœ¬åœ°æˆ–è‡ªå®šä¹‰ CI ä¸­ä¾èµ– `fork` å¯èƒ½å¯¼è‡´è¡Œä¸ºå·®å¼‚ã€‚ | ä½ | - åœ¨ README / CI æ–‡æ¡£ä¸­è¯´æ˜è¯¥ env var ä¸º **æµ‹è¯•ä¸“ç”¨**ï¼Œç”Ÿäº§ä¸­ä»ä¿æŒé»˜è®¤ã€‚ |

---

## ğŸ’¡ å…³æ³¨å»ºè®®  

1. **éªŒè¯ç­‰ä»·æ€§**  
   - åœ¨æœ¬åœ°æˆ–å†…éƒ¨ CIï¼ˆä½¿ç”¨å®Œæ•´çš„ `tests/compile/distributed/test_fusions_e2e.py`ï¼‰è·‘ä¸€æ¬¡ï¼Œç¡®ä¿æ–° `fusions_e2e` æµ‹è¯•æ•è·åˆ°ç›¸åŒçš„æ—¥å¿—æ¨¡å¼ä¸è®¡æ•°ã€‚  
   - è‹¥å‘ç°é—æ¼ï¼Œè¡¥å……å¯¹åº”çš„ `custom_ops_combos` æˆ– `matches_check` é¡¹ã€‚

2. **CI é•œåƒç®¡ç†**  
   - é”å®š `torch` ç‰ˆæœ¬åˆ° `>=2.9`ï¼ˆæˆ–åœ¨ `Dockerfile` ä¸­åŠ å…¥ `pip install "torch>=2.9"`ï¼‰ä»¥é¿å… `inductor_graph_partition` è¢«æ„å¤– skipã€‚  
   - åŒæ­¥æ›´æ–° **Buildkite** é•œåƒçš„ CUDA é©±åŠ¨ä¸ `flashinfer`

---

### feat: Add ColBERT late interaction model support (#33686)
**SHA**: `439afa4` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/439afa4eea14db2be232a9ce78eacc2c7bbfac77)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º vLLM æ·»åŠ  ColBERTâ€¯Lateâ€‘Interactionï¼ˆMaxSimï¼‰æ¨¡å‹çš„å®Œæ•´æ”¯æŒï¼‰  

**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´â€¯é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨æ–‡æ¡£ã€ç¤ºä¾‹ã€æµ‹è¯•ä¸­åŠ å…¥ ColBERTï¼ˆtokenâ€‘levelï¼‰æ¨¡å‹çš„ä½¿ç”¨è¯´æ˜ä¸åœ¨çº¿è°ƒç”¨ç¤ºä¾‹ã€‚  
- åœ¨æ¨¡å‹æ³¨å†Œè¡¨ä¸­åŠ å…¥ `HF_ColBERT`ï¼Œå¹¶é€šè¿‡æ–°çš„ `supports_late_interaction` æ ‡è®°è®©æ¡†æ¶è®¤çŸ¥è¯¥æ¨¡å‹ç±»å‹ã€‚  
- å®ç° `ColBERTModel`ï¼ˆç»§æ‰¿è‡ª `BertEmbeddingModel`ï¼‰ï¼Œåœ¨ forward æœŸé—´åŠ å…¥ ColBERTâ€‘ä¸“ç”¨çº¿æ€§æŠ•å½±å±‚ï¼Œæ”¯æŒ `token_embed` ä»»åŠ¡ã€‚  
- æ–°å¢ `compute_maxsim_score` ä¸ä¸€ç³»åˆ— lateâ€‘interaction scoring ä»£ç è·¯å¾„ï¼ˆ`_late_interaction_score`ã€`ServingScores._late_interaction_score`ï¼‰ï¼Œä½¿ API èƒ½ç›´æ¥è¿”å› MaxSim åˆ†æ•°ã€rerank ç»“æœä»¥åŠåŸå§‹ token embeddingsã€‚  
- æ‰©å±• `model_config`ï¼ˆ`is_late_interaction`ï¼‰å’Œ `supports_late_interaction` åè®®ï¼Œä½¿å…¶å®ƒç»„ä»¶èƒ½å¤Ÿç»Ÿä¸€åˆ¤æ–­ã€‚  
- æ–°å¢å•å…ƒ/é›†æˆæµ‹è¯•è¦†ç›–æ¨¡å‹åŠ è½½ã€tokenâ€‘embedã€MaxSim è®¡ç®—ã€ä¸ HuggingFace åŸºå‡†å¯¹é½ã€é”™è¯¯è·¯å¾„ç­‰ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
| æ¨¡å—/ç»„ä»¶ | å—å½±å“ç¨‹åº¦ |
|----------|------------|
| `vllm/model_executor/models/registry.py`ã€`interfaces.py` | æ–°å¢æ¨¡å‹æ³¨å†Œã€ç‰¹æ€§æŸ¥è¯¢ï¼ˆ`supports_late_interaction`ï¼‰ |
| `vllm/model_executor/models/colbert.py` | æ–°å¢æ¨¡å‹å®ç°ã€æƒé‡åŠ è½½ã€æŠ•å½±å±‚åˆå§‹åŒ– |
| `vllm/config/model.py` | æ–°å¢ `is_late_interaction` å±æ€§ |
| `vllm/entrypoints/llm.py`ã€`pooling/score/serving.py`ã€`pooling/score/utils.py` | æ–°å¢ Lateâ€‘Interaction scoring æµç¨‹ã€`compute_maxsim_score` |
| `vllm/entrypoints/pooling/__init__.py`ã€`pooling/score/api_router.py` | API è·¯ç”±æ¡ä»¶æ‰©å±•ï¼ˆæ”¯æŒ `token_embed`ï¼‰ |
| æ–‡æ¡£ã€ç¤ºä¾‹ã€æµ‹è¯• | æ–°å¢ ColBERT ä½¿ç”¨ç¤ºä¾‹ã€å®Œæ•´è‡ªåŠ¨åŒ–æµ‹è¯• |
| `tests/models/registry.py` | æ³¨å†Œè¡¨éªŒè¯ï¼ˆåŠ å…¥ `HF_ColBERT`ï¼‰ |

---

### ğŸ” æŠ€æœ¯æ´å¯Ÿ  

#### æ¶æ„å½±å“  
1. **æ¨¡å‹å±‚æ¬¡** â€“ å¼•å…¥äº†ä¸“é—¨çš„ `ColBERTModel`ï¼Œåœ¨ `BertEmbeddingModel` åŸºç¡€ä¸Šæ·»åŠ äº†çº¿æ€§æŠ•å½± (`colbert_linear`) å¹¶å¼ºåˆ¶ `token_embed` ä¸ºå”¯ä¸€ poolerã€‚  
2. **ç‰¹æ€§æ ‡è®°ä½“ç³»** â€“ æ–°å¢ `supports_late_interaction` åè®®åŠå¯¹åº” `is_late_interaction` å±æ€§ï¼Œä½¿ç‰¹æ€§åˆ¤æ–­ç»Ÿä¸€åŒ–ï¼Œé¿å…æ•£ä¹±çš„ç¡¬ç¼–ç æ£€æŸ¥ã€‚  
3. **è¯„åˆ†è·¯å¾„åˆ†æ”¯** â€“ `LLM.score` ä¸ `ServingScores` ç°åœ¨æ ¹æ® `model_config.is_late_interaction` é€‰æ‹© `_late_interaction_score`ï¼Œä¸åŸæœ‰çš„ crossâ€‘encoderã€embedding è·¯å¾„å¹¶è¡Œã€‚  
4. **è¯·æ±‚è°ƒåº¦** â€“ Lateâ€‘Interaction é€šè¿‡ä¸¤æ¬¡ `encode`ï¼ˆä¸€æ¬¡ queryã€ä¸€æ¬¡ docsï¼‰å¹¶åœ¨æœåŠ¡å™¨ä¾§å®Œæˆ MaxSim è®¡ç®—ï¼Œä¿æŒä¸ç°æœ‰ `engine.encode` æ¥å£ä¸€è‡´ï¼Œæ‰©å±•å¹…åº¦æœ€å°ã€‚  
5. **API å¯è§æ€§** â€“ åœ¨ `register_pooling_api_routers` ä¸­åŠ å…¥ `token_embed` åˆ¤æ–­ï¼Œä½¿ `POST /rerank`ã€`/score`ã€`/pooling` åœ¨ ColBERT ä¸Šå¯ç”¨ï¼Œä¸”å¯¹é ColBERT æ¨¡å‹ä»ä¿æŒåŸæœ‰è¡Œä¸ºã€‚  

#### æ€§èƒ½å½±å“  
| ç»´åº¦ | æ­£é¢å½±å“ | è´Ÿé¢/æ³¨æ„ç‚¹ |
|------|----------|-------------|
| **åå** | å¯¹äºå•æ¡ queryâ€‘doc å¯¹æ¯”ï¼Œå»¶è¿Ÿä»…ç›¸å½“äºä¸€æ¬¡ `token_embed` æ¨ç† + ä¸€æ¬¡çŸ©é˜µä¹˜ (`qÂ·dáµ€`) â†’ è®¡ç®—é‡ `O(L_qÂ·L_dÂ·d)`ï¼Œåœ¨çŸ­å¥ï¼ˆ<â€¯128 tokenï¼‰å’Œ 96â€‘dim å…¸å‹é…ç½®ä¸‹æå°ã€‚ | å½“ **Nâ€¯â‰«â€¯1**ï¼ˆä¾‹å¦‚ 1â€¯:â€¯1000ï¼‰ä¼šäº§ç”Ÿ 1000 æ¬¡ tokenâ€‘level forward ä¸ O(NÂ·L_qÂ·L_d) çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¯èƒ½æˆä¸ºç“¶é¢ˆã€‚å»ºè®®åœ¨ä¸šåŠ¡å±‚åš topâ€‘k é¢„è¿‡æ»¤æˆ–æ‰¹é‡ encodeã€‚ |
| **æ˜¾å­˜** | `token_embed` åªä¿ç•™ perâ€‘token 96â€‘dim å‘é‡ï¼Œæ˜¾å­˜å ç”¨çº¦ `LÂ·96Â·4B`ï¼ˆfp32ï¼‰ï¼Œæ¯”è·¨ç¼–ç ï¼ˆå…¨åºåˆ—æ‹¼æ¥ï¼‰æ˜¾è‘—é™ä½ã€‚ | å¯¹é•¿æ–‡æ¡£ï¼ˆ>â€¯512 tokenï¼‰æ˜¾å­˜å³°å€¼ä¼šéš token é•¿åº¦çº¿æ€§å¢é•¿ï¼Œéœ€è¦åœ¨é…ç½®ä¸­é™åˆ¶ `max_model_len`ï¼ˆå·²é€šè¿‡å‚æ•°ä¼ é€’ï¼‰ã€‚ |
| **CPU/IO** | `compute_maxsim_score` é‡‡ç”¨çº¯ PyTorch çŸ©é˜µä¹˜ï¼Œé¿å… Python å¾ªç¯ï¼Œæ€§èƒ½æ¥è¿‘ BLASã€‚ | è‹¥éƒ¨ç½²åœ¨æ²¡æœ‰ GPUâ€‘accelerated BLAS çš„ CPU ç¯å¢ƒï¼ŒçŸ©é˜µä¹˜ä»£ä»·ä¼šæå‡ï¼Œéœ€è¯„ä¼°æœåŠ¡èŠ‚ç‚¹çš„ç¡¬ä»¶åŒ¹é…ã€‚ |
| **å¹¶å‘** | é€šè¿‡ `asyncio`+`merge_async_iterators` å®ç°æ‰¹é‡å¼‚æ­¥è¯·æ±‚ï¼Œä¿æŒä¸å…¶ä»–æ¨¡å‹ç›¸åŒçš„å¹¶å‘æ¨¡å‹ã€‚ | ä»ç„¶æ˜¯ **é¡ºåº** è°ƒç”¨ä¸¤æ¬¡ `engine.encode`ï¼Œåœ¨é«˜å¹¶å‘æƒ…å†µä¸‹æ¯æ¡è¯·æ±‚ä¼šå ç”¨ä¸¤æ¬¡è°ƒåº¦ slotï¼Œæ½œåœ¨è°ƒåº¦ç«äº‰ã€‚ |

#### å®‰å…¨è€ƒè™‘  
- **è¾“å…¥éªŒè¯**ï¼šåœ¨ `_late_interaction_score` ä¸­å¯¹ `ScoreData` å¼ºåˆ¶è¦æ±‚æ˜¯ `str`ï¼Œå¯¹å¤šæ¨¡æ€è¾“å…¥æŠ›å‡º `NotImplementedError`ï¼Œé¿å…æ„å¤–è§¦å‘æœªå®ç°çš„è·¯å¾„ã€‚  
- **DoS é˜²æŠ¤**ï¼šæœªå¯¹ `query_len`ã€`doc_len` è¿›è¡Œä¸Šé™æ£€æŸ¥ï¼›è‹¥æ”»å‡»è€…å‘é€æé•¿æ–‡æœ¬ï¼ˆè¶…è¿‡ `max_model_len`ï¼‰ï¼Œä¼šåœ¨ tokenization é˜¶æ®µè¢«æˆªæ–­æˆ–è§¦å‘ `RuntimeError`ã€‚å»ºè®®åœ¨ API å±‚åŠ ä¸Šæ˜¾å¼é•¿åº¦é˜ˆå€¼ï¼ˆå·²æœ‰ `max_model_len` å‚æ•°ï¼‰ã€‚  
- **æ•°å€¼å®‰å…¨**ï¼š`compute_maxsim_score` é‡‡ç”¨ `torch.matmul` + `amax` + `sum`ï¼Œåœ¨ fp16/bf16 åœºæ™¯ä¸‹å¯èƒ½å‡ºç°è½»å¾®æ•°å€¼æ¼‚ç§»ï¼Œå·²åœ¨æµ‹è¯•ä¸­ä½¿ç”¨ `rtol=1e-2` æ”¾å®½å®¹å·®ã€‚  
- **éšç§**ï¼šæ–°æ¥å£ä¼šè¿”å› **åŸå§‹ token embeddings**ï¼ˆ`token_embed`ï¼‰ï¼Œå¦‚æœä¸šåŠ¡ä¸å¸Œæœ›æ³„éœ²ç»†ç²’åº¦å‘é‡ï¼Œéœ€è¦åœ¨æ–‡æ¡£å’Œæƒé™å±‚é¢è¯´æ˜ã€‚  

---

### âš ï¸ æ½œåœ¨é£é™©  

1. **å…¼å®¹æ€§å›é€€**  
   - æ–°å¢ `supports_late_interaction

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (19)

### [Refactor] Clean up input preprocessing (#33687)
**SHA**: `7bd42e6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/7bd42e609d24501f59a8b405229ed91f4ca8037c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `vllm.inputs.data` ä¸­æ–°å¢ `MultiModalEncDecInputs` ç±»å‹ï¼Œå¹¶åœ¨ `EncoderDecoderInputs` ä¸­æ”¹ä¸ºä½¿ç”¨è¯¥ç±»å‹ã€‚  
2. `vllm.inputs.parse` æ–°å¢ `split_enc_dec_prompt`ï¼Œç»Ÿä¸€æ‹†åˆ†æ˜¾å¼ encoderâ€‘decoder æç¤ºï¼Œç®€åŒ–åç»­æµç¨‹ã€‚  
3. `vllm.inputs.preprocess` å¤§å¹…ç²¾ç®€ï¼š  
   - `get_decoder_start_token_id` ç›´æ¥è¿”å› `int`ï¼Œä¸å†è¿”å› `None`ï¼Œè‹¥ç¼ºå¤±ä¼šæŠ›å¼‚å¸¸ã€‚  
   - ç§»é™¤è€çš„ â€œé»˜è®¤ decoder promptâ€ é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨ `decoder_start_token_id`ã€‚  
   - å°†åŸå…ˆçš„ `_build_enc_dec_llm_inputs`ã€`_split_enc_dec_mm_inputs` åˆå¹¶ä¸ºæ›´æ˜ç¡®çš„ `_validate_*` ä¸ `_build_enc_dec_inputs`ï¼Œå¹¶åŠ å…¥å¤šæ¨¡æ€è¾“å…¥æ ¡éªŒã€‚  
4. `vllm.multimodal.inputs` ä¸º `MultiModalEncDecInputs` æ·»åŠ è¯´æ˜ï¼Œæ ‡è®°å³ä½¿çº¯æ–‡æœ¬æ¨¡å‹ä¹Ÿä»¥å¤šæ¨¡æ€å½¢å¼å®ç°ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm.inputs.data`ã€`vllm.inputs.parse`ã€`vllm.inputs.preprocess`ï¼ˆæ ¸å¿ƒè¾“å…¥é¢„å¤„ç†è·¯å¾„ï¼‰  
- å¤šæ¨¡æ€å¤„ç†ç›¸å…³ (`vllm.multimodal.inputs`ã€`vllm.multimodal.processing`)  
- ä»»ä½•ä¾èµ– `EncoderDecoderInputs`ã€`get_decoder_start_token_id` æˆ–æ˜¾å¼ encoderâ€‘decoder æç¤ºçš„ä¸Šå±‚æ¨¡å‹/æµ‹è¯•ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤æ—§ç‰ˆè°ƒç”¨ `get_decoder_start_token_id` ä»ç„¶èƒ½æ­£å¸¸æ•è·å¼‚å¸¸ï¼Œé¿å…å› è¿”å› `None` è€Œå¯¼è‡´éšè—é”™è¯¯ã€‚  
2. **å¤šæ¨¡æ€æ³¨å†Œ**ï¼šç¡®ä¿æ‰€æœ‰è‡ªå®šä¹‰ `EncDecMultiModalProcessor` åœ¨ `MultiModalEncDecInputs` ä¸­å®ç° `encoder_prompt_token_ids`ï¼Œå¦åˆ™ä¼šè§¦å‘ RuntimeErrorã€‚  
3. **å•å…ƒæµ‹è¯•**ï¼šæ–°å¢å¯¹ `split_enc_dec_prompt`ã€çº¯æ–‡æœ¬ encoderâ€‘decoderã€ä»¥åŠå¤šæ¨¡æ€ encoderâ€‘decoder åœºæ™¯çš„è¦†ç›–ï¼›ç‰¹åˆ«éªŒè¯åœ¨ç¼ºå¤± `decoder_start_token_id` æ—¶æŠ›å‡ºçš„é”™è¯¯ä¿¡æ¯ã€‚  
4. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨ API æ–‡æ¡£å’Œä½¿ç”¨æ‰‹å†Œä¸­æ ‡æ˜ `EncoderDecoderInputs` ç°å·²ä½¿ç”¨ `MultiModalEncDecInputs`ï¼Œå¹¶è¯´æ˜é»˜è®¤ decoder prompt å·²è¢«ç§»é™¤ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡é‡æ„æå‡äº†ç±»å‹å®‰å…¨å’Œä»£ç å¯è¯»æ€§ï¼Œä½†ä¹Ÿå¼•å…¥äº†æ›´ä¸¥æ ¼çš„è¿è¡Œæ—¶æ ¡éªŒï¼Œéœ€é€šè¿‡å……åˆ†çš„å›å½’æµ‹è¯•ç¡®ä¿ä¸å½±å“ç°æœ‰ encoderâ€‘decoder ä¸å¤šæ¨¡æ€æ¨¡å‹çš„ä½¿ç”¨ã€‚

---

### [perf] Integrate flashinfer concat_mla_k (#31171)
**SHA**: `59a5cb3` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/59a5cb387ae4c11c73855d505adb0b2c7cd3861d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆæ–°å¢ flashinferâ€¯`concat_mla_k`â€¯è‡ªå®šä¹‰ç®—å­ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨ `mla_attention.py` ä¸­åŠ å…¥ `self._use_flashinfer_concat_mla_k` æ ‡å¿—ï¼Œä»…åœ¨æ¨¡å‹ç»´åº¦åŒ¹é… DeepSeekâ€‘V3ï¼ˆnum_headsâ€¯=â€¯128ã€nope_dimâ€¯=â€¯128ã€rope_dimâ€¯=â€¯64ï¼‰ä¸”æ£€æµ‹åˆ° flashinfer åº“æ—¶å¯ç”¨ä¸“ç”¨æ‹¼æ¥ kernelã€‚  
- å®ç°äº†åŸºäº `torch.ops.vllm.flashinfer_concat_mla_k` çš„é«˜æ•ˆå†…å­˜æ‹·è´è·¯å¾„ï¼Œæœªæ»¡è¶³æ¡ä»¶æ—¶ä¿æŒåŸæœ‰é€å…ƒç´ å¤åˆ¶é€»è¾‘ã€‚  
- åœ¨ `utils/flashinfer.py` ä¸­é€šè¿‡ `direct_register_custom_op` æ³¨å†Œè¯¥è‡ªå®šä¹‰ç®—å­åŠå…¶â€œç©ºå®ç°â€ï¼Œå¹¶å¯¹å¤–æä¾› `torch.library.custom_op` æœºåˆ¶çš„å…¼å®¹å…¥å£ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **æ ¸å¿ƒæ¨¡å—**ï¼š`vllm/model_executor/layers/attention/mla_attention.py`ï¼ˆprefill é˜¶æ®µçš„ Kâ€‘tensor æ‹¼æ¥ï¼‰ã€‚  
- **å·¥å…·æ¨¡å—**ï¼š`vllm/utils/flashinfer.py`ï¼ˆè‡ªå®šä¹‰ op æ³¨å†Œã€flashinfer æ£€æµ‹ï¼‰ã€‚  
- ä¾èµ– **flashinfer** çš„ç¯å¢ƒä¼šè§¦å‘æ–°è·¯å¾„ï¼›å…¶ä»–ç¯å¢ƒä»èµ°åŸä»£ç ï¼Œä¸ä¼šäº§ç”Ÿå›å½’ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **éªŒè¯ç»´åº¦é™åˆ¶**ï¼šå¦‚æœæœªæ¥æ¨¡å‹æ”¹ä¸ºä¸åŒ head/ç»´åº¦ï¼Œéœ€è¦æ›´æ–° `_use_flashinfer_concat_mla_k` æ¡ä»¶æˆ–æä¾›é€šç”¨å®ç°ï¼Œå¦åˆ™ä»ä¼šå›é€€åˆ°æ…¢è·¯å¾„ã€‚  
2. **å…¼å®¹æ€§æµ‹è¯•**ï¼šåœ¨æœªè£… flashinfer çš„ CI ç¯å¢ƒç¡®è®¤ `has_flashinfer()` æ­£ç¡®è¿”å› `False`ï¼Œé¿å…å‡ºç° `torch.ops.vllm.flashinfer_concat_mla_k` æœªå®šä¹‰çš„å¼‚å¸¸ã€‚  
3. **æ€§èƒ½åŸºå‡†**ï¼šå¯¹æ¯”å¼€å¯/å…³é—­ flashinfer æ—¶çš„é¢„å¡«å……ååç‡ï¼Œç¡®ä¿è‡ªå®šä¹‰ op çš„åŠ é€Ÿå¹…åº¦ç¬¦åˆé¢„æœŸï¼ˆå°¤å…¶åœ¨å¤§æ‰¹é‡ DeepSeekâ€‘V3 åœºæ™¯ï¼‰ã€‚  
4. **é”™è¯¯å®¹é”™**ï¼šè‹¥ flashinfer è¿è¡Œæ—¶æŠ›å¼‚å¸¸ï¼Œè€ƒè™‘æ•è·å¹¶å›é€€åˆ°åŸå®ç°ï¼Œä»¥é˜²å•æœºå´©æºƒã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨åœ¨ç‰¹å®šæ¨¡å‹/ç¡¬ä»¶ä¸‹å¯æ˜¾è‘—æå‡é¢„å¡«å……é˜¶æ®µçš„å†…å­˜æ‹·è´æ•ˆç‡ï¼Œé£é™©ä¸»è¦æ¥è‡ªç»´åº¦ç¡¬ç¼–ç å’Œå¯¹å¤–éƒ¨åº“çš„ä¾èµ–ï¼Œå»ºè®®åœ¨å¤šæ¨¡å‹ã€å¤šç¯å¢ƒä¸‹åšå¥½å›é€€éªŒè¯ã€‚

---

### Enable Cross layers KV cache layout at NIXL Connector V2 (#33339)
**SHA**: `8322d4e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8322d4e47f89f7985b9b3b808fc4ba8549d6afcd)

**å˜æ›´æ¦‚è§ˆ**  
æœ¬æ¬¡æäº¤åœ¨ NIXL KVâ€‘connector V2 ä¸­å®ç°äº† *Crossâ€‘layers KVâ€‘cache layout*ã€‚æ ¸å¿ƒå®ç°åŒ…æ‹¬ï¼š

1. **é…ç½®å…¥å£** â€“ `kv_connector_extra_config.enable_cross_layers_blocks`ï¼ˆæ–‡æ¡£å·²è¡¥é½ï¼‰ã€‚  
2. **å…¼å®¹æ€§å“ˆå¸Œ** â€“ `compute_nixl_compatibility_hash` æ–°å¢ `cross_layers_blocks` å‚æ•°ï¼Œä½¿ä¸åŒå¸ƒå±€çš„è¿›ç¨‹åœ¨æ¡æ‰‹é˜¶æ®µèƒ½å¤Ÿè¢«åŒºåˆ†ã€‚  
3. **TpKVTopology** â€“ æ–°å¢ `tensor_shape`ã€`_cross_layers_blocks`ã€`block_size_position`ã€`cross_layers_blocks` ç­‰å±æ€§ï¼Œå®ç°è·¨å±‚ç»´åº¦çš„è‡ªåŠ¨æ£€æµ‹ä¸å—å¤§å°å®šä½ã€‚  
4. **NixlConnector** â€“  
   - `prefer_cross_layer_blocks` åˆ¤æ–­åç«¯æ˜¯å¦æ”¯æŒä»¥åŠ KVâ€‘layout æ˜¯å¦ä¸º HNDã€‚  
   - `register_cross_layers_kv_cache` è®©å¤–éƒ¨å¯ä¸€æ¬¡æ€§æ³¨å†Œè·¨å±‚ç»Ÿä¸€ç¼“å­˜ã€‚  
   - `register_kv_caches` å»¶è¿Ÿæ„é€  `kv_topo`ï¼Œå¹¶åœ¨é¦–æ¬¡æ³¨å†Œæ—¶è®¡ç®—å…¼å®¹æ€§å“ˆå¸Œã€‚  
5. **æµ‹è¯•** â€“ å‚æ•°åŒ– `enable_cross_layers`ï¼Œè¦†ç›– FlashAttentionã€FlashInferã€TRITON_ATTN ä¸‰ä¸ªåç«¯çš„æ™®é€š/è·¨å±‚ä¸¤ç§è·¯å¾„ã€‚  
6. **è„šæœ¬/æ–‡æ¡£** â€“ å¢åŠ  `--enable-cross-layers` å¼€å…³çš„ä½¿ç”¨ç¤ºä¾‹ã€‚

**å½±å“èŒƒå›´**  
- `vllm/distributed/kv_transfer/kv_connector/*`ï¼ˆå°¤å…¶æ˜¯ `utils.py`ã€`v1/nixl_connector.py`ï¼‰  
- KVâ€‘cache ç›¸å…³é…ç½® (`KVTransferConfig`)ã€å¹³å°å±‚ (`current_platform`)  
- å•å…ƒæµ‹è¯• `test_nixl_connector.py`ã€é›†æˆè„šæœ¬ `config_sweep_accuracy_test.sh`ã€`run_accuracy_test.sh`  
- æ–‡æ¡£ `docs/features/nixl_connector_usage.md`

**æ½œåœ¨é£é™© & å»ºè®®**  

| å…³æ³¨ç‚¹ | è¯´æ˜ | å»ºè®® |
|--------|------|------|
| **åˆå§‹åŒ–é¡ºåº** | `kv_topo` ä»…åœ¨ `register_kv_caches` ä¸­åˆ›å»ºï¼Œéƒ¨åˆ†è·¯å¾„ï¼ˆå¦‚ `handshake`ã€`post_process_device_kv_on_receive`ï¼‰åœ¨æœªè°ƒç”¨ `register_kv_caches` å‰å°±ä¼šè®¿é—® `self.kv_topo`ï¼Œå¯¼è‡´ `AssertionError`ã€‚ | åœ¨ `__init__` ä¸­æå‰åˆ›å»º `TpKVTopology`ï¼ˆä½¿ç”¨å ä½ `tensor_shape=None`ï¼‰ï¼Œæˆ–åœ¨æ‰€æœ‰ä½¿ç”¨å‰ç»Ÿä¸€è°ƒç”¨ `_ensure_topology()`ã€‚ |
| **å—å¤§å°å®šä½** | `block_size_position` ä¾èµ– `tensor_shape` ä¸åç«¯è¿”å›çš„ `kv_cache_shape`ï¼Œä½†åœ¨éè·¨å±‚æ¨¡å¼ä¸‹ä»ä½¿ç”¨ `self.kv_topo.block_size_position`ï¼Œè‹¥åç«¯è¿”å›çš„ç»´åº¦é¡ºåºå˜åŒ–å¯èƒ½äº§ç”Ÿé”™ä½ã€‚ | ä¸ºéè·¨å±‚è·¯å¾„ä¿ç•™åŸæœ‰ `block_size_position` é€»è¾‘ï¼ˆ-2/-3ï¼‰ï¼Œä»…åœ¨ `cross_layers_blocks=True` æ—¶ä½¿ç”¨æ–°è®¡ç®—æ–¹å¼ï¼Œé˜²æ­¢æ„å¤–å›é€€ã€‚ |
| **åç«¯å…¼å®¹æ€§** | `prefer_cross_layer_blocks` åªåœ¨ FlashAttention/FlashInfer ä¸” `kv_cache_layout=="HND"` æ—¶è¿”å› `True`ï¼Œä½† `register_kv_caches` ä¸­æœªæ˜¾å¼æ£€æŸ¥è¯¥æ¡ä»¶ï¼Œè·¨å±‚ç¼“å­˜ä»å¯èƒ½åœ¨ä¸æ”¯æŒçš„åç«¯è¢«æ³¨å†Œã€‚ | åœ¨ `register_cross_layers_kv_cache` å‰åŠ å…¥ `if not self.prefer_cross_layer_blocks: raise RuntimeError(...)`ï¼Œå¹¶åœ¨ `register_kv_caches` ä¸­è®°å½• `self.kv_topo.cross_layers_blocks` ä¸ `self.prefer_cross_layer_blocks` æ˜¯å¦ä¸€è‡´ã€‚ |
| **å…¼å®¹æ€§å“ˆå¸Œ** | æ–°å¢ flag è¿›å“ˆå¸Œåï¼Œè€ç‰ˆæœ¬ï¼ˆæœªå¼€å¯è·¨å±‚ï¼‰ä»ä¼šç”Ÿæˆä¸åŒ hashï¼Œå¯¼è‡´æ¡æ‰‹å¤±è´¥ã€‚ | åœ¨æ—§èŠ‚ç‚¹çš„ `kv_connector_extra_config` ä¸­é»˜è®¤å¡«å…¥ `"enable_cross_layers_blocks":"False"`ï¼Œæˆ–åœ¨ `compute_nixl_compatibility_hash` ä¸­å¯¹ç¼ºå¤±é”®åšå®¹é”™ã€‚ |
| **æ€§èƒ½å½±å“** | è·¨å±‚å¸ƒå±€ä½¿æ¯å±‚å…±äº«åŒä¸€å—å†…å­˜ï¼Œç†è®ºä¸Šå¯å‡å°‘ `register_kv_caches` æ—¶çš„å—æ³¨å†Œæ•°é‡ï¼ˆä» 2/4 æ¡ä¸‹é™åˆ° 1 æ¡ï¼‰ï¼Œä½†åœ¨è¯»å†™æ—¶ä»éœ€è·¨å±‚ permuteï¼Œå¯èƒ½å¸¦æ¥é¢å¤–æ‹·è´ã€‚ | åœ¨ `post_process_device_kv_on_receive` æ³¨é‡Šä¸­æ ‡æ˜ â€œè·¨å±‚éœ€é¢å¤– permuteâ€ï¼Œå¹¶åœ¨ benchmark ä¸­åŠ å…¥å¯¹æ¯”ã€‚ |
| **ä»£ç æ•´æ´** | å¤šå¤„ `assert self.kv_topo is not None` ä¸é‡å¤çš„ `self.compat_hash` åˆ¤ç©ºï¼Œå¯å°è£…ä¸ºå±æ€§ `self._ensure_ready()`ï¼›`import` ä¸­å‡ºç°æœªä½¿ç”¨çš„ `AttentionMetadata`ã€‚ | æ¸…ç†æ— ç”¨å¯¼å…¥ï¼Œæç‚¼é‡å¤æ£€æŸ¥ä¸ºç§æœ‰æ–¹æ³•ï¼Œæé«˜å¯è¯»æ€§ã€‚ |

**æ€»ä½“ç»“è®º**  
æœ¬æ¬¡æ”¹åŠ¨ä¸º NIXL KVâ€‘connector å¼•å…¥äº†è·¨å±‚ç¼“å­˜å¸ƒå±€ï¼Œèƒ½å¤Ÿåœ¨æ”¯æŒçš„åç«¯ä¸‹æ˜¾è‘—å‡å°‘å—æ³¨å†Œæ•°é‡ï¼Œå¯¹å¤§æ¨¡å‹çš„åˆå§‹åŒ–å¼€é”€æœ‰æ­£å‘å¸®åŠ©ã€‚ä»£ç æ”¹åŠ¨è¦†ç›–é¢å¹¿ï¼Œæ¶‰åŠæ‹“æ‰‘è®¡ç®—ã€å…¼å®¹æ€§æ ¡éªŒä»¥åŠå¤šçº§æµ‹è¯•ï¼Œæ•´ä½“ç»“æ„å·²åŸºæœ¬å®Œæˆï¼Œä½†ä»éœ€æ³¨æ„åˆå§‹åŒ–é¡ºåºå’Œåç«¯å…¼å®¹æ€§æ£€æŸ¥ï¼Œä»¥é˜²åœ¨éâ€‘HNDã€éâ€‘Flash ç³»åˆ—åç«¯ä¸Šå‡ºç°éšè”½é”™è¯¯ã€‚å»ºè®®åœ¨ä¸»åˆ†æ”¯åŠ å…¥ä¸Šè¿°é˜²å¾¡æ€§æ£€æŸ¥ï¼Œå¹¶åœ¨ CI ä¸­åŠ å…¥ä¸æ”¯æŒè·¨å±‚çš„åç«¯å¿«é€Ÿå¤±è´¥ç”¨ä¾‹ï¼Œä»¥ä¿è¯å‘åå…¼å®¹ã€‚

---

### [Refactor] Move `task` outside of `PoolingParams.verify` (#33796)
**SHA**: `038914b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/038914b7c891c0b5b2853ec0574062dc3bea8073)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æ­¤æ¬¡æäº¤æŠŠâ€¯`PoolingParams.verify`â€¯ä¸­å¯¹â€¯`task`â€¯çš„å¤„ç†æŠ½ç¦»åˆ°è°ƒç”¨æ–¹ï¼Œå¹¶åœ¨å¤šä¸ªå…¥å£ï¼ˆLLMã€Scoreã€Embedã€Classifyã€Engineã€Workerï¼‰ç»Ÿä¸€ä½¿ç”¨ `PoolingParams(task=â€¦)` æˆ–åœ¨ `request.to_pooling_params()` æ—¶æ˜¾å¼è®¾å®š taskã€‚ç›¸åº”åœ°ï¼Œæµ‹è¯•ç”¨ä¾‹ä¹ŸåŒæ­¥æ›´æ–°ï¼Œé”™è¯¯ä¿¡æ¯æ”¹ä¸º `"Unsupported task: â€¦"`ï¼Œå¹¶æ–°å¢å¯¹ `POOLING_TASKS`ã€`SupportedTask` çš„ç»Ÿä¸€æ ¡éªŒã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  

| æ¨¡å— | å…³é”®æ”¹åŠ¨ |
|------|----------|
| `vllm/pooling_params.py` | `verify` å‚æ•°ç­¾åä» `(task, model_config)` â†’ `(model_config)`ï¼›å†…éƒ¨ä¸å†è‡ªè¡Œè¦†ç›– taskï¼Œåªåœ¨ `_set_default_parameters`/`_verify_valid_parameters` ä¸­æ£€æŸ¥å·²è®¾å®šçš„ taskã€‚ |
| `vllm/entrypoints/*` | æ‰€æœ‰ `to_pooling_params` å®ç°æ–°å¢ `task` å‚æ•°å¹¶ç§»é™¤æ—§çš„ `verify(task, â€¦)` è°ƒç”¨ã€‚|
| `vllm/v1/engine/*` | `add_request`ã€`process_inputs` ç­‰å‡½æ•°æ–°å¢ `supported_tasks` å‚æ•°ï¼Œç”¨äºåœ¨ `PoolingParams` æœªæ˜¾å¼æŒ‡å®š task æ—¶è‡ªåŠ¨æ¨æ–­ã€‚|
| `vllm/v1/engine/input_processor.py` | æ–°å¢å¯¹ `supported_tasks` çš„æ ¡éªŒé€»è¾‘ï¼Œç¡®ä¿ä»…åœ¨æœåŠ¡ç«¯æ”¯æŒçš„ pooling task ä¸Šåˆ›å»º `PoolingParams`ã€‚|
| æµ‹è¯•å¥—ä»¶ | æ–°å¢ `tests/test_pooling_params.py`ï¼Œåˆ é™¤æ—§çš„ `test_task`ï¼Œç»Ÿä¸€ä½¿ç”¨ `PoolingParams(task=â€¦)`ï¼›é”™è¯¯ä¿¡æ¯æ£€æŸ¥æ”¹ä¸º `Unsupported task:`ã€‚|
| CI é…ç½® | CI è„šæœ¬åŠ å…¥ `test_pooling_params.py` çš„æ‰§è¡Œã€‚|

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **å‘åå…¼å®¹**ï¼šè™½ç„¶å¤§å¤šæ•°å…¥å£å·²æ”¹ä¸ºæ˜¾å¼ä¼  taskï¼Œä½†ä»æœ‰å°‘é‡æ—§ä»£ç ï¼ˆå¦‚ç¬¬ä¸‰æ–¹æ’ä»¶æˆ–è‡ªå®šä¹‰ `PoolingParams` å®ä¾‹ï¼‰å¯èƒ½ä»è°ƒç”¨ `verify(task,â€¦)`ã€‚å»ºè®®ä¿ç•™ä¸€ä¸ªå…¼å®¹åŒ…è£…ï¼ˆå¦‚åœ¨ `verify` ä¸­æ£€æµ‹å¤šä½™å‚æ•°å¹¶å‘å‡º deprecation è­¦å‘Šï¼‰ï¼Œé˜²æ­¢æ„å¤– `TypeError`ã€‚  
2. **æ–‡æ¡£åŒæ­¥**ï¼šæ›´æ–° `vllm.pooling_params.PoolingParams` ä¸å„ API æ–‡æ¡£ï¼Œæ˜ç¡®è¯´æ˜ `task` å¿…é¡»åœ¨å®ä¾‹åŒ–æ—¶æä¾›æˆ–åœ¨ `request.to_pooling_params()` ä¸­è‡ªåŠ¨æ³¨å…¥ã€‚  
3. **é”™è¯¯ä¿¡æ¯ç»Ÿä¸€**ï¼šç›®å‰é”™è¯¯ä¿¡æ¯å·²ç»Ÿä¸€ä¸º `Unsupported task: â€¦`ï¼Œä½†éƒ¨åˆ†æ—§ä»£ç ä»ä½¿ç”¨æ—§æ–‡æœ¬ï¼ˆå¦‚ `"Task embed is not supported"`ï¼‰ã€‚ç¡®ä¿æ‰€æœ‰å¼‚å¸¸è·¯å¾„å‡è°ƒç”¨ `create_error_response`ï¼Œå¹¶åœ¨åç«¯ç»Ÿä¸€åŒ…è£…ä¸º `BadRequestError`ã€‚  
4. **æ€§èƒ½æ£€æŸ¥**ï¼šåœ¨ `Engine.add_request` ä¸­ç¼“å­˜ `supported_tasks`ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚é‡å¤å¼‚æ­¥è°ƒç”¨ã€‚å¯åœ¨ `LLMEngine` åˆå§‹åŒ–æ—¶å°±é¢„å–ä¸€æ¬¡ï¼Œä»¥è¿›ä¸€æ­¥é™ä½è°ƒç”¨å¼€é”€ã€‚  
5. **æµ‹è¯•è¦†ç›–**ï¼šæ–°å¢çš„ `test_pooling_params.py` å·²è¦†ç›–æ­£åä¾‹ï¼Œä½†å»ºè®®è¡¥å……è·¨ä»»åŠ¡ç»„åˆï¼ˆå¦‚ `token_embed` ä¸ `token_classify` åŒæ—¶å­˜åœ¨çš„åœºæ™¯ï¼‰ä»¥åŠ `plugin` ä»»åŠ¡çš„ç‰¹ä¾‹ï¼Œç¡®ä¿ `task=None` æ—¶çš„è‡ªåŠ¨æ¨æ–­é€»è¾‘å§‹ç»ˆç¬¦åˆ `supported_tasks`ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡é‡æ„è§£è€¦äº†ä»»åŠ¡ä¸å‚æ•°æ ¡éªŒï¼Œä½¿ `PoolingParams` æ›´ä¸“æ³¨äºè‡ªèº«çº¦æŸï¼Œæå‡äº†ä»£ç å¯ç»´æŠ¤æ€§å’Œé”™è¯¯ä¿¡æ¯çš„ä¸€è‡´æ€§ã€‚åªè¦æ³¨æ„å…¼å®¹æ€§ä¸æ–‡æ¡£åŒæ­¥ï¼Œé£é™©å¯æ§ã€‚

---

### [KV Connector][Metrics] Do not count local prefix cache hits in connector queries (#30522)
**SHA**: `2abd975` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/2abd97592f947c041ba70329532f0cf62dd8971f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / ä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¿®æ­£ KVâ€¯Connector å‰ç¼€ç¼“å­˜ç»Ÿè®¡ï¼šæœ¬åœ°ç¼“å­˜å‘½ä¸­çš„ token ä¸å†è®¡å…¥ `connector_prefix_cache` çš„ `queries`ã€‚  
2. å°†ç»Ÿè®¡æ”¶é›†ä»åŸæ¥çš„ `_update_connector_prefix_cache_stats` æ–¹æ³•è¿ç§»åˆ° `Scheduler.schedule()` ä¸»æµç¨‹ä¸­ï¼Œç¡®ä¿ä¸€æ¬¡è°ƒåº¦åªè®°å½•å®é™…çš„å¤–éƒ¨æŸ¥è¯¢æ•°é‡ã€‚  
3. ä¸º `make_stats()` æ·»åŠ å¯¹ `connector_prefix_cache_stats` çš„åˆ·æ–°é€»è¾‘ï¼ŒåŒæ—¶åˆ é™¤å·²åºŸå¼ƒçš„ç§æœ‰æ–¹æ³•ã€‚  
4. æ‰©å……å•å…ƒæµ‹è¯•ï¼Œè¦†ç›–æœ¬åœ°ç¼“å­˜å‘½ä¸­ã€å¼‚æ­¥/åŒæ­¥ä¸¤ç§è°ƒåº¦æ¨¡å¼ä¸‹çš„ç»Ÿè®¡è¡Œä¸ºï¼Œå¹¶åœ¨ KVâ€‘Connector å¤±æ•ˆ/å—æ’¤é”€çš„åœºæ™¯ä¸­æ ¡éªŒç»Ÿè®¡ç»“æœã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/core/sched/scheduler.py`ï¼ˆè°ƒåº¦ä¸ç»Ÿè®¡æ ¸å¿ƒï¼‰  
- KVâ€¯Connector ç›¸å…³æµ‹è¯• `tests/v1/core/test_scheduler.py`ã€`tests/v1/kv_connector/unit/test_invalid_blocks_correctness.py`  
- å¯èƒ½æ¶‰åŠåˆ°ä½¿ç”¨ `Scheduler`ï¼ˆå¦‚æœåŠ¡å™¨å…¥å£ã€benchmark è„šæœ¬ï¼‰è·å– `scheduler_stats` çš„ä¸šåŠ¡ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **ç»Ÿè®¡å‡†ç¡®æ€§**ï¼šç¡®è®¤ `self.connector_prefix_cache_stats` åœ¨ Scheduler å®ä¾‹åŒ–æ—¶å·²æ­£ç¡®åˆ›å»ºï¼Œå¦åˆ™åœ¨é¦–æ¬¡è°ƒåº¦æ—¶å¯èƒ½å‡ºç° `None` æŠ¥é”™ã€‚  
2. **å¹¶å‘å®‰å…¨**ï¼š`schedule()` ç°åœ¨ç›´æ¥ä¿®æ”¹ `connector_prefix_cache_stats`ï¼Œè‹¥å®ä¾‹åœ¨å¤šçº¿ç¨‹/å¤šè¿›ç¨‹ç¯å¢ƒä¸‹å…±äº«ï¼Œéœ€è¦ç¡®ä¿è¯¥å¯¹è±¡æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼ˆç›®å‰å¤§å¤šæ•° vLLM ä½¿ç”¨å•è¿›ç¨‹æ¨¡å‹ï¼Œä½†ä»å»ºè®®åŠ æ³¨è¯´æ˜ï¼‰ã€‚  
3. **é‡ç½®æ—¶æœº**ï¼š`make_stats()` ä¼šåœ¨è¿”å›å‰æŠŠ `connector_prefix_cache_stats` ç½®ä¸ºæ–°å®ä¾‹ï¼Œç¡®ä¿åç»­è°ƒåº¦çš„ç»Ÿè®¡ä¸ä¼šè¢«ä¸Šä¸€æ¬¡çš„ç»“æœæ±¡æŸ“ã€‚è‹¥æœ‰å¤–éƒ¨ä»£ç åœ¨ `make_stats()` ä¹‹åä»æŒæœ‰æ—§å¯¹è±¡ï¼Œè¯·æ³¨æ„æ­¤è¡Œä¸ºã€‚  
4. **æ–‡æ¡£ä¸æ—¥å¿—**ï¼šå»ºè®®åœ¨ `Scheduler.__init__` ä¸­æ·»åŠ æ³¨é‡Šï¼Œè¯´æ˜ `connector_prefix_cache_stats` è®°å½•çš„ä»…æ˜¯â€œå¤–éƒ¨ KVâ€¯Connector æŸ¥è¯¢â€ã€‚è‹¥æœ‰ç›‘æ§ä»ªè¡¨ç›˜ä¾èµ–æ—§ç»Ÿè®¡å£å¾„ï¼Œéœ€åŒæ­¥æ›´æ–°ã€‚  
5. **å›å½’æµ‹è¯•**ï¼šè¿è¡Œå®Œæ•´çš„ `pytest -m kv_connector`ï¼Œå°¤å…¶æ˜¯å¼‚æ­¥è·¯å¾„ï¼Œç¡®ä¿ `initial_ecos` ä¸åç»­ `ecos` ä»ä¿æŒä¸€è‡´çš„ç»Ÿè®¡ç»“æ„ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº†å‰ç¼€ç¼“å­˜æŒ‡æ ‡çš„çœŸå®æ€§ï¼Œä»£ç ç»“æ„æ›´ç®€æ´ï¼Œä½†éœ€å…³æ³¨å¯¹è±¡åˆå§‹åŒ–å’Œå¹¶å‘å®‰å…¨çš„ç»†èŠ‚ã€‚è‹¥æ— å…¶ä»–å‰¯ä½œç”¨ï¼Œå»ºè®®åˆå¹¶ã€‚

---

### [Perf] Optimize the performance of structured output + reasoning (#33557)
**SHA**: `6abb045` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6abb0454adb531de0b081bbf65ccf907e4bd560d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ€§èƒ½ä¼˜åŒ–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ OpenAI Chat Completion æ¥å£ä¸­å¼•å…¥â€¯`ReasoningParser`â€¯çš„å®ä¾‹åŒ–ä¸å¤ç”¨é€»è¾‘ï¼Œé¿å…åœ¨æ¯æ¬¡æµå¼ç”Ÿæˆæ—¶é‡å¤åˆ›å»ºè§£æå™¨ï¼›åœ¨ `EngineCoreRequest` ä¸ `Request` ä¸­æ–°å¢ `reasoning_ended` æ ‡è®°ï¼Œæ”¯æŒåœ¨æ¨ç†é˜¶æ®µæå‰ç»“æŸç»“æ„åŒ–è¾“å‡ºçš„ bitmask å¡«å……ï¼Œä»è€Œæå‡å¸¦æœ‰æ¨ç†/å·¥å…·è°ƒç”¨çš„è¯·æ±‚ååä¸å»¶è¿Ÿã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/entrypoints/openai/chat_completion/serving.py`ï¼ˆæµå¼ä¸éæµå¼è·¯å¾„ï¼‰  
- `vllm/v1/engine/__init__.py`ï¼ˆè¯·æ±‚ç»“æ„ä½“ï¼‰  
- `vllm/v1/request.py`ï¼ˆæ„é€ å‡½æ•°ä¸è½¬åŒ–ï¼‰  
- `vllm/v1/structured_output/__init__.py`ï¼ˆbitmask å¡«å……åˆ¤æ–­ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼š`reasoning_parser_cls` ç°åœ¨å¯èƒ½ä¸º `None`ï¼Œè¯·ç¡®è®¤æ—§ç‰ˆéƒ¨ç½²ä»èƒ½æ­£å¸¸èµ°æ— è§£æå™¨è·¯å¾„ã€‚  
2. **é”™è¯¯å¤„ç†**ï¼š`ReasoningParser` åˆ›å»ºå¤±è´¥æ—¶è¿”å›é”™è¯¯å“åº”ï¼Œå»ºè®®åœ¨ç›‘æ§ä¸­æ•è·è¯¥æ—¥å¿—ï¼Œé˜²æ­¢å› å•æœºå¼‚å¸¸å¯¼è‡´æ•´ä½“æœåŠ¡ä¸å¯ç”¨ã€‚  
3. **æµ‹è¯•è¦†ç›–**ï¼šæ–°å¢å±æ€§ `reasoning_ended` å½±å“ `StructuredOutputRequest` çš„ bitmask é€»è¾‘ï¼ŒåŠ¡å¿…è¡¥å…¨åŒ…å«æ¨ç†ç»“æŸã€å·¥å…·è‡ªåŠ¨é€‰æ‹©ã€ä»¥åŠçº¯æ–‡æœ¬çš„å•å…ƒæµ‹è¯•ã€‚  
4. **æ€§èƒ½åŸºå‡†**ï¼šå¯¹æ¯”å¼€å¯/å…³é—­ `reasoning_parser` å‰åçš„ latency ä¸ tokenâ€‘throughputï¼Œç¡®ä¿ä¼˜åŒ–æ•ˆæœåœ¨ä¸åŒæ¨¡å‹ã€batch size ä¸‹ä¿æŒæ­£å‘æå‡ã€‚  

ä»¥ä¸Šæ”¹åŠ¨ä¸»è¦æå‡äº†æ¨ç†æœŸé—´ç»“æ„åŒ–è¾“å‡ºçš„æµå¼è§£ææ•ˆç‡ï¼Œå½±å“èŒƒå›´å±€é™äº OpenAIâ€‘å…¼å®¹å…¥å£åŠç»“æ„åŒ–è¾“å‡ºæ¨¡å—ï¼Œä¿æŒå‘åå…¼å®¹çš„å‰æä¸‹è¿›è¡Œå……åˆ†æµ‹è¯•å³å¯ã€‚

---

### [Bugfix] Fix ScoreMultiModalParam multi-document scoring returning single result (#33837)
**SHA**: `1f70313` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/1f70313e59bfae08588bb503b69c249a5ebd1e01)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ï¼ˆScoreMultiModalParam å¤šæ–‡æ¡£è¯„åˆ†è¿”å›å•ä¸€ç»“æœï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- `create_score_multimodal_param` ç”±è¿”å›å•ä¸ª `ScoreMultiModalParam` æ”¹ä¸ºè¿”å› **åˆ—è¡¨**ï¼Œä»¥åŒ¹é…å¤šæ–‡æ¡£åœºæ™¯ã€‚  
- è°ƒæ•´äº†è¿”å›å€¼çš„æ„é€ æ–¹å¼ï¼šæ¯ä¸ªæ–‡æ¡£å¯¹åº”ä¸€ä¸ª `ScoreMultiModalParam` å®ä¾‹ã€‚  
- `_run_hf` é‡æ–°å®ç°äº†å¯¹æ–‡æ¡£çš„é€æ¡æ‰“åˆ†é€»è¾‘ï¼šä¸å†å…ˆæŠŠæ–‡æ¡£æŒ‰ç±»å‹æ‹†åˆ†å†æ‰¹é‡è°ƒç”¨ `compute_score`ï¼Œè€Œæ˜¯éå† `document_strs`ï¼Œå¯¹æ¯æ¡æ–‡æ¡£å•ç‹¬è°ƒç”¨ `compute_score` å¹¶æŠŠç»“æœç›´æ¥è¿½åŠ åˆ° `scores`ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **tests/models/multimodal/pooling/test_jinavl_reranker.py**ï¼ˆä»…æµ‹è¯•æ–‡ä»¶ï¼‰  
- **ScoreMultiModalParam** çš„å…¬å¼€ APIï¼ˆè¿”å›ç±»å‹å˜åŒ–ï¼‰  
- é€šè¿‡ `hf_model.model.compute_score` æ‰¹é‡æ‰“åˆ†çš„å†…éƒ¨å®ç°è·¯å¾„  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **API å‘åå…¼å®¹**  
   - `create_score_multimodal_param` ä» `ScoreMultiModalParam` â†’ `list[ScoreMultiModalParam]` çš„ç­¾åå˜æ›´æ˜¯ **ç ´åæ€§çš„**ã€‚å¦‚æœåº“çš„å…¶å®ƒæ¨¡å—æˆ–ç”¨æˆ·ä»£ç ä»å‡è®¾è¿”å›å•å¯¹è±¡ï¼Œä¼šåœ¨è¿è¡Œæ—¶æŠ›å‡ºç±»å‹é”™è¯¯ã€‚å»ºè®®åœ¨ `vllm` æ ¸å¿ƒåº“ä¸­æä¾›ä¸€ä¸ªå…¼å®¹åŒ…è£…ï¼ˆå¦‚ä¿æŒåŸå‡½æ•°åå¹¶è¿”å›å•å¯¹è±¡ï¼Œæˆ–æ–°å¢ `create_score_multimodal_params`ï¼‰å¹¶åœ¨æ–‡æ¡£ä¸­å£°æ˜è¿ç§»è·¯å¾„ã€‚  

2. **`compute_score` è¿”å›å€¼ä¸€è‡´æ€§**  
   - å½“å‰å®ç°å‡è®¾ `compute_score` åœ¨åªä¼ å…¥å•æ¡ `[query, doc]` æ—¶è¿”å› **æ ‡é‡** `float`ã€‚å¦‚æœåº•å±‚å®ç°ä»è¿”å› `list[float]`ï¼ˆå³ä¾¿é•¿åº¦ä¸º 1ï¼‰ï¼Œ`scores.append(score)` å°†äº§ç”Ÿ `list[float]` ç»“æ„ï¼Œå¯¼è‡´åç»­æ–­è¨€æˆ–æ¯”è¾ƒå‡ºé”™ã€‚è¯·ç¡®è®¤ `compute_score` åœ¨å•æ¡è¾“å…¥æ—¶çš„è¿”å›ç±»å‹ï¼Œå¿…è¦æ—¶æ”¹ä¸º `score = hf_model.model.compute_score(...)[0]`ã€‚  

3. **æ€§èƒ½å½±å“**  
   - ç”±åŸæ¥çš„æ‰¹é‡è°ƒç”¨æ”¹ä¸ºé€æ¡è°ƒç”¨ï¼Œä¼šå¯¼è‡´æ˜¾è‘—çš„ **è°ƒç”¨å¼€é”€**ï¼ˆç‰¹åˆ«æ˜¯è¿œç¨‹æœåŠ¡å™¨æˆ–å¤§å‹æ¨¡å‹ï¼‰ï¼Œå¯èƒ½é™ä½ååã€‚è‹¥æ‰¹é‡æ‰“åˆ†ä»å¯è¡Œï¼Œå»ºè®®ä¿ç•™æ‰¹é‡è·¯å¾„ï¼Œæˆ–åœ¨å†…éƒ¨æ£€æµ‹ `len(document_strs) > 1` å†å†³å®šæ˜¯æ‰¹é‡è¿˜æ˜¯å•æ¡ã€‚  

4. **æµ‹è¯•è¦†ç›–**  
   - åªä¿®æ”¹äº†ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ï¼ŒæœªéªŒè¯å…¶å®ƒå¤šæ¨¡æ€æ¨¡å‹æˆ– Reranker å®ç°ã€‚å»ºè®®åœ¨ `tests/models/multimodal` ç›®å½•ä¸‹æ·»åŠ  **è·¨ç±»å‹æ··åˆï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰** çš„å®Œæ•´å›å½’æµ‹è¯•ï¼Œç¡®ä¿ `scores` é¡ºåºä¸åŸå§‹æ–‡æ¡£é¡ºåºä¿æŒä¸€è‡´ã€‚  

5. **æ–‡æ¡£ä¸è¿ç§»æŒ‡å—**  
   - åœ¨ `README` æˆ– API æ–‡æ¡£ä¸­è¡¥å…… `ScoreMultiModalParam` çš„ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯´æ˜åœ¨å¤šæ–‡æ¡£è¯„åˆ†åœºæ™¯ä¸‹åº”ä½¿ç”¨åˆ—è¡¨å½¢å¼çš„å‚æ•°ï¼Œä»¥åŠæ—§ä»£ç çš„å‡çº§æ–¹æ³•ã€‚  

**æ€»ç»“**  
æ­¤æ¬¡æäº¤ä¿®å¤äº†å¤šæ–‡æ¡£æƒ…å½¢ä¸‹åªè¿”å›å•ä¸€è¯„åˆ†çš„é”™è¯¯ï¼Œä½†å¼•å…¥äº†å‡½æ•°ç­¾åçš„ç ´åæ€§æ›´æ”¹ä»¥åŠé€æ¡è°ƒç”¨æ½œåœ¨çš„æ€§èƒ½å›é€€ã€‚å›¢é˜Ÿåº”åœ¨æ ¸å¿ƒåº“ä¸­æä¾›å‘åå…¼å®¹å±‚ï¼Œç¡®è®¤åº•å±‚ `compute_score` çš„è¿”å›ç±»å‹ä¸€è‡´æ€§ï¼Œå¹¶è¡¥å…¨ç›¸åº”çš„å›å½’æµ‹è¯•ä¸æ–‡æ¡£è¯´æ˜ï¼Œä»¥é˜²æ­¢ç”Ÿäº§ç¯å¢ƒå‡ºç°æ„å¤–è¡Œä¸ºã€‚

---

### [CI/Build] Parallelize CPU CI tests (#33778)
**SHA**: `07daee1` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/07daee132b30140bb7c5b28d7f8c856036d2baad)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / CI æ”¹è¿›  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- å°†åŸæœ‰çš„å•ä¸€ `intel.yaml` CPU æµ‹è¯•æ‹†åˆ†ï¼Œæ–°å¢ `cpu.yaml`ï¼Œåœ¨ Buildkite ä¸­å¹¶è¡Œè¿è¡Œå¤šç±» CPU æµ‹è¯•ï¼ˆkernelã€æ¨¡å‹ã€é‡åŒ–ã€åˆ†å¸ƒå¼ TP/DP/PPã€Multiâ€‘Modalï¼‰ï¼Œå¹¶ä¿ç•™åŸæœ‰ Arm æµ‹è¯•å…¥å£ã€‚  
- é‡æ„ `run-cpu-test.sh`ï¼Œå–æ¶ˆå¤šé•œåƒæ„å»ºä¸å®¹å™¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œæ”¹ä¸ºä¸€æ¬¡æ€§æ„å»ºé•œåƒååœ¨å®¹å™¨å†…ç›´æ¥æ‰§è¡Œä¼ å…¥çš„æµ‹è¯•å‘½ä»¤ï¼Œæ”¯æŒè¶…æ—¶æ§åˆ¶ã€‚  
- åœ¨ `cpu_worker.py` ä¸­åŠ å…¥ `VLLM_CPU_SIM_MULTI_NUMA` ç¯å¢ƒå˜é‡ï¼Œç”¨äºåœ¨ CI ç¯å¢ƒæ¨¡æ‹Ÿå¤š NUMA èŠ‚ç‚¹ï¼Œé¿å…å› çœŸå®æœºå™¨ NUMA ä¸è¶³å¯¼è‡´æ–­è¨€å¤±è´¥ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- CI é…ç½®æ–‡ä»¶ï¼š`.buildkite/hardware_tests/*`ã€`.buildkite/scripts/hardware_ci/*`  
- é•œåƒæ„å»ºè„šæœ¬ï¼š`run-cpu-test.sh`ï¼ˆDocker æ„å»ºã€è¿è¡Œé€»è¾‘ï¼‰  
- CPU å·¥ä½œè¿›ç¨‹ï¼š`vllm/v1/worker/cpu_worker.py`ï¼ˆNUMA ç»‘å®šç®—æ³•ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **CI ç¨³å®šæ€§**ï¼šæ–°å¹¶è¡Œæ­¥éª¤å¢åŠ äº†è¿è¡Œæ—¶é—´æ³¢åŠ¨ï¼Œå»ºè®®åœ¨ Buildkite ä¸­ç›‘æ§å„æ­¥éª¤çš„è€—æ—¶ï¼Œå¿…è¦æ—¶å¯¹è¶…æ—¶é˜ˆå€¼ (`TIMEOUT_VAL`) åšè°ƒä¼˜ã€‚  
2. **NUMA æ¨¡æ‹Ÿ**ï¼š`VLLM_CPU_SIM_MULTI_NUMA` åªåœ¨ CI ç¯å¢ƒä½¿ç”¨ï¼Œç”Ÿäº§ç¯å¢ƒåº”ç¡®ä¿ä¸æ³„æ¼è¯¥å˜é‡ï¼›å¯åœ¨ä»£ç ä¸­åŠ å…¥ `os.unsetenv` é˜²æ­¢è¯¯ä¼ ã€‚  
3. **é•œåƒç¼“å­˜**ï¼šç°åœ¨æ¯æ¬¡ CI éƒ½é‡æ–°æ„å»ºé•œåƒï¼Œè‹¥æ„å»ºæ—¶é—´æˆä¸ºç“¶é¢ˆï¼Œå¯è€ƒè™‘å¼€å¯ Buildkite çš„é•œåƒç¼“å­˜æˆ–ä½¿ç”¨å±‚ç¼“å­˜ã€‚  
4. **è„šæœ¬å…¼å®¹æ€§**ï¼š`run-cpu-test.sh` ç°åœ¨ä¾èµ– `$1` ä¸ `$2` å‚æ•°ï¼Œè°ƒç”¨æ–¹å¿…é¡»ä¿è¯é¡ºåºæ­£ç¡®ï¼›å»ºè®®åœ¨è„šæœ¬å¼€å¤´åŠ å…¥å‚æ•°æ£€æŸ¥å¹¶ç»™å‡ºå‹å¥½é”™è¯¯ä¿¡æ¯ã€‚  
5. **å›å½’æµ‹è¯•**ï¼šç¡®è®¤æ–°å¢çš„åˆ†å¸ƒå¼ Smoke Test ä¸æ—§çš„ TP/PP æµ‹è¯•åœ¨å¤šèŠ‚ç‚¹æœºå™¨ä¸Šä»èƒ½æ­£ç¡®å¯åŠ¨å¹¶å…³é—­ï¼Œé¿å…æ®‹ç•™è¿›ç¨‹å¯¼è‡´åç»­æ­¥éª¤å¤±è´¥ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨æ˜¾è‘—æå‡äº† CPU ç«¯çš„æµ‹è¯•è¦†ç›–åº¦ä¸å¹¶è¡Œåº¦ï¼Œä½†ä¹Ÿå¼•å…¥äº†å¯¹ç¯å¢ƒå˜é‡å’Œè¶…æ—¶ç®¡ç†çš„ä¾èµ–ï¼Œè¯·åœ¨ CI ç›‘æ§å’Œæœ¬åœ°è°ƒè¯•ä¸­ç•™æ„ç›¸åº”çš„ç»†èŠ‚ã€‚

---

### [2/N] move responses/serving _make_response_output_items logic to parser (#33281)
**SHA**: `9595afd` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9595afda183bdd79b0ee2d38d2b0049fe86e6628)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆå°†å“åº”ç”Ÿæˆé€»è¾‘æŠ½è±¡åˆ°â€¯Parserï¼Œç»Ÿä¸€â€¯Responsesâ€¯ä¸â€¯ChatCompletionsâ€¯çš„å®ç°ï¼‰  

**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. æŠŠ `serving._make_response_output_items` ä¸­çš„æ¨ç†ã€å·¥å…·è°ƒç”¨ã€logprob è®¡ç®—ç­‰å®Œæ•´å“åº”æ„é€ æ¬åˆ°äº† `vllm.parser.abstract_parser.Parser`ã€‚  
2. åœ¨ `Parser` ä¸­æ–°å¢æŠ½è±¡æ–¹æ³• `extract_response_outputs`ï¼Œå¹¶åœ¨åŸºç±»å®ç°äº†ç»Ÿä¸€çš„æ¨ç†â€‘å†…å®¹â€‘å·¥å…·è°ƒç”¨è§£æä»¥åŠå¯¹åº”çš„ OpenAI ç±»å‹å¯¹è±¡åˆ›å»ºã€‚  
3. `serving.py` åªè´Ÿè´£æ—¥å¿—è®°å½•å’Œåœ¨æ²¡æœ‰ parser æ—¶è¿”å›æœ€ç®€ `ResponseOutputMessage`ï¼Œå…¶ä½™å…¨éƒ¨äº¤ç»™ parser å¤„ç†ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/entrypoints/openai/responses/serving.py`ï¼ˆå“åº”æµå¼/éæµå¼è¾“å‡ºçš„å…¥å£ï¼‰  
- `vllm/parser/abstract_parser.py`ï¼ˆæ ¸å¿ƒæŠ½è±¡å±‚ï¼Œæ–°å¢æŠ½è±¡æ–¹æ³•ä¸å®ç°ï¼‰  
- ä¾èµ– `Parser` çš„æ’ä»¶æˆ–è‡ªå®šä¹‰ parserï¼ˆå¦‚è‡ªç ” ReasoningParserã€ToolParserï¼‰  
- ä¸æ—¥å¿—ã€logprob ç›¸å…³çš„å•å…ƒæµ‹è¯•ä¸ Benchmarks  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **å‘åå…¼å®¹**ï¼šåœ¨æ²¡æœ‰ `self.parser` çš„è€ç‰ˆæœ¬è·¯å¾„ä»è¿”å› `ResponseOutputMessage`ï¼Œä½†ä»éœ€ç¡®è®¤æ—§çš„ `make_tool_call_id`ã€`tool_call_id_type` ç­‰å‚æ•°åœ¨æ–°è·¯å¾„ä¸‹ä¿æŒè¡Œä¸ºä¸€è‡´ã€‚  
2. **æ€§èƒ½**ï¼šæ¯æ¬¡è¯·æ±‚éƒ½ä¼š `self.parser(tokenizer)` å®ä¾‹åŒ–ä¸€æ¬¡ parserï¼Œè‹¥ parser æ„é€ ä»£ä»·è¾ƒé«˜ï¼ˆå¦‚åŠ è½½å¤§å‹æ¨¡å‹æˆ–ç¼“å­˜ï¼‰ï¼Œå»ºè®®æ”¹ä¸º `cached_property` æˆ–åœ¨ `Engine` å±‚å¤ç”¨å®ä¾‹ã€‚  
3. **æ—¥å¿—ä¸å¼‚å¸¸**ï¼šåŸå…ˆçš„ `try/except` åŒ…è£¹æ¨ç† parser ç°åœ¨è¢«ç§»é™¤ï¼Œè‹¥ `reasoning_parser` åˆ›å»ºå¤±è´¥ä¼šç›´æ¥æŠ›å¼‚å¸¸ã€‚è€ƒè™‘åœ¨ `extract_response_outputs` ä¸­åŠ å…¥æ—¥å¿— & æ•è·ï¼Œä»¥å…å› å•æ¬¡æ¨ç†è§£æé”™è¯¯å¯¼è‡´æ•´ä¸ªå“åº”ä¸­æ–­ã€‚  
4. **logprob è®¡ç®—**ï¼šè¯·æ±‚ `include_output_logprobs` æ—¶ï¼Œlogprob åªåœ¨ parser åˆ†æ”¯è®¡ç®—ã€‚è¯·ç¡®è®¤ `request.is_include_output_logprobs()` åœ¨æœªä½¿ç”¨ parser æ—¶ä»èƒ½è¿”å› `None`ï¼ˆç¬¦åˆæ—§è¡Œä¸ºï¼‰ã€‚  
5. **å·¥å…·è°ƒç”¨è§£æ**ï¼šæ–°å¢ `_parse_tool_calls` åˆå¹¶äº† ChatCompletion ä¸ Responses ä¸¤å¥—è§„åˆ™ï¼Œç¡®ä¿ `ToolChoiceFunction`ã€`ChatCompletionNamedToolChoiceParam`ã€`required`ã€`auto` ç­‰åˆ†æ”¯åœ¨æ‰€æœ‰ OpenAI SDK ç‰ˆæœ¬ä¸‹éƒ½èƒ½å¾—åˆ°é¢„æœŸçš„ JSON/FunctionCallã€‚å»ºè®®æ·»åŠ è¦†ç›–è¿™äº›åˆ†æ”¯çš„é›†æˆæµ‹è¯•ã€‚  
6. **æ–‡æ¡£**ï¼šæ›´æ–° `Parser` æ¥å£è¯´æ˜ã€`Responses` API ç¤ºä¾‹ä»¥åŠ `--disable-log-outputs` ç­‰æ——æ ‡çš„å½±å“èŒƒå›´ï¼Œé˜²æ­¢ç”¨æˆ·è¯¯ä»¥ä¸ºæ—¥å¿—è¡Œä¸ºæœªå˜ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡è¿ç§»æå‡äº†ä»£ç å¤ç”¨åº¦å’Œå¯æµ‹è¯•æ€§ï¼Œä½†éœ€è¦å…³æ³¨å®ä¾‹åŒ–å¼€é”€ã€å¼‚å¸¸è·¯å¾„ä»¥åŠä¸æ—§ç‰ˆæ’ä»¶çš„å…¼å®¹æ€§ã€‚å»ºè®®åœ¨ CI ä¸­åŠ å…¥ parserâ€‘only ä¸ fallbackâ€‘only ä¸¤å¥—æ‰§è¡Œè·¯å¾„çš„å¯¹æ¯”æµ‹è¯•ï¼Œä»¥ä¿è¯åŠŸèƒ½ä¸€è‡´æ€§ã€‚

---

### [Minor] Include `StreamingInput` in inputs package (#33856)
**SHA**: `add9f1f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/add9f1fbd920611c2b909fe10d9b44b59650f8b7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šå°† `StreamingInput` ä»å†…éƒ¨ `vllm.inputs.data` æ¨¡å—æå‡è‡³ `vllm.inputs` åŒ…çš„å…¬å¼€æ¥å£ï¼Œå¹¶åœ¨ç›¸åº”çš„æµ‹è¯•å’Œæ ¸å¿ƒä»£ç ä¸­ç»Ÿä¸€ä½¿ç”¨æ–°å¯¼å…¥è·¯å¾„ã€‚åŒæ­¥æ›´æ–° `__all__`ï¼Œç¡®ä¿è¯¥ç±»å¯¹å¤–å¯è§ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm.inputs` åŒ…ï¼ˆæ–°å¢å¯¼å‡º `StreamingInput`ï¼‰  
- `vllm/v1/engine/async_llm.py`ï¼ˆåˆå¹¶å¯¼å…¥ï¼‰  
- æµ‹è¯•ç›®å½•ä¸‹çš„ `test_streaming_input.py`ã€`test_async_llm_streaming.py`ï¼ˆæ”¹ä¸ºæ–°è·¯å¾„ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
- æ£€æŸ¥é¡¹ç›®å…¶ä»–æ¨¡å—æ˜¯å¦ä»ä½¿ç”¨æ—§è·¯å¾„ `vllm.inputs.data.StreamingInput`ï¼Œå¦‚æœ‰éœ€åŒæ­¥è¿ç§»æˆ–æ·»åŠ å…¼å®¹åˆ«åä»¥é˜²æ­¢ç ´åå·²æœ‰ç”¨æˆ·ä»£ç ã€‚  
- æ–‡æ¡£å’Œç¤ºä¾‹åº”æ›´æ–°ä¸º `from vllm.inputs import StreamingInput`ï¼Œé¿å…æ··æ·†ã€‚  
- å…³æ³¨æäº¤ä¸­ç•™ä¸‹çš„ TODOï¼ˆé¿å…å¯¹é‡å¤ä½¿ç”¨çš„ `SamplingParams` å†æ¬¡æ ¡éªŒï¼‰ï¼Œåç»­å¯åœ¨ `AsyncLLM` ä¸­å®ç°ç¼“å­˜æˆ–ä¸€æ¬¡æ€§æ ¡éªŒé€»è¾‘ï¼Œæå‡æ€§èƒ½ã€‚  
- è¿è¡Œå®Œæ•´æµ‹è¯•çŸ©é˜µï¼Œç¡®è®¤å¯¼å‡º `StreamingInput` åä¸ä¼šå½±å“åºåˆ—åŒ–/ååºåˆ—åŒ–æˆ–å¤šæ¨¡æ€è¾“å…¥çš„å¤„ç†ã€‚

---

### Revert "[Attention][FA3] Update FA3 to include new swizzle optimization" (#33841)
**SHA**: `e3bf79f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/e3bf79ffa080a5052aa61fce71b70b11fb7f9d1e)

**ğŸ”§ å˜æ›´ç±»å‹**ï¼šé‡æ„ / ä»£ç å›æ»š  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤å°†å…ˆå‰å›  FA3 æ–° swizzle ä¼˜åŒ–å¼•å…¥çš„ä»£ç å›é€€ã€‚ä¸»è¦æ”¹åŠ¨åŒ…æ‹¬ï¼š  
1. å°† `flash-attention` å­æ¨¡å—çš„ Git tag ä» `2adfc8câ€¦` å›é€€åˆ° `188be16â€¦`ï¼ˆå¯¹åº”æœªå¼€å¯æ–°ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼‰ã€‚  
2. åœ¨ `flash_attn.py` ä¸ `flashattn_mla.py` ä¸­ï¼Œæ¢å¤åŸæ¥çš„ `scheduler_metadata` é•¿åº¦è®¡ç®—æ–¹å¼ï¼Œå»é™¤å¯¹ `max_cudagraph_size` ä¸ `max_num_seqs` å–æœ€å¤§åå†ä¹˜ 4 çš„é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨ `scheduler_config.max_num_seqs + 1`ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `cmake/external_projects/vllm_flash_attn.cmake`ï¼ˆå¤–éƒ¨ä¾èµ–ç‰ˆæœ¬ï¼‰  
- `vllm/v1/attention/backends/flash_attn.py`  
- `vllm/v1/attention/backends/mla/flashattn_mla.py`  
è¿™äº›æ–‡ä»¶æ¶‰åŠ Flashâ€‘Attention åç«¯çš„æ„å»ºåŠ CUDAâ€‘graph è°ƒåº¦å…ƒæ•°æ®åˆ†é…ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§éªŒè¯**ï¼šå›é€€åéœ€ç¡®è®¤åœ¨ä½¿ç”¨ `max_cudraph_size`ï¼ˆå¦‚å¼€å¯å…¨å›¾ CUDAâ€‘graphï¼‰æ—¶ä»èƒ½æ»¡è¶³è¿è¡Œæ—¶éœ€æ±‚ï¼Œé˜²æ­¢å› å…ƒæ•°æ®é•¿åº¦ä¸è¶³å¯¼è‡´æº¢å‡ºé”™è¯¯ã€‚  
2. **æ€§èƒ½å›å½’æ£€æŸ¥**ï¼šç¡®ä¿å›é€€è‡³æ—§ç‰ˆ flashâ€‘attention ä¸ä¼šæ˜¾è‘—é™ä½ååæˆ–å¢åŠ æ˜¾å­˜å ç”¨ï¼›å»ºè®®åœ¨å…¸å‹å·¥ä½œè´Ÿè½½ï¼ˆå•/å¤šåºåˆ—ï¼‰ä¸Šè·‘åŸºå‡†å¯¹æ¯”ã€‚  
3. **æ–‡æ¡£åŒæ­¥**ï¼šæ›´æ–° README/CHANGELOG ä¸­å…³äº FA3 swizzle ä¼˜åŒ–çš„è¯´æ˜ï¼Œé˜²æ­¢ç”¨æˆ·è¯¯ä»¥ä¸ºå·²å¯ç”¨ã€‚  
4. **æœªæ¥æ¢å¤**ï¼šè‹¥åç»­å†æ¬¡å¯ç”¨ swizzleï¼Œéœ€è¦é‡æ–°è¯„ä¼° `scheduler_metadata` çš„å¤§å°è®¡ç®—ï¼Œé¿å…ä¹‹å‰çš„ç¡¬ç¼–ç ä¹˜ 4 äº§ç”Ÿçš„æ½œåœ¨é”™è¯¯ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡å›æ»šæ¶ˆé™¤äº†å› æ–°ä¼˜åŒ–å¼•å…¥çš„å…ƒæ•°æ®é•¿åº¦è®¡ç®—é”™è¯¯ï¼Œé£é™©ä¸»è¦åœ¨äºå¯èƒ½ç¨å¾®ä¸‹é™çš„æ€§èƒ½å’Œå¯¹å·²æœ‰ CUDAâ€‘graph é…ç½®çš„å…¼å®¹æ€§ï¼Œå»ºè®®é€šè¿‡ CI/åŸºå‡†æµ‹è¯•è¿›è¡ŒéªŒè¯åå†å‘å¸ƒã€‚

---

### [CI][Bugfix]: return McpCall for built-in MCP tools in non-streaming mode (#32762)
**SHA**: `fb1270f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/fb1270f1f821603402a8868e3067b3c3342455e7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / ç¨³å®šæ€§å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. åœ¨éæµå¼æ¨¡å¼ä¸‹ï¼Œä¿®æ­£å†…ç½® MCP å·¥å…·ï¼ˆpythonã€browserã€containerï¼‰è¿”å› `McpCall` çš„æ˜ å°„ï¼Œä½¿å…¶ä¸æµå¼è·¯å¾„ä¿æŒä¸€è‡´ã€‚  
2. ä¸º `RemoteOpenAIServer` å¢åŠ æ›´æ¸©å’Œçš„è¿›ç¨‹ç»“æŸé€»è¾‘ä»¥åŠ GPU å†…å­˜é‡Šæ”¾æ£€æµ‹ï¼Œé˜²æ­¢å­è¿›ç¨‹æ®‹ç•™å¯¼è‡´åç»­æµ‹è¯• OOMã€‚  
3. æµ‹è¯•ç”¨ä¾‹ç»Ÿä¸€æ”¹ä¸ºæ›´å¤§æ•°å€¼ï¼ˆ123â€¯Ã—â€¯456ï¼‰å¹¶åŠ å…¥ flaky é‡è¯•ã€ä¾èµ–å…³ç³»ç­‰ï¼Œä»¥è¦†ç›–æ–°è·¯å¾„ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/entrypoints/openai/parser/harmony_utils.py`ï¼ˆMCP è°ƒç”¨è§£æï¼‰  
- `tests/entrypoints/openai/responses/*`ï¼ˆæ‰€æœ‰å“åº”ç›¸å…³çš„å•å…ƒæµ‹è¯•ï¼‰  
- `tests/utils.py`ï¼ˆRemoteOpenAIServer ç®¡ç†ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
- ç¡®è®¤ `_BUILTIN_TOOL_TO_MCP_SERVER_LABEL` ä¸åç«¯å®é™…æ ‡ç­¾ä¿æŒä¸€è‡´ï¼Œé˜²æ­¢æœªæ¥æ–°å¢å†…ç½®å·¥å…·å¿˜è®°æ˜ å°„ã€‚  
- `RemoteOpenAIServer._wait_for_gpu_memory_release` ä¾èµ– nvml/rocmï¼Œå»ºè®®åœ¨ CI ç¯å¢ƒä¸­æå‰æ£€æµ‹åº“å¯ç”¨ï¼Œé¿å…å› æŸ¥è¯¢å¤±è´¥å¯¼è‡´æµ‹è¯•è¯¯æŠ¥ã€‚  
- æµ‹è¯•ä¸­ä½¿ç”¨çš„å¤§æ•°ä¹˜æ³•ç»“æœå·²ç¡¬ç¼–ç ä¸º `"56088"`ï¼Œè‹¥åç»­æ”¹åŠ¨æ›´æ¢é»˜è®¤å·¥å…·å®ç°ï¼Œéœ€è¦åŒæ­¥æ›´æ–°æ–­è¨€ã€‚  
- ç»´æŒ `flake8`/`mypy` æ£€æŸ¥ï¼Œæ–°å¢çš„å‡½æ•°è¿”å›ç±»å‹ä½¿ç”¨äº† Union (`float | None`) éœ€ç¡®ä¿é¡¹ç›® Python ç‰ˆæœ¬å§‹ç»ˆ â‰¥3.10ã€‚  

æ•´ä½“æ”¹åŠ¨æå‡äº†éæµå¼ MCP å·¥å…·çš„å…¼å®¹æ€§å¹¶é™ä½äº†èµ„æºæ³„éœ²é£é™©ï¼Œå»ºè®®åœ¨ CI ä¸­è·‘å®Œæ•´å¥— GPUâ€‘enabled æµ‹è¯•ä»¥éªŒè¯ GPU å†…å­˜å›æ”¶é€»è¾‘ã€‚

---

### [Core] Don't schedule spec tokens with prefill chunks (#33652)
**SHA**: `fa4e0fb` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/fa4e0fb028460cf5f4eb9cc90e206d0d6f35b026)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ chunkedâ€‘prefill åœºæ™¯ä¸‹ï¼Œä¹‹å‰ä¼šæŠŠ draftï¼ˆspecï¼‰ä»¤ç‰Œé”™è¯¯åœ°ä¸å‰©ä½™çš„ prefill ä»¤ç‰Œä¸€èµ·è°ƒåº¦ï¼Œå¯¼è‡´å¤šè°ƒåº¦äº† spec ä»¤ç‰Œã€‚æ–°å¢ `Request.is_prefill_chunk` æ ‡è®°å¹¶åœ¨è°ƒåº¦ã€draftâ€‘token æ›´æ–°ä»¥åŠ asyncâ€‘scheduler çš„åç½®å¤„ç†é‡Œè¿‡æ»¤æ‰è¿™äº› spec ä»¤ç‰Œï¼Œä»è€Œä¿è¯åªæœ‰åœ¨ prefill å®Œæˆåæ‰ä¼šä½¿ç”¨ draft ä»¤ç‰Œã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/request.py`ï¼šæ–°å¢ `is_prefill_chunk` æ ‡è®°ã€‚  
- `vllm/v1/core/sched/scheduler.py`ï¼šåœ¨ `_update_after_schedule` ä¸­ç»´æŠ¤è¯¥æ ‡è®°ï¼›åœ¨ `update_draft_token_ids` ä¸­å¯¹ prefillâ€‘chunk è¯·æ±‚ç›´æ¥ä¸¢å¼ƒ draft ä»¤ç‰Œã€‚  
- `vllm/v1/core/sched/async_scheduler.py`ï¼šç®€åŒ–ç»“æ„åŒ–è¾“å‡ºæ ‡è®°çš„è®¡ç®—ï¼Œé¿å…å¯¹ prefillâ€‘chunk è¿›è¡Œ placeholder æ›´æ–°ã€‚  
- `vllm/v1/worker/gpu/spec_decode/utils.py`ï¼šæ”¹è¿› draftâ€‘token æ¡ä»¶è½¬ç§»ï¼Œä¿è¯å¼‚æ­¥å¤åˆ¶åœ¨æ— ç»“æ„åŒ–è¾“å‡ºæ—¶ä¹Ÿèƒ½è¿”å›åˆæ³•çš„å ä½æ•°ç»„ã€‚  
- æ–°å¢å•å…ƒæµ‹è¯• `test_no_spec_tokens_scheduled_for_prefill_chunks`ï¼Œè¦†ç›– bug åœºæ™¯ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æ ‡è®°ç”Ÿå‘½å‘¨æœŸ**ï¼š`is_prefill_chunk` ä¾èµ– `num_computed_tokens` ä¸ `num_tokens + num_output_placeholders` çš„æ¯”è¾ƒï¼Œç¡®ä¿åœ¨æ‰€æœ‰ä»£ç è·¯å¾„ï¼ˆä¾‹å¦‚è¯·æ±‚è¢«å–æ¶ˆæˆ–æå‰ç»“æŸï¼‰ä¸­è¯¥æ¯”è¾ƒä»ç„¶æˆç«‹ï¼Œå¦åˆ™å¯èƒ½å‡ºç°è¯¯åˆ¤ã€‚  
2. **ç»“æ„åŒ–è¾“å‡ºæ ‡è®°**ï¼šasyncâ€‘scheduler ä¸­å»æ‰äº† `has_structured_output_requests` çš„å‰ç½®è®¡ç®—ï¼Œæ”¹ä¸ºåœ¨ `_update_after_schedule` é‡Œç›´æ¥å†™å…¥ `scheduler_output.has_structured_output_requests`ã€‚è¯·æ£€æŸ¥å…¶å®ƒä½¿ç”¨è¯¥å­—æ®µçš„è·¯å¾„ï¼ˆå¦‚ GPU workerï¼‰æ˜¯å¦ä»èƒ½æ­£å¸¸å·¥ä½œã€‚  
3. **æ€§èƒ½å½±å“**ï¼šæ–°å¢çš„å¸ƒå°”æ£€æŸ¥å‡ ä¹ä¸äº§ç”Ÿé¢å¤–å¼€é”€ï¼Œä½† `update_draft_token_ids` ç°åœ¨ä¼šåœ¨æ¯ä¸ª prefillâ€‘chunk è¯·æ±‚ä¸Šæ¸…ç©º `spec_token_ids`ï¼Œç¡®è®¤è¿™ä¸ä¼šå¯¼è‡´ä¸å¿…è¦çš„å†…å­˜æ‹·è´æˆ–é¢å¤–çš„åŒæ­¥ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šå½“å‰åªæ·»åŠ äº†å•ä¸€åœºæ™¯çš„æµ‹è¯•ï¼Œå»ºè®®è¡¥å……ä»¥ä¸‹è¾¹ç•Œï¼š  
   - å¤šè¯·æ±‚äº¤å‰çš„ chunkedâ€‘prefillï¼›  
   - ç»“æ„åŒ–è¾“å‡ºè¯·æ±‚åœ¨ prefillâ€‘chunk åç«‹å³è¿›å…¥ decodeï¼›  
   - å¼‚æ­¥è°ƒåº¦å…³é—­æ—¶ `SpecDecodeUtils.get_draft_tokens` çš„å›é€€è·¯å¾„ã€‚  
5. **æ–‡æ¡£æ›´æ–°**ï¼šå¯¹å¤–è¯´æ˜ `chunked_prefill` æ¨¡å¼ä¸‹ draft ä»¤ç‰Œçš„ç”Ÿæ•ˆæ—¶æœºï¼Œä»¥å…ç”¨æˆ·è¯¯ä»¥ä¸ºå¯ä»¥åœ¨ prefill æœŸé—´å°±å¼€å¯ speculative decodingã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨å®šä½ç²¾å‡†ï¼Œä»£ç æ”¹åŠ¨é›†ä¸­ï¼Œé£é™©å¯æ§ï¼Œåªè¦å®Œæˆä¸Šè¿°ç»†èŠ‚éªŒè¯å³å¯å®‰å…¨åˆå…¥ã€‚

---

### Revert "[torch.compile] Significantly speed up cold start times" (#33820)
**SHA**: `9f14c92` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9f14c9224d3d6664e2f5a2e7fecd012fd048fcb1)

**å˜æ›´ç±»å‹**ï¼šğŸ”§ å…¶ä»–ï¼ˆæ’¤é”€æ­¤å‰çš„ `torch.compile` å†·å¯åŠ¨åŠ é€Ÿå®ç°ï¼‰  
**é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**æ ¸å¿ƒæ”¹åŠ¨**  
1. **ç¼“å­˜é”®ç”± `graph_hash` æ”¹ä¸º `graph_index`**ï¼šåœ¨ `CompilerManager` ä¸­ï¼Œç¼“å­˜å­—å…¸çš„é”®ä» `(runtime_shape, graph_hash, backend_name)` å˜ä¸º `(runtime_shape, graph_index, backend_name)`ï¼Œç›¸åº”åœ°åˆ é™¤äº†åŸæ¥çš„ `graph_hash` è®¡ç®—é€»è¾‘ï¼ˆå»æ‰äº†å¯¹ `AOTAutogradCachePickler` çš„ä¾èµ–ï¼‰ã€‚  
2. **åŠ è½½/å­˜å‚¨æµç¨‹åŒæ­¥ä¿®æ”¹**ï¼š`load`ã€`compile`ã€ä»¥åŠåº•å±‚ `compiler_interface.load` æ¥å£å‡åŠ å…¥ `graph_index` å‚æ•°ï¼Œå¹¶æ”¹ç”¨æ–°é”®æ¥æŸ¥æ‰¾æˆ–å†™å…¥ç¼“å­˜ã€‚  
3. **æµ‹è¯•æœŸæœ›æ›´æ–°**ï¼š`tests/compile/test_cold_start.py` ç°åœ¨æ£€æŸ¥å†·å¯åŠ¨æœŸé—´ 3 æ¬¡ç¼“å­˜ missã€å…¶ä½™ 30 æ¬¡ cache hitï¼Œåæ˜ äº†ç¼“å­˜å‘½ä¸­ç‡çš„å˜åŒ–ã€‚  

**å½±å“èŒƒå›´**  
- ç¼–è¯‘ç¼“å­˜ç®¡ç†ï¼ˆ`vllm/compilation/backends.py`ã€`vllm/compilation/compiler_interface.py`ï¼‰ã€‚  
- å†·å¯åŠ¨æµ‹è¯•è„šæœ¬ã€‚  
- ä»»ä½•ä¾èµ–ç¼–è¯‘ç¼“å­˜æŒä¹…åŒ–çš„è¿è¡Œæ—¶ï¼ˆä¾‹å¦‚å¤šè¿›ç¨‹å¯åŠ¨ã€è·¨ä¼šè¯å¤ç”¨ç¼“å­˜ï¼‰å°†å—åˆ°é”®ç»“æ„å˜åŒ–çš„å½±å“ã€‚  

**å…³æ³¨å»ºè®®**  
- **é”®å”¯ä¸€æ€§**ï¼š`graph_index` å¿…é¡»åœ¨åŒä¸€æ¨¡å‹ç»“æ„ä¸‹ä¿æŒç¨³å®šï¼Œå¦åˆ™å¯èƒ½å‡ºç°è¯¯ç¼“å­˜æˆ–å¤±æ•ˆã€‚å»ºè®®åœ¨æ¨¡å‹æ‹†åˆ†é˜¶æ®µæ˜¾å¼è®°å½•å¹¶å›ºå®šå­å›¾é¡ºåºã€‚  
- **å‘åå…¼å®¹**ï¼šè€ç¼“å­˜æ–‡ä»¶ä»ä½¿ç”¨ `graph_hash` é”®ï¼Œå½“å‰å®ç°ç›´æ¥ä¸¢å¼ƒæ—§ç¼“å­˜ã€‚è‹¥éœ€è¦å¹³æ»‘è¿ç§»ï¼Œå¯åœ¨ `initialize_cache` ä¸­æ£€æµ‹æ—§é”®å¹¶è¿ç§»ã€‚  
- **æ€§èƒ½å›å½’**ï¼šæ’¤é”€ `graph_hash` åï¼Œå†·å¯åŠ¨æ—¶ä»ä¼šäº§ç”Ÿ 30 æ¬¡å‘½ä¸­ï¼Œä½†æ•´ä½“å¯åŠ¨æ—¶é—´å¯èƒ½å›å‡ã€‚è¯·åœ¨ä¸åŒç¡¬ä»¶/æ¨¡å‹è§„æ¨¡ä¸‹è·‘åŸºå‡†ï¼Œç¡®è®¤ä¸å‡ºç°æ˜¾è‘—å›é€€ã€‚  
- **æ–‡æ¡£æ›´æ–°**ï¼šè¯´æ˜ç¼“å­˜é”®çš„å®šä¹‰åŠå…¶å¯¹ `torch.compile` å†·å¯åŠ¨çš„å½±å“ï¼Œæé†’ç”¨æˆ·åœ¨æ›´æ”¹æ¨¡å‹å­å›¾åˆ’åˆ†æ—¶æ¸…é™¤ç¼“å­˜ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡å›æ»šé€šè¿‡ä½¿ç”¨å­å›¾ç´¢å¼•ç®€åŒ–ç¼“å­˜é€»è¾‘ï¼Œå´å¯èƒ½å¼•å…¥é”®ä¸ç¨³å®šé£é™©ã€‚åŠ¡å¿…åœ¨ CI ä¸­åŠ å…¥å¯¹ç¼“å­˜è¿ç§»å’Œé”®ç¨³å®šæ€§çš„æ£€æµ‹ã€‚

---

### [Model] Add transcription support for Qwen3-Omni (#29828)
**SHA**: `535de06` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/535de06cb1d90ed1c48246a512e74c87fe1768e4)

**ğŸ› ï¸ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º Qwen3â€‘Omni å¼•å…¥éŸ³é¢‘è½¬å†™/ç¿»è¯‘èƒ½åŠ›ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- åœ¨æ–‡æ¡£ä¸­åŠ å…¥ Qwen3â€‘Omniâ€¯MoeThinker çš„æ”¯æŒè¯´æ˜ã€‚  
- `vllm/model_executor/models/qwen3_omni_moe_thinker.py` å¢åŠ  `SupportsTranscription` æ¥å£å®ç°ï¼Œæ–°å¢è¯­è¨€æ˜ å°„ã€`get_speech_to_text_config`ã€`get_generation_prompt` ç­‰æ–¹æ³•ï¼Œå®ç°éŸ³é¢‘ â†’ æ–‡æœ¬ï¼ˆè½¬å†™æˆ–ç¿»è¯‘ï¼‰çš„ Prompt æ„é€ ã€‚  
- æ–°å¢å¯¹ `PromptType`ã€`cached_processor_from_config` çš„å¼•ç”¨ï¼Œç¡®ä¿åœ¨æ¨ç†æ—¶èƒ½å¤Ÿè‡ªåŠ¨åŠ è½½å¯¹åº”çš„å¤„ç†å™¨å¹¶ç”Ÿæˆå¸¦ `<|audio_start|>` ç­‰å ä½ç¬¦çš„å¯¹è¯æ¨¡æ¿ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/models/qwen3_omni_moe_thinker.py`ï¼ˆæ ¸å¿ƒæ¨¡å‹å®ç°ï¼‰  
- å…¬å…±æ¥å£ `SupportsTranscription`ã€`PromptType`ã€`SpeechToTextConfig`ï¼ˆæ–°å¢/ä¿®æ”¹ï¼‰  
- æ–‡æ¡£ `docs/contributing/model/transcription.md`ã€`docs/models/supported_models.md`ï¼ˆå¯¹å¤–è¯´æ˜ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å•å…ƒ/é›†æˆæµ‹è¯•**ï¼šè¡¥å…¨é’ˆå¯¹ `get_generation_prompt` çš„æµ‹è¯•ï¼ŒéªŒè¯ä¸åŒ `task_type`ã€`language`ã€`to_language` ç»„åˆä¸‹ Prompt è¯­æ³•æ˜¯å¦ç¬¦åˆæ¨¡å‹æœŸæœ›ã€‚  
2. **å…¼å®¹æ€§**ï¼š`SupportsTranscription` æ˜¯æ–°æ¥å£ï¼Œç¡®è®¤å…¶å®ƒæ¨¡å‹å®ç°æˆ–è°ƒåº¦é€»è¾‘æœªè¯¯å°†å…¶è§†ä¸ºå¿…éœ€ï¼Œä»¥å…å‡ºç° `AttributeError`ã€‚å¦‚æœæœ‰é€šç”¨ä»£ç æ£€æŸ¥ `hasattr(model, "get_generation_prompt")`ï¼Œä¿æŒå‘åå…¼å®¹ã€‚  
3. **è¯­è¨€æ˜ å°„**ï¼šISO639_1_SUPPORTED_LANGS åªåˆ—å‡ºå°‘æ•°è¯­è¨€ï¼Œè‹¥æ¨¡å‹å®é™…æ”¯æŒæ›´å¤šè¯­è¨€ï¼Œæ–‡æ¡£å’Œæ˜ å°„éœ€åŒæ­¥æ›´æ–°ï¼Œå¦åˆ™ç”¨æˆ·å¯èƒ½å¾—åˆ°ç©ºå­—ç¬¦ä¸²çš„æŒ‡ä»¤ã€‚  
4. **Processor ç¼“å­˜**ï¼š`cached_processor_from_config` ä¼šåœ¨é¦–æ¬¡è°ƒç”¨æ—¶å®ä¾‹åŒ– Processorï¼Œç¡®ä¿åœ¨å¤šè¿›ç¨‹/TP ç¯å¢ƒä¸‹å®‰å…¨ï¼ˆé¿å…è¿›ç¨‹é—´å…±äº«çŠ¶æ€ï¼‰ã€‚å¯åœ¨ `__init__` ä¸­é¢„çƒ­ä¸€æ¬¡ã€‚  
5. **Prompt é•¿åº¦**ï¼šéŸ³é¢‘å ä½ç¬¦ä¸æŒ‡ä»¤ä¸€èµ·è®¡å…¥ token é•¿åº¦ï¼Œå»ºè®®åœ¨ `get_speech_to_text_config` ä¸­åŠ å…¥å¯¹ `max_new_tokens` çš„åˆç†é™åˆ¶ï¼Œé˜²æ­¢è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ã€‚  

æ€»ä½“æ¥è¯´ï¼Œæ­¤æ¬¡æäº¤ä¸º Qwen3â€‘Omni å¢æ·»äº†å®Œæ•´çš„è½¬å†™/ç¿»è¯‘æµæ°´çº¿ï¼Œæ˜¯ä¸€æ¬¡æœ‰ä»·å€¼çš„åŠŸèƒ½æ‰©å±•ï¼Œåªéœ€åœ¨æµ‹è¯•ã€å…¼å®¹æ€§ä¸æ–‡æ¡£ç»†èŠ‚ä¸Šå†åšå¥½æŠŠæ§å³å¯é¡ºåˆ©å‘å¸ƒã€‚

---

### [Bugfix] Support `RotaryEmbedding` CustomOp for gpt-oss (#33800)
**SHA**: `4292c90` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4292c90a2a188121ccbfd132def62031283d9d8a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / Bug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šä¸º `RotaryEmbedding` æ·»åŠ äº†è‡ªå®šä¹‰ç®—å­åœ¨ `torch.compile`ï¼ˆVLLMâ€‘compileï¼‰ä¸‹çš„æ”¯æŒã€‚æ ¸å¿ƒå®ç°ä¿®æ”¹ä¸ºåœ¨ `torch.compile` è·Ÿè¸ªæœŸé—´é¿å…å¯¹ç¼“å­˜ buffer çš„åŸåœ°ä¿®æ”¹ï¼Œ`_match_cos_sin_cache_dtype` æ”¹ä¸ºè¿”å›é€‚é…åçš„ `cos_sin_cache` è€Œä¸æ˜¯ç›´æ¥å†™å›ï¼Œå¹¶åœ¨æ‰€æœ‰ Rotaryâ€‘Embedding å­ç±»ä¸­ä½¿ç”¨è¿”å›å€¼ã€‚æ–°å¢å¯¹åº”çš„ç¼–è¯‘æµ‹è¯•ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/model_executor/layers/rotary_embedding/*`ï¼ˆbaseã€deepseek_scaling_ropeã€mrope ç­‰ï¼‰  
- ç¼–è¯‘è·¯å¾„ `vllm/compilation/*`ï¼ˆè‡ªå®šä¹‰ç®—å­æ³¨å†Œï¼‰  
- æµ‹è¯•å¥—ä»¶ `tests/compile/test_rotary_embedding_compile.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
1. **åŠŸèƒ½éªŒè¯**ï¼šåœ¨å¤šå¡ã€ä¸åŒ dtypeï¼ˆfp16/bf16ï¼‰ä»¥åŠ CPU fallback åœºæ™¯ä¸‹è·‘å®Œæ•´æ¨ç†é“¾è·¯ï¼Œç¡®ä¿è¿”å›çš„ `cos_sin_cache` ä¸åŸå®ç°ç­‰ä»·ï¼Œé˜²æ­¢å› æœªå†™å›å¯¼è‡´ç¼“å­˜å¤±æ•ˆã€‚  
2. **æ€§èƒ½å›å½’**ï¼šå¯¹æ¯”å¼€å¯/å…³é—­ `VLLM_USE_BYTECODE_HOOK`ã€`VLLM_USE_AOT_COMPILE` æ—¶çš„ååä¸å»¶è¿Ÿï¼Œç¡®ä¿æ–°è·¯å¾„æ²¡æœ‰å¼•å…¥é¢å¤–æ‹·è´å¼€é”€ã€‚  
3. **ä»£ç å…¼å®¹**ï¼šæ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç›´æ¥è®¿é—® `self.cos_sin_cache` çš„å¤–éƒ¨ä»£ç ï¼ˆå¦‚è‡ªå®šä¹‰æ’ä»¶ï¼‰å¯èƒ½å› ä¸ºä¸å†å¼ºåˆ¶æ›´æ–°è€Œå‡ºç°ä¸ä¸€è‡´ã€‚å¿…è¦æ—¶åœ¨æ–‡æ¡£ä¸­æ˜ç¡® â€œåœ¨ torch.compile ç¯å¢ƒä¸‹è¯·ä½¿ç”¨è¿”å›å€¼â€ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šå»ºè®®è¡¥å…… CPU ç¯å¢ƒä¸‹çš„ç­‰ä»·æ€§æµ‹è¯•ï¼Œä»¥åŠå¯¹ `torch.compile` çš„ AOT ç¼–è¯‘æ¨¡å¼è¿›è¡ŒéªŒè¯ï¼Œä»¥é˜²æ­¢æœªæ¥çš„ç¼–è¯‘åç«¯æ›´æ”¹å¯¼è‡´å›é€€ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤ä¿®æ”¹æå‡äº† VLLM åœ¨ä½¿ç”¨ `torch.compile` æ—¶çš„ç¨³å®šæ€§ï¼Œå½±å“å±€é™äº Rotaryâ€‘Embedding ç›¸å…³è·¯å¾„ï¼Œé£é™©å¯é€šè¿‡ä¸Šè¿°éªŒè¯æ‰‹æ®µè¿›è¡Œç®¡æ§ã€‚

---

### Implement zero-copy GQA for multimodal and CPU (#33732)
**SHA**: `6e98f6d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6e98f6d8b64984d5e3a8a9b323321f1095bac8b3)

**ğŸ›  å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆZeroâ€‘Copy GQA æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- å°†åŸå…ˆåœ¨ `maybe_reshape_qkv_to_4d` ä¸­å¯¹ **GQA/MQA** é€šè¿‡ `torch.repeat_interleave` è¿›è¡Œçš„å¤åˆ¶é€»è¾‘æŠ½ç¦»ï¼Œæ”¹ä¸ºåœ¨åç«¯ SDPA/Flashâ€‘Attn ä¸­é€šè¿‡æ–°å¢ `enable_gqa` æ ‡è®°å®ç° **é›¶æ‹·è´**ã€‚  
- é‡å‘½å `maybe_reshape_qkv_to_4d` ä¸º `view_qkv_to_4d`ï¼Œä»…è´Ÿè´£å½¢çŠ¶å˜æ¢ï¼›åœ¨å¤šæ¨¡æ€ï¼ˆ`mm_encoder_attention.py`ï¼‰å’Œ `molmo2`ã€CPU åç«¯ä¸­ç»Ÿä¸€ä¼ é€’ `enable_gqa=self.num_heads > self.num_kv_heads`ã€‚  
- `vit_attn_wrappers` ä¸åº•å±‚ C++/CUDA æ¥å£ `torch_sdpa_wrapper` å¢åŠ å¯¹åº”å‚æ•°ï¼Œä¿è¯è·¨å¹³å°ï¼ˆGPUã€CPUï¼‰éƒ½èƒ½ä½¿ç”¨ GQA çš„é›¶æ‹·è´è·¯å¾„ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **Attention å±‚**ï¼š`vllm/model_executor/layers/attention/mm_encoder_attention.py`ã€`vllm/model_executor/models/molmo2.py`ã€‚  
- **CPU åç«¯**ï¼š`vllm/v1/attention/backends/cpu_attn.py`ã€‚  
- **é€šç”¨åŒ…è£…å±‚**ï¼š`vllm/v1/attention/ops/vit_attn_wrappers.py`ï¼ˆSDPAã€Flashâ€‘Attnã€Fakeã€Torchâ€‘SDPAï¼‰ã€‚  
- å…³è”çš„ C++/CUDA æ¥å£ï¼ˆ`torch.ops.vllm.torch_sdpa_wrapper`ï¼‰éœ€è¦èƒ½å¤Ÿæ¥å— `enable_gqa` å‚æ•°ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤å½“å‰è¿è¡Œç¯å¢ƒçš„ PyTorch ç‰ˆæœ¬åœ¨ `scaled_dot_product_attention` ä¸­å·²å®ç° `enable_gqa` å‚æ•°ï¼›è‹¥ä¸æ”¯æŒï¼Œéœ€è¦åœ¨ `setup.py` æˆ–æ–‡æ¡£ä¸­æ³¨æ˜æœ€ä½ç‰ˆæœ¬ã€‚  
2. **åŠŸèƒ½å›å½’**ï¼šåœ¨é GQA åœºæ™¯ï¼ˆ`num_heads == num_kv_heads`ï¼‰ä¸‹è¿è¡Œå®Œæ•´å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ—§çš„å¤åˆ¶è·¯å¾„è¢«å®Œå…¨æ›¿ä»£ä¸”ç»“æœä¸€è‡´ã€‚  
3. **æ€§èƒ½åŸºå‡†**ï¼šå¯¹æ¯”å¼€å¯/å…³é—­ `enable_gqa` å‰åçš„æ˜¾å­˜å ç”¨å’Œæ¨ç†ååï¼ŒéªŒè¯â€œé›¶æ‹·è´â€å¸¦æ¥çš„å®é™…æ”¶ç›Šã€‚  
4. **æ–‡æ¡£ä¸ç¤ºä¾‹**ï¼šåœ¨æ¨¡å‹é…ç½®æˆ– API æ–‡æ¡£ä¸­è¯´æ˜ `enable_gqa` çš„æ„ä¹‰åŠä½•æ—¶ä¼šè‡ªåŠ¨è§¦å‘ï¼Œé¿å…ä½¿ç”¨è€…è¯¯ä»¥ä¸ºéœ€è¦æ‰‹åŠ¨ä¼ å‚ã€‚  
5. **åç«¯å®ç°**ï¼šæ£€æŸ¥ `torch.ops.vllm.torch_sdpa_wrapper` çš„ C++ å®ç°å·²æ¥å—å¹¶æ­£ç¡®ä½¿ç”¨ `enable_gqa`ï¼Œé˜²æ­¢å‚æ•°ä¸¢å¤±å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ã€‚  

è½å®ä¸Šè¿°æ£€æŸ¥åï¼Œå¯ç¡®ä¿æ–°é›¶æ‹·è´ GQA åœ¨å¤šæ¨¡æ€ã€CPU ä¸ GPU è·¯å¾„ä¸Šå®‰å…¨ã€å¿«é€Ÿåœ°æŠ•å…¥ä½¿ç”¨ã€‚

---

### [rocm][ray] Fix: Unify Ray device visibility handling across CUDA and ROCm (#33308)
**SHA**: `2f6d17c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/2f6d17cb2f4a49e29aae5c3c1ff64623d66d0257)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / BUG ä¿®å¤ï¼ˆç»Ÿä¸€ Ray åœ¨ CUDA ä¸ ROCm ä¸Šçš„è®¾å¤‡å¯è§æ€§å¤„ç†ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `Platform` åŸºç±»ä¸­åŠ å…¥ `ray_noset_device_env_vars`ï¼Œç”¨äºåœ¨ Ray å¯åŠ¨æ—¶å¼ºåˆ¶è®¾ç½® `RAY_EXPERIMENTAL_NOSET_*` ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢ Ray è‡ªåŠ¨è¦†ç›– `CUDA_VISIBLE_DEVICES`ã€`HIP_VISIBLE_DEVICES`ã€`ROCR_VISIBLE_DEVICES`ã€‚  
2. `CudaPlatform`ã€`RocmPlatform` å„è‡ªå£°æ˜å„è‡ªéœ€è¦çš„ noset ç¯å¢ƒå˜é‡ï¼›`ray_executor` åœ¨åˆ›å»º worker å‰ç»Ÿä¸€å†™å…¥è¿™äº›å˜é‡ï¼Œå¹¶åœ¨ `sort_by_driver_then_worker_ip` ä¸­æ˜¾å¼æŠŠæ‰€æœ‰ GPU ID é€šè¿‡ `CUDA_VISIBLE_DEVICES` æš´éœ²ç»™æ¯ä¸ª workerã€‚  
3. ç§»é™¤ Dockerfile ä¸­çš„ç¡¬ç¼–ç  noset å˜é‡ï¼Œæ”¹ç”±ä»£ç ç»Ÿä¸€ç®¡ç†ï¼Œé¿å…é•œåƒå±‚é¢çš„ä¸ä¸€è‡´ã€‚  
4. å¢åŠ æµ‹è¯•é…ç½®ï¼Œç¡®ä¿åœ¨æœªæ¥ Ray ç‰ˆæœ¬é»˜è®¤è¡Œä¸ºä¸‹ä»èƒ½ä¿æŒå¯è§è®¾å¤‡ä¸è¢«è¦†ç›–ã€‚  
5. åˆ é™¤ `worker_base` ä¸­å¯¹ `CUDA_VISIBLE_DEVICES` çš„ç‰¹æ®Šåˆ é™¤é€»è¾‘ï¼Œæ”¹ç”±å¹³å°ç»Ÿä¸€æ§åˆ¶ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/platforms/*`ï¼ˆCUDAã€ROCmï¼‰ä»¥åŠå…¬å…± `interface.py`ã€‚  
- Ray åˆ†å¸ƒå¼æ‰§è¡Œå™¨ `vllm/v1/executor/ray_executor.py`ã€‚  
- Docker é•œåƒæ„å»ºè„šæœ¬ `docker/Dockerfile.rocm`ã€‚  
- å•å…ƒæµ‹è¯• `tests/config/test_config_generation.py`ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å…¼å®¹æ€§**ï¼šç¡®ä¿åœ¨æœªä½¿ç”¨ Rayï¼ˆæˆ–ä½¿ç”¨æ—§ç‰ˆ Rayï¼‰çš„ç¯å¢ƒä¸‹ï¼Œæ–°å¢çš„ `RAY_EXPERIMENTAL_NOSET_*` å˜é‡ä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨ï¼›å¯é€šè¿‡åœ¨ `Platform` åˆå§‹åŒ–æ—¶åšä¸€æ¬¡æ£€æµ‹å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯ã€‚  
- **æ–‡æ¡£**ï¼šæ›´æ–°éƒ¨ç½²è¯´æ˜ï¼Œè¯´æ˜åœ¨è‡ªå®šä¹‰ Ray é›†ç¾¤æ—¶æ— éœ€æ‰‹åŠ¨è®¾ç½® `RAY_EXPERIMENTAL_NOSET_*`ï¼ŒvLLM ä¼šè‡ªåŠ¨æ³¨å…¥ã€‚  
- **æµ‹è¯•**ï¼šåœ¨ CI ä¸­åŠ å…¥ä¸åŒ Ray ç‰ˆæœ¬ï¼ˆåŒ…æ‹¬é¢„è§ˆç‰ˆï¼‰ä»¥åŠçº¯ CPU ç¯å¢ƒçš„çŸ©é˜µæµ‹è¯•ï¼ŒéªŒè¯ `device_control_env_var` åœ¨æ—  GPU çš„æœºå™¨ä¸Šä»ä¿æŒåˆç†è¡Œä¸ºã€‚  
- **æ€§èƒ½**ï¼šæš´éœ²å…¨éƒ¨ GPU ç»™æ¯ä¸ª worker å¯èƒ½å¯¼è‡´ `CUDA_VISIBLE_DEVICES` åˆ—è¡¨è¿‡é•¿ï¼›è‹¥å‡ºç°å¼‚å¸¸ï¼Œå¯è€ƒè™‘åœ¨ `RayDistributedExecutor` ä¸­åŠ å…¥è£å‰ªé€»è¾‘ï¼Œä»…ä¿ç•™åˆ†é…ç»™è¯¥ worker çš„ GPUã€‚  
- **å›æ»š**ï¼šè‹¥ç”¨æˆ·ä¾èµ–æ—§çš„ Dockerfile ç¯å¢ƒå˜é‡ï¼Œéœ€è¦åœ¨å‡çº§æŒ‡å—ä¸­æç¤ºå…¶è¿ç§»åˆ°æ–°å¹³å°å®ç°æ–¹å¼ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ç»Ÿä¸€äº†ä¸åŒç¡¬ä»¶åç«¯ä¸‹ Ray å¯¹å¯è§è®¾å¤‡çš„ç®¡ç†ï¼Œå‡å°‘äº†å› ç¯å¢ƒå˜é‡è¢«æ„å¤–è¦†ç›–å¯¼è‡´çš„ GPU ä¸å¯è¾¾é”™è¯¯ï¼Œé£é™©å¯æ§ã€‚è¯·é‡ç‚¹å…³æ³¨è·¨å¹³å° CI ä¸æ–‡æ¡£åŒæ­¥ã€‚

---

### [Bugfix] Fix interns1-pro initialization and PP (#33793)
**SHA**: `192ad46` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/192ad4648b2066ebdf1fa04ad84f24bdf0cd6533)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¸º `internlm/Internâ€‘S1â€‘Pro` æ·»åŠ æµ‹è¯•è·³è¿‡ï¼Œé¿å… CI å› åˆ†è¯æˆ–å¼ é‡ schema é—®é¢˜æŠ¥é”™ã€‚  
2. åœ¨æ¨¡å‹æ³¨å†Œè¡¨ä¸­æ’¤é”€ `min_transformers_version="5.0.0"` ä¸ `is_available_online=False`ï¼Œä½¿è¯¥æ¨¡å‹åœ¨é»˜è®¤è·¯å¾„ä¸‹å¯è¢«å‘ç°ã€‚  
3. ä¿®æ­£ `interns1_pro` æ¨¡å‹çš„åˆå§‹åŒ–ï¼š  
   - è°ƒæ•´æ³¨æ„åŠ›å±‚å¯¼å…¥è·¯å¾„ã€å»é™¤ä¸å†ä½¿ç”¨çš„ `RoutingMethodType` å‚æ•°ã€‚  
   - æ­£ç¡®è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼ˆ`super(Qwen3MoeForCausalLM, self).__init__()` / `super(Qwen3VLForConditionalGeneration, self).__init__()`ï¼‰ï¼Œé˜²æ­¢ MRO é”™è¯¯ã€‚  
   - å°† FOPEï¼ˆFineâ€‘grainedâ€¯Rotaryâ€¯Positionalâ€¯Embeddingï¼‰å‚æ•°æ˜ å°„æŠ½ç¦»ä¸º `get_frope_params_map`ï¼Œå¹¶åœ¨ `load_weights` ä¸­ä½¿ç”¨è‡ªå®šä¹‰ `WeightsMapper`ã€‚  
   - åœ¨è§†è§‰å­æ¨¡å—åˆå§‹åŒ–æ—¶ï¼Œå®¹é”™å¤„ç† `multimodal_config` ç¼ºå¤±çš„æƒ…å†µã€‚  
4. ä¸º `qwen3_vl` ä¸ `qwen3_vl_moe` åœ¨ Pipeline Parallel (PP) ç¯å¢ƒä¸‹åŠ å…¥ `hasattr` æ£€æŸ¥ï¼Œé˜²æ­¢åœ¨æ²¡æœ‰ `deepstack_visual_indexes` å±æ€§æ—¶è§¦å‘ `AttributeError`ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/models/interns1_pro.py`ï¼ˆæ¨¡å‹æƒé‡åŠ è½½ã€FOPE å‚æ•°æ˜ å°„ã€å±‚åˆå§‹åŒ–ï¼‰ã€‚  
- `vllm/model_executor/models/qwen3_vl*.py`ï¼ˆPP ç¯å¢ƒä¸‹çš„å±‚åˆ‡åˆ†æ£€æŸ¥ï¼‰ã€‚  
- æµ‹è¯•å¥—ä»¶ `tests/models/multimodal/processing/*` ä¸ `tests/models/multimodal/processing/test_tensor_schema.py`ã€‚  
- æ¨¡å‹æ³¨å†Œè¡¨ `tests/models/registry.py`ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æµ‹è¯•**ï¼šç›®å‰å¯¹ Internâ€‘S1â€‘Pro ç›´æ¥ `skip`ï¼Œå»ºè®®åœ¨åç»­æäº¤ä¸­è¡¥å…¨åˆ†è¯ä¸å¼ é‡ schema çš„é€‚é…å®ç°ï¼Œé¿å…é•¿æœŸè·³è¿‡å¯¼è‡´åŠŸèƒ½ç¼ºå¤±ã€‚  
2. **çˆ¶ç±»åˆå§‹åŒ–**ï¼š`super(Qwen3MoeForCausalLM, self).__init__()` å½¢å¼è™½å¯è¿è¡Œï¼Œä½†æ›´ç›´è§‚çš„å†™æ³•æ˜¯ `super().__init__()`ï¼›ç¡®ä¿æ‰€æœ‰å­ç±»å‡ä½¿ç”¨ç»Ÿä¸€é£æ ¼ï¼Œé˜²æ­¢é—æ¼å‚æ•°ã€‚  
3. **FOPE å‚æ•°æ˜ å°„**ï¼š`get_frope_params_map` ä¾èµ–å‚æ•°åä¸­å‡ºç° `rotary_emb.sin_coef` / `cos_coef`ï¼Œè‹¥æœªæ¥ HF ç‰ˆæœ¬æ”¹åï¼Œæ˜ å°„ä¼šå¤±æ•ˆã€‚å»ºè®®åŠ å•å…ƒæµ‹è¯•éªŒè¯æ˜ å°„å®Œæ•´æ€§ã€‚  
4. **è·¯ç”±æ–¹æ³•**ï¼šå»é™¤ `routing_method_type=RoutingMethodType.Renormalize`ï¼Œç¡®è®¤ MoE è°ƒåº¦ä»ä¿æŒæœŸæœ›è¡Œä¸ºï¼Œå¿…è¦æ—¶åœ¨é…ç½®æ–‡ä»¶ä¸­æ˜¾å¼å£°æ˜é»˜è®¤è·¯ç”±ç­–ç•¥ã€‚  
5. **PP é€»è¾‘**ï¼š`hasattr` æ£€æŸ¥æå‡äº†å®¹é”™æ€§ï¼Œä½†è‹¥æ¨¡å‹çœŸçš„ç¼ºå°‘ `deepstack_visual_indexes`ï¼Œå¯èƒ½å¯¼è‡´è§†è§‰å±‚åœ¨éé¦–ä½è¿›ç¨‹è¢«é”™è¯¯åˆ‡åˆ†ã€‚å»ºè®®åœ¨æ–‡æ¡£é‡Œæ˜ç¡®æ­¤å±æ€§çš„å¿…è¦æ€§æˆ–æä¾›å›é€€æ–¹æ¡ˆã€‚  
6. **æ³¨å†Œè¡¨å˜æ›´**ï¼šåˆ é™¤ `min_transformers_version` åï¼Œç¡®ä¿é¡¹ç›®çš„ `requirements.txt` ä¸ CI ç¯å¢ƒä¸­çš„ transformers ç‰ˆæœ¬ä»å…¼å®¹ï¼›è‹¥æœ‰ç‰ˆæœ¬å†²çªï¼Œå¯èƒ½å¯¼è‡´è¿è¡Œæ—¶å¯¼å…¥é”™è¯¯ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æäº¤æˆåŠŸè§£å†³äº† Internâ€‘S1â€‘Pro åœ¨ Pipeline Parallel ç¯å¢ƒä¸‹çš„åˆå§‹åŒ–å´©æºƒï¼Œå¹¶æå‡äº†ä»£ç å¯¹ç¼ºå¤±é…ç½®çš„å®¹é”™èƒ½åŠ›ã€‚åç»­åº”å…³æ³¨æ¢å¤å®Œæ•´æµ‹è¯•è¦†ç›–ä»¥åŠéªŒè¯ FOPE å‚æ•°æ˜ å°„åœ¨ä¸åŒ HF ç‰ˆæœ¬ä¸‹çš„é²æ£’æ€§ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (15)

### [Docs] Add bart-plugin to docs (#33905)
**SHA**: `81a90e5` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/81a90e52776503c6cbdccd30fbe53f61c9179bdf)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨æ’ä»¶ç³»ç»Ÿè¯´æ˜ã€æ”¯æŒæ¨¡å‹è¡¨æ ¼å’Œä½¿ç”¨æŒ‡å—ä¸­åŠ å…¥äº†å®˜æ–¹â€¯bartâ€‘pluginâ€¯ç¤ºä¾‹ï¼Œæ›´æ–°äº†å¯¹ Encoderâ€‘Decoder æ¨¡å‹çš„æ–‡æ¡£è¯´æ˜ã€‚

---

### [Bugfix] Fix corner case of sparse embedding  (#33886)
**SHA**: `1c3a221` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/1c3a221d3b0f7a82cd9a6d56e10ea360e2435a1c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„ / Bugfix  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `special.py` ä¸­å°† `pooled_data.squeeze()` æ”¹ä¸º `squeeze(-1)`ï¼Œé˜²æ­¢åœ¨ç¨€ç–åµŒå…¥çš„è§’è½æ¡ˆä¾‹ï¼ˆå• tokenï¼‰æ—¶é”™è¯¯å»é™¤ç»´åº¦ï¼›æ–°å¢å¯¹åº”çš„å¼‚æ­¥æµ‹è¯• `test_bge_m3_api_server_sparse_embedding_corner_case` éªŒè¯ä¿®å¤ã€‚

---

### [Bugfix] Fix Kimi-K2.5 NVFP4 checkpoints weight loading (#33876)
**SHA**: `a252283` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a2522839d87d2b81b57458dfdbbcb27afb8191ae)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®å¤ Kimiâ€‘K2.5 NVFP4 æ£€æŸ¥ç‚¹æƒé‡åŠ è½½é”™è¯¯ï¼Œæ–°å¢ `name` åˆ¤ç©ºæ£€æŸ¥ï¼›åœ¨æ¨¡å‹ç±»ä¸­åŠ å…¥ `SupportsQuant`ï¼Œå°†æƒé‡æ˜ å°„å™¨æ”¹åä¸º `hf_to_vllm_mapper` å¹¶æ·»åŠ å‰ç¼€å…¼å®¹æ˜ å°„ã€‚

---

### [ROCm][Bugfix][CI] Fix hybrid models and their tests (Mamba/Jamba/Bamba) (#32710)
**SHA**: `3e472e8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/3e472e81f99b5bcf494369ee2d26ee9d6ceeffe3)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¸º ROCm ç¯å¢ƒä¿®å¤æ··åˆæ¨¡å‹ï¼ˆMamba/Jamba/Bambaï¼‰åœ¨ split äº§ç”Ÿçš„éè¿ç»­å¼ é‡å¯¼è‡´çš„ GEMM é”™è¯¯ï¼Œå¹¶åœ¨ç›¸å…³æµ‹è¯•ä¸­åŠ å…¥ ROCmâ€‘ç‰¹å®šçš„ `max_num_seqs` é™åˆ¶ï¼Œä»¥é™ä½æ‰¹æ¬¡æ–¹å·®å½±å“ã€‚

---

### [Bugfix] Kimi-K2 grouped_topk usage for Flashinfer monolithic kernels. (#33858)
**SHA**: `d2f4a71` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d2f4a71cd54418369f617a174e6c839a71a47ed8)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šç®€åŒ– DeepSeekâ€‘V2 MoE åˆå§‹åŒ–ï¼Œç»Ÿä¸€ä½¿ç”¨ `config.n_group` ä¸ `config.topk_group`ï¼Œå»é™¤å†—ä½™åˆ¤æ–­ã€‚

---

### [CI/Build] Fix CPU CI test case title (#33870)
**SHA**: `db6f71d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/db6f71d4c9efc4679b05311c9a8fcc594b187c06)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `.buildkite/hardware_tests/cpu.yaml` ä¸­å°†æ­¥éª¤æ ‡ç­¾ä» **CPU-TP/DP/PP Tests** ä¿®æ”¹ä¸º **CPU-Distributed Tests**ï¼Œä»…æ›´æ–° CI æ­¥éª¤çš„æè¿°ï¼Œä¸å½±å“åŠŸèƒ½ã€‚

---

### [CPU][BugFix] Allow w8a8 oneDNN quantized matmul to support 3D inputs (#33727)
**SHA**: `fd03538` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/fd03538bf97cd7f4fedd6da4584c89635878174f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ CPU é‡åŒ–çŸ©é˜µä¹˜å®ç°ä¸­åŠ å…¥å¯¹ 3D è¾“å…¥çš„ reshape ä¸æ¢å¤ï¼Œä¿®å¤ w8a8 oneDNN é‡åŒ– matmul åœ¨ä¸‰ç»´å¼ é‡ä¸Šçš„é”™è¯¯ã€‚

---

### [CI][AMD][BugFix] Ensure VLLM_ROCM_USE_AITER is set so test_rocm_aiter_topk.py can run correctly (#33840)
**SHA**: `c1395f7` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/c1395f72cd22d97eb39ecd67d9d22f2af3d20bda)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ ROCm æµ‹è¯• `test_rocm_aiter_topk.py` ä¸­åŠ å…¥å¹³å°æ£€æŸ¥ä¸ç¯å¢ƒå˜é‡ `VLLM_ROCM_USE_AITER=1`ï¼Œå¹¶åœ¨ç¼ºå°‘ `aiter` åŒ…æ—¶ç›´æ¥è·³è¿‡ï¼Œç¡®ä¿è¯¥æµ‹è¯•ä»…åœ¨ ROCm ç¯å¢ƒä¸‹ä¸”å·²æ³¨å†Œ AITER ops æ—¶è¿è¡Œã€‚

---

### [docs] fix unintentional misspellings (#33863)
**SHA**: `007b183` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/007b183d745f5b37aeb6cdf936c3b590b0c29fde)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®æ­£äº† `basic.md`ã€`multimodal.md`ã€`quickstart.md` ä¸­çš„æ‹¼å†™é”™è¯¯å’Œç”¨è¯ä¸å½“ï¼Œæå‡æ–‡æ¡£å¯è¯»æ€§ä¸å‡†ç¡®æ€§ã€‚

---

### [release] Minor fixes to release annotation (#33849)
**SHA**: `72bb24e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/72bb24e2db2acd98a49adcb9e3f1dc6f1bbef4c0)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®æ­£ `annotate-release.sh` æ³¨é‡Šæ ¼å¼ï¼Œè¡¥å…¨ CPU é•œåƒæ‹‰å–ï¼Œé‡æ–°ç»„ç»‡ CUDA/ROCm æ ‡ç­¾åŠå¤šæ¶æ„ manifest åˆ›å»ºæµç¨‹ã€‚

---

### [Bugfix] fix DeepSeek R1 with CUTLASS MLA Broken on B200 (#33637)
**SHA**: `a7be77b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a7be77beef5f59d9d349818b4f2860483551b255)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåˆ é™¤ `q_pad_num_heads` å‚æ•°çš„ç›´æ¥ä¼ é€’ï¼Œæ”¹ä¸ºåœ¨å®ç°å±‚ä¸­è·å–ï¼Œä»¥ä¿®å¤ DeepSeek R1 åœ¨ B200 ä¸Šä½¿ç”¨ CUTLASS MLA æ—¶çš„é”™è¯¯ã€‚

---

### [Bugfix] Disable TRTLLM attention when KV transfer is enabled (#33192)
**SHA**: `bbe0574` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/bbe0574d8e51c1c5935aeff9e92040c61d1d59c5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„/ Bugfix  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ FlashInfer æ³¨æ„åŠ›å®ç°ä¸­ï¼Œæ£€æµ‹åˆ°å¼€å¯ KVâ€¯Transfer æ—¶ KV ç¼“å­˜å¯èƒ½éè¿ç»­ï¼Œå¯¼è‡´ TRTLLM æ³¨æ„åŠ›å¤±æ•ˆã€‚æ–°å¢å¯¹ `kv_transfer_config` çš„æ£€æµ‹ï¼Œç¦ç”¨ TRTLLM æ³¨æ„åŠ›å¹¶è®°å½•æ—¥å¿—ï¼Œç¡®ä¿åœ¨ KVâ€¯Transfer åœºæ™¯ä¸‹è¿è¡Œå®‰å…¨ã€‚

---

### Change the type signature of MixtureOfExperts.expert_weights to MutableSequence[Sequence[Tensor]] (#33573)
**SHA**: `ce498a6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ce498a6d61a083b1bbbd1cc92754961b43860ff6)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `MixtureOfExperts.expert_weights` åŠç›¸å…³å‡½æ•°çš„ç±»å‹ç­¾åä» `Iterable[Tensor]` è°ƒæ•´ä¸º `Sequence[Tensor]`ï¼Œå¹¶ç›¸åº”ä¿®æ”¹å¯¼å…¥ã€æ–­è¨€åŠéå†å†™æ³•ï¼Œæé«˜ç±»å‹æ˜ç¡®æ€§å’Œå¯å˜åºåˆ—çš„çº¦æŸã€‚

---

### [Misc] Delay deprecation of CommonAttentionMetadata properties (#33801)
**SHA**: `0e92298` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0e922986222d9c25ce320fbdd0aff20279c2cb93)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `CommonAttentionMetadata` ä¸­çš„å±æ€§å»é™¤ç‰ˆå·æç¤ºï¼Œæ”¹ä¸ºâ€œè¯·å°½å¿«è¿ç§»â€ï¼Œä»¥å»¶è¿Ÿè¿™äº›å±æ€§çš„åºŸå¼ƒæ—¶é—´ã€‚

---

### [Bugfix] Fix ubatch wrapper num_tokens calculate (#33694)
**SHA**: `87d9a26` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/87d9a261664705e0c9635014b4e2d49eddc8a056)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®å¤ `gpu_ubatch_wrapper.py` ä¸­ `num_tokens` è®¡ç®—æ–¹å¼ï¼Œæ”¹ä¸ºå¯¹æ‰€æœ‰ `ubatch_slice` çš„ `num_tokens` æ±‚å’Œï¼Œé¿å…é”™è¯¯çš„å€æ•°è®¡ç®—ã€‚

---

