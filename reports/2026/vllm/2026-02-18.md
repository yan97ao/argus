# 每日更新报告（2026-02-18）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-18 23:42:36 | Robert Shaw | [Model Bash] DeepSeek R1 BF16 Min Latency QKV A GEMM (0.5% E2E Speedup) (#34758) |
| 2026-02-18 19:22:49 | Burkhard Ringlein | Add unit tests for fp8 output fusion of triton_attn (#34228) |
| 2026-02-18 16:49:21 | Nick Hill | [Model Runner V2] Avoid prepare prefill kernel launch overhead (#34780) |
| 2026-02-18 16:35:04 | Cyrus Leung | [Renderer] Deprecate code paths for old input processing (#34775) |
| 2026-02-18 15:43:44 | Asaf Joseph Gardin | [Quantization] - Added uses_meta_device_weights to quant config (#34645) |
| 2026-02-18 15:39:46 | Marek Michalowski | [Bugfix] fix activation in cpu_fused_moe_torch call (#34696) |
| 2026-02-18 15:39:15 | Michael Goin | [Bugfix] Fix prefix creation for Qwen3.5 (#34723) |
| 2026-02-18 15:35:04 | ElizaWszola | [Bugfix] Fix quant RMS norm fusion for quantization with TMA-aligned scales (#33255) |
| 2026-02-18 13:39:07 | Nick Hill | [Model Runner V2] A bit more PP simplification (#34766) |
| 2026-02-18 12:19:11 | Cyrus Leung | [CI/Build] Remove use of `skip_v1` (#34699) |
| 2026-02-18 11:59:53 | Andreas Karatzas | [ROCm][CI] Removed hard-coded attn backend requirement for Qwen VL (#34753) |
| 2026-02-18 11:53:35 | Russell Bryant | [Core] Fix SSRF bypass via backslash-@ URL parsing inconsistency (#34743) |
| 2026-02-18 11:29:15 | Luka Govedič | [torch.compile] Turn on silu+fp4 quant fusion by default for O1+ (#34718) |
| 2026-02-18 11:19:41 | Hongxia Yang | [BugFix] [Build] fix string literals comparison in indexer_k_quant_and_cache calling site (#34653) |
| 2026-02-18 11:18:55 | Cyrus Leung | [Renderer] Move MM Hash parsing into Renderer (#34711) |
| 2026-02-18 10:58:18 | Amr Mahdi | [CI] Remove unused precompiled wheel args from image build (#34767) |
| 2026-02-18 09:06:54 | Matthew Bonanni | [Attention] Refactor `check_and_update_config` (#33600) |
| 2026-02-18 08:27:15 | Wentao Ye | [Feature] Decode Context Parallel support for GPU model runner v2 (#34179) |
| 2026-02-18 07:18:18 | Woosuk Kwon | [Model Runner V2] Further simplification for PP (#34724) |
| 2026-02-18 07:14:30 | Jongseok Park | [Kernel] Triton-based Top-k and Top-p sampler kernels (#33538) |
| 2026-02-18 03:01:27 | Matthew Bonanni | [Bugfix][MTP][Sparse MLA] Allow sparse MLA with MTP to run with FULL cudagraphs (#34457) |
| 2026-02-18 02:42:52 | Flora Feng | [CI] Fix flaky test_parsable_context (#34717) |
| 2026-02-18 01:07:56 | Richard Zou | [BugFix] Fix sp tests (#34716) |

### 📊 统计摘要
> 本日共 23 个提交 | 🔴高 2 | 🟡中 10 | 🟢低 11
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[Model Bash] DeepSeek R1 BF16 Min Latency QKV A GEMM (0.5...](#6874638)
    - [[Kernel] Triton-based Top-k and Top-p sampler kernels (#3...](#c656ba3)
  - [🟡 中重要度变更 (10)](#-🟡-中重要度变更-10)
    - [Add unit tests for fp8 output fusion of triton_attn (#34228)](#e24663c)
    - [[Renderer] Deprecate code paths for old input processing ...](#a766b30)
    - [[Quantization] - Added uses_meta_device_weights to quant ...](#1faa8cb)
    - [[Bugfix] Fix quant RMS norm fusion for quantization with ...](#a88b3be)
    - [[CI/Build] Remove use of `skip_v1` (#34699)](#30ebe0d)
    - [[Core] Fix SSRF bypass via backslash-@ URL parsing incons...](#6f3b204)
    - [[Renderer] Move MM Hash parsing into Renderer (#34711)](#a0d8d94)
    - [[Attention] Refactor `check_and_update_config` (#33600)](#7743152)
    - [[Feature] Decode Context Parallel support for GPU model r...](#ab33d2a)
    - [[Model Runner V2] Further simplification for PP (#34724)](#be3af2d)
  - [🟢 低重要度变更 (11)](#-🟢-低重要度变更-11)
    - [[Model Runner V2] Avoid prepare prefill kernel launch ove...](#c50e105)
    - [[Bugfix] fix activation in cpu_fused_moe_torch call (#34696)](#e89a91d)
    - [[Bugfix] Fix prefix creation for Qwen3.5 (#34723)](#909b147)
    - [[Model Runner V2] A bit more PP simplification (#34766)](#a49ea5a)
    - [[ROCm][CI] Removed hard-coded attn backend requirement fo...](#cef65f0)
    - [[torch.compile] Turn on silu+fp4 quant fusion by default ...](#02e8f26)
    - [[BugFix] [Build] fix string literals comparison in indexe...](#4a00a51)
    - [[CI] Remove unused precompiled wheel args from image buil...](#df3f537)
    - [[Bugfix][MTP][Sparse MLA] Allow sparse MLA with MTP to ru...](#dc5fa77)
    - [[CI] Fix flaky test_parsable_context (#34717)](#1e4a084)
    - [[BugFix] Fix sp tests (#34716)](#7967e85)
#### 🔴 高重要度变更 (2)

### [Model Bash] DeepSeek R1 BF16 Min Latency QKV A GEMM (0.5% E2E Speedup) (#34758)
**SHA**: `6874638` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6874638bc443ae99ee900072e0bb039fa5f7f0e7)

**🎯 变更类型**：功能增强 / 性能优化  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 在 CUDA 构建链中新增 **DeepSeek V3 fused A GEMM** kernel（`csrc/dsv3_fused_a_gemm.cu`），仅在 SM 9.0+（Hopper 及以后）上编译，提供 BF16‑only、1‑16 token 场景的最小延迟矩阵乘。  
2. 为该 kernel 暴露 C++ 接口 `dsv3_fused_a_gemm`，并通过 `torch.ops._C` 注册为自定义 PyTorch OP。  
3. 在 Python 层添加包装函数 `vllm._custom_ops.dsv3_fused_a_gemm`，并在 DeepSeek‑V2 模型的 QKV‑A 投影实现 `DeepSeekV2FusedQkvAProj` 中做运行时检测与透明回退。  

**🎯 影响范围**  
- **GPU 代码生成**：`csrc/dsv3_fused_a_gemm.cu`、CMake 构建脚本。  
- **自定义算子注册**：`csrc/ops.h`、`csrc/torch_bindings.cpp`。  
- **Python 绑定层**：`vllm/_custom_ops.py`、`vllm/model_executor/layers/mla.py`。  
- **模型实现**：`vllm/model_executor/models/deepseek_v2.py`（新增 `DeepSeekV2FusedQkvAProj`）。  
- **仅在 CUDA 环境**（非 ROCm）启用；不影响已有 CPU/ROCm 路径。

---

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | - 新增的 kernel 采用 **Hopper**（SM 9.0）以及 CUDA 13+ 的 **cp.async**、**mbarrier**、**HMMA** 指令，实现 4‑stage 双缓冲和 128 Byte 预取。<br>- 通过 `-DENABLE_DSV3_FUSED_A_GEMM=1` 编译标记，仅在满足 `CUDA_ARCHS` 包含 `9.0a/10.0/11.0` 时加入。<br>- 在模型代码中通过 `DeepSeekV2FusedQkvAProj` 进行运行时检测，若不满足条件则回退到原有 `MergedColumnParallelLinear` 实现，保持向后兼容。 |
| **性能影响** | - 对 **DeepSeek‑V2/V3** 的 QKV‑A 投影（`[num_tokens, 7168] × [7168, 2112]`）在 **1‑16** token 小批量时可实现 **≈0.5% End‑to‑End 加速**，主要来自降低 kernel 启动延迟、批次内部共享‑SMEM 双缓冲以及 PDL（Program‑Dependent‑Launch）并行化。<br>- 动态共享内存需求约 **< 200 KB**（`stage_cnt` 动态计算），在 48 KB 以下的 GPU 仍可运行，但会触发 `cudaFuncSetAttribute` 设置更大的 `MaxDynamicSharedMemorySize`。 |
| **安全考虑** | - 仅在 BF16、SM ≥ 9.0 的设备上启用；不涉及内存越界或未检查的指针。<br>- 环境变量 `TRTLLM_ENABLE_PDL` 控制是否开启 PDL，若误设为非 ‘1’ 仍安全，只是失去 PDL 优化。<br>- 编译时对 `USE_ROCM` 做了 `#ifndef` 条件屏蔽，避免在 ROCm 环境编译出不兼容代码。 |
| **可维护性** | - 核心实现来源于 NVIDIA/TensorRT‑LLM 与 sgl‑project，已带有完整注释，代码量 ~750 行，单独文件易于定位。<br>- Python 层包装保持与原有自定义 OP 接口一致，调用方式不变，便于单元测试和 CI。<br>- 新增的 CMake 条件逻辑稍显复杂，建议在 `CMakeLists.txt` 中加入注释解释 `cuda_archs_loose_intersection` 的意义。 |

---

**⚠️ 潜在风险**  

1. **硬件兼容性**  
   - 若用户在不支持 SM 9.0（如 V100、A100）上运行，`dsv3_fused_a_gemm` 将不会被编译；但在运行时仍可能因 **`hasattr(self, "weight")`** 判断失误而进入该路径，导致 `cudaErrorInvalidDeviceFunction`。当前实现通过 `self._use_min_latency_gemm` 检查设备能力，风险已降至最低。  

2. **共享内存超限**  
   - `stage_cnt` 随 `gemm_k / tile_k` 变化，最坏情况（`tile_k=256`、`gemm_k=7168`）约 **13 stage**，共享内存约 **190 KB**。在显存 48 KB SM（旧显卡）上会触发 `cudaFuncSetAttribute` 警告或运行时错误。  

3. **Env‑PDL 误用**  
   - `TRTLLM_ENABLE_PDL` 若被误设为非 `'1'`，PDL 将被禁用，导致潜在的性能回退，但不影响正确性。  

4. **输入形状/布局错误**  
   - 该 kernel 假设 **`mat_a` 行主序、`mat_b` 列主序**（即 `weight.T`），若外部代码传入转置错误的张量会产生 **数据错位**，而不会抛出显式错误。  

5. **编译时间与二进制体积**  
   - 新增的 CUDA 源文件和编译标记会稍微增加编译时间和最终库体积，尤在 CI 环境中可能导致超时。  

---

**💡 关注建议**  

1. **单元/集成测试**  
   - 为 `dsv3_fused_a_gemm` 新增 **shape‑检查** 与 **SM 版本检查** 的单元测试，覆盖 1、8、16 token 边界。  
   - 在 CI 中加入 **Hopper 模拟器**（或使用 `cuda-memcheck`）验证共享内存尺寸与 barrier 使用的正确性。  

2. **运行时保护**  
   - 在 `dsv3_fused_a_gemm` 前添加 `TORCH_CHECK(getSMVersion() >= 90, ...)` 与 `TORCH_CHECK(num_tokens <= 16, ...)`，防止误用。  
   - 若 `stage_cnt` 计算后需要的共享内存 > 48 KB，提前 `TORCH_CHECK` 给出友好错误提示。  

3. **文档与使用指引**  
   - 在项目 README 或模型说明中明确标注 **“仅在 Hopper (SM 9.0+) + BF16 环境、token ≤ 16 才会生效”。**  
   - 说明 `TRTLLM_ENABLE_PDL` 环境变量的意义与默认行为。  

4. **代码维护**  
   - 将 `CMakeLists.txt` 中的 arch 判定逻辑抽取为函数或变量，便于后续加入更高

---

### [Kernel] Triton-based Top-k and Top-p sampler kernels (#33538)
**SHA**: `c656ba3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c656ba3b4d2cda82ca753eefde4b10cbf04c0a3f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 在 `vllm/v1/sample/ops` 中新增基于 Triton 的 **Top‑k / Top‑p 采样 kernel**，实现了通过二分/三分搜索+高斯近似的快速截断算法。  
- 改造 `Sampler` 使其在 batch >= 8 且环境支持 Triton 时自动使用该 kernel，否则回退到原有 PyTorch 排序实现。  
- 新增大量单元测试、benchmark 脚本以及对 `math_utils` 的小幅简化，以验证在各种极端‑inf、重复 logits、极端 k/p 值等场景下的正确性和数值稳定性。  

**🎯 影响范围**：  
- `vllm/v1/sample/ops/topk_topp_triton.py`（全新 Triton kernel）  
- `vllm/v1/sample/ops/topk_topp_sampler.py`（调度逻辑、`apply_top_k_top_p` 与 `apply_top_k_top_p_pytorch`）  
- `vllm/triton_utils.py`（TRITON 可用性标记）  
- 单元测试 `tests/v1/sample/test_topk_topp_sampler.py`（Triton‑specific 测试）  
- 基准脚本 `benchmarks/benchmark_topk_topp.py`（对比 Triton 与 PyTorch 实现）  
- `vllm/utils/math_utils.py`（`next_power_of_2` / `prev_power_of_2` 合并简写）  

---

### 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | - 引入 **Triton** 作为可选后端，新增了两层缓存：<br> 1️⃣ ` _TRITON_TABLE_CACHE`（常量表）<br> 2️⃣ `_TRITON_BUFFER_CACHE`（临时 buffer），分别以 device、dtype、vocab 为键进行复用。<br>- `Sampler` 现在在运行时检查 `HAS_TRITON` 并依据 batch 大小决定使用哪条路径，保持向后兼容。 |
| **性能影响** | - Triton kernel 通过 **pivot‑based 截断**（基于 Gaussian 统计估计 top‑k/‑p 的阈值）避免全局排序，理论复杂度从 `O(V log V)` 降至 `O(V)` 并大幅减少内存搬运。<br>- 代码在 `apply_top_k_top_p` 中对 **小 batch (<8)** 直接走 PyTorch 实现，防止 launch 开销逆转收益。<br>- `benchmark_topk_topp.py` 已提供多批次/词表规模基准，可观察到在 32k–128k 词表、batch ≥ 64 时常见 **2×‑5×** 的加速。 |
| **安全考虑** | - 新增的 Triton kernel 只在 **CUDA** 环境下执行，不涉及网络、文件系统等外部资源，安全风险有限。<br>- 为防止 **NaN**、**-inf** 传播， kernel 在计算均值/方差、pivot 搜索时显式过滤 `-inf` 并在全部为 `-inf` 时将 `min_logit = max_logit`，保证搜索收敛到 `-inf`（即不掩码）。<br>- 对极端 `k`、`p`、重复 logits 等情况加入 guard（`final_pivot >= max_logit` → 直接不掩码），避免意外过滤全部 token。 |
| **可维护性** | - 代码量约 1 k 行，逻辑集中在单一 Triton kernel，内部使用大量局部变量与分支，阅读门槛较高。<br>- 通过 **`apply_top_k_top_p`** 统一入口，将 Triton 与 PyTorch 实现解耦，后续可单独升级或替换。<br>- 单元测试覆盖了 top‑k、top‑p、组合、全部‑inf、极少‑finite、相同 logits、近似相同 logits 等八类场景，降低回归风险。 |
| **兼容性** | - 仅在 **CUDA + Triton** 环境下生效，未检测到 Triton 时保持原有行为。<br>- 对 ROCm/HIP 保持 `forward_hip`（未改动）路径不受影响。<br>- `apply_top_k_top_p_pytorch` 现在接受 `allow_cpu_sync` 标记，防止在 CPU 运行时误调用 GPU‑sync 的 top‑k 实现。 |

---

### ⚠️ 潜在风险

1. **依赖 Triton 版本**  
   - Triton API 变化或安装错误会导致运行时 `ImportError`。需要在文档/CI 中明确最低兼容版本。  
2. **GPU‑sync 开销**  
   - `apply_top_k_only` 为 GPU→CPU 的 `torch.topk`，在启用 Triton 但 batch < 8 时仍会走该路径，可能导致异步调度性能下降。  
3. **数值偏差**  
   - Triton 采用 **近似 pivot**（高斯估计 + ternary search），在极端概率分布（如大量相同 logits）下可能产生轻微的概率偏差。虽已加入容忍阈值（≤ 0.5%），但在对采样精度极端敏感的业务（如对话安全过滤）需额外验证。  
4. **内存缓存泄漏**  
   - `_TRITON_BUFFER_CACHE` 按 device、dtype、vocab 大小缓存，若连续创建不同 vocab 大小的模型，缓存会逐渐膨胀至 `num_sm * max_vocab`，可能占用显存。需要在进程退出或模型卸载时清理。  
5. **多进程/分布式环境**  
   - 缓存是进程级全局变量，跨进程（如 torch.multiprocessing）会分别创建副本，理论安全，但在 **fork** 场景下可能出现未初始化的 Triton context。  

---

### 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | 1️⃣ 在 `setup.cfg` / `requirements.txt` 中声明 `triton>=2.0`（或项目当前使用的版本）为 **可选依赖**。<br>2️⃣ 提供 `vllm.enable_triton(False)` 之类的 API，让用户显式关闭 Triton，便于调试/兼容性排查。<br>3️⃣ 在 `apply_top_k_top_p` 中记录一次性日志（`logger.info`）说明是使用 Triton 还是 PyTorch，实现可观测性。 |
| **CI / 测试** | - 将 Triton‑based 测试标记为 `cuda+triton`，在 CI 中使用具备对应 GPU 的 runner。<br>- 加入 **statistical equivalence** 测试：对随机 logits 运行 10 k 次，两实现的采样分布 KS‑test < 0.01。 |
| **运维 / 部署** | - 在生产镜像中预装 Triton，或提供 “no‑triton” 轻

---

#### 🟡 中重要度变更 (10)

### Add unit tests for fp8 output fusion of triton_attn (#34228)
**SHA**: `e24663c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e24663c5a958f9ad2cf787ba2e9b1da0ba558768)

**🎯 变更类型**：功能增强（新增单元测试）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `tests/kernels/attention/test_triton_unified_attention.py` 中加入 127 条测试代码，专门验证 Triton‑实现的统一注意力算子在 **FP16 输入 → FP8 输出** 场景下的正确性与数值尺度融合（`output_scale`）。测试覆盖多组 head‑size、head‑num、块大小、滑动窗口、软上限、块数量、3‑D 序列阈值等组合，并使用 `ref_paged_attn` 作为参考实现进行对比，容忍度设为 `atol=rtol=2e‑1`。

---

### 核心变更

| 位置 | 关键改动 | 含义 |
|------|----------|------|
| `NUM_HEADS` | 新增 `(5, 1)` 组合 | 验证非整除的 query/k​v 头数比例（仍满足 `num_query_heads % num_kv_heads == 0`） |
| 新增 `FP8_DTYPE = current_platform.fp8_dtype()` | 动态获取当前平台支持的 FP8 数据类型（`torch.float8_e4m3fnuz` 或 `None`） | 仅在支持 FP8 的 CUDA/ROCM 环境下运行 |
| `test_triton_unified_attn_fp16_input_fp8_output` | 完整的参数化测试函数 | - 输入保持 FP16，输出分配为 FP8 <br> - 通过 `output_scale`（0.5）完成 FP8→FP16 的数值恢复 <br> - 与参考实现 `ref_paged_attn` 对比，容差放宽到 0.2，防止 FP8 精度限制导致误报 |
| 软化段（softmax seg）相关张量的创建 | 为 3‑D softmax 分段路径准备辅助 buffer | 保证即使在 `seq_threshold_3D` 触发 3‑D 实现时仍能正常工作 |
| `unified_attention` 调用参数 | 新增 `output_scale`、`softmax_segm_*` 等 | 直接测试算子内部的 FP8 融合路径与分段 softmax 逻辑 |

### 受影响范围

| 模块 | 影响描述 |
|------|----------|
| `vllm.v1.attention.ops.triton_unified_attention` | 新增对 `output_scale` 与 FP8 输出的调用路径进行覆盖。若算子内部实现对 FP8 支持不完整或在特定硬件上缺失，测试会直接报错。 |
| `vllm.utils.torch_utils` | 通过 `current_platform.is_rocm()` 与 `fp8_dtype()` 判断平台兼容性，若平台不支持 FP8，测试会被自动跳过。 |
| CI / 测试框架 | 该文件引入 **大量参数化组合**（约 2 k+ 运行实例），可能显著延长 CI 时长；建议在 CI 中使用 `-k fp8` 或类似标签仅在具备 FP8 GPU 的 runner 上执行。 |

### 关注建议

1. **硬件依赖**：FP8 仅在 Ampere+（CUDA 11.8+）或相应 ROCm 设备上可用。若 CI 环境不具备这些硬件，测试会被标记为 `skip`，但在本地开发时请确认 GPU 支持 `torch.float8_e4m3fnuz`。  
2. **数值容差**：目前容差设为 `2e‑1`（20%），在极端序列长度或大块数下仍可能出现误差。若未来算子精度提升，可适当收紧 `atol/rtol`，并在 CI 中监控误差分布。  
3. **性能与资源**：参数组合导致 **≈ 2500 次** 实例执行，且每次会分配多个 3‑D softmax 缓冲区（每个 `seq_threshold_3D` 大小）。在资源受限的机器上运行可能 OOM，请根据实际显存情况调低 `NUM_HEADS`/`HEAD_SIZES` 或 `NUM_BLOCKS`。  
4. **未来扩展**：若计划加入 **FP8 输入 → FP8 输出** 或 **INT8** 路径，建议基于本测试模板添加相应的参数化组合，保持测试覆盖一致性。  
5. **代码维护**：当前测试直接使用 `torch.inference_mode()`，确保不触发 autograd；若算子内部出现梯度相关改动（如训练支持），需补充 `torch.enable_grad()` 的对应对照测试。  

总体而言，此提交通过系统化的参数化组合，提升了 Triton‑Attention 在 FP8 融合路径上的可靠性验证，对后续在 FP8 量化部署（如 LLM 推理加速）具有重要支撑。请在具备 FP8 GPU 的 CI runner 上开启该测试，并关注其运行时显存与时长消耗。

---

### [Renderer] Deprecate code paths for old input processing (#34775)
**SHA**: `a766b30` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a766b303497bb743defee3f7d0b3477f13477418)

**🎯 变更类型**：功能增强 / 向后兼容性清理（Deprecation）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将旧的输入处理路径逐步废弃：`Platform.validate_request` 的 `prompt` 参数、`tokenization_kwargs`、`truncate_prompt_tokens` 等全部转移到渲染器层。  
- 删除 `tokenization_kwargs` 相关调用，统一通过 `Renderer.render_*` 传递 token 化选项。  
- 对 `EngineCoreRequest` 直接传入 `LLMEngine.add_request` / `AsyncLLM.add_request` 给出弃用警告，计划在 `v0.18` 移除。  
- 文档同步，新增 `prompt` 参数在 `Platform.validate_request` 中的弃用说明。  

**🎯 影响范围**  
- **入口层**：`vllm/entrypoints/llm.py`（参数签名与调用删减）  
- **平台接口**：`vllm/platforms/interface.py`（validate_request 重签名）  
- **异步引擎**：`vllm/v1/engine/async_llm.py`（兼容旧 `EngineCoreRequest`、警告）  
- **同步引擎**：`vllm/v1/engine/llm_engine.py`（同上）  
- **输入处理器**：`vllm/v1/engine/input_processor.py`（合并平台验证、发出兼容警告）  
- **渲染器**：`vllm/renderers`（不再需要 `tokenization_kwargs`）  

**💡 关注建议**  
1. **用户**：尽快改用 `Renderer.render_cmpl()` / `Renderer.render_chat()` 生成的 `ProcessorInputs`，并在调用 `LLMEngine.generate`、`AsyncLLM.generate` 时去掉 `tokenization_kwargs`、`prompt` 参数。  
2. **插件/平台实现者**：更新 `Platform.validate_request` 实现，仅接受 `(processed_inputs, params)`，移除对 `prompt` 的依赖。若仍需兼容，可参考 `input_processor.py` 中的兼容包装。  
3. **内部开发**：检查所有自定义插件、LoRA/LoRA‑request 代码是否仍显式使用 `truncate_prompt_tokens`，改为通过渲染器的 `tokenization_kwargs`（随后也会被去除）。  
4. **测试**：运行完整单元/集成测试，确保在无 `tokenization_kwargs`、`prompt` 参数的情况下仍能通过。  
5. **文档**：关注即将发布的 v0.18 迁移指南，更新使用示例。  

及时完成上述迁移可避免在 v0.18 发布后出现 `TypeError` 或功能缺失。

---

### [Quantization] - Added uses_meta_device_weights to quant config (#34645)
**SHA**: `1faa8cb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1faa8cb73cad1346224284002c136fb060dc89c9)

**🎯 变更类型**：功能增强（为在线量化加入 meta‑device 权重创建）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `QuantizeMethodBase` 中新增 `uses_meta_device` 标记，默认 `False`。Fp8 在线量化实现 (`Fp8OnlineLinearMethod`、`Fp8OnlineMoEMethod`) 将该标记设为 `True`，表明它们会在 meta 设备上先创建权重，再在 `process_weights_after_loading` 中逐层量化。`initialize_dummy_weights` 依据该标记判断是否需要跳过对 meta 参数的 dummy 初始化，避免在加载阶段产生不必要的内存占用。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/quantization/base_config.py`（基类）  
- `vllm/model_executor/layers/quantization/fp8.py`（Fp8 在线量化实现）  
- `vllm/model_executor/model_loader/weight_utils.py`（权重初始化逻辑）  

**💡 关注建议**  
1. **代码审阅**：确认所有新加入的 `uses_meta_device` 标记在未来新增的在线量化实现中被正确设置，防止遗漏导致仍在 CPU/GPU 上创建大尺寸权重。  
2. **兼容性测试**：在非在线量化（如离线量化或纯 fp16）路径下，`initialize_dummy_weights` 仍应如旧行为工作，建议加入单元测试覆盖两种分支。  
3. **文档更新**：在量化配置说明中加入 “meta device 权重创建” 的概念，提醒用户在使用 `fp8` 在线量化时可获得显著的加载时内存峰值下降。  
4. **性能监控**：在大模型加载（> 70B）时对比加载前后显存峰值，确保 `process_weights_after_loading` 能正确触发且不会出现遗漏的 meta 参数。  

总体而言，此次改动通过在模型加载阶段把权重保持在 meta 设备上，降低了峰值显存，属于一次重要的内存优化功能增强。保持对新标记的完整覆盖与文档同步，可确保未来量化后端的平滑扩展。

---

### [Bugfix] Fix quant RMS norm fusion for quantization with TMA-aligned scales (#33255)
**SHA**: `a88b3be` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a88b3be7c4c494c896f4a88ca8e6bfc1083625e0)

**🎯 变更类型**：功能增强（修复 RMSNorm‑Quant 融合在 TMA‑aligned scale 情形下的错误）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 C++/CUDA 接口中为 `per_token_group_quant_fp8`、`rms_norm_per_block_quant` 等函数新增 `dummy_is_scale_transposed`、`dummy_is_tma_aligned` 参数，并在内部传播 `outer_scale_stride` 以支持外层 stride 为 1 的转置/对齐情形。  
2. 计算尺度时改用 `scale_rows = (gridDim.x+stride‑1)/stride*stride`，确保 TMA‑aligned（stride = 4）时写入正确的行数。  
3. Python 层 (`_custom_ops.rms_norm_per_block_quant`) 根据 `tma_alignment` 动态构造转置或 TMA‑aligned 的 `scales` 张量。  
4. 编译器融合匹配器(`MatcherQuantFP8`、`FusedAddRMSNormGroupQuantPattern`、`RMSNormGroupQuantPattern`) 增加 `is_tma_aligned` 标记，匹配时把 dummy 参数传给底层 op。  
5. 单元测试扩展至不同 `tma_alignment`、`group_size`、`scale_ub` 场景，并在 DeepGEMM 环境下做跳过处理。  

**🎯 影响范围**  
- `csrc/ops.h`、`csrc/quantization/*`（CUDA kernel）  
- `vllm/_custom_ops.py`、`vllm/compilation/passes/fusion/*`（融合匹配与 pattern 注册）  
- `vllm/model_executor/layers/quantization/utils/fp8_utils.py`（调用入口）  
- 相关测试 `test_fused_quant_layernorm.py`、`test_tp1_quant.py`  

**💡 关注建议**  
1. **兼容性**：默认 `dummy_is_scale_transposed=False、dummy_is_tma_aligned=False`，保持现有路径不变；若有外部代码直接调用 `per_token_group_fp8_quant`，请检查是否需要显式传入这两个新参数。  
2. **性能验证**：TMA‑aligned 路径在 Hopper/Blackwell 上可利用 `wmma`‐style load，建议在真实模型推理中对比吞吐量，确认 `outer_scale_stride` 计算不导致额外填充。  
3. **文档更新**：在 Ops/FP8 章节说明 `tma_alignment` 参数含义、取值范围（0 / 4）以及何时需要开启（group‑wise FP8、scale 转置为列主序）。  
4. **测试覆盖**：CI 已加入 `tma_alignment` 参数组合，确保多卡、不同 `group_size`、`scale_ub` 情况全覆盖；如有新增自定义 kernel，务必在 `tests/kernels/core` 增加对应 case。  
5. **异常处理**：`rms_norm_per_block_quant` 中对 `scales.stride(1) > 1` 作了断言，若后续实现支持非转置 stride，需同步更新此检查。  

总体来看，此次改动解决了 TMA‑aligned scale 与 RMSNorm‑Quant 融合失配的问题，提升了在 Hopper/Blackwell 上的量化性能；请在发布前完成跨平台（CUDA 12.x、ROCm）回归，确保新参数不会破坏已有的 CUDA‑only 工作流。

---

### [CI/Build] Remove use of `skip_v1` (#34699)
**SHA**: `30ebe0d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/30ebe0dc3c24e016b5e6f2f4532939867719097e)

**🎯 变更类型**：CI/Build 调整 + 测试标记删减（功能提升 / 维护性改进）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在多处 Buildkite CI 配置文件中加入对 `tests/detokenizer` 的依赖并显式运行对应测试，同时统一去除 `skip_v1` 标记，将原本排除在 V1 运行的测试全部恢复。`pyproject.toml` 中也删除了 `skip_v1` 标记的声明，`tests/detokenizer/test_disable_detokenization.py` 相应移除装饰器。

**🎯 影响范围**  
- CI 配置（`.buildkite/*.yaml`）——所有 AMD、CPU、sampler、misc 等测试流水线。  
- `pyproject.toml` 中的 pytest markers 配置。  
- `tests/detokenizer/` 目录下的单元测试以及任何使用 `skip_v1` 标记的测试。

**💡 关注建议**  
1. **兼容性确认**：`skip_v1` 原本用于防止在 V1 引擎上运行不兼容的测试，删除后请确保这些测试在 V1 环境下仍然通过，或在 CI 中明确只跑 V2。建议在本地或 CI 中跑一次完整的 V1/ V2 对比，以防潜在回归。  
2. **CI 稳定性**：新增对 `detokenizer` 的显式调用会延长流水线时长，确认该目录下的测试耗时在可接受范围内，必要时可考虑分片或并行。  
3. **文档同步**：README、CONTRIBUTING 或 pytest marker 文档需要移除 `skip_v1` 的说明，避免新贡献者误用。  
4. **代码清理**：若项目中还有其他使用 `skip_v1` 的地方（如自定义插件或 CI 脚本），请一并清理，以防残留导致 “unknown marker” 警告。  
5. **回归测试**：在下一个发布前执行全套 `pytest -m samplers`、`detokenizer`、`multimodal` 等组合，确认所有测试在不同硬件（CPU、GPU、AMD）上均通过。

总体来看，此次改动提升了 CI 的测试覆盖度，去除过时的 `skip_v1` 标记提升了代码库的整洁度，但需要额外关注 V1 兼容性和 CI 时长。建议在合并前做一次完整的全平台回归，以确保改动不会引入隐藏的破坏。

---

### [Core] Fix SSRF bypass via backslash-@ URL parsing inconsistency (#34743)
**SHA**: `6f3b204` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6f3b2047abd4a748e3db4a68543f8221358002c0)

**🎯 变更类型**：Bug修复（安全漏洞）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交修复了 MediaConnector 在 URL 解析时的 SSRF 绕过漏洞。`\@` 形态的 URL 会在 urllib3 与 aiohttp/yarl 之间产生解析不一致，导致 `allowed_media_domains` 检查失效。现在在发起请求前统一使用 urllib3 解析后的 `url_spec.url`，并在同步/异步路径上均做同样处理。新增两组测试验证：①后端仍访问本地资产服务器但路径被编码，抛出 HTTP 错误；②当攻击者控制的域名位于 urllib3 解析的主机位置时直接触发 `ValueError`。  

**🎯 影响范围**  
- `vllm/multimodal/media/connector.py`（URL 规范化逻辑）  
- 测试套件 `tests/multimodal/media/test_connector.py`（新增安全相关单元测试）  
- 可能影响所有使用 `MediaConnector` 进行图片/视频拉取的业务模块。  

**💡 关注建议**  
1. **代码审查**：确认 `url_spec.url` 在所有入口均已替换，避免遗漏导致旧实现残留。  
2. **文档**：在媒体拉取章节注明对 URL 的严格校验规则及已修复的 `\@` 绕过情形。  
3. **安全审计**：进一步检查其他特殊字符（如 `%2f`、空字符）在不同解析库之间是否仍存在差异。  
4. **回归测试**：在 CI 中启用新加入的 async/ sync SSRF 测试，确保未来改动不再破坏此检查。  
5. **部署监控**：上线后监控异常外部请求日志，验证该漏洞已彻底关闭。  

---

### [Renderer] Move MM Hash parsing into Renderer (#34711)
**SHA**: `a0d8d94` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a0d8d944e2659cedd52bccef63fbc7f764be4cf6)

**核心变更概览**  
本次 PR 将 **多模态 UUID** 的解析与归一化从渲染器内部抽离为 `vllm.multimodal.parse.parse_mm_uuids`，并新增类型别名 `MultiModalUUIDItems`（已归一化为 `dict[str, Sequence[str|None]]`）。随后，几乎所有涉及多模态处理的入口（`Renderer._process_multimodal`、`InputsPreprocess._process_multimodal`、各模型的 `apply`/`_cached_apply_hf_processor`）统一改为接受 **mm_uuid_items** 而非原来的 `MultiModalUUIDDict`，并在内部统一使用 `mm_data_items` 与 `mm_uuid_items`。  
同时：

* `vllm/multimodal/inputs.py` 将 `MultiModalUUIDDict` 的值类型从 `list[str|None]` 扩展为 `Sequence[str|None]`，便于兼容 tuple、numpy array 等。  
* `vllm/multimodal/parse.py` 增加 `parse_mm_uuids` 与 `MultiModalUUIDItems`，并在 `MultiModalDataItems` 中加入 `select` 辅助方法。  
* 处理流程中加入对 **空数据项 vs. 缺失 UUID** 的细粒度校验，保证 `None` 项必须对应显式 `None` UUID。  
* 渲染器 `_process_multimodal` 现在先调用 `parse_mm_uuids`，再依据 `mm_uuid_items` 生成默认 UUID（仅在缓存关闭且前缀缓存关闭时生效）。  

**受影响范围**  
- 渲染层 `vllm/renderers/base.py`、`tests/renderers/test_process_multi_modal_uuids.py`  
- 输入预处理 `vllm/inputs/preprocess.py`  
- 所有多模态模型实现：`clip.py`, `deepseek_vl2.py`, `h2ovl.py`, `llava.py`, `paligemma.py`, `pixtral.py`, `siglip.py`, `terratorch.py`, `transformers/multimodal.py`, `voxtral.py`  
- 多模态解析与处理核心 `vllm/multimodal/parse.py`、`vllm/multimodal/processing/processor.py`  

**关注点与建议**  

1. **向后兼容**  
   - 仍保留 `MultiModalUUIDDict` 类型别名，但多数实现已改为 `MultiModalUUIDItems`。若外部库直接使用旧签名，可能产生 `TypeError`。建议在 `__init__.py` 中提供兼容包装或发出 deprecation 警告。  

2. **默认参数处理**  
   - 多处将可选 `hf_processor_mm_kwargs`、`tokenization_kwargs` 的默认值改为 `None` 并在内部自行初始化，避免了 mutable‑default 参数问题。请确认所有调用路径已相应更新（包括用户自定义的插件）。  

3. **性能与内存**  
   - `MultiModalDataItems.select` 在每次 HF 处理前都会复制子字典。若数据规模大（如视频帧），复制成本可能不可忽视。可以考虑在 `select` 中返回视图或直接在 `processor` 里过滤，而不是完整拷贝。  

4. **异常信息**  
   - 新增的校验在 `Renderer._validate_mm_uuids` 中已经非常细致，但错误信息仍使用硬编码字符串，建议抽取为常量或使用统一的错误构造函数，以便后续本地化或统一日志。  

5. **单元测试**  
   - 已更新 `tests/renderers/test_process_multi_modal_uuids.py`，但仍缺少针对 `parse_mm_uuids` 本身的独立测试（包括空 dict、单字符串、混合类型）。建议补充，以防止未来对 `parse_mm_uuids` 的改动破坏行为。  

6. **文档同步**  
   - 新增类型 `MultiModalUUIDItems` 与 `parse_mm_uuids` 必须在 API 文档中标注为推荐使用方式，旧 `mm_uuids` 参数应标记为已废弃。  

7. **线程安全**  
   - `Renderer._mm_req_counter.inc(1)` 仍在多线程环境下使用，若 `mm_uuid_items` 的默认生成逻辑被并发调用，需确认计数器为原子操作（目前使用 `AtomicInteger`，但请再次核实）。  

**结论**  
此次改动显著提升了多模态 UUID 处理的解耦与可测试性，统一了内部数据结构并强化了校验逻辑。只要确保向后兼容、验证性能开销并补齐文档和独立测试，整体迁移风险较低，可在下一次发布中推广。

---

### [Attention] Refactor `check_and_update_config` (#33600)
**SHA**: `7743152` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7743152957236f21fc36f0402f9678159976ccc5)

**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：对 KV‑cache 的 `block_size` 配置与注意力后端的关联逻辑进行全面重构。将原先在 `CudaPlatform.check_and_update_config` 中硬编码的后端‑块大小映射抽象为统一的后端选择流程，并新增 `select_attention_backend` 与 `_update_block_size_for_backend` 两个辅助方法。`CacheConfig.block_size` 类型从受限的 `Literal` 放宽为 `int`，并在平台检查时统一统一设置默认值。后端实现类现在提供 `get_preferred_block_size` 与 `supports_block_size`，去除对 `BlockSize` Literal 的依赖。

**🎯 影响范围**  
- `vllm/config/cache.py`（`CacheConfig`、`BlockSize` 类型去除）  
- `vllm/engine/arg_utils.py`（`EngineArgs.block_size` 类型改为 `int`）  
- `vllm/platforms/cuda.py`（核心配置检查、块大小自动校准、后端选择抽象）  
- `vllm/v1/attention/backend.py`（后端支持判断、首选块大小计算）  

**💡 关注建议**  

1. **兼容性**：`CacheConfig.block_size` 现在接受 `None` → `int`，但仍保持在引擎启动前必定被填充。请在文档与 CLI 参数帮助中明确说明：`--block-size` 可省略，系统会自动依据后端选择填充。  
2. **测试**：新增单元测试覆盖以下情形：  
   - 用户显式指定 `--block-size` 与兼容/不兼容的 `--attention-backend` 的错误提示。  
   - 未指定块大小时，后端自动选取并正确设置 `cache_config.block_size`（FlashMLA→64、CUTLASS_MLA→128 等）。  
   - Hybrid（attention+mamba）模型跳过块大小校准。  
3. **日志**：`logger.info/warning` 已加入，对不匹配的 `--block-size` 给出建议，确认在 CI 中日志级别不会导致误报。  
4. **后端实现**：所有自定后端需实现 `get_preferred_block_size` 与 `get_supported_kernel_block_sizes`（若已有则保持不变），确保新逻辑不会因缺失方法而在运行时抛错。  
5. **文档/示例**：更新 README/CLI 手册，说明块大小的默认值、何时需要手动指定以及与不同后端的关联。  

整体来看，此次重构提升了配置校验的可维护性与错误提示的可读性，但需要充分的回归测试来防止在特定 GPU（尤其 Blackwell 系列）上出现后端‑块大小不匹配导致的启动失败。祝调试顺利！

---

### [Feature] Decode Context Parallel support for GPU model runner v2 (#34179)
**SHA**: `ab33d2a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ab33d2a629be6eca2dd946b1628af4d23d39c547)

**变更类型**：功能增强（Decode Context Parallel 支持）  
**重要程度**：🟡 中  

**核心改动**  
1. **attn_utils** 新增 `prepare_dcp_local_seq_lens`，在 CUDA‑graph 安全的前提下填充每个请求在 DCP 条件下的局部 seq_len；`build_attn_metadata` 增加 `dcp_local_seq_lens` 参数并向下传递。  
2. **block_table** 引入 `cp_kv_cache_interleave_size`、`dcp_world_size`、`dcp_rank`，并在计算 slot‑mapping 时使用 “virtual block size” 与 “本地/跨 CP” 的映射逻辑，确保 KV 在多卡解码时被正确切分。  
3. **cudagraph_utils / model_runner** 在捕获图前和普通输入准备时分别调用 `prepare_dcp_local_seq_lens`，并把 `dcp_local_seq_lens` 传入 `build_attn_metadata`。  
4. **input_batch** 为每个请求新增 `dcp_local_seq_lens` 缓冲区。  
5. **模型初始化** 将 `cp_kv_cache_interleave_size` 传递给 `BlockTables`，并在启动时检查注意力层与 CP 的兼容性。  

**影响范围**  
- KV‑cache 管理（`BlockTables`、`KVCacheConfig`）  
- 注意力后端构建（`AttentionMetadataBuilder`）  
- CUDA‑graph 捕获路径和推理调度（`ModelRunner`、`cudagraph_utils`）  
- 分布式并行状态（`vllm.distributed`）  

**关注建议**  
1. **单元/集成测试**：在 DCP‑size > 1、不同 `cp_kv_cache_interleave_size` 组合下跑完整的 decode 流程，确保 slot‑mapping 不产生 PAD‑ID 漏洞且序列长度在所有 rank 上保持一致。  
2. **CUDA‑graph 安全**：验证 `prepare_dcp_local_seq_lens` 在图捕获期间不触发同步或非法写（使用 `torch.cuda.is_current_stream_capturing()` 检查）。  
3. **兼容性检查**：`check_attention_cp_compatibility` 必须覆盖所有注意力实现（Flash‑Attn、Xformers 等），防止在 CP + DCP 环境下出现未对齐的 block 大小。  
4. **性能基准**：对比无 DCP 与开启 DCP 时的吞吐、显存占用，特别关注 KV‑shard 的额外通信开销。  
5. **文档/配置**：在 README/Config 文档中说明 `decode_context_parallel_size` 与 `cp_kv_cache_interleave_size` 的推荐取值及限制。  

总体而言，改动为 DCP 引入了必要的分片与本地 seq_len 维护，逻辑清晰，但需要严格的分布式/图捕获测试以及对注意力后端的兼容性验证。

---

### [Model Runner V2] Further simplification for PP (#34724)
**SHA**: `be3af2d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/be3af2d29e2507f32b2190fe015cd6609b348caa)

**🎯 变更类型**：重构 / 功能简化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 删除原来的 `PPHandler` 类（109 行），将管线并行（PP）相关的广播/接收逻辑抽取为独立的函数 `pp_broadcast`、`pp_receive` 放在新模块 `pp_utils.py`。  
2. `ModelRunner` 中相应调用改为直接使用这两个函数，去掉了 `self.pp_handler` 成员及其初始化。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`（核心模型调度逻辑）  
- 新增 `vllm/v1/worker/gpu/pp_utils.py`（广播/接收实现）  
- 删除 `vllm/v1/worker/gpu/pp_handler.py`（已不再使用）  

**💡 关注建议**  
1. **功能等价性**：`pp_broadcast` 将 `num_sampled`、`num_rejected` 合并为一个二维张量一次广播，接收端对应解包。请确认在 **speculative decoding** 场景下，原来的三次 `broadcast` 与现在一次 `stack` 再 `broadcast` 的顺序、对齐方式完全一致（张量 dtype、设备、contiguous 等）。  
2. **异常路径**：`pp_receive` 在非最后 PP rank 返回 `None`，调用方已保持原有 `assert received is not None`，但若未来在其它路径（如调试或单机模式）误调用，可能触发 AssertionError。建议在上层加上更友好的错误提示或容错。  
3. **性能评估**：一次性广播合并张量可以略微降低通信次数，理论上提升吞吐。建议在多节点/多卡环境下跑一次基准，验证是否真正收益。  
4. **代码可维护性**：移除 `PPHandler` 简化了对象生命周期，降低了成员变量的显式管理；但也失去了类封装的可扩展性（如后续需要添加更多 PP 相关状态）。如果后续需求增多，考虑将 utils 再封装为轻量类或保持函数式模块化。  
5. **向后兼容**：外部代码若仍 `import PPHandler` 将报错。请在项目文档或迁移指南中说明已废弃 `PPHandler`，并提供 `from …pp_utils import pp_broadcast, pp_receive` 的使用示例。  

总体来看，此次改动通过函数抽取降低了代码复杂度，保持了原有功能；重点关注广播数据的等价性和多卡环境的实际性能变化。

---

#### 🟢 低重要度变更 (11)

### [Model Runner V2] Avoid prepare prefill kernel launch overhead (#34780)
**SHA**: `c50e105` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c50e105a8843907c8c89f95ee29b8cc5e3935bae)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `prepare_inputs` 中加入 `any_prefills` 检查，仅在存在 prefill 时才调用 `prepare_prefill_inputs`，避免不必要的 kernel 启动开销；在 `states.py` 新增 `any_prefills` 方法实现该判断，并做少量格式化调整。

---

### [Bugfix] fix activation in cpu_fused_moe_torch call (#34696)
**SHA**: `e89a91d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e89a91d9275cd8ac086fe04476b41675a9ebbd5c)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `cpu_fused_moe_torch` 调用中的 `activation` 参数改为 `activation.value`，修正 CPU 融合 MoE 层的激活传递错误。

---

### [Bugfix] Fix prefix creation for Qwen3.5 (#34723)
**SHA**: `909b147` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/909b14719725f9647591b63151c45ff396fb4524)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 Qwen3.5 模型前缀生成逻辑，避免在已有 `model` 前缀时产生重复；同时为 `Qwen3_5ForConditionalGeneration` 与其 MoE 子类将默认前缀统一为 `"model"`，确保模型参数路径正确。

---

### [Model Runner V2] A bit more PP simplification (#34766)
**SHA**: `a49ea5a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a49ea5a58fc0f8170027abd79168d6f7ca3e4789)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：简化了流水线并行（PP）相关逻辑：将 `pp_receive` 返回值直接解构，去除空返回分支；在 `pp_broadcast` 中改为断言确保仅在最后 PP rank 调用；相应注释更新，使代码更清晰、减少不必要的条件判断。

---

### [ROCm][CI] Removed hard-coded attn backend requirement for Qwen VL (#34753)
**SHA**: `cef65f0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cef65f0715927d5a5137c34f9a95d6aa5be26cc2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/models/multimodal/generation/test_common.py` 中删除了针对 ROCm 平台的硬编码注意力后端配置，使 Qwen VL 测试不再强制使用 `ROCM_AITER_FA`，提升了跨平台兼容性。

---

### [torch.compile] Turn on silu+fp4 quant fusion by default for O1+ (#34718)
**SHA**: `02e8f26` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/02e8f26ceaa3af0382b9de6b40825c4ad49ef5b7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `enable_act_fusion` 中新增对 FP4 量化模型的检测，使其默认开启 SiLU+Mul 或 FP4 量化融合。

---

### [BugFix] [Build] fix string literals comparison in indexer_k_quant_and_cache calling site (#34653)
**SHA**: `4a00a51` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4a00a511bbf707fcb484d655b3b5eec0ed0ca308)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `csrc/cache_kernels.cu` 中将硬编码字符串 `"fp8_e4m3"` 替换为静态变量 `kv_cache_dtype`，以避免字符串字面量比较错误，修复了 `indexer_k_quant_and_cache` 调用处的类型分派问题。

---

### [CI] Remove unused precompiled wheel args from image build (#34767)
**SHA**: `df3f537` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/df3f537a666cd4014359414ed2766b4aaea0fa60)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：删除构建镜像脚本中未使用的预编译 wheel 参数，简化 `image_build.sh` 的调用签名并相应修改 Buildkite 步骤配置。

---

### [Bugfix][MTP][Sparse MLA] Allow sparse MLA with MTP to run with FULL cudagraphs (#34457)
**SHA**: `dc5fa77` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dc5fa77a4eb6680339cb77abe713fb22d7795560)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在文档表格中加入 FlashInferMLASparse；将 DeepseekV32Indexer 的 CUDA‑graph 支持改为 UNIFORM_BATCH；限制稀疏 MLA 的 `num_speculative_tokens` ≤ 1 并抛出错误；在解码元数据中对 `block_table` 进行 clamp，防止 OOB 访问。

---

### [CI] Fix flaky test_parsable_context (#34717)
**SHA**: `1e4a084` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1e4a084c8e53bfb422f64e2394fc94c4ed5b6cbf)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `test_parsable_context` 改为基于输出结构而非固定索引进行断言，支持多轮 reasoning/mcp_call，放宽输出数量检查，提升测试稳定性，防止 flaky。

---

### [BugFix] Fix sp tests (#34716)
**SHA**: `7967e85` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7967e854da4868872b4b13d2b7f039061fee50fe)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_sequence_parallel.py` 中，将启用 eager 模式的参数 `--enforce-eager` 替换为 `-cc.cudagraph_mode=none`，修正了 SP（序列并行）相关测试的启动参数。

---

