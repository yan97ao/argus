# 每日更新报告（2026-02-21）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-21 20:49:41 | Roman | [Frontend] Add automatic language detection for Whisper transcription (#34342) |
| 2026-02-21 20:48:14 | Huy Do | [Bugfix] Gate 256-bit instructions to CUDA 12.9+ (#34791) |
| 2026-02-21 18:31:58 | Cyrus Leung | [Benchmark] Improve benchmarks (#35012) |
| 2026-02-21 17:57:53 | petrpechman | [Doc] Fix example of eagle3 (#34960) |
| 2026-02-21 17:38:28 | Nick Hill | [Core] Minor structured-output related scheduler optimization (#34765) |
| 2026-02-21 17:34:57 | Nicolò Lucchesi | [PD] Change kv_load_failure_policy Default from "recompute" to "fail" (#34896) |
| 2026-02-21 16:34:55 | BADAOUI Abdennacer | [ROCm] Enable bitsandbytes quantization support on ROCm (#34688) |
| 2026-02-21 12:32:05 | jennyyyyzhen | [ROCM] Optimize ROCM_AITER_FA spec decode eagle performance (#34541) |
| 2026-02-21 12:25:23 | Andreas Karatzas | [ROCm][CI] Fix spec decode logprobs flakiness and parametrize tree attention backends (#34599) |
| 2026-02-21 12:25:07 | Andreas Karatzas | [ROCm][AITER] Fix aiter paged_attention_v1 decode for sliding window and head_size < 64 (#34570) |
| 2026-02-21 12:12:05 | Andreas Karatzas | [CI] Fix ColBERT HF comparison tests on AMD CI + refactor (#34567) |
| 2026-02-21 12:11:40 | zhongdaor-nv | [feat] Add per-block extra_keys to KV events (#33304) |
| 2026-02-21 12:03:32 | Andreas Karatzas | [CI][MCP][Harmony] Heavy refactoring Harmony & MCP response tests and stabilizing with deterministic test infrastructure (#33949) |
| 2026-02-21 12:01:40 | Kata Coder | [Frontend] Support multimodal inputs for late-interaction scoring (ColQwen3) + NewModel: nvidia/nemotron-colembed (#34574) |
| 2026-02-21 11:59:42 | pougetat | [Realtime] Add Qwen3-ASR realtime streaming support (#34613) |
| 2026-02-21 11:59:06 | Xin Yang | [Kernel] Optimize sample_recovered_tokens_kernel (#34974) |
| 2026-02-21 11:57:38 | Li | Support prompt_embeds for pooling requests in output processor (#34904) |
| 2026-02-21 11:56:33 | Taneem Ibrahim | [Misc] Fix mypy errors in vllm/profiler and remove from exclude list (#34959) |
| 2026-02-21 11:56:16 | Rohan Potdar | [ROCm][Bugfix]: Only save unpadded sizes for shared_experts in MoERunner to fix rmsnorm pad fusion (#34636) |
| 2026-02-21 11:55:51 | Yanan Cao | [Kernel] [Helion] [9/N] Canonicalize GPU variant names to base model names (#34928) |
| 2026-02-21 11:54:55 | Vlad Tiberiu Mihailescu | [CI/Build] Add opentelemetry libs in default vllm build (requirements/common.txt) (#34466) |
| 2026-02-21 11:54:35 | yugong333 | [LoRA] Support Quantized Adapters (#30286) |
| 2026-02-21 09:19:19 | Lucas Wilkinson | Revert "[Llama4,Quantization] Simplify and generalize logic for Q/K permutations in quantized self-attn layers " (#34997) |
| 2026-02-21 05:37:31 | Wei Zhao | Bump Flashinfer Version and Re-enable DeepSeek NVFP4 AR+Norm Fusion (#34899) |
| 2026-02-21 05:33:04 | Ryan Rock | [AMD][CI] Fix test_custom_allreduce for A100 testgroup (#34735) |
| 2026-02-21 05:25:50 | Lucas Wilkinson | [CI] Revert PRs 34818 and 33600 (#34979) |
| 2026-02-21 02:51:58 | Wei Zhao | [Test] Add FP8 KV Cache Testing for MLA Backends (#34473) |
| 2026-02-21 02:17:42 | Michael Goin | [CI] Remove failing prime-rl integration test (#34843) |
| 2026-02-21 00:47:14 | Zhengxu Chen | [compile] Fix torch.compile time discrepancy in logging. (#34912) |
| 2026-02-21 00:46:45 | Zhengxu Chen | [compile] Move torch_aot_compile directory under torch_compile_cache (#34831) |
| 2026-02-21 00:36:51 | Yanan Cao | [Kernel] [Helion] [6/N] Add num_tokens dimension to silu_mul autotuning and dispatching (#34185) |

### 📊 统计摘要
> 本日共 31 个提交 | 🔴高 3 | 🟡中 19 | 🟢低 9
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[CI][MCP][Harmony] Heavy refactoring Harmony & MCP respon...](#991d6bf)
    - [[Frontend] Support multimodal inputs for late-interaction...](#5719a4e)
    - [[CI] Revert PRs 34818 and 33600 (#34979)](#aaefc58)
  - [🟡 中重要度变更 (19)](#-🟡-中重要度变更-19)
    - [[Frontend] Add automatic language detection for Whisper t...](#98b0205)
    - [[Benchmark] Improve benchmarks (#35012)](#f74f157)
    - [[PD] Change kv_load_failure_policy Default from "recomput...](#ab6f348)
    - [[ROCm] Enable bitsandbytes quantization support on ROCm (...](#8dc8a99)
    - [[ROCM] Optimize ROCM_AITER_FA spec decode eagle performan...](#2aab2bb)
    - [[ROCm][CI] Fix spec decode logprobs flakiness and paramet...](#54254f7)
    - [[CI] Fix ColBERT HF comparison tests on AMD CI + refactor...](#89358f0)
    - [[feat] Add per-block extra_keys to KV events (#33304)](#a0fe7ea)
    - [[Realtime] Add Qwen3-ASR realtime streaming support (#34613)](#11be2c7)
    - [[Kernel] Optimize sample_recovered_tokens_kernel (#34974)](#7a5adad)
    - [[Misc] Fix mypy errors in vllm/profiler and remove from e...](#d38cd3d)
    - [[Kernel] [Helion] [9/N] Canonicalize GPU variant names to...](#9d7577b)
    - [[LoRA] Support Quantized Adapters (#30286)](#a55caf6)
    - [Revert "[Llama4,Quantization] Simplify and generalize log...](#0e22cd6)
    - [Bump Flashinfer Version and Re-enable DeepSeek NVFP4 AR+N...](#ea5f903)
    - [[Test] Add FP8 KV Cache Testing for MLA Backends (#34473)](#f24b2de)
    - [[CI] Remove failing prime-rl integration test (#34843)](#fac1507)
    - [[compile] Fix torch.compile time discrepancy in logging. ...](#f863994)
    - [[Kernel] [Helion] [6/N] Add num_tokens dimension to silu_...](#a6d0299)
  - [🟢 低重要度变更 (9)](#-🟢-低重要度变更-9)
    - [[Bugfix] Gate 256-bit instructions to CUDA 12.9+ (#34791)](#272b535)
    - [[Doc] Fix example of eagle3 (#34960)](#bebfe55)
    - [[Core] Minor structured-output related scheduler optimiza...](#820d781)
    - [[ROCm][AITER] Fix aiter paged_attention_v1 decode for sli...](#cf93c1a)
    - [Support prompt_embeds for pooling requests in output proc...](#59c6233)
    - [[ROCm][Bugfix]: Only save unpadded sizes for shared_exper...](#ded333f)
    - [[CI/Build] Add opentelemetry libs in default vllm build (...](#e739c29)
    - [[AMD][CI] Fix test_custom_allreduce for A100 testgroup (#...](#0632ed8)
    - [[compile] Move torch_aot_compile directory under torch_co...](#e4a5d8c)
#### 🔴 高重要度变更 (3)

### [CI][MCP][Harmony] Heavy refactoring Harmony & MCP response tests and stabilizing with deterministic test infrastructure (#33949)
**SHA**: `991d6bf` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/991d6bff38ff02f7cf47a3833efce58b27db8bb8)

**🎯 变更类型**：重构 / 性能优化 / 安全修复  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 为 Harmony 与 MCP Responses API 编写 **确定性测试基础设施**：新增 `retry_for_tool_call`、`retry_streaming_for`、日志诊断函数等，实现在非确定性模型输出下的自动重试与全链路可观测。  
- **测试代码大幅重构**：集中到 `tests/entrypoints/openai/responses/conftest.py`，统一环境变量、日志、工具函数；删除冗余手工实现的事件栈检查，改用统一 `validate_streaming_event_stack`。  
- **核心库改动**：  
  - `vllm/entrypoints/openai/parser/harmony_utils.py`：引入日志、统一 `MCP_BUILTIN_TOOLS` 定义、系统消息日期来源改为 `VLLM_SYSTEM_START_DATE` 环境变量，提升可复现性。  
  - `vllm/entrypoints/openai/responses/serving.py` 与 `context.py`：在请求层面过滤仅**请求启用**的 builtin tool，防止未授权工具被意外调用。  
  - `vllm/entrypoints/openai/responses/serving.py`：在多轮对话构造时避免向模型发送空字符串输入。  
  - `tests/utils.py`（`RemoteOpenAIServer`）：改进进程组管理、增加 GPU‑memory 预/后‑监控、两阶段等待策略以及孤儿进程清理，显著降低服务器泄露导致的 OOM。  
- **配置扩展**：`vllm/envs.py` 新增 `VLLM_SYSTEM_START_DATE` 环境变量，用于固定系统消息中的对话开始日期，消除因日期变化导致的非确定性。  
- **业务价值**：提升测试可靠性、加速 CI 通过率；提供更明确的工具调用边界，降低潜在安全风险；加强 GPU 资源回收，提升多实例并行测试的稳定性。

---

### 🎯 影响范围
- **测试模块**：`tests/entrypoints/openai/responses/*`（Harmony 与 MCP 相关的全部集成测试）。  
- **核心库**：`vllm/entrypoints/openai/parser/harmony_utils.py`、`vllm/entrypoints/openai/responses/serving.py`、`vllm/entrypoints/openai/responses/context.py`、`vllm/envs.py`、`tests/utils.py`（RemoteOpenAIServer）。  
- **运行时环境**：新增/修改的环境变量 (`VLLM_SYSTEM_START_DATE`、`BASE_TEST_ENV` 中的 `VLLM_SYSTEM_START_DATE`) 会影响所有通过 `Responses` API 调用的实例。  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** |- **工具过滤**：`serving.create_responses` 现在只把请求中出现的 builtin tool（`code_interpreter`, `web_search_preview`, `container`）加入 `builtin_tool_list`。<br>- **统一日志/诊断**：`log_response_diagnostics` 与 `retry_*` 系列函数在测试层提供统一的可观察性，未来可复用到生产监控。<br>- **系统消息生成**：`get_system_message` 已改为使用 `VLLM_SYSTEM_START_DATE`，使系统消息内容在不同机器/时间点保持一致。 |
| **性能影响** |- **GPU 资源回收**：`RemoteOpenAIServer` 在 `__exit__` 中采用两阶段（SIGTERM → SIGKILL → orphan‑kill）并监控 GPU 使用，显著降低因残留进程导致的 OOM；但引入的 `time.sleep(1.0)` 与更长的 GPU‑memory 超时（60 s）会略微延长测试套件的总执行时间。<br>- **重试逻辑**：`retry_for_tool_call`/`retry_streaming_for` 默认最多 3 次重试，增加了最坏情况的请求次数（最多 3×），但在 CI 中可以避免因一次性“噪声”导致的失败。 |
| **安全考虑** |- **工具调用白名单**：通过 `extract_tool_types(request.tools)` 并只把请求显式声明的 builtin tool 暴露给服务器，防止模型利用未授权的工具执行恶意代码。<br>- **外部网络调用**：`tests/entrypoints/openai/responses/test_harmony.py` 中保留了对 `api.open-meteo.com` 的请求，但已加入异常捕获并在失败时返回固定值，防止网络波动泄露 CI 环境信息或导致卡死。<br>- **进程组终止**：在 Windows 平台 `os.killpg` 可能不可用，代码已做 `try/except` 包装，避免因系统调用异常导致的子进程残留。 |
| **可维护性** |- **代码复用**：大量事件栈检查、工具调用判定抽象为函数，避免在多个测试文件中重复实现。<br>- **日志结构化**：`log_response_diagnostics` 以 JSON 形式记录模型行为，便于未来的审计或自动化分析。<br>- **环境变量统一**：`BASE_TEST_ENV` 集中管理常用 env，防止各 test 文件散落的硬编码。 |
| **兼容性** |- **生产代码**：核心改动（工具过滤、系统消息日期）在默认情况下保持原行为（如果未设置 env，则回退到当前日期），对现有部署不产生破坏性影响。<br>- **测试依赖**：新增 `psutil` 用于孤儿进程清理，若 CI 镜像未预装该库，可能导致 `ImportError`；建议在 `requirements-dev.txt` 中显式声明。 |
| **潜在副作用** |- **重试次数**：在极端情况下（模型持续不返回期望的 tool type），重试耗时会累积导致测试超时。<br>- **GPU 等待策略**：如果机器上有其他进程占用 GPU，仍可能在 60 s 内未达到 “基线 + 2 GiB” 目标，导致 CI 报错。<br>- **系统消息日期**：若业务代码依赖 `system.start_date` 的变动（例如时间敏感的 prompt），将失去原有的“每日增长”特性，需要在业务层自行处理。 |

---

### ⚠️ 潜在风险

1. **Windows 平台的进程组终止**  
   - `os.killpg` 在 Windows 上会抛 `AttributeError`，虽然已被 `try/except` 包裹，但若 `pgid` 为 `None`，仅调用 `self.proc.terminate()`，可能留下子进程。  
   - **建议**：在 Windows 环境中使用 `subprocess.CREATE_NEW_PROCESS_GROUP` 并在 `TerminateProcess` 中发送 `CTRL_BREAK_EVENT`，或直接使用 `psutil` 的 `children(recursive=True)` 统一清理。

2. **GPU‑memory 基线不准确**  
   - 基线在服务器启动前采集，若同机器上已有其他占用 GPU 的任务，基线会被高估，导致后续清理判断不准确（误判为已释放）。  
   - **建议**：在 CI 中使用专用的空闲 GPU 节点，或在基线采集前强制 `nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits` 清零。

3. **`VLLM_SYSTEM_START_DATE` 环境变量未设导致非确定性**  
   - 虽然回退为当前日期，但在低温度（`temperature=0`）情况下，模型的 token 生成会因日期而略有差异，导致测试不稳定。  
   - **建议**

---

### [Frontend] Support multimodal inputs for late-interaction scoring (ColQwen3) + NewModel: nvidia/nemotron-colembed (#34574)
**SHA**: `5719a4e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5719a4e4e601fb91274294d25370b7aad656d629)

**🎯 变更类型**：功能增强 / 多模态支持 / 模型新增  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 为 ColQwen3 系列模型的 **late‑interaction scoring**（/score 与 /rerank 接口）加入多模态输入能力，支持 **text × image**、**image × text** 等组合；新增模型 `Qwen3VLNemotronEmbedModel`（nvidia/nemotron‑colembed‑vl）。  
- 重构底层批处理逻辑：实现 `score_data_to_prompts`、`parse_score_data_single`，让查询/文档在 **独立** 编码阶段能够携带各自的 `multi_modal_data`。  
- 文档、示例、单元测试同步更新，覆盖纯文本、纯图像、混合场景。  

**🎯 影响范围**：  
- `vllm/entrypoints/llm.py`（late‑interaction 入口）  
- `vllm/entrypoints/pooling/score/serving.py`（async 处理、预处理）  
- `vllm/entrypoints/pooling/score/utils.py`（解析与转换）  
- 模型注册：`registry.py`、`config.py`、`colqwen3.py`（投影层容错）  
- 文档 `docs/models/pooling_models.md`、示例脚本、测试套件 `tests/models/multimodal/...`  

---

## 🔍 技术洞察

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • 在 **late‑interaction** 路径引入了 **多模态数据流**：每个 `ScoreData` 现在可以是字符串或 `ScoreMultiModalParam`（包含 image_url + text）。<br>• 新增 **`score_data_to_prompts`** 与 **`parse_score_data_single`**，实现 **独立** 的多模态 Prompt 生成，使查询与文档分别进行 token‑level 编码，再在服务端做 MaxSim 聚合。<br>• 注册表与配置层加入 `Qwen3VLNemotronEmbedModel`，保持模型统一走 `ColQwen3Model` 实现。 |
| **性能影响** | • 多模态路径会触发 **图像解码（PIL）** 与 **base64 解码**，相较纯文本多出 CPU 与内存开销，预计 **10‑30%** 的请求时延提升（取决于图片尺寸/数量）。<br>• `gpu_memory_utilization` 参数在测试中被显式设为 **0.7**，防止因额外的 MultiModalData 张量导致 OOM。<br>• 仍然使用 **Late‑Interaction** 的 token‑wise MaxSim，核心向量运算保持 O(N·M) 级别，没有改变原有的 GPU 加速路径。 |
| **安全考虑** | • 接收 **外部 base64 图像**，如果未做尺寸/类型校验，可能导致 **内存/CPU DoS**（巨型图片、恶意嵌入）。<br>• 代码目前未限制 `image_url` 的协议，仅支持 data‑uri；若将来开放网络 URL，需加入 **网络安全**（超时、重定向、CORS）检查。<br>• 引入 `Pillow`，需关注其已知安全 CVE（如 `Image.open` 对特制 PNG 的解析漏洞），建议锁定安全的 Pillow 版本并在容器镜像中审计。 |
| **可维护性** | • 多模态解析逻辑集中在 `utils.py`，新增的 `_ensure_str`、`parse_score_data_single`、`score_data_to_prompts` 为纯函数，易于单元测试。<br>• `colqwen3.py` 的投影层容错（`if self.custom_text_proj is not None`）提升了对不同模型变体的兼容性，降低未来模型迭代的破坏风险。<br>• 文档、示例、测试同步更新，覆盖 3 种输入组合，保持 **回归安全**。 |

---

## ⚠️ 潜在风险

1. **向后兼容性**  
   - `late_interaction_score` 以前对非字符串 `ScoreData` 抛 `NotImplementedError`；现在改为多模态解析。如果用户传入自定义非标准结构（未实现 `ScoreMultiModalParam`），可能出现 `ValueError`，导致未捕获异常。  
2. **资源消耗**  
   - 大批量图像（尤其高分辨率）会导致 **GPU 显存** 与 **CPU** 急增，可能触发 OOM 或服务卡顿。  
3. **模型配置冲突**  
   - `Qwen3VLNemotronEmbedModel` 仍走 `colqwen3` 实现，若未来该模型使用不同的投影层或额外视觉 encoder，当前代码的投影容错逻辑可能不够，导致隐藏层 dtype 不匹配。  
4. **依赖升级风险**  
   - 新增 `Pillow`、`base64` 处理，若底层依赖升级（如 Pillow 移除 `Image.ANTIALIAS`），可能破坏图像解码路径。  
5. **安全漏洞**  
   - 未对 `data:image/png;base64,...` 的大小进行限制，攻击者可发送几百 MB 的 Base64，导致 **内存耗尽**。  

---

## 💡 关注建议

| 角色 | 建议 |
|------|------|
| **开发者** | - 在 `parse_score_data_single` 与 `score_data_to_prompts` 增加 **输入大小校验**（最大图片尺寸、Base64 长度上限），并在报错信息中提示。<br>- 为 `late_interaction_score` 增设 **兼容模式** 参数，保留旧的 `NotImplementedError` 行为，以便渐进式迁移。<br>- 将 `Pillow` 版本锁定在已知安全的 `>=10,<11` 范围，并在 CI 中加入安全审计。 |
| **模型维护者** | - 若后续推出 **视觉‑语言混合投影**（如跨模态对齐层），请在 `colqwen3.py` 中为 `custom_text_proj` 与潜在的 `custom_image_proj` 分别提供容错逻辑，避免隐藏层 dtype 不匹配。 |
| **运维/产品** | - 在 API 网关层对 **/score**、**/rerank** 的请求体大小设定上限（例如 5 MB），防止恶意大图攻击。<br>- 监控 `gpu_memory_utilization`，在多模态请求激增时动态调节 `max_model_len` 或开启 **分页**。 |
| **测试** | - 增加 **异常路径** 测试：非法 `image_url`、超大 Base64、缺失 `type` 字段等，确保系统返回 4xx 而非 5xx。<br>- 添加 **并发** 测试，验证在高并发下多模态解码不会导致共享状态竞争。 |

---

**结论**  
此次改动为 vLLM 引入了 **跨模态 late‑interaction scoring** 能力，显著提升了检索/重排序场景的适用性，同时在模型注册与底层实现上保持了统一的代码路径。风险主要集中在资源消耗与输入验证上，建议尽快在入口层加固安全检查，并在监控告警中关注多模态请求的显存占用情况。整体而言，改动价值高，技术实现稳健，只要按上述建议进行细化与防护，即可安全上线。

---

### [CI] Revert PRs 34818 and 33600 (#34979)
**SHA**: `aaefc58` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aaefc58ee0f023ec7bd3671ca83aae1b8a8f271d)

**🎯 变更类型**：重构（回退/撤销先前的 block size 强制与验证逻辑）  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交撤销了之前 PR（34818、33600）中对 KV‑cache block size 的强制检查与自动调整逻辑，恢复了 “如果用户未显式指定则使用默认 16” 的老行为。与此同时引入了 `BlockSize` 类型别名（`Literal[1,8,16,32,64,128,256]`），在配置层面对合法值做类型约束，并相应更新了 `EngineArgs`、Attention backend 选择以及若干内部实现（如 `ChunkedLocalAttention`、MLA metadata、GPU model runner）的默认参数取值。所有与 `update_block_size_for_backend` 相关的调用也被移除。

**🎯 影响范围**  
- `vllm/config/cache.py`、`vllm/config/vllm.py`（配置对象）  
- `vllm/platforms/cuda.py`（平台级默认填充值与 backend‑specific 强制）  
- `vllm/engine/arg_utils.py`（CLI Args 默认）  
- Attention 相关实现：`vllm/model_executor/layers/attention/*`（ChunkedLocal、MLA）  
- 执行器入口：`vllm/v1/executor/*`（multiproc、ray、uniproc）  
- 测试模块以及若干工具函数  

**🔍 技术洞察**  

- **架构影响**  
  - **配置层抽象**：通过 `BlockSize` Literal 限定合法取值，提升类型安全，但移除 `validate_block_size` 及平台层的自动调节后，`VllmConfig` 不再在统一入口校验 block size 与后端兼容性，导致 **配置与运行时的耦合度上升**，易出现 “配置合法但运行时不兼容” 的情形。  
  - **平台平台职责变化**：`Platform.check_and_update_config` 仍会在 CUDA 上为 `None` 的情况设为 16，但不再统一处理 **MLA / DCP / FlashInfer** 等后端的强制 block size，要依赖各 backend 自身的 `supports_block_size` 检查。  

- **性能影响**  
  - 对于 **MLA、FlashInfer、CUTLASS_MLA** 等后端，原先的强制 `block_size`（64/128/32）可确保 KV‑cache 与 kernel 最佳对齐。撤销后若用户保持默认 16，可能导致 **子最优的内存对齐**，出现显著的吞吐量下降或显存碎片化。  
  - 另一方面，恢复默认 16 对 **非‑MLA、纯 Transformer** 模型没有负面影响，仍保持原有的 “块大小可随需求自行调节” 灵活性。  

- **安全考虑**  
  - 变更不涉及外部输入的安全检查或权限控制，**安全风险基本为零**。唯一需关注的是 **错误的块大小可能导致进程崩溃**（如 CUDA 非法内存访问），从而产生服务可用性问题。  

**⚠️ 潜在风险**  

1. **后端兼容性错误**  
   - MLA、FlashInfer 等后端在运行时会抛出 `assert` 或 `RuntimeError`，因为 `CacheConfig.block_size` 未满足 `block_size % interleave == 0` 等约束。  
2. **性能退化**  
   - 未显式设置合适 block size 时，可能导致 KV‑cache 频繁拆分、内存搬运增加，整体吞吐量下降。  
3. **隐藏配置错误**  
   - 由于缺少统一的 `validate_block_size`，错误的配置只能在模型首次 forward 时才被捕获，调试成本提升。  
4. **测试覆盖不足**  
   - 之前依赖 `update_block_size_for_backend` 的单元测试已被删除，若未补齐相应断言，回归风险增加。  

**💡 关注建议**  

- **开发者**  
  1. 在文档或 CLI 帮助中明确提示：使用 MLA、FlashInfer、DCP 等后端时 **推荐 block size ≥ 32 且满足后端对齐要求**。  
  2. 如项目仍需自动调节，可在业务侧自行调用 `CacheConfig.block_size = <backend_preferred>`，或恢复 `validate_block_size` 的关键检查。  
  3. 为关键路径（`EngineArgs`、`CacheConfig`）新增单元测试，确保在未显式设置 block size 时仍能通过后端兼容性校验。  

- **用户**  
  1. 对使用 **多模态 / Mamba / MLA** 模型的部署，务必在启动参数中显式指定 `--block-size`（如 64、128），或根据后端文档手动调整。  
  2. 如遇 `AssertionError: Block_size(...) should be greater than ...`，请检查并对齐 `cache_config.block_size` 与 `parallel_config.cp_kv_cache_interleave_size`。  

- **维护者**  
  1. 考虑在后端选择阶段（`Platform.get_attn_backend_cls`）加入 **fallback 警告**，当用户提供的 block size 与所有可用后端不匹配时给出明确提示。  
  2. 评估是否在未来重新引入 **轻量级的统一校验**（例如仅在 `VllmConfig.validate` 时做一次兼容性检查），以兼顾灵活性与安全性。  

---  

---

#### 🟡 中重要度变更 (19)

### [Frontend] Add automatic language detection for Whisper transcription (#34342)
**SHA**: `98b0205` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/98b0205c3c934849d96922e162e65f3178e0886b)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为 Whisper 转写加入自动语言检测：在 `SpeechToTextEndpoint` 中若请求未携带 `language`，会先对音频块执行一次受限生成，仅得到语言 token；随后将检测到的语言写回请求并继续正常转写。为此在 `SupportsTranscription` 接口新增 `supports_explicit_language_detection`、`get_language_detection_prompt`、`parse_language_detection_output`、`get_language_token_ids` 四个抽象成员；`WhisperForConditionalGeneration` 实现这些成员并在 `validate_language` 中改为默认返回 `None`（触发检测），同时新增对应单元测试。

**🎯 影响范围**  
- `vllm/entrypoints/openai/speech_to_text/speech_to_text.py`（调用检测逻辑）  
- `vllm/model_executor/models/interfaces.py`（接口扩展）  
- `vllm/model_executor/models/whisper.py`（实现检测相关方法）  
- 新增/修改测试：`tests/entrypoints/openai/test_transcription_validation_whisper.py`、`tests/models/multimodal/generation/test_whisper.py`  

**💡 关注建议**  

1. **兼容性**：`supports_explicit_language_detection` 默认 `False`，确保其他转写模型在不实现新接口时仍能正常工作。若未来为其它模型添加检测，需要同步实现三方法，否则会在运行时抛 `NotImplementedError`。  
2. **错误容错**：`_detect_language` 只在 `language is None` 且模型声明支持检测时调用，若检测过程返回空 `token_ids` 或解析失败会抛 `AssertionError/IndexError`，建议捕获并回退到默认语言（如 `en`），避免整个转写请求失败。  
3. **性能影响**：额外的前向传播会消耗一次模型推理时间，约占总耗时的 5‑10%。在高并发场景下可考虑在配置中关闭自动检测，或在已经缓存语言信息的情况下复用。  
4. **日志与可观察性**：已加入 `logger.info`，但建议在 `DEBUG` 级别记录检测的原始 token、采样参数，以便排查语言误判。  
5. **单元/集成测试**：现有两语言（en、it）覆盖基本路径，建议补充非支持语言、长音频分片以及多 GPU 环境下的并发检测测试，确保 `allowed_token_ids` 约束在不同 tokenizer 实现上保持一致。  
6. **文档更新**：在 API 文档中说明 `language` 参数为可选，未提供时会触发自动检测，并列出模型当前支持的语言列表。  

总体而言，此次改动为 Whisper 引入了实用的语言自动识别功能，接口抽象保持向后兼容，只要在新增模型时遵循新抽象即可。后续关注检测失败的回退策略和对高并发的性能评估。

---

### [Benchmark] Improve benchmarks (#35012)
**SHA**: `f74f157` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f74f1572ca3a0973d8db2187f0064bfecb6d5df2)

**🎯 变更类型**：功能增强 / 小幅 Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `benchmarks/datasets.py` 中将对数据集路径的解析提前到循环外，并改用 `tokenizer.encode` 统一获取 token 长度，避免在每次迭代里重复查找解析函数并兼容 `transformers` 新版 tokenizer。  
2. 对 `benchmarks/sweep/plot*` 的 import 进行容错拆分，分别捕获 `matplotlib`、`pandas`、`seaborn` 的缺失，防止一次性 ImportError 导致整个脚本失效。  
3. `serve_sla.py` 在 SLA 计算历史记录时加入 `sla_data.append(past_iter_data)`，并在搜索入口打印出当前的 Serve 与 Bench 参数，提升可观测性。  

**🎯 影响范围**  
- `vllm/benchmarks/datasets.py`（所有基准数据集采样逻辑）  
- `vllm/benchmarks/sweep/plot.py`、`plot_pareto.py`（绘图脚本）  
- `vllm/benchmarks/sweep/serve_sla.py`（SLA 求解与搜索流程）  

**💡 关注建议**  
- **兼容性**：`tokenizer.encode` 取代 `tokenizer(...).input_ids`，请确认自定义 tokenizer 仍实现 `encode` 接口，或在文档中明确要求。  
- **错误处理**：提前抛出 “Unsupported dataset path” 更易定位问题，但若未来加入新数据集，需要同步更新 `SUPPORTED_DATASET_PATHS`。  
- **性能**：把 `parser_fn` 移出循环可略微提升采样速度，建议在其他类似循环中也检查是否有重复的查询。  
- **绘图脚本**：拆分的 ImportError 逻辑已完善，建议在 CI 中加入 `import‑failure` 的测试场景，确保占位模块的属性访问不导致隐藏错误。  
- **日志信息**：`serve_sla.py` 新增的参数打印有助于调试，但在生产环境可能产生噪声，可考虑加入 `--verbose` 开关或使用 `logging` 而非直接 `print`。  

总体来看，这次提交提升了基准采样的稳健性与可维护性，兼容性改动集中在 tokenizer 与 import 处理，影响范围局限于 benchmark 相关模块，风险较低。建议在下一个 release 前跑全量基准测试，确认采样长度与历史 SLA 数据的累积行为未被意外改变。

---

### [PD] Change kv_load_failure_policy Default from "recompute" to "fail" (#34896)
**SHA**: `ab6f348` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ab6f3487a6146b325cb836711e34f40f341278e4)

**🎯 变更类型**：功能增强 / 行为默认值调整  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 KV 缓存加载失败的默认处理策略 `kv_load_failure_policy` 从 `"recompute"` 改为 `"fail"`，使在分布式预填充场景中，加载异常时直接报错而不是重新计算。  
- 相关文档、示例、单元测试以及 `KVTransferConfig` 的默认值同步更新。  

**🎯 影响范围**  
- `vllm/config/kv_transfer.py`（默认配置）  
- `vllm/config/vllm_config.py`（创建 `VllmConfig` 时的参数）  
- KV‑Connector 示例与测试代码  
- 文档 `docs/features/nixl_connector_usage.md`  

**💡 关注建议**  
1. **兼容性**：默认改为 `"fail"` 可能导致已有部署在 KV 加载失败时行为改变，请在升级说明中提醒用户显式设置 `kv_load_failure_policy="recompute"` 以维持旧行为。  
2. **告警**：建议在 `KVTransferConfig` 初始化时加入一次性 **deprecation warning**，提示即将删除 `"recompute"` 为默认值，帮助用户提前适配。  
3. **类型安全**：已使用 `Literal`，可进一步封装为 `Enum` 提升可读性与 IDE 自动完成。  
4. **测试覆盖**：确认所有涉及 KV 加载的路径（包括异步加载、离线推理等）在两种策略下均有对应的单元测试，防止因默认改动引入回归。  
5. **文档同步**：文档已更新，建议在 Release Note 中突出此默认变化，提供迁移指南（如 `--kv-load-failure-policy fail`）。  

总体来说，改动清晰且影响范围受限，注意兼容性提示即可安全发布。

---

### [ROCm] Enable bitsandbytes quantization support on ROCm (#34688)
**SHA**: `8dc8a99` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8dc8a99b56e1a8427c83217a37595d0cd12b1ff2)

**🔧 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 BitsAndBytes 量化的最低版本提升至 `0.49.2`（在 ROCm 环境下）并在代码中统一使用 `_check_bitsandbytes_version()` 进行版本校验。  
- 解除原先对 ROCm（gfx9）上 BitsAndBytes 的硬性禁用，`RocmPlatform.supported_quantization` 中加入 `"bitsandbytes"`，并同步文档、测试和依赖文件。  
- 去除原有的显式 `bitsandbytes` 版本检查逻辑，改为统一函数；相应测试中删除了对 ROCm 平台的 `skip`。

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/bitsandbytes.py`（量化实现与版本检查）  
- `vllm/platforms/rocm.py`（平台支持列表）  
- 文档 `docs/features/quantization/bnb.md`  
- 依赖列表 `requirements/*.txt`、`requirements/rocm-test.txt`  
- 单元测试 `tests/models/test_transformer.py`（加入 ROCm 下的 BNB 测例）  

**💡 关注建议**  
1. **兼容性验证**：在不同 GPU 架构（尤其是 gfx9 与更高）上跑一次完整的 `nightly_torch_test`，确认新加入的 `"bitsandbytes"` 不会因底层 kernel 限制导致崩溃。  
2. **版本冲突**：`bitsandbytes` 0.49.2 对 CUDA 与 ROCm 的二进制兼容性不同，建议在 CI 中分别使用 `pip install bitsandbytes>=0.49.2 --extra-index-url https://download.pytorch.org/whl/rocm5.6`（或对应 ROCm 镜像）以防误装错平台包。  
3. **文档同步**：README 与快速入门示例中应标明 “在 ROCm 环境下请使用 bitsandbytes≥0.49.2”。  
4. **错误提示**：当前 `_check_bitsandbytes_version()` 会在导入失败时抛出 `ImportError`，可考虑在异常信息中加入 `torch.version.hip` 等平台信息，帮助用户快速定位缺少的 ROCm‑specific wheels。  
5. **回归测试**：新增的 `unsloth/tinyllama-bnb-4bit` 示例已在测试中加入，确保在多卡、MOE、异构部署等场景下仍能正常加载。  

总体来说，此次改动为 ROCm 用户打开了 BitsAndBytes 量化的大门，只要在 CI 与文档中做好平台‑版本对应的说明，即可安全上线。

---

### [ROCM] Optimize ROCM_AITER_FA spec decode eagle performance (#34541)
**SHA**: `2aab2bb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2aab2bb54366c5a26add1b07f107b86f7fe28ff5)

**🎯 变更类型**：功能增强/性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 `AiterFlashAttentionMetadataBuilder` 的 CUDA‑graph 支持从 `UNIFORM_SINGLE_TOKEN_DECODE` 调整为 `UNIFORM_BATCH`，以匹配批量解码的统一策略。  
2. 在构造函数中通过 `_init_reorder_batch_threshold(1, supports_spec_as_decode=True)` 初始化 `reorder_batch_threshold`，并在 `build` 中加入断言确保其已被设定。  
3. 新增 `build_for_drafting`，专为 EAGLE draft 模式提供 “全解码‑统一批” 的元数据构造，省去 `split_decodes_prefills_and_extends` 与 CPU‑GPU 同步，提升捕获 CUDA‑graph 时的性能。  

**🎯 影响范围**  
- `vllm/v1/attention/backends/rocm_aiter_fa.py`（ROCM AITer‑FA 实现）  
- 由此间接涉及的统一注意力调度、图捕获以及 `CommonAttentionMetadata` 的使用路径。  

**💡 关注建议**  
1. **兼容性检查**：确认其他后端（如 CUDA）或旧代码仍能接受 `UNIFORM_BATCH`，避免因枚举变化导致运行时异常。  
2. **阈值初始化**：`_init_reorder_batch_threshold` 现在强制 `supports_spec_as_decode=True`，请复核该标志在非‑draft 场景下的意义，防止误把非统一批次当作统一批处理。  
3. **单元测试**：新增针对 `build_for_drafting` 的测试，覆盖：  
   - uniform‑decode 输入 → 正确填充 `decode_metadata`、`num_decodes` 等字段；  
   - 非 uniform 场景仍走旧的 `build` 路径，且 `reorder_batch_threshold` 不被错误使用。  
4. **文档与部署**：在 README/CHANGELOG 中标明该优化仅在 ROCm‑EAGLE drafting 场景可见，提醒用户在其他模式下仍使用原 `build`。  
5. **性能基准**：提供对比基准（有/无 `build_for_drafting`）以验证图捕获延迟和整体吞吐的提升，防止因逻辑错误导致回退。  

整体来看，此次改动在保证统一解码批次前提下显著减少 CPU‑GPU 同步开销，预计对 ROCm‑EAGLE drafting 的吞吐提升明显。但需注意枚举和阈值初始化的兼容性，以及完善相应的测试和文档。

---

### [ROCm][CI] Fix spec decode logprobs flakiness and parametrize tree attention backends (#34599)
**SHA**: `54254f7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/54254f7a6155002b1a493c3cefda9752bc9ce92f)

**🎯 变更类型**：功能增强 / Bug 修复 / 稳定性提升  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 通过 `with VllmRunner(...) as runner` 把多个原先手动管理的 `VllmRunner` 包装为上下文管理器，确保在测试结束后自动释放 GPU 显存并清理分布式环境。  
2. 修复 **spec‑decode** 产生的 logprob 不确定性：在 ROCm 环境下强制关闭 non‑deterministic “skinny GEMM”，并在测试结束时显式 `torch.cuda.empty_cache()` 与 `cleanup_dist_env_and_memory()`。  
3. 为树注意力（tree attention）单元测试加入 **后端参数化**：  
   - 新增 `_get_platform_default_backend`、`_get_available_reference_backends`、`_adapt_kv_cache_for_backend` 等工具函数，实现不同平台、不同后端（FLASH、TRITON、ROCM_AITER_FA 等）的自动发现与 KV‑cache 布局适配。  
   - 在测试中依据后端自动选择参考实现，统一比较 tree‑attention 与对应的基准实现。  
4. 细微代码整理：使用集合常量表达 KV‑cache 布局映射，改写部分硬编码的 token 判断，提升可读性。

**🎯 影响范围**  
- `tests/v1/sample/test_logprobs.py`、`tests/v1/spec_decode/test_tree_attention.py`（测试层）  
- `vllm/v1/attention/backend` 与 `vllm/platforms`（后端适配逻辑）  
- `vllm/v1/spec_decode`（靠 spec‑decode 输出的 logprob 对比）  

**💡 关注建议**  
1. **资源回收**：上下文管理器已加入，但仍建议在所有新建的 `LLM` / `VllmRunner` 实例的异常路径中确保 `torch.cuda.empty_cache()` 与 `cleanup_dist_env_and_memory()` 被调用，防止 CI GPU 泄漏。  
2. **后端兼容性**：当前对 ROCm 的不兼容后端列入 `_INCOMPATIBLE_REFERENCE_BACKENDS`，若未来添加新后端，请同步更新该集合并在 `_adapt_kv_cache_for_backend` 中加入相应布局转换。  
3. **环境变量**：`VLLM_ROCM_USE_SKINNY_GEMM=0` 只在此测试中强制关闭，建议在文档里说明此变量的作用及其对性能的影响，防止生产环境误用。  
4. **测试可靠性**：`pytest.mark.parametrize` 已覆盖多后端，建议在 CI 中为每个平台分别跑一遍，确保 `current_platform.is_rocm()` 的分支不被遗漏。  
5. **代码可读性**：KV‑cache 布局的转换逻辑已抽取为函数，后续若出现其它布局（如 paged）可直接在 `_adapt_kv_cache_for_backend` 中扩展，保持单点修改。  

总体而言，此次改动显著提升了 **spec‑decode logprobs** 的确定性并为树注意力提供了跨后端一致性校验，风险主要集中在 ROCm 特定实现的兼容性上，建议持续监控对应平台的 CI 通过率。

---

### [CI] Fix ColBERT HF comparison tests on AMD CI + refactor (#34567)
**SHA**: `89358f0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/89358f0d35e7923cf1554d4d652094c4ad2e80de)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 ColBERT 系列模型新增了 “HF 比对” 测试，统一抽取出模型加载、投影权重下载、embedding 计算及结果校验的工具函数，并在 `COLBERT_MODELS` 配置中加入 `hf_comparison` 块。原先针对每个模型的重复实现被删除，测试结构更简洁，可在 AMD CI（CPU）环境下运行。

**🎯 影响范围**  
- `tests/models/language/pooling/test_colbert.py`（新增/重构的测试逻辑）  
- 依赖的核心模块：`vllm.runner`（用于启动模型），以及 `transformers`/`huggingface_hub`/`safetensors`（在测试中下载并加载权重）  

**💡 关注建议**  
1. **依赖与环境**：确保 CI 镜像已预装 `huggingface_hub`、`safetensors` 与对应的 `transformers` 版本，否则测试会因 ImportError 失败。  
2. **网络下载耗时**：HF 权重下载会显著延长测试时长，建议在 CI 中启用缓存（`HF_HUB_CACHE`）或使用 `--skip-download` 参数的本地镜像。  
3. **CPU 兼容**：函数 `_load_hf_model` 在 CPU 上强制使用 `attn_implementation="eager"`，避免 Flash‑Attention 报错；如果后续加入其他自定义注意力实现，需要同步更新此处。  
4. **异常处理**：下载或加载权重失败时当前会抛异常导致整个套件挂掉，建议在测试中使用 `pytest.importorskip` 或捕获 `HFHubError`，在网络不通时标记为 “skip”。  
5. **代码可读性**：`_assert_embeddings_close` 已在文件顶部实现，后面的重复定义已删除，保持单一定义即可；若后续需要复用，可考虑抽到 `tests/utils.py`。  
6. **dtype 与容错**：比较容差 (`rtol=1e-2`, `atol=1e-2`) 对 half‑precision 可能偏宽，建议在高精度（float32）下运行，以捕获潜在数值偏差。  

总体来看，此次改动显著提升了 ColBERT 模型在不同实现之间的一致性验证，同时通过抽象函数降低了维护成本。只要保证 CI 环境具备网络与依赖，风险有限。

---

### [feat] Add per-block extra_keys to KV events (#33304)
**SHA**: `a0fe7ea` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a0fe7ea2f052bb44820bc06a5635456b8d1383af)

**变更概述**  
本次提交在 KV‑Cache 相关事件中新增 `extra_keys`，并对 prompt‑embeds 的哈希逻辑做了抽象与缓存。`BlockStored` 现在携带每个块的额外哈希信息，`generate_block_hash_extra_keys` 统一返回该信息并在 `Request` 上缓存每块的 embed‑hash，避免重复计算。相应的测试也改为校验哈希值而非原始字节。

**影响范围**  
- `vllm/distributed/kv_events.py`、`vllm/v1/core/block_pool.py`、`vllm/v1/core/kv_cache_utils.py`、`vllm/v1/request.py`：核心 KV‑Cache 生产与消费路径。  
- 示例 `examples/online_serving/kv_events_subscriber.py` 与单元测试。

**关键点评**  
1. **API 向后兼容**：`BlockStored.extra_keys` 为可选字段，默认 `None`，对旧消费方影响最小。  
2. **哈希实现**：改用 `hashlib.sha256` 对 `tensor_data` 结果直接做摘要，返回 `bytes`，比直接使用原始 `bytes` 更节省内存并保证长度固定。  
3. **缓存机制**：在 `Request` 上新增 `_prompt_embeds_per_block_hashes`，通过 `generate_block_hash_extra_keys` 的 `start_mm_idx` 维持 MM‑input 的递增，避免对同一块重复计算。测试已验证一次调用计数为 1，说明缓存生效。  
4. **块级 extra_keys 生成**：`block_pool.cache_full_blocks` 为每块单独生成 `extra_keys` 并保存到事件中，确保 MM、LoRA、cache_salt 等信息在跨节点复用时可还原。  
5. **性能**：哈希一次后缓存，整体开销主要在第一次 `tensor_data` 调用；对大 batch 的 prompt‑embeds 仍保持 O(1) 额外空间。  
6. **序列化/对比**：`KVCacheEvent.__hash__` 现将 `extra_keys` 以 `tuple` 包装参与哈希，若 `extra_keys` 包含大量大对象（如大型 embed‑hash），应确保不导致不可预期的内存增长。  

**建议**  
- 在文档或 CHANGELOG 中说明 `BlockStored.extra_keys` 的语义与使用场景，特别是外部 KV‑Cache 消费方如何利用它重建块哈希。  
- 对 `extra_keys` 为 `None` 的情况在 `__hash__` 中使用 `0` 或固定 sentinel，避免 `tuple(None)` 带来的歧义。  
- 考虑在 `Request` 中提供清理接口，以防长期运行的服务因缓存的 `_prompt_embeds_per_block_hashes` 累积导致内存泄漏（例如在请求结束后删除对应条目）。  
- 运行完整的性能基准，确认对大规模多模态请求（含大量 MM‑inputs）引入的额外哈希开销在可接受范围。  
- 若后续需要对 `extra_keys` 进行序列化（如日志、持久化），建议统一采用 `bytes`（已实现）或 base64 编码，以保持跨语言兼容性。  

总体来看，本次改动为 KV‑Cache 事件提供了更丰富的块级元信息，提升了多模态、LoRA、缓存盐等特性的可追溯性，同时通过缓存减少了重复计算，对现有功能的影响有限。只要按上述建议完善文档与内存管理，即可安全合入。

---

### [Realtime] Add Qwen3-ASR realtime streaming support (#34613)
**SHA**: `11be2c7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/11be2c74dc1eb08aaaeb260f84a31c2b36bbd454)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Qwen‑3‑ASR 模型加入实时流式识别支持。实现了专用的 `Qwen3ASRRealtimeGeneration` 类，新增音频缓冲、分段读取以及对 `SupportsRealtime` 接口的实现；更新模型注册表、测试清单以及 OpenAI‑realtime 接口，使生成时 `max_tokens` 依据模型的 `realtime_max_tokens` 动态设定。  

**🎯 影响范围**  
- `vllm/model_executor/models/interfaces.py`（新增 `realtime_max_tokens`）  
- `vllm/model_executor/models/qwen3_asr_realtime.py`（核心实现）  
- `vllm/model_executor/models/registry.py`（注册新模型）  
- `vllm/entrypoints/openai/realtime/connection.py`（使用新 `max_tokens`）  
- `tests/models/registry.py`（添加测试条目）  

**💡 关注建议**  
1. **模型兼容性**：`realtime_max_tokens` 默认 1，子类需显式覆盖。建议在文档中注明所有实时模型必须提供该常量，以防误用。  
2. **音频分段长度**：当前硬编码 5 s，可能不适用于低采样率或高延迟场景，可考虑在配置或请求中暴露 `segment_duration_s`。  
3. **异常处理**：`buffer_realtime_audio` 在读取结束后如果音频为空会直接返回 `None`；若上层依赖非空返回，需添加防护。  
4. **性能**：缓冲区会按需扩容，建议在高频流式场景下监控内存增长，并在 `flush` 后及时释放。  
5. **测试覆盖**：目前仅在注册表中加入了模型条目，建议补充端到端的实时流式单元测试（音频分块、prompt 构造、生成截断），确保 `max_tokens` 与 `realtime_max_tokens` 的配合正确。  

整体来看，此次提交为 Qwen‑3‑ASR 引入了实时语音转写能力，改动集中且风险可控，关注上述细节后即可在生产环境推广。

---

### [Kernel] Optimize sample_recovered_tokens_kernel (#34974)
**SHA**: `7a5adad` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7a5adad48026d130348064ae7d41072ff999d1bf)

**🎯 变更类型**：性能优化 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `sample_recovered_tokens` 引入 Triton 加速内核 `sample_recovered_tokens_kernel`，改为在每个 batch‑request 上并行遍历词表块，使用 `inv_q = q.reciprocal()` 直接在 kernel 中完成 `prob * inv_q` 的乘法，去掉了原先在 Python 层的除法。  
2. 将 kernel 的 `PADDED_VOCAB_SIZE` 参数替换为固定 `BLOCK_SIZE = 8192`，并在循环中分块读取词表，以降低寄存器压力。  
3. 新增 `native_sample_recovered_tokens` 参考实现以及对应的单元测试，确保 Triton 实现与原生 Python 实现完全等价。  

**🎯 影响范围**  
- `vllm/v1/sample/rejection_sampler.py`（采样逻辑）  
- `tests/v1/sample/test_rejection_sampler.py`（增加对比测试）  
- 依赖 Triton 的运行时环境（GPU、CUDA 兼容性）  

**💡 关注建议**  

1. **兼容性**：`BLOCK_SIZE` 固定为 8192，若 vocab‑size > 2⁶⁴‑1 或远大于 8192，仍能通过分块循环工作，但在极端大 vocab（如 100k）下可能导致较多循环迭代，建议在文档中说明性能随 vocab 大小的线性关系。  
2. **数值稳定性**：`inv_q` 在 Python 层已做 `q.reciprocal()`，而 kernel 仍使用 `inv_q` 乘法，避免了除 0 的风险。但 `q` 是指数分布的随机数，极小值可能导致 `inv_q` 非常大，建议在生成 `q` 时加入上界裁剪，或在 kernel 中对 `inv_q` 做 `tl.where(inv_q==0, 0, inv_q)` 防止 overflow。  
3. **可重复性**：代码在生成 `q` 前保存并恢复每个 request 的 RNG 状态，保证在无 draft‑tokens 的请求上不产生随机数，这一点在多卡并行时尤为重要，保持不变即可。  
4. **内存与调度**：`recovered_token_ids` 与 `draft_token_ids` 同步占用 `int32`，但 `prob`、`target_probs`、`draft_probs` 仍在 float32 上下文，若显存紧张（大 batch、large vocab），请监控显存占用。  
5. **测试覆盖**：新增的对比测试覆盖了不同 batch‑size、vocab‑size、max_spec_len 以及是否提供 draft_probs 四种组合，建议在 CI 中保持该测试，并在未来加入更大 vocab (如 32k、64k) 的随机抽样，以防止隐藏的块划分错误。  

总体来看，此次改动在保持原有采样概率逻辑不变的前提下，显著提升了 GPU 上的并行度和吞吐，适用于 Spec‑Decode 场景的高频次恢复采样。但请确保部署环境装有兼容的 Triton 版本，并对极端词表大小进行基准评估。

---

### [Misc] Fix mypy errors in vllm/profiler and remove from exclude list (#34959)
**SHA**: `d38cd3d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d38cd3dde549b8e421d7d0390799b985e13bd8ab)

**🎯 变更类型**：Bug 修复 / 重构（消除 mypy 报错）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 `vllm/profiler` 从 pre‑commit 的 mypy 排除列表中移除，使其必须通过静态类型检查。  
2. 为 `layerwise_profile.py` 中的统计树结构引入 `Generic` 与 `TypeVar`，完善 `StatsEntry` 的类型约束；统一函数签名为 `dict[str, int] | None` 等 PEP‑604 写法。  
3. 修正可变默认值（`children` 使用 `field(default_factory=list)`），并显式标注 `parent` 为可能的 `None`。  

**🎯 影响范围**  
- `vllm/profiler`（尤其是 `layerwise_profile.py`）  
- `tools/pre_commit/mypy.py`（检查范围更广）  

**💡 关注建议**  
- 运行完整的单元/集成测试，确认加入类型检查后功能仍保持一致，尤其是树的构建与遍历路径。  
- 注意 `Generic` 实例化后对序列化（pickle）或跨进程传递可能产生的兼容性，若涉及请补充相应测试。  
- 若项目中有对 `_StatsTreeNode` 直接实例化的旧代码，需更新为 `...[_StatsTreeNode[ModelStatsEntry]]` 或相应具体类型。  
- 继续在 CI 中保留 mypy 检查，防止未来再次出现未捕获的类型错误。

---

### [Kernel] [Helion] [9/N] Canonicalize GPU variant names to base model names (#34928)
**SHA**: `9d7577b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9d7577b2bdf9de1155f1078e591446453723a88c)

**🎯 变更类型**：功能增强 / 其他  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 Helion kernel 引入 GPU 名称规范化（canonicalization）逻辑，统一把带有型号、显存、接口等后缀的变体名称映射到基准型号（如 `nvidia_h100`）。  
2. 新增 `_GPU_NAME_ALIASES` 表，在 `utils.py` 中实现映射，并在 `canonicalize_gpu_name` 中使用。  
3. 更新测试覆盖新的映射规则。  
4. 改进 `ConfigManager.get_config` 错误提示，给出手动添加别名或运行 autotune 脚本的指南。

**🎯 影响范围**  
- `vllm/kernels/helion/utils.py`（核心函数 `canonicalize_gpu_name`、别名表）。  
- `vllm/kernels/helion/config_manager.py`（错误信息增强）。  
- `tests/kernels/helion/test_utils.py`（新增/修改单元测试）。  
- 相关的 Helion kernel 配置文件（通过 `scripts/autotune_helion_kernels.py` 生成）。

**💡 关注建议**  
- **兼容性**：确保在旧版 GPU 名称未被列入别名表时仍能正常使用（当前仍返回归一化字符串），避免因未覆盖的新变体导致 `KeyError`。可以在 `canonicalize_gpu_name` 中加入 `log.warning` 提示缺失映射。  
- **维护**：别名表随硬件迭代会增长，建议把它抽离为可配置的 JSON/YAML 文件或添加 `--gpu-aliases` CLI 参数，便于社区自行补齐。  
- **测试**：现有测试只覆盖部分变体，建议在 CI 中加入多厂商（AMD、Intel）以及不同驱动报告格式的随机抽样，以防映射遗漏。  
- **文档**：在项目 README 或 Helion kernel 文档中说明别名表的作用、如何贡献新映射以及 `scripts/autotune_helion_kernels.py` 的使用方法。  

总体来看，此次改动提升了平台自动检测的鲁棒性，避免因显卡型号细分导致配置缺失的错误。但后续需要做好别名表的持续维护和可配置化，以防维护成本飙升。

---

### [LoRA] Support Quantized Adapters (#30286)
**SHA**: `a55caf6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a55caf6ae9a561dd816692060ef49681d9d6786d)

**🎯 变更类型**：功能增强（为 LoRA 引入量化适配器并实现基于 FP8 的 fused‑moe‑lora 内核）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `vllm.lora.ops.triton_ops` 目录新增 `fused_moe_lora_fp8_op.py`，实现了 FP8 / INT8‑W8A8 / INT8‑W8A16 三种量化路径的 fused‑MoE‑LoRA 前向计算。  
- 对外在 `__init__.py` 中导出 `fused_moe_lora_fp8`、`fused_moe_lora_shrink_fp8`、`fused_moe_lora_expand_fp8`。  
- 通过 `direct_register_custom_op` 将这三个函数注册为自定义 Torch OP，提供 fake 实现以兼容没有 Triton 编译器的环境。  
- 新增 `USE_B_L2_CACHE`（.ca cache）和 `naive_block_assignment` 逻辑，以提升显存访问效率并兼容不提供 `sorted_token_ids` 的场景。  

**🎯 影响范围**  
- **LoRA 相关模块**：`vllm/lora/ops/triton_ops/*`、`vllm/lora/ops/__init__.py`。  
- **分布式/模型并行**：利用 `tensor_model_parallel_all_reduce` / `all_gather` 进行全局聚合。  
- **推理后端**：Triton 编译器 (≥3.1) 与 FP8 硬件（支持 `torch.float8_e4m3fn` / `torch.float8_e5m2` 的 GPU）。  

**💡 关注建议**  

1. **兼容性检查**  
   - 确认目标 GPU 支持 FP8（Ampere+），并在不满足时有明确回退路径（如使用原 FP16/F32 kernel）。  
   - `direct_register_custom_op` 的 `except AttributeError` 兜底实现应保持同步；若未来 Torch 引入新的注册机制，记得更新。  

2. **参数校验与错误信息**  
   - 新增的 `use_fp8_w8a8 / use_int8_w8a8 / use_int8_w8a16` 互斥检查可以在函数入口做统一断言，防止用户误传组合。  
   - 对 `block_shape` 与 `lora_*_scale_stacked` 大小不匹配的情况提供更友好的提示。  

3. **性能基准**  
   - 由于引入了 `.ca` L2 缓存加载、分块量化以及 GDC（grid‑dependent‑collectives），建议在不同 GPU（A100, H100, RTX 4090）上跑完整的吞吐量/延迟基准，确保新路径确实带来预期提升。  

4. **单元/回归测试**  
   - 为 `fused_moe_lora_fp8*` 添加数值正确性测试（与 FP16/F32 参考实现对比误差）以及梯度检查（即使在 `torch.inference_mode`，仍应确保 backward 在训练模式下可用）。  
   - 在 CI 中加入 “triton‑available” 条件，以防止因缺少编译环境导致构建失败。  

5. **文档和示例**  
   - 更新 LoRA 使用手册，说明如何在 `vllm.Engine` 配置 `quant_config` 启用 FP8 适配器，示例中展示 `adapter.enabled = True` 与 `quant_mode = "fp8"` 的配合。  

6. **代码维护**  
   - 该文件行数 >1000，逻辑较为复杂。建议抽取公共子函数（如 `stride_*` 计算、mask 构造）到 `utils.py`，提升可读性并便于后续扩展。  
   - `torch.inference_mode()` 包裹的函数如果未来需要支持训练，需注意 `tensor_model_parallel_all_reduce` 的梯度传播路径。  

**结论**  
本次 PR 为 LoRA 引入了高效的量化路径，打开了在大模型推理中使用 FP8/INT8‑W8A8 的可能。只要在硬件兼容性、参数校验和性能回归上做好把控，新增功能将为 vLLM 的吞吐量提升提供显著价值。建议在合并前补全上述测试与文档，随后在正式发布前进行跨 GPU 的基准验证。

---

### Revert "[Llama4,Quantization] Simplify and generalize logic for Q/K permutations in quantized self-attn layers " (#34997)
**SHA**: `0e22cd6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0e22cd618b5da36404365518aad5a522aea008e7)

**🎯 变更类型**：Bug 修复 / 代码回滚  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：此次提交将之前在 *Llama4* 量化自注意力层中对 Q/K 权重排列的简化实现撤回，恢复了更细致的排列逻辑。新增对 **Compressed‑Tensors**（int8/fp8）以及 **NV‑FP4** 权重尺度的专门处理，并在文件顶部引入 `compressed_tensors` 包。  

**🎯 影响范围**  
- `vllm/model_executor/models/llama4.py`（权重加载与排列）  
- 量化配置相关：`QuantizationConfig`、`CompressedTensorsConfig`  
- 受影响的模型：Llama‑4 系列，尤其使用 FP4、int8/fp8 压缩张量的 checkpoint。  

**💡 关注建议**  
1. **功能验证**：在使用 FP4、int8/fp8 或普通 GPTQ 量化权重的 Llama‑4 checkpoint 上跑完整的加载‑推理链路，确保旋转嵌入（RoPE）仍能正确对齐。  
2. **回归测试**：添加或更新单元测试，分别覆盖 `weight`, `weight_scale`（NV‑FP4）以及 `CompressedTensors` 的三种情况，防止再次因简化导致维度误差。  
3. **性能评估**：虽然回滚增加了一段分支逻辑，但对大模型的加载时间影响微乎其微。建议在高并发部署场景下监控加载时延。  
4. **文档同步**：在模型说明或量化指南中标明，此实现已恢复对 FP4/Compressed‑Tensors 的兼容，避免使用旧版文档误导用户。  
5. **代码可维护性**：考虑将排列逻辑抽离为独立函数或类（例如 `WeightPermuter`），避免未来再次出现“大改”导致的回滚。  

总体来看，此次回滚恢复了对多种量化格式的兼容性，必要且风险可控，重点在于确保相应的测试覆盖完整。

---

### Bump Flashinfer Version and Re-enable DeepSeek NVFP4 AR+Norm Fusion (#34899)
**SHA**: `ea5f903` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ea5f903f80fec5afd4960a3846b8a84b0e53ca6e)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 FlashInfer 依赖从 0.6.3 升级到 0.6.4，并同步更新 Dockerfile、`versions.json`、`requirements/cuda.txt`。  
2. 移除 `DeepseekV3ForCausalLM` 中对 NVFP4 量化下 *All‑reduce‑RMS*（AR+Norm）融合的强制关闭逻辑，改为默认开启，只有 `DeepseekV32ForCausalLM` 直接使用 `VerifyAndUpdateConfig`。  

**🎯 影响范围**  
- Docker 镜像构建与运行时环境（FlashInfer 相关库）。  
- `vllm/model_executor/models/config.py` 中的模型配置验证路径。  
- `vllm/model_executor/models/__init__.py` 中的模型类映射（`DeepseekV3ForCausalLM` 被移除）。  

**💡 关注建议**  
1. **兼容性验证**：确保使用 `DeepseekV3`（若仍被外部调用）在 NVFP4 量化下不会出现原先的融合错误；若不再支持，需在文档中注明此模型已被移除。  
2. **回归测试**：在升级到 FlashInfer 0.6.4 后跑全套单元/集成测试，特别关注含有 AR‑RMS 融合的 DeepSeek 系列模型的推理正确性与性能。  
3. **镜像缓存**：Dockerfile 仍未对 FlashInfer 构建产物做缓存，推荐在 CI 中使用 `--mount=type=cache` 或预构建好的镜像，以降低 nightly 构建时间。  
4. **发布说明**：在 Release Note 中明确说明 “NVFP4 量化下的 DeepSeek AR+Norm 融合已恢复”，并提醒用户在旧版环境（FlashInfer 0.6.3）仍会保持禁用行为。  

总体来看，此次改动提升了 DeepSeek 系列模型的性能潜力，但需做好兼容性回归，防止因旧模型或旧环境引入回归错误。

---

### [Test] Add FP8 KV Cache Testing for MLA Backends (#34473)
**SHA**: `f24b2de` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f24b2de3d3812301932645e3002adba46a8c0055)

**🎯 变更类型**：功能增强（为 MLA 后端加入 FP8 KV‑Cache 的单元测试及代码路径）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `tests/v1/attention/test_mla_backends.py` 中加入对 FP8 KV‑Cache（`fp8`、`fp8_e4m3`、`fp8_ds_mla`）的完整测试，包括参数化、后端过滤、缓存布局填充与解码路径。  
2. 入口层面引入 `_DecodeConcatQuantFP8` 与 `GroupShape`，在 `MLAAttention` 和 `MLAAttentionBase` 中根据 `kv_cache_dtype` 动态走 FP8 分支，使用 `ops.concat_and_cache_mla` 进行缓存写入，并在解码时对查询做 FP8 量化拼接。  
3. 增加 `kv_cache_dtype` 参数向下传递至 `run_attention_backend` 与 `vllm_config.cache_config`，并在测试中根据后端的 `supported_kv_cache_dtypes` 自动跳过不支持的组合。  

**🎯 影响范围**  
- `vllm.model_executor.layers.attention.mla_attention`（新增 `_DecodeConcatQuantFP8` 实例、FP8 相关分支）  
- `vllm.model_executor.layers.attention_layer_base`（KV‑Cache 写入逻辑）  
- `vllm.utils.torch_utils`（dtype 映射）  
- 测试目录 `tests/v1/attention/`（新增大量 FP8 场景）  
- 相关配置 `vllm.config`（`cache_config.cache_dtype` 默认仍为 `auto`，但在测试中被覆盖）  

**💡 关注建议**  
1. **兼容性**：`kv_cache_dtype.startswith("fp8")` 假设 `kv_cache_dtype` 为字符串，若后端返回 `None` 会触发 `AttributeError`。建议在使用前加 `isinstance(..., str)` 或统一使用 `str(kv_cache_dtype)`.  
2. **性能**：`_DecodeConcatQuantFP8` 在每次 forward 时实例化一次（虽为 static），首次编译可能导致额外开销。建议在全局单例或缓存层面复用已编译的 kernel。  
3. **后端支持**：新增 `supported_kv_cache_dtypes` 的检查逻辑依赖每个 backend 正确声明该属性，确保所有已实现的 backend（包括 FlashMLA、XFormers 等）已同步更新，否则测试会因 `skip` 过多而失去意义。  
4. **缓存视图**：`kv_cache = kv_cache.view(current_platform.fp8_dtype())` 仅在非 `fp8_ds_mla` 场景下执行，需确认 `kv_cache` 在内存布局上是 contiguous，避免出现隐式复制或错误的字节解释。  
5. **文档与 CI**：更新 README / doc 中关于 FP8 KV‑Cache 的说明，并在 CI 中加入相应的硬件/模拟器依赖（如支持 FP8 的 GPU），确保 CI 环境不因缺少 FP8 支持而频繁报错。  

总体而言，此次改动为 MLA 系列后端提供了完整的 FP8 KV‑Cache 验证，提升了 vLLM 对低精度推理的覆盖率。但需留意以上兼容性与性能细节，确保在不同硬件和后端组合下的平滑运行。

---

### [CI] Remove failing prime-rl integration test (#34843)
**SHA**: `fac1507` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fac1507f03c78d8717853c8a15ad1d887d71cc1d)

**🎯 变更类型**：CI/测试脚本维护（功能增强‑移除失效的集成测试）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 删除了 `.buildkite/scripts/run‑prime‑rl‑test.sh` 脚本以及所有在 Buildkite CI 配置文件中调用该脚本的步骤。  
- 目的在于剔除长期失败且不再维护的 *Prime‑RL* 集成测试，防止 CI 因该测试卡住或产生误报。  

**🎯 影响范围**  
- **CI 配置**：`.buildkite/test-amd.yaml`、`.buildkite/test_areas/e2e_integration.yaml` 中的 RL 测试节点被移除。  
- **项目根目录**：`run‑prime‑rl‑test.sh` 脚本文件被完全删除。  
- **文档/README**（若有提及 Prime‑RL 测试）可能需要同步更新。  

**💡 关注建议**  

1. **确认依赖清理**  
   - 检查是否还有其他 CI 步骤或文档仍然引用 `run‑prime‑rl‑test.sh`（如 `source_file_dependencies`、README、CONTRIBUTING）。  
   - 若有残留引用，及时删除或注释，防止 CI 报错 `file not found`。  

2. **回归验证**  
   - 在本地或临时 CI 环境执行一次完整的 Buildkite pipeline（除去已删除步骤），确保剩余测试仍能顺利通过且没有因缺少依赖而中断。  

3. **监控后续构建**  
   - 合并后观察几次完整的 CI 运行，确保总耗时不受影响且没有出现新的 flaky 报告。  
   - 若曾经依赖 Prime‑RL 的功能（如 RL‑related 示例或 benchmark）在文档中出现，需要在文档中标记 “已移除”。  

4. **潜在的功能缺失**  
   - 该测试原本验证 vLLM 在 RL 场景下的 GPU 兼容性，移除后相当于放弃了这块验证。如果团队后期仍需支持 RL，建议在单独的分支或外部仓库维护对应的集成测试，而不是在主 CI 中出现频繁失败。  

5. **代码库整洁**  
   - 由于本次提交只删除文件和 CI 配置，未涉及业务代码，风险极低。但请在后续 PR 中保持 `buildkite` 目录的结构清晰，避免残余的 “dead code”。  

总体来看，此次改动是一次安全的 CI 清理，影响仅限于 Buildkite 的配置与脚本，业务代码未受影响。只要确认没有遗漏的引用并监控几轮 CI 即可放心合并。

---

### [compile] Fix torch.compile time discrepancy in logging. (#34912)
**SHA**: `f863994` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f8639940844bcc10e3f374d2bb5aa33ae52a2624)

**🎯 变更类型**：功能增强（修复日志计时不一致）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将编译过程及 `torch.compile` 相关的时间统计从 `time.time()` 换为 `time.perf_counter()`，消除因系统时间调整导致的计时误差。  
- 在 `piecewise_backend.py` 中记录保存编译管理器缓存的耗时，并在耗时 > 1 s 时输出日志。  

**🎯 影响范围**  
- `vllm.compilation.backends`、`vllm.compilation.monitor`、`vllm.compilation.piecewise_backend` 三个核心编译模块。  
- 依赖 `CompilationConfig.compilation_time`、`torch_compile_start_time` 等全局变量的日志、监控与调试功能。  

**💡 关注建议**  
1. **兼容性**：`perf_counter` 在所有受支持平台均可用，但返回的是相对时间，确保外部依赖（如测试中对 `compilation_time` 做数值断言）已同步更新。  
2. **线程安全**：全局变量仍在并发场景下使用，若以后加入多实例或并行编译，需考虑加锁或改为上下文对象。  
3. **日志频率**：新增对缓存保存耗时的 `info_once`，请确认在高频调用路径上不会引入不必要的 I/O 开销。  
4. **回归测试**：增加对 `torch.compile` 总耗时日志的断言，确保 `total_compile_time` 与 `CompilationConfig.compilation_time` 的差值在预期误差范围内。  
5. **文档/示例**：更新 README 或调试指南，明确新日志字段的含义（如 “torch.compile takes X s in total”），帮助用户定位编译瓶颈。  

整体来看，此次改动提升了计时的准确性，并对缓存保存进行可观测化，对调优和问题定位都有积极帮助。只要注意上述兼容与并发细节，即可安全合并。

---

### [Kernel] [Helion] [6/N] Add num_tokens dimension to silu_mul autotuning and dispatching (#34185)
**SHA**: `a6d0299` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a6d0299c75f2c0687334d50d302801ade083c784)

**🎯 变更类型**：功能增强（为 `silu_mul_fp8` 引入 `num_tokens` 维度的自调优与调度）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原先仅依据 `intermediate_size` 与固定 batchsize 的配置键改为 `"intermediate_{size}_numtokens_{tokens}"`，并在 `pick_silu_mul_fp8_config` 中实现：① 按最近的 `intermediate_size` 匹配；② 在对应的 `num_tokens` 列表中选取最小的 ≥ 输入 token 数，若全部不足则取最大；③ 对非法键抛出 `ValueError`。相应测试覆盖了精确、取上限、取上限回退、异常等情况，并更新了 autotuning 输入生成以及配置 JSON。

**🎯 影响范围**  
- `vllm/kernels/helion/ops/silu_mul_fp8.py`（核心调度逻辑）  
- `vllm/kernels/helion/configs/silu_mul_fp8.json`（配置键格式）  
- `tests/kernels/helion/test_silu_mul_fp8.py`（新增/修改测试）  

**💡 关注建议**  
1. **向后兼容**：旧键 `"*_batchsize_*"` 已被全部移除，若仍有历史 autotune 产物或外部脚本依赖，需提供迁移或兼容层。  
2. **错误信息**：`pick_silu_mul_fp8_config` 抛出 `ValueError` 时信息明确，但在调用方未捕获时会导致运行时异常，建议在上层（如 kernel 调用入口）加入安全兜底或回退到 `"default"`。  
3. **性能验证**：新增的 `num_tokens` 选取逻辑会在每次调用时做一次排序和遍历，开销极小但可在热路径上使用 `bisect` 加速。  
4. **配置管理**：`generate_silu_mul_fp8_inputs` 生成的键数量激增（数千），确保 `silu_mul_fp8.json` 与实际键保持同步，防止因缺失导致意外回退。  
5. **文档更新**：在 README/Kernel 使用说明中注明新键格式及 `num_tokens` 取值范围，帮助用户自行扩展 autotune。  

整体改动合理，提升了对不同序列长度的适配能力，只要注意兼容性和文档同步即可顺利上线。

---

#### 🟢 低重要度变更 (9)

### [Bugfix] Gate 256-bit instructions to CUDA 12.9+ (#34791)
**SHA**: `272b535` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/272b535ab3315a2ed3cd1a5e9803df2b86da4f07)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `csrc/activation_kernels.cu` 中为 256 位加载/存储指令新增 CUDA 12.9+ 检测，防止在旧版 CUDA 编译时报错。

---

### [Doc] Fix example of eagle3 (#34960)
**SHA**: `bebfe55` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bebfe55b1c17c2e0fedb1b402df1dddfc1a04684)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/features/speculative_decoding/eagle.md` 示例中，将 `method` 参数从 `"eagle"` 更正为 `"eagle3"`，保证文档与实际模型名称对应。

---

### [Core] Minor structured-output related scheduler optimization (#34765)
**SHA**: `820d781` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/820d7815ebd5e88118e5be02870af9ce49a314b1)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正调度器对结构化输出的预填充块判断，避免无效位掩码生成，提升调度性能。

---

### [ROCm][AITER] Fix aiter paged_attention_v1 decode for sliding window and head_size < 64 (#34570)
**SHA**: `cf93c1a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cf93c1a12849693d6bcee3e2c917c02e1fc9a47f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm AITER 实现中，针对 head_size < 64 或滑动窗口注意力，新增统一 attention（triton）回退路径，并在使用 shuffle KV 缓存时加入断言，防止不兼容。这样修复了 paged_attention_v1 解码在小 head_size 场景下的错误。

---

### Support prompt_embeds for pooling requests in output processor (#34904)
**SHA**: `59c6233` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/59c62332978fcce318784df499713764f14c7bc1)

**变更类型**：代码重构  
**重要程度**：🟢低  
**摘要**：在 `output_processor.py` 中为 pooling 请求加入 prompt_embeds 支持，使用占位 token ids 以兼容嵌入输入。

---

### [ROCm][Bugfix]: Only save unpadded sizes for shared_experts in MoERunner to fix rmsnorm pad fusion (#34636)
**SHA**: `ded333f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ded333fb9b903e9de9f1cc5d82d2b5c5ab726750)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `MoERunner` 中仅在 `shared_experts` 存在时保存原始 (未填充) 的 hidden_states 与其维度，防止 rmsnorm pad fusion 时出现尺寸错误。

---

### [CI/Build] Add opentelemetry libs in default vllm build (requirements/common.txt) (#34466)
**SHA**: `e739c29` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e739c29ea451869e073a31b5d8cbc6b88f162e8d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `requirements/common.txt` 中加入 OpenTelemetry 依赖，并在示例 README 中说明这些核心包已随 vLLM 打包，无需手动安装。

---

### [AMD][CI] Fix test_custom_allreduce for A100 testgroup (#34735)
**SHA**: `0632ed8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0632ed8778cab44de6152eb873d09fa40c241962)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/distributed/test_custom_all_reduce.py` 中新增对 `HIP_VISIBLE_DEVICES` 环境变量的清除，确保在 AMD CI 环境下运行测试时不受该变量影响。

---

### [compile] Move torch_aot_compile directory under torch_compile_cache (#34831)
**SHA**: `e4a5d8c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e4a5d8c653fc00adb06922bddcb7fec14b01a62b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 AOT 编译缓存路径从 `VLLM_CACHE_ROOT/torch_aot_compile` 调整为 `VLLM_CACHE_ROOT/torch_compile_cache/torch_aot_compile`，并相应更新注释与路径拼接。

---

