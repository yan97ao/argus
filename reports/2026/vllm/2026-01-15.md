# 每日更新报告（2026-01-15）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-15 23:29:53 | Pleaplusone | [ROCm][Perf] Enable shuffle kv cache layout and assembly paged attention kernel for `AiterFlashAttentionBackend` (#29887) |
| 2026-01-15 23:25:55 | Dipika Sikka | [Quant] Support MXFP4 W4A16 for compressed-tensors MoE models  (#32285) |
| 2026-01-15 22:49:57 | Matthew Bonanni | [Attention][MLA] Make `FLASHINFER_MLA` the default MLA backend on Blackwell, and TRTLLM the default prefill (#32339) |
| 2026-01-15 20:16:00 | brian033 | [ROCm] Improve error handling while loading quantized model on gfx120… (#31715) |
| 2026-01-15 19:52:12 | Cyrus Leung | [3/N] Group together media-related code (#32406) |
| 2026-01-15 19:05:54 | rasmith | [CI][BugFix][AMD][FP8] Fix test_rms_norm so it runs correctly on ROCm (#32372) |
| 2026-01-15 19:01:40 | Douglas Lehr | [ROCM] Add ROCm image build to release pipeline (#31995) |
| 2026-01-15 18:49:31 | Chauncey | [Refactor] [11/N] to simplify the mcp architecture (#32396) |
| 2026-01-15 17:25:46 | rongfu.leng | [Benchmark] [Feature] add vllm bench sweep startup command (#32337) |
| 2026-01-15 17:02:30 | Cyrus Leung | [2/N] Move cache factories to MM registry (#32382) |
| 2026-01-15 17:01:59 | Cyrus Leung | [Model] Avoid token selection in SigLIP pooling head (#32389) |
| 2026-01-15 16:45:59 | seeksky | fix: avoid crash on zero-arg tool calls in glm4 parser (#32321) |
| 2026-01-15 16:31:16 | dtc | [Bugfix] Strengthen the check of X-data-parallel-rank in Hybrid LB mode (#32314) |
| 2026-01-15 15:41:34 | Chauncey | [Refactor] [10/N] to simplify the vLLM openai completion serving architecture (#32369) |
| 2026-01-15 15:19:34 | Andreas Karatzas | [ROCm][CI] Pin transformers 4.57.3 to fix jina test failures (#32350) |
| 2026-01-15 14:39:37 | Ofir Zafrir | [Bugfix] Fix stale `common_attn_metadata.max_seq_len` in speculative decoding with Eagle (#32312) |
| 2026-01-15 14:32:22 | Lucas Wilkinson | [BugFix] Fix DeepSeek-V3.1 + DeepGEMM incompatible scale shapes (#32361) |
| 2026-01-15 13:29:34 | Ning Xie | [code clean] remove duplicate check (#32376) |
| 2026-01-15 13:04:34 | rasmith | [CI][AMD][Quantization][BugFix] Fix fp8 max in quant_utils.py and update test_fp8_quant.::test_static_fp8_quant_group_2d to use correct fp8 dtype and adjust atol/rtol (#32201) |
| 2026-01-15 12:53:43 | Micah Williamson | [ROCm][CI] Disable async scheduling on ROCm for test_structured_output[meta-llama/Meta-Llama-3.1-8B-Instruct-xgrammar-auto-speculative_config9] (#32355) |
| 2026-01-15 12:50:48 | kzwrime | [Bugfix] Add CpuCommunicator.dispatch and combine to fix DP+MoE inference (#31867) |
| 2026-01-15 12:29:56 | Li Wang | [Misc] Remove redundant line (#32366) |
| 2026-01-15 12:07:26 | Shiyan Deng | Support configure skip_special_tokens in openai response api (#32345) |
| 2026-01-15 12:05:48 | baonudesifeizhai | Fix optional parameter parsing in MiniMax M2 tool parser #32278 (#32342) |
| 2026-01-15 12:01:42 | Ryan Rock | [CI/Build][Hardware][AMD] Fix v1/shutdown (#31997) |
| 2026-01-15 04:46:56 | dolpm | [compile] raise on compile_size implicit padding (#32343) |
| 2026-01-15 04:10:01 | Lumosis | [BugFix] Assign page_size_padded when unifying kv cache spec. (#32283) |
| 2026-01-15 03:32:48 | vllmellm | [Bugfix][ROCm][performance] Resolve the performance regression issue of the Qwen3-Next-80B-A3B-Thinking under rocm_atten (#32336) |
| 2026-01-15 02:20:52 | Aleksandr Samarin | [MODEL] Fix handling of multiple channels for gpt-oss with speculative decoding  (#26291) |
| 2026-01-15 00:53:36 | qli88 | [CI] Move rixl/ucx from Dockerfile.rocm_base to Dockerfile.rocm (#32295) |

### 📊 统计摘要
> 本日共 30 个提交 | 🔴高 2 | 🟡中 14 | 🟢低 14
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[3/N] Group together media-related code (#32406)](#2845978)
    - [[Refactor] [10/N] to simplify the vLLM openai completion ...](#4c1c501)
  - [🟡 中重要度变更 (14)](#-🟡-中重要度变更-14)
    - [[ROCm][Perf] Enable shuffle kv cache layout and assembly ...](#130d6c9)
    - [[Quant] Support MXFP4 W4A16 for compressed-tensors MoE mo...](#361dfdc)
    - [[Attention][MLA] Make `FLASHINFER_MLA` the default MLA ba...](#8ebfaca)
    - [[Refactor] [11/N] to simplify the mcp architecture (#32396)](#707b44c)
    - [[Benchmark] [Feature] add vllm bench sweep startup comman...](#3a4e10c)
    - [[2/N] Move cache factories to MM registry (#32382)](#cbbae38)
    - [fix: avoid crash on zero-arg tool calls in glm4 parser (#...](#a52d139)
    - [[Bugfix] Strengthen the check of X-data-parallel-rank in ...](#1e58482)
    - [[code clean] remove duplicate check (#32376)](#9d7ae3f)
    - [[CI/Build][Hardware][AMD] Fix v1/shutdown (#31997)](#15422ed)
    - [[compile] raise on compile_size implicit padding (#32343)](#8471b27)
    - [[BugFix] Assign page_size_padded when unifying kv cache s...](#66652e8)
    - [[MODEL] Fix handling of multiple channels for gpt-oss wit...](#d084e9f)
    - [[CI] Move rixl/ucx from Dockerfile.rocm_base to Dockerfil...](#3a61232)
  - [🟢 低重要度变更 (14)](#-🟢-低重要度变更-14)
    - [[ROCm] Improve error handling while loading quantized mod...](#b89275d)
    - [[CI][BugFix][AMD][FP8] Fix test_rms_norm so it runs corre...](#8853a50)
    - [[ROCM] Add ROCm image build to release pipeline (#31995)](#c5891b5)
    - [[Model] Avoid token selection in SigLIP pooling head (#32...](#cdba4c7)
    - [[ROCm][CI] Pin transformers 4.57.3 to fix jina test failu...](#ae1eba6)
    - [[Bugfix] Fix stale `common_attn_metadata.max_seq_len` in ...](#e9ec2a7)
    - [[BugFix] Fix DeepSeek-V3.1 + DeepGEMM incompatible scale ...](#2c9b4cf)
    - [[CI][AMD][Quantization][BugFix] Fix fp8 max in quant_util...](#3c26856)
    - [[ROCm][CI] Disable async scheduling on ROCm for test_stru...](#773d707)
    - [[Bugfix] Add CpuCommunicator.dispatch and combine to fix ...](#edadca1)
    - [[Misc] Remove redundant line (#32366)](#d86fc23)
    - [Support configure skip_special_tokens in openai response ...](#375e598)
    - [Fix optional parameter parsing in MiniMax M2 tool parser ...](#19b251f)
    - [[Bugfix][ROCm][performance] Resolve the performance regre...](#e27078e)
#### 🔴 高重要度变更 (2)

### [3/N] Group together media-related code (#32406)
**SHA**: `2845978` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/28459785ff905648f08a42049904d7d162646398)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将所有多模态（audio / image / video）相关的实现从分散的 `vllm.multimodal.*` 模块统一迁移至新子包 `vllm.multimodal.media`，包括 `MediaIO`、`MediaWithBytes`、`AudioMediaIO`、`AudioEmbeddingMediaIO`、`ImageMediaIO`、`ImageEmbeddingMediaIO`、`VideoMediaIO`。  
- 更新项目内部及测试代码的导入路径，修正了若干旧模块的引用（`base` → `media`），并在 `tools/pre_commit/check_pickle_imports.py` 中加入新的 pickle‑兼容测试文件。  
- 为 `MediaWithBytes` 添加了 `__getstate__`/`__setstate__` 实现，解决了之前在 pickle/unpickle 时的 `RecursionError`（issue #30818）。  
- 对 `ImageMediaIO` 增加了 RGBA 背景颜色可配置、参数校验以及向后兼容的默认白色。  
- 删除了旧的 `audio.py`、`image.py`、`video.py` 中的多媒体 IO 实现，改为在 `media/` 包中重新实现，保持功能等价并加入详细单元测试。  

**🎯 影响范围**：  
- **核心模块**：`vllm/multimodal/*`（`audio.py、image.py、video.py、base.py、parse.py、utils.py、inputs.py、hasher.py`）全部改为引用 `vllm.multimodal.media.*`。  
- **测试套件**：所有原有多模态测试文件已改为使用新路径，新加入 `tests/multimodal/media/*`（audio、image、video、base）。  
- **工具链**：`tools/pre_commit/check_pickle_imports.py` 需要同步更新路径。  
- **对外使用者**：任何直接 `import vllm.multimodal.base`、`vllm.multimodal.image`、`vllm.multimodal.audio`、`vllm.multimodal.video` 的代码将在升级后失效，需迁移至 `vllm.multimodal.media`。  

---

### 🔍 技术洞察

| 维度 | 影响概述 |
|------|----------|
| **架构影响** | - **模块化提升**：通过 `media` 包把所有 IO、包装类统一，形成清晰的 “媒体层” 与 “业务层” 分离，后续添加新模态（如 `text`、`3d`）只需在该包内扩展。<br>- **依赖层次**：`media` 包内部仍然依赖 `vllm.logger`、`vllm.utils.import_utils` 以及第三方库（`librosa`、`soundfile`、`PIL`、`torch`），但这些依赖已集中在子包入口，降低了根包的导入成本。<br>- **向后兼容**：旧的 `vllm.multimodal.audio`、`.image`、`.video` 文件仍然存在（但已被清空或仅保留版权头），因此外部仍能 import 但得到空模块；这会在运行时抛 `ImportError`，需要升级指南。 |
| **性能影响** | - **几乎无运行时开销**：仅是 import 路径的变化，类实现逻辑保持不变。<br>- **pickle 改进**：新增 `__getstate__/__setstate__` 消除了递归调用导致的栈溢出，提升序列化/分布式通信的可靠性。<br>- **图片背景颜色校验**：在 `ImageMediaIO.__init__` 中添加一次小的类型检查，成本极低（O(1)）。 |
| **安全考虑** | - **Base64 解码**：统一在 `media` 包内使用 `base64`（或 `pybase64`）进行解码，已加入 `validate=True` 参数，防止异常字符导致的缓冲区错误。<br>- **稀疏张量安全**：`AudioEmbeddingMediaIO`、`ImageEmbeddingMediaIO` 在加载 tensor 时保持 `torch.sparse.check_sparse_tensor_invariants()`，未受本次改动影响。<br>- **环境变量控制**：`VideoMediaIO` 现在会 **pop** `video_backend` 参数，防止该键意外泄漏到底层 loader，降低潜在的后门攻击面。 |
| **可维护性** | - **单一入口**：所有媒体 IO 类集中，文档、类型提示、单元测试只需要维护一个位置。<br>- **测试覆盖**：新增 4 + 3 + 1 + 1 = 9 个测试文件，覆盖了加载、编码、参数校验、pickle，提升回归安全性。<br>- **代码重复度降低**：原来 `audio.py`、`image.py`、`video.py` 中的 `MediaIO` 基类引用重复，现在统一在 `media/base.py`。 |
| **兼容性 & 迁移成本** | - **内部**：项目内部所有导入已经更新，CI 通过。<br>- **外部**：使用者需要把 `from vllm.multimodal.image import ImageMediaIO` 改为 `from vllm.multimodal.media import ImageMediaIO`（同理 audio、video、MediaWithBytes）。<br>- **重大风险**：如果外部代码仍依赖已删除的模块，会在运行时抛 `ImportError`。建议在发布说明中提供 **兼容 shim**（例如在 `vllm/multimodal/__init__.py` 中重新导出新类）以平滑迁移。 |
| **回归风险** | - **pickle**：虽然已实现 `__getstate__/__setstate__`，但旧版本的 pickle 文件（未使用新实现）在升级后仍应可解码；需要在 CI 中加入向后兼容性测试。<br>- **视频后端选择**：`VideoMediaIO` 现在在 `__init__` 中 `pop` `video_backend`，若用户自行在 `kwargs` 中传递 `video_backend` 并期望它在 loader 中可见（旧实现可能会把它原样传递），行为会改变。<br>- **图片背景颜色**：默认仍为白色，保持兼容；但若用户依赖 `rgba_background_color` 为列表而未检测类型，新的校验会抛 `ValueError`。 |
| **文档 & 迁移建议** | - 更新项目 README 与 API 文档，列出新导入路径。<br>- 在 `vllm/multimodal/__init__.py` 添加向后兼容的别名，例如 `from .media import ImageMediaIO as ImageMediaIO`，并在文档中注明该别名将在下个 major 版本移除。<br>- 在发布 notes 中强调 `MediaWithBytes` 现在支持 pickle，旧的 `RecursionError` 已修复。 |

---

### ⚠️ 潜在风险

1. **破坏外部导入**  
   - 直接引用旧模块的第三方库会因 `ImportError` 停止工作。  
   - 解决方案：在主 `v

---

### [Refactor] [10/N] to simplify the vLLM openai completion serving architecture (#32369)
**SHA**: `4c1c501` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4c1c501a7ee1d5efbad945ea62a702ce5cefb799)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：本次提交对 vLLM 的 OpenAI 接口层进行大幅重构，拆分出 `vllm.entrypoints.openai.models` 包，统一模型/LoRA 路径定义，迁移 Completion 相关协议至 `openai/completion` 子包，并新增 Anthropic API 路由。所有原有 import、路由注册以及内部服务引用均同步更新，以实现「模型/协议」与「业务逻辑」的解耦，提升代码可维护性与可扩展性。

**🎯 影响范围**：  
- `vllm/entrypoints/openai/*`（包括 chat、completion、transcriptions、models、responses 等）  
- `vllm/entrypoints/anthropic/*`（新增 `api_router.py` 与路由挂载）  
- 多数 `tests/entrypoints/openai/*` 单元测试文件的 import 调整  
- `vllm/entrypoints/openai/engine/protocol.py`（删除冗余 Completion 定义）  
- 相关 utils、serve、sagemaker、elastic‑ep、disagg、tokenize 等子模块的 import 同步改动  

---

## 🔍 技术洞察

| 维度 | 影响 | 说明 |
|------|------|------|
| **架构影响** | ✅ 大幅模块化 | 将模型路径、LoRA 路径抽离至 `models` 子包，形成 **“协议层”**（protocol） ↔ **“服务层”**（serving） 的清晰分层。<br>Completion 请求/响应从 `engine/protocol` 移至 `completion/protocol`，避免跨业务（chat vs. completion）混用同一文件，提升可读性。 |
| **路由组织** | ✅ 新增独立 API Router | `openai/models/api_router.py`、`openai/completion/api_router.py`、`anthropic/api_router.py` 分别负责 `/v1/models`、`/v1/completions`、`/v1/messages`，在 `api_server.build_app` 中统一挂载，便于后续插件/多模型集成。 |
| **代码重复** | ✅ 消除重复定义 | 之前 `CompletionRequest`/`CompletionResponse` 同时出现在 `engine/protocol.py` 与新 `completion/protocol.py`，本次删除旧实现，所有引用统一指向新文件，降低维护成本。 |
| **性能影响** | ⚪ 近乎零 | 重构只涉及 import 重排与路由组织，运行时逻辑未改变；StreamingResponse、EngineClient 调用路径保持不变。 |
| **安全考虑** | ✅ 维持原有安全模型 | - 继续使用 `validate_json_request` 对所有入口进行 JSON 验证。<br>- Anthropic 路由复用 OpenAI 错误协议转换，保持统一错误码/信息。<br>- 新增 `AnthropicServingMessages` 与 `OpenAIServingModels` 的状态检查，避免未实现模型的意外调用。 |
| **兼容性** | ⚠ 破坏性 | - 公开的 import 路径已改变（如 `vllm.entrypoints.openai.serving_models` → `vllm.entrypoints.openai.models.*`），外部插件或自定义脚本若直接引用旧路径将抛 `ImportError`。<br>- 单元测试已同步更新，仍需在 CI 中确保所有用户代码（如第三方集成）也完成迁移。 |
| **可维护性** | ✅ 大幅提升 | - 每类协议集中在对应子包，新增协议或模型只需在 `models/protocol.py` 中定义。<br>- 路由结构化后，新增服务（例如 Ollama、Gemini）只需实现对应 `serving` 与 `api_router` 即可。 |
| **潜在风险** | 1. 循环导入风险<br>2. 未同步的旧 import 在运行时可能导致 `AttributeError`<br>3. 文档/示例仍引用旧路径 | 详细见下方“潜在风险”。 |

---

## ⚠️ 潜在风险

1. **循环导入**  
   - `completion/protocol.py` 需要引用 `engine/protocol` 的 `LogitsProcessors` 等，而 `engine/protocol.py` 已经不再依赖 `completion`，但在某些模块（如 `utils.py`）中同时导入两者，可能产生间接循环。若出现 `ImportError: cannot import name ...`，需要检查导入顺序或改为局部导入。  
2. **向后兼容性破坏**  
   - 任何外部代码仍旧使用 `vllm.entrypoints.openai.serving_models`（如插件、用户脚本）将失效。虽然项目内部已统一更新，但经常有 downstream 项目直接引用这些路径。  
3. **文档/示例未同步**  
   - 官方文档、README、example notebooks 中的 import 示例若保持旧路径，会导致新手用户跑不通示例。  
4. **测试遗漏**  
   - 大量文件的 import 改动后，若有未被覆盖的内部模块仍残留旧路径，会在运行时触发错误（尤其是动态 import 场景，如 `load_aware_call` 通过 `importlib` 加载模块路径）。  
5. **Anthropic 接口的错误映射**  
   - `AnthropicErrorResponse` 直接映射 OpenAI `ErrorResponse`，若后续 OpenAI 错误模型扩展而未同步到 Anthropic，可能出现响应字段缺失。  

---

## 💡 关注建议

| 类别 | 建议 |
|------|------|
| **兼容层** | 在 `vllm/entrypoints/openai/__init__.py` 中保留一个**轻量兼容 shim**：<br>`from .models.serving import OpenAIServingModels as _LegacyOpenAIServingModels`，并在 `__all__` 中导出旧名称，以便外部临时兼容。 |
| **循环导入** | 将 `LogitsProcessors`、`Logprob` 等共用类型抽到独立 `vllm/entrypoints/openai/common/protocol.py`，供 `engine` 与 `completion` 共用，彻底破除循环依赖。 |
| **文档同步** | 更新以下位置的 import 示例：<br>• `README.md` / `examples/`<br>• API 文档生成脚本（若使用 sphinx autodoc）<br>• 官方博客/教程 | 
| **CI/测试** | 在 CI 中加入 **import 兼容性检查**：<br>```bash<br>python - <<EOF<br>import importlib, pkgutil, sys<br>try: importlib.import_module('vllm.entrypoints.openai.serving_models')<br>except ModuleNotFoundError: pass  # Expected after refactor<br>EOF<br>```<br>确保仅在特定分支保留兼容 shim。 |
| **迁移指南** | 为下游用户提供 *Migration Guide*，列出旧路径 → 新路径对照表，并提示执行 `grep -R "serving_models"` 的全项目检查。 |
| **Anthropic 路由** | 将 `AnthropicErrorResponse` 的模型定义放入 `anthropic/protocol.py`，并在 `api_router` 中统一使用 `translate_error_response`，防止以后错误结构改动遗漏。 |
| **性能监控** | 虽然运行时无性能变化，但增加的路由层次会略微增加启动时间（额外的 import）。建议在 `vllm/entrypoints/utils.py` 中使用延迟 import（如 `from importlib import import_module`）来保持启动速度。 |
| **版本号** | 由于公开 API 路径变更，建议将 **major** 版本号提升（如 0.4 → 0.5）或在 release note 中标记 *Breaking Change*，提醒 downstream。 |

---

## 综合评价

此次 PR 通过 **模块化、职责分离** 的方式，对 vLLM 的 OpenAI

---

#### 🟡 中重要度变更 (14)

### [ROCm][Perf] Enable shuffle kv cache layout and assembly paged attention kernel for `AiterFlashAttentionBackend` (#29887)
**SHA**: `130d6c9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/130d6c9514856cb5a152329f0382d60ff6e8d97e)

**🎯 变更类型**：功能增强（ROCm AIter后端加入 KV‑Cache shuffle 布局及汇编实现的分页注意力）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 新增环境变量 `VLLM_ROCM_SHUFFLE_KV_CACHE_LAYOUT`（默认 `False），用于在 ROC m 上开启 KV‑Cache 的 *shuffle* 布局。  
- 在 `vllm._aiter_ops` 中加入对应的开关 `is_shuffle_kv_cache_enabled`。  
- `rocm_aiter_fa.py` 重构：  
  - `cp_mha_gather_cache_kernel` 支持两种布局 `NHD` 与 `SHUFFLE`，实现了对 `head_id` 的并行读取与写入，简化了原先的循环与 mask。  
  - 新增 `reshape_and_cache_shuffle_kernel` 与 `reshape_and_cache_shuffle_triton`，负责把 `key/value` 按 `shuffle` 布局写入 KV‑Cache，并在 fp8 量化场景下兼容 scale。  
  - 注意力前向中根据开关自动选择 `NHD`（原实现）或 `SHUFFLE` 布局；在 decode 阶段若打开 shuffle，则调用 `aiter.pa_fwd_asm` 汇编实现的分页注意力。  
- `AttentionMetadata` 新增 `k_scale/v_scale` 用于 fp8 shuffle 布局的 per‑token 量化尺度。  
- 相关构造函数在 `AiterFlashAttentionMetadataBuilder` 中为 shuffle 布局分配 scale 张量。  

**🎯 影响范围**  
- `vllm/_aiter_ops.py`、`vllm/envs.py`（环境变量）  
- `vllm/v1/attention/backends/rocm_aiter_fa.py`（核心注意力实现）  
- `vllm/attention` 及其调用路径：`Attention`、`AiterFlashAttentionMetadata`、`AiterFlashAttentionMetadataBuilder`、`forward` 逻辑。  

**💡 关注建议**  
1. **兼容性**：默认关闭，已有代码保持行为不变。若开启，需要确认模型使用的 KV‑Cache dtype 为 fp8（或其它支持的 dtype），否则会触发 assert。  
2. **性能验证**：shuffle 布局主要提升 FP8 场景下的缓存访存局部性，建议在 MI 3/MI 4 GPU 上对比 `NHD` 与 `SHUFFLE` 的吞吐与延迟，关注 `block_size`、`x = 16/element_size` 的取值是否合理。  
3. **测试覆盖**：添加单元测试验证 `cp_mha_gather_cache_kernel` 在两种布局下的输出等价；对 `reshape_and_cache_shuffle_triton` 做恰当的 shape‑cast 与数值误差检查。  
4. **文档 & 环境**：在 README/文档中说明新环境变量的含义、适用模型（fp8）以及已知限制（如 sliding‑window 暂不支持 shuffle 布局）。  
5. **故障排查**：若出现 “Sliding window with shuffle layout is not supported yet.” 的断言，请确认未在滑动窗口模型上开启该开关。  

总体来看，此次改动为 ROCm 环境下的 FlashAttention 引入了更高效的 KV‑Cache 布局与汇编实现，若配合 fp8 量化可期待显著的带宽提升。开发者在开启前务必检查模型 dtype 与 sliding‑window 兼容性，并补充对应的测试与文档。

---

### [Quant] Support MXFP4 W4A16 for compressed-tensors MoE models  (#32285)
**SHA**: `361dfdc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/361dfdc9d81ef16f00cf9f51dc104b55e382ee61)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 MoE（Mixture‑of‑Experts）模型新增 MXFP4 W4A16 量化路径，支持 compressed‑tensors‑based MoE 的 MXFP4 方案。  
- 在 `nvfp4.py` 中加入 `make_mxfp4_moe_quant_config`，并在 `compressed_tensors_moe.py` 实现 `CompressedTensorsW4A4Mxfp4MoEMethod`，完成权重创建、scale 处理、Marlin 后端准备及核调用。  
- 同步新增测试配置 `Qwen3‑30B‑A3B‑MXFP4A16.yaml` 并在模型列表中登记。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/oracle/nvfp4.py`（量化配置）  
- `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`（MoE 方法实现）  
- `vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py`（Marlin 准备函数）  
- 测试目录 `tests/evals/gsm8k/configs/`（新增模型配置）  

**💡 关注建议**  
1. **后端兼容性**：当前实现默认使用 `NvFp4MoeBackend.MARLIN`，请确认运行环境已编译并加载 Marlin‑fp4 kernel，否则会在加载 MXFP4 模型时报错。  
2. **scale 数据类型**：`w13_weight_scale`、`w2_weight_scale` 仍采用 `torch.uint8`，若后续改为 BF16/FP16，请同步 `make_mxfp4_moe_quant_config` 参数类型。  
3. **内存布局**：权重采用 2 fp4 打包（`hidden_size // 2`、`intermediate_size // 2`），在自定义模型或更改 hidden/intermediate 大小时，需要确认维度为偶数，否则会触发 shape 错误。  
4. **测试覆盖**：新增的 GSM8K 配置已加入 `models‑small.txt`，建议在 CI 中加入对应的性能/精度基准，确保与原始 FP16/FP8 MoE 结果保持在阈值内（`accuracy_threshold: 0.88`）。  
5. **文档和用户指引**：更新 README/Quantization 章节，说明如何在 `vllm` 启动时通过 `--quantization=mxfp4`（或相应 flag）激活此路径，并提醒用户在多卡部署时保持 `--enforce-eager` 以避免 Marlin 的调度冲突。  

总体来看，此次提交为 MXFP4‑W4A16 量化提供了完整的 MoE 支持，若环境准备充分，能够在保持较低显存占用的同时提供接近 FP16 的推理准确率。后续关注 kernel 编译、尺度数据匹配以及 CI 精度回归即可。

---

### [Attention][MLA] Make `FLASHINFER_MLA` the default MLA backend on Blackwell, and TRTLLM the default prefill (#32339)
**SHA**: `8ebfaca` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8ebfacaa7524dd9641cef9c5e4151214e0912dfd)

**🎯 变更类型**：功能增强 / 默认行为调整  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 `AttentionConfig.use_trtllm_ragged_deepseek_prefill` 的默认值改为 `True`，并在 MLA 初始化时优先使用 TRT‑LLM ragged DeepSeek prefill。  
2. 在 Blackwell（SM100）设备上，默认的 MLA 后端从 **CUTLASS_MLA** 改为 **FLASHINFER_MLA**，并相应地调整后端优先级列表。  
3. 将日志级别统一为 `info_once`（并加入 `scope="local"`），提升可观测性。  

**🎯 影响范围**  
- `vllm/config/attention.py`（默认配置）  
- `vllm/model_executor/layers/attention/mla_attention.py`（prefill 路径选择）  
- `vllm/platforms/cuda.py`（后端优先级与 Blackwell 默认后端）  

**💡 关注建议**  
- **依赖检查**：使用 FLASHINFER_MLA 前必须确保 FlashInfer 库已正确安装并兼容当前 CUDA 版本；使用 TRT‑LLM ragged DeepSeek prefill 前同样需要相应的 TRT‑LLM 包。  
- **回退验证**：在非 Blackwell GPU（如 SM80/90）或在稀疏（DSv3.2）模式下，仍需验证仍会回退到原有后端，防止意外崩溃。  
- **性能回归**：建议在 Blackwell 实例上跑一次基准（prefill + decode）对比旧的 CUTLASS_MLA，确认期望的吞吐/延迟提升。  
- **文档与示例**：更新 README/Config 文档，说明默认行为已改为 FlashInfer‑MLA 与 TRT‑LLM prefill，并提供关闭方式（`use_trtllm_ragged_deepseek_prefill=False`、`backend=CUTLASS_MLA`）。  
- **日志监控**：由于日志从 `debug_once` 改为 `info_once`，生产环境会多出这些信息，确保日志采集策略能接受。  

总体而言，此次改动旨在在 Blackwell GPU 上利用 FlashInfer‑MLA 与 TRT‑LLM 的更高效实现，若环境满足依赖，预计能带来显著的预填充性能提升。请在升级前做好依赖、回退与性能验证。

---

### [Refactor] [11/N] to simplify the mcp architecture (#32396)
**SHA**: `707b44c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/707b44cc281ce5db8c079962795912b6c337a7f0)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原有的 MCP（Multi‑Channel Processor）相关实现搬迁至 `vllm.entrypoints.mcp` 包，统一了工具、工具服务器、响应上下文等模块的路径，并相应更新了所有内部引用及单元测试。  

**🎯 影响范围**  
- `vllm.entrypoints.mcp.*`（新增 / 重命名）  
- `vllm.entrypoints.openai.responses.*`（导入路径改动）  
- `vllm.entrypoints.context` 相关引用全部改为 `openai.responses.context`  
- 推理层 (`vllm.reasoning.*`) 与工具服务器的耦合点  
- 多处测试文件、`api_server.py`、`engine/serving.py`、`responses/serving.py` 等入口代码  

**💡 关注建议**  
1. **兼容性**：如果外部用户仍依赖旧路径（如 `vllm.entrypoints.tool_server`），建议保留薄包装模块或在 `__init__` 中提供向后兼容的导入，以免突发 `ImportError`。  
2. **文档同步**：更新 README/开发者文档、API 参考以及示例代码中的 import 示例，确保开发者能快速定位新路径。  
3. **导出统一**：在 `vllm/entrypoints/mcp/__init__.py` 中显式导出 `ToolServer、Tool、HarmonyBrowserTool、HarmonyPythonTool` 等核心类，避免在其他子包再次硬编码路径。  
4. **测试覆盖**：所有受影响的单元测试已修改，但请在 CI 中跑一次完整的集成测试，确保工具调用、流式解析和响应构造在新路径下保持行为一致。  
5. **性能检查**：虽然改动仅是 import 重构，仍建议对 `engine/serving.py` 中的上下文创建路径进行轻量基准，以确认没有因循环导入导致的额外加载开销。  

总体而言，此次重构提升了代码组织清晰度，但需注意对外使用者的迁移成本，建议提供短期的兼容层并完善文档。

---

### [Benchmark] [Feature] add vllm bench sweep startup command (#32337)
**SHA**: `3a4e10c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3a4e10c8477c329b9e75ba55ff205a1f258cbd01)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `vllm bench sweep` 增加了 **startup** 子命令，能够在不同 engine 参数组合下批量测量模型冷/热启动时延。  
- 新增 `vllm/benchmarks/sweep/startup.py` 实现参数读取、过滤（支持 `--strict-params`）、多次运行、结果归档与 CSV 汇总。  
- 文档 `docs/benchmarking/sweches.md` 同步说明使用方式。  

**🎯 影响范围**  
- `vllm/benchmarks/sweep/cli.py`（子命令注册）  
- 新增模块 `vllm/benchmarks/sweep/startup.py`（核心逻辑）  
- 文档 `docs/benchmarking/sweeps.md`  
- 依赖 `pandas`（通过占位模块包装，避免硬依赖）

**💡 关注建议**  
1. **恢复（resume）实现缺失**：`run_main` 只创建时间戳目录，但 `run_combs`/`run_comb` 并未检查已有子目录或结果文件，导致 `--resume` 实际无效。建议在 `run_combs` 中加入目录遍历与跳过已完成组合的逻辑。  
2. **错误信息与日志**：当前 `print` 用于报告参数过滤与命令执行，生产环境建议换成 `logging`，并统一日志级别。  
3. **参数过滤边界**：`_is_supported_param` 只检测前缀，若提供 `model.name` 之类的嵌套键会误报为支持，建议在文档中明确支持的键或改进检测。  
4. **测试覆盖**：新增功能涉及子进程调用、文件 I/O、参数组合，建议补充单元测试（mock `subprocess.run`、`Path.open`），防止回归。  
5. **兼容性**：占位 `pandas` 可避免 ImportError，但后续调用 `.DataFrame` 必须确保实际环境已装包，否则会在运行时抛异常。可在入口处提前检查并给出友好提示。  

总体来看，功能实现完整，文档与 CLI 接口保持一致，若上述细节得到改进，可提升可用性与可靠性。

---

### [2/N] Move cache factories to MM registry (#32382)
**SHA**: `cbbae38` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cbbae38f9368b6c35d9b9295bf4ceee1e6452750)

**🎯 变更类型**：重构（将多模态缓存工厂迁移到 `MultiModalRegistry`）

**⚡ 重要程度**：🟡 中

**📋 变更摘要**  
- 在 `vllm.multimodal.registry` 中新增 `_get_cache_type` 与一系列 `processor_cache_from_config / engine_receiver_cache_from_config / worker_receiver_cache_from_config / processor_only_cache_from_config` 方法，实现缓存实例的统一创建。  
- 移除 `vllm.multimodal.cache` 中原有的工厂函数（约 100 行），相应调用改为 `MULTIMODAL_REGISTRY.xxx_from_config(...)`。  
- 多处模块（`core.py、input_processor.py、gpu_model_runner.py、worker/utils.py、worker/worker_base.py`）以及测试代码已同步更新 import 与调用方式。  

**🎯 影响范围**  
- `vllm/multimodal/cache.py`（功能被抽走）  
- `vllm/multimodal/registry.py`（核心实现）  
- `vllm/v1/engine/*`, `vllm/v1/worker/*`（调用点改动）  
- 相关单元测试 `tests/multimodal/*`、`tests/v1/engine/*`  

**💡 关注建议**  
1. **兼容性检查**：外部项目或插件若直接 import `processor_cache_from_config` 等旧函数，会出现 `ImportError`。建议在文档中说明迁移路径，或在 `cache.py` 保留轻量的兼容别名（已在 PR 中删除，需要自行评估是否恢复）。  
2. **注册表实例使用**：确保所有入口都通过 `MULTIMODAL_REGISTRY`（单例）调用，防止出现多个注册表导致状态不一致。  
3. **测试覆盖**：运行完整测试套件，特别是多模态输入的 LRU 与 SHM 缓存路径，以验证新实现的 `_get_cache_type` 逻辑在并行配置下仍然正确。  
4. **性能回归**：虽然功能未变，但缓存对象的实例化方式改变，建议在高并发场景下对 `MultiModalProcessorSenderCache`、`ShmObjectStoreReceiverCache` 等做一次基准测试，以排除潜在的初始化开销。  
5. **文档更新**：在 `vllm.multimodal` 模块的使用说明中加入 “Cache factories are now methods of `MultiModalRegistry`” 的提示，帮助使用者快速定位新 API。  

总体来看，此次迁移提升了缓存创建的集中管理，可减少散落的函数入口，后续若需扩展缓存类型或统一配置，只需在 `MultiModalRegistry` 中调整即可。务必确保所有旧调用已完成改写并通过 CI，以避免运行时异常。

---

### fix: avoid crash on zero-arg tool calls in glm4 parser (#32321)
**SHA**: `a52d139` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a52d1396a7108d9bb8a7afdadb524d72c59fff5c)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `glm4_moe_tool_parser.py` 中加入防御式代码，避免在解析零参数工具调用时出现 `NoneType` 异常。若正则未匹配到函数细节则记录警告并跳过；若参数字符串为空则直接使用空列表，防止 `findall` 报错。  

**🎯 影响范围**：  
- `vllm/tool_parsers/glm4_moe_tool_parser.py`（工具调用解析器）  
- 受此解析器影响的所有使用 GLM‑4‑MoE 工具调用的推理路径。  

**💡 关注建议**：  
1. 为零参数的工具调用补充单元测试，确保在各种边界输入下不会抛异常。  
2. 检查项目中是否有对 `logger` 的统一配置，防止在未初始化的环境下出现 `NameError`。  
3. 关注日志级别，生产环境可能不希望大量 `warning`，可考虑将其调至 `debug` 或在异常统计中归类。  
4. 如有其他解析器（如其他模型的 tool parser）也可能遇到类似情况，建议审视并统一防御逻辑。  

此改动提升了对异常输入的鲁棒性，风险较低，只要保持日志配置即可平滑发布。

---

### [Bugfix] Strengthen the check of X-data-parallel-rank in Hybrid LB mode (#32314)
**SHA**: `1e58482` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1e584823f8f9f7b8504c90dfa7689061537280da)

**🎯 变更类型**：Bugfix（加强 Hybrid LB 场景下对 `data_parallel_rank` 的检查）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `ParallelConfig` 中新增只读属性 `local_engines_only`，统一表示 “Hybrid / External DP LB” 两种只使用本地 Engine 的模式。所有涉及 DP‑rank 计算、断言和选路的代码均改为使用该属性，避免了原先对 `data_parallel_hybrid_lb`、`data_parallel_external_lb` 的分散判断。  
**🎯 影响范围**：`vllm/config/parallel.py`、`vllm/entrypoints/cli/serve.py`、`vllm/v1/engine/coordinator.py`、`vllm/v1/engine/core_client.py`、`vllm/v1/engine/input_processor.py`、`vllm/v1/engine/utils.py` 以及对应单元测试。  

**💡 关注建议**  
1. **属性语义**：`local_engines_only` 仍然等价于 “外部 / Hybrid LB”，但其实现是 “`external_lb or hybrid_lb`”。请确认未来若出现纯内部 LB（即 `external_lb=False && hybrid_lb=False`) 时仍保持 `False`，避免误判。  
2. **断言逻辑**：`serve.py` 中的 `assert parallel_config.local_engines_only or dp_rank == 0` 取代了原来的三路判断，需要确认在 `offline_mode`（本地  + 远端）下 `dp_rank` 为非 0 时仍能正常启动。  
3. **DP‑rank 范围校验**：`input_processor.py` 现在以 `num_ranks = dp_local_size if local_engines_only else dp_size` 进行边界检查。请确保 `data_parallel_size_local` 在所有配置路径下已正确填充，否则可能出现 `num_ranks` 为 0 的异常。  
4. **文档/示例**：更新 README / API 文档，说明 `local_engines_only` 为高级配置入口，旧的 `data_parallel_hybrid_lb`、`data_parallel_external_lb` 仍保留但建议使用新属性。  
5. **测试覆盖**：新增或扩展针对 Hybrid LB、External LB、纯内部 LB 三种模式的单元测试，验证 `dp_rank` 为非 0 时仅在 `local_engines_only` 为 `False` 的情况下通过。  

总体而言，此次改动通过统一属性减少了重复判断，提升了代码可维护性，但需留意属性值的初始化以及对离线模式的兼容性，适当补充文档与测试即可平稳发布。

---

### [code clean] remove duplicate check (#32376)
**SHA**: `9d7ae3f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9d7ae3fcdbdecf828c5189f0152808fcbe6dbc86)

**🎯 变更类型**：代码清理 / 功能微调  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 删除了 `model_config.skip_tokenizer_init` 的重复判断。  
- `InputProcessor`、`AsyncLLM`、`LLMEngine` 均改为直接调用 `cached_tokenizer_from_config(model_config)`，不再在 `skip_tokenizer_init` 为 True 时传入 `None`。  
- 同时在 `_create_processing_ctx` 中去掉了对应的 `if tokenizer is None and not model_config.skip_tokenizer_init` 条件，只保留 `if tokenizer is None:`。  

**🎯 影响范围**  
- `vllm/multimodal/registry.py`（输入处理上下文创建）  
- `vllm/v1/engine/async_llm.py`（异步引擎初始化）  
- `vllm/v1/engine/llm_engine.py`（同步引擎初始化）  

**💡 关注建议**  
1. **功能验证**：`skip_tokenizer_init` 过去用于在模型只需推理而不做分词的场景下节约启动时间和显存。现在始终会尝试加载 tokenizer，确保 `cached_tokenizer_from_config` 能在该 flag 为 True 时安全返回可用对象或快速短路。建议在单元测试和 CI 中加入对 `skip_tokenizer_init=True` 的启动路径验证，防止出现意外的加载或异常。  
2. **文档同步**：若该 flag 已失去实际意义，需要在官方文档、示例代码和 `ModelConfig` 注释中说明其已被内部统一处理或即将废弃，避免用户产生误导。  
3. **向后兼容**：检查其他模块（如自定义 `IOProcessor`、插件）是否仍假设 `tokenizer=None` 能表示“未初始化”。如有，需要相应更新或在 `cached_tokenizer_from_config` 内部返回 `None`，保持兼容。  
4. **性能评估**：在大模型（如 70B）启动时，对比开启/关闭 `skip_tokenizer_init` 前后的启动时间和显存占用，确认去除判断后没有引入不必要的开销。  

整体来看，此次修改消除了冗余条件，代码更简洁，但需确保 `skip_tokenizer_init` 的预期行为在新实现中仍然得到满足，避免因强制加载 tokenizer 而导致启动慢或显存溢出。

---

### [CI/Build][Hardware][AMD] Fix v1/shutdown (#31997)
**SHA**: `15422ed` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/15422ed3f748b2c26a9446db388979027a254199)

**🎯 变更类型**：其他（CI/构建 & ROCm 兼容性补丁）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `tests/v1/shutdown` 目录新增了 `conftest.py`，并在两个 shutdown 相关的单元测试中加入了 ROCm‑specific 的 fixture `rocm_evil_forward` / `rocm_evil_method`。通过 `sitecustomize.py` 动态注入错误实现（把 `LlamaForCausalLM.forward` 或 `load_weights` 替换为会抛异常的函数），从而验证在 ROCm 环境下模型前向/启动错误时能够正确触发 shutdown 与显存释放。  

**🎯 影响范围**  
- **测试层**：`tests/v1/shutdown/*`（新增与修改）  
- **平台检测**：`vllm.platforms.current_platform`（判断 ROCm）  
- **环境变量**：`PYTHONPATH` 在 fixture 中被临时追加，以确保 `sitecustomize.py` 能被 Python 自动执行。  

**💡 关注建议**  
1. **fixture 可靠性**：`rocm_sitecustomize_factory` 只在 ROCm 环境返回真实安装函数，非 ROCm 时返回 `lambda _: None`。请确认 CI 中的 ROCm 环境变量配置正确，否则可能导致测试在非 ROCm 平台意外通过。  
2. **清理工作**：`sitecustomize.py` 会被写入临时目录并通过 `PYTHONPATH` 注入。虽然 `tmp_path` 在每个测试结束后自动清理，但建议在 fixture 结束时显式恢复原 `PYTHONPATH`，防止对后续测试产生副作用。  
3. **代码可读性**：使用 `inspect.getsource` 动态获取函数定义虽简洁，但在代码审查时不易追踪。建议在注释中标明为何采用此方式，并考虑将错误实现抽离为独立的辅助模块，以便复用。  
4. **错误路径覆盖**：当前仅在 ROCm 平台覆盖了 forward 与 load_weights 的异常路径。若项目后续支持其他加速后端（如 CUDA），可考虑抽象出平台无关的 fixture，统一测试异常 shutdown 行为。  

总体而言，此次改动提升了 vLLM 在 ROCm 环境下异常 shutdown 的可靠性，影响局限于测试代码和 CI 配置，风险可控。继续关注平台检测与环境变量的恢复即可。

---

### [compile] raise on compile_size implicit padding (#32343)
**SHA**: `8471b27` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8471b27df97c3eb79f891802fc0e858f8f7ac6a0)

**🎯 变更类型**：功能增强 / 可靠性提升  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `CompilationConfig.post_init_cudagraph_sizes` 中加入对 `compile_sizes` 的校验，确保所有编译尺寸在开启 cuGraph 时不会被自动填充（padding）。  
- 当 `cudagraph_mode` 为 `NONE` 或者 `max_cudagraph_capture_size` 为 0 时，跳过该校验，保持向后兼容。  
- 新增单元测试覆盖合法、非法以及在关闭 cuGraph 时的行为。

**🎯 影响范围**  
- `vllm/config/compilation.py`（`CompilationConfig`）  
- `tests/compile/test_config.py`（测试用例）  

**💡 关注建议**  
1. **类型安全**：`compile_sizes` 仍支持使用 `"cudagraph_capture_sizes"` 这种占位符字符串。当前实现在 `cudagraph_mode != NONE` 时直接遍历并比较 `size <= max_cudagraph_capture_size`，若 `size` 为字符串会抛出 `TypeError`。建议在校验前先统一解析（如调用已有的字符串展开逻辑），或在循环前做 `isinstance(size, int)` 检查。  
2. **错误信息**：异常信息已指明会被填充的目标值，建议在文档或 `README` 中同步说明该限制，帮助用户在配置 `compile_sizes` 时避免误用。  
3. **测试覆盖**：新增的测试已验证关闭 cuGraph 时不进行校验，建议再补充一次 `compile_sizes` 包含混合字符串和整数的情况，确保异常路径的健壮性。  
4. **后向兼容**：该校验默认在开启 cuGraph 时生效，对已有使用 `compile_sizes` 的部署没有影响，但在升级后若用户误配置会直接报错，需在升级指南中提示。  

总体来说，此变更提升了 `CompilationConfig` 在使用 cuGraph 时的安全性，改动局部且对核心运行时性能无影响，只需留意上面的类型检查与文档同步即可。

---

### [BugFix] Assign page_size_padded when unifying kv cache spec. (#32283)
**SHA**: `66652e8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/66652e8082b69ba7d1e6aca7c234433de55f1b9b)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `unify_hybrid_kv_cache_specs` 中统一 KV‑Cache 规格时，遗漏了 `page_size_padded` 参数的拷贝，导致混合注意力（Full + SlidingWindow / ChunkedLocal）场景下生成的 `FullAttentionSpec` 缺失该信息。  
- 新增对该字段的显式传递，并在测试 `test_unify_hybrid_kv_cache_specs` 中验证四种混合组合的正确性。  

**🎯 影响范围**  
- `vllm/v1/core/kv_cache_utils.py`（统一规格的核心实现）  
- `vllm/v1/kv_cache_interface.py`（`FullAttentionSpec`、`ChunkedLocalAttentionSpec` 等定义）  
- 单元测试 `tests/v1/core/test_kv_cache_utils.py`（新增大量参数化构造函数及统一化测试）  

**💡 关注建议**  

1. **兼容性检查**：`FullAttentionSpec` 现在默认接受 `page_size_padded=None`，但外部代码若自行实例化该类仍需显式传参或保持默认，避免出现意外的 `None` 行为。  
2. **文档同步**：在 `kv_cache_interface` 与配置文档中补充说明 `page_size_padded` 在混合注意力模式下的意义与使用方式。  
3. **序列化/加载**：确认 `KVCacheConfig` 的 JSON/YAML 序列化/反序列化路径已覆盖 `page_size_padded`，防止旧版本配置文件丢失该字段导致运行时错误。  
4. **完整回归**：运行全套 `pytest -m core` 以及模型推理基准，验证在实际推理过程中（尤其是使用 `sliding_window` 或 `chunked_local_attention` 的模型）缓存大小仍符合预期。  
5. **性能评估**：`page_size_padded` 影响块分配和对齐，建议在不同硬件（GPU/CPU）上测量内存占用和吞吐率，确保修复未引入额外开销。  

总体来看，此次修改修复了在混合 KV‑Cache 配置下缓存分页对齐信息丢失的 Bug，影响主要局限在 KV‑Cache 规格统一逻辑和相关单元测试。开发者在更新后需检查自定义 KV‑Cache 配置的兼容性，并同步文档与序列化实现。

---

### [MODEL] Fix handling of multiple channels for gpt-oss with speculative decoding  (#26291)
**SHA**: `d084e9f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d084e9fca7d5d40cbb62eb5fe8ab5cbc6c769cf0)

**核心变更**  
1. **测试层面**  
   - 将原有的函数式测试改为类式组织，并新增 `gptoss_speculative_server`、`gptoss_speculative_client` fixture以及 `test_gpt_oss_speculative_reasoning_leakage`，用于验证在 **speculative decoding** 场景下推理内容不会泄露。  
   - 对 GPT‑OSS 多轮、工具调用、tool‑choice、数组化 tool 消息等用例进行细化，覆盖了 “content vs. tool‑calls” 的多通道处理。  

2. **流式解析层**  
   - 新增 `TokenState`（`NamedTuple`），在 `chat_completion/serving.py` 中每个 token 记录 **channel、recipient、文本**，取代原来的 `delta_text` 累加方式。  
   - `extract_harmony_streaming_delta` 接口从 `(cur_channel, cur_recipient, delta_text)` 改为接受 `token_states: list[TokenState]`，并实现 **分组合并**、**工具调用索引计算**、**reasoning 内容聚合**，返回单一 `DeltaMessage`。  
   - `serving.py` 中日志输出改为拼接 **content / reasoning / tool‑args**，提升可读性。  

3. **兼容性**  
   - 所有调用 `extract_harmony_streaming_delta` 的位置已同步改为使用 `token_states`，但外部代码若直接引用旧签名将失效。  

**影响范围**  
- `vllm/entrypoints/openai/chat_completion/serving.py`（流式生成器）  
- `vllm/entrypoints/openai/chat_completion/stream_harmony.py`（核心解析逻辑）  
- 相关单元测试文件 `tests/entrypoints/openai/*.py`  
- 可能的用户自定义插件或监控工具如果依赖旧的 `extract_harmony_streaming_delta` 参数也会受影响。  

**关注建议**  
1. **兼容层**：考虑为旧签名提供包装函数或 `*args, **kwargs` 兼容，以免破坏已有集成。  
2. **索引计算**：`base_index`、`next_tool_index` 与 `ongoing_tool_index` 的逻辑较为复杂，建议加入更多单元测试（包括跨批次、跨 token 交叉的 tool‑call 场景）以及注释说明。  
3. **性能**：分组与字符串拼接在每个 token 上进行，理论上略增开销；可在高吞吐环境下对 `TokenState` 列表做一次性 `''.join`，已实现，但仍需监控 CPU/内存峰值。  
4. **日志**：新的 `delta_content_parts` 拼接方式更灵活，但仍需确保 `reasoning_content` 与 `tool_calls` 的字段命名与 OpenAI 规范保持一致。  
5. **文档**：更新 README/CHANGELOG，说明 `extract_harmony_streaming_delta` 已改为接受 `TokenState` 列表，并给出迁移示例。  

总体来看，此次改动显著提升了 **多通道、工具调用及推理内容的准确分离**，并为 **speculative decoding** 增加了安全检测；只要注意兼容性与索引细节，即可在生产环境平滑替换。

---

### [CI] Move rixl/ucx from Dockerfile.rocm_base to Dockerfile.rocm (#32295)
**SHA**: `3a61232` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3a612322eb9351594bc0c35fe5b85426931be36e)

**🔧 变更类型**：其他（CI/Dockerfile 重构）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 RIXL 与 UCX 的编译、安装以及 wheel 打包流程从 `docker/Dockerfile.rocm_base` 中抽离，迁移到 `docker/Dockerfile.rocm` 的新阶段 `build_rixl`。  
- 同时在 `test` 阶段新增对生成的 RIXL wheel 的安装步骤。  
- 删除了 `Dockerfile.rocm_base` 中原有的 RIXL 构建相关代码，简化了基础镜像的体积与构建时间。  

**🎯 影响范围**  
- **Docker 镜像构建流程**：`rocm_base` 镜像不再包含 RIXL/UCX，只有在最终 `rocm` 镜像里才会编译并打包。  
- **CI/CD**：涉及的 Buildkite 步骤会使用新的 `Dockerfile.rocm`，旧的 `Dockerfile.rocm_base` 仍保留但不再承担 RIXL 构建。  
- **运行时环境**：`vllm` 的测试镜像会自动安装 RIXL wheel，确保功能可用。  

**💡 关注建议**  
1. **镜像体积**：虽然把编译过程放在 `rocm` 阶段可以避免在基础镜像中留下编译工具，但最终镜像仍会包含 UCX、RIXL 运行时库。若目标是最小化镜像体积，建议在 `final` 阶段使用 `--mount=type=cache` 或多阶段复制，仅保留运行时文件。  
2. **构建缓存**：`build_rixl` 阶段的依赖安装（apt、uv pip、git clone）未使用缓存层。可以在 `RUN apt-get update && apt-get install … && rm -rf /var/lib/apt/lists/*` 前加入 `--mount=type=cache,target=/var/cache/apt`，提升 CI 重跑速度。  
3. **参数一致性**：`Dockerfile.rocm` 中硬编码了 `RIXL_BRANCH="f33a5599"`、`UCX_BRANCH="da3fac2a"`，而 `rocm_base` 仍保留旧 ARG 定义（已被删除的代码中）。若后续仍需要在 CI 中覆盖这些分支，应在文件顶部重新声明对应 `ARG`，否则 `docker build --build-arg` 将失效。  
4. **测试验证**：在 `test` 阶段新增 `uv pip install --system /rixl_install/*.whl`，请确认 `vllm` 的单元测试已有对 RIXL 接口的覆盖，否则可能出现遗漏的运行时错误。  
5. **文档同步**：README 或 CI 文档中提及的 “RIXL/UCX 已在 rocm_base 中构建” 需要更新为 “在 rocm 镜像的 `build_rixl` 阶段”。  

总体来看，此次改动把较重的 RIXL/UCX 编译工作从通用基础镜像中抽离，提高了镜像可复用性和 CI 可维护性，只要注意缓存、参数暴露以及文档同步即可顺利上线。

---

#### 🟢 低重要度变更 (14)

### [ROCm] Improve error handling while loading quantized model on gfx120… (#31715)
**SHA**: `b89275d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b89275d018796170acbb91b8bcddbc8b8213025e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `quark_ocp_mx.py` 中扩展异常捕获，加入 `RuntimeError` 并在捕获时记录警告，防止在不支持的平台上因加载量化模型而导致崩溃。

---

### [CI][BugFix][AMD][FP8] Fix test_rms_norm so it runs correctly on ROCm (#32372)
**SHA**: `8853a50` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8853a50af2a3d2a12700740aa6def37fed3142da)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_fused_quant_layernorm.py` 中改用 `current_platform.fp8_dtype()` 统一 FP8 类型，并在测试启动时显式设置 CUDA 设备，修复 ROCm 环境下 RMSNorm 测试运行错误。

---

### [ROCM] Add ROCm image build to release pipeline (#31995)
**SHA**: `c5891b5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c5891b54306669db6e78e7df32250b6eee7d34b5)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 Release Pipeline 中新增 ROCm 镜像构建步骤，修改注释脚本以推送并标记 ROCm 镜像，并在 `Dockerfile.rocm` 中加入官方入口点，使 vLLM 支持 ROCm 发行版。

---

### [Model] Avoid token selection in SigLIP pooling head (#32389)
**SHA**: `cdba4c7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cdba4c74b30824b22d9d7cd22b187d5e4f690404)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `SiglipVisionTransformer` 的 `forward` 中去掉对第一个 token 的显式选择，改为直接返回完整的 `hidden_state`，相关逻辑交由 `resolve_visual_encoder_outputs` 处理。

---

### [ROCm][CI] Pin transformers 4.57.3 to fix jina test failures (#32350)
**SHA**: `ae1eba6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ae1eba6a9a2342c9660731e8a5674447ad35f757)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `requirements/rocm-test.txt` 中新增 `transformers==4.57.3` 锁定版本，以解决 CI 中 Jina 测试失败。

---

### [Bugfix] Fix stale `common_attn_metadata.max_seq_len` in speculative decoding with Eagle (#32312)
**SHA**: `e9ec2a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e9ec2a72d845e1f3374c6de68e361edc6258c891)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Eagle 预测中，若序列超限后仍统一递增 `common_attn_metadata.max_seq_len`，并限制不超过模型最大长度，以避免 stale（过时）的 max_seq_len 导致的注意力计算错误。

---

### [BugFix] Fix DeepSeek-V3.1 + DeepGEMM incompatible scale shapes (#32361)
**SHA**: `2c9b4cf` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2c9b4cf5bf844de0471f77e6579e16c7bc3ee0d0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 DeepSeek‑V3.1 与 DeepGEMM 的兼容性修复中，新增 `layer.quant_method.use_deep_gemm` 检查，防止因 DeepGEMM 转换的 scale 布局与 `scaled_dequantize` 不兼容而导致错误。

---

### [CI][AMD][Quantization][BugFix] Fix fp8 max in quant_utils.py and update test_fp8_quant.::test_static_fp8_quant_group_2d to use correct fp8 dtype and adjust atol/rtol (#32201)
**SHA**: `3c26856` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3c2685645ed602e3db8010cd62d88d3130115f06)

**🎯 变更类型**：代码重构/测试修改  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `quant_utils.py` 中使用 `get_fp8_min_max()` 的 `fp8_max` 计算量化比例，纠正了之前使用 `finfo.max`导致的错误；相应地在单元测试 `test_fp8_quant.py` 中使用平台正确的 FP8 dtype 并放宽误差容限，以验证修正后的量化行为。

---

### [ROCm][CI] Disable async scheduling on ROCm for test_structured_output[meta-llama/Meta-Llama-3.1-8B-Instruct-xgrammar-auto-speculative_config9] (#32355)
**SHA**: `773d707` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/773d7073aea7de91710dad24928f46497017a8f6)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 环境下，为 `test_structured_output_generate.py` 中的 `run_generate_test` 添加 `async_scheduling=False` 参数，禁用异步调度，以确保该测试在 ROCm 上的稳定运行。

---

### [Bugfix] Add CpuCommunicator.dispatch and combine to fix DP+MoE inference (#31867)
**SHA**: `edadca1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/edadca109cf8b41aed3bb9cad11aaca058a6a4d0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `CpuCommunicator` 中实现 all2all 调度与合并，新增 `dispatch` 与 `combine` 方法，并在基类签名支持返回额外张量；对非 `naive` 后端做回退并记录日志，修复 DP+MoE 推理错误。

---

### [Misc] Remove redundant line (#32366)
**SHA**: `d86fc23` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d86fc23bdd8a67a0f466d73f7ea6c50871abe232)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/attention/layer.py` 中删除冗余的 `output_shape = output_shape if output_shape is not None else query.shape` 代码行，功能保持不变。

---

### Support configure skip_special_tokens in openai response api (#32345)
**SHA**: `375e598` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/375e5984fec8f79f1ec4190c2fd76cc185f6a58f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 OpenAI 响应 API 的请求模型中新增 `skip_special_tokens` 参数（默认 True），并在转化为采样参数时传递该标志，实现对特殊标记的跳过控制。

---

### Fix optional parameter parsing in MiniMax M2 tool parser #32278 (#32342)
**SHA**: `19b251f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/19b251fe3d26ec9c8df6cb633af9dd196cc6ae18)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `MiniMax M2` 工具解析器中，简化可选参数的空值处理，统一将 `"null"、"none"、"nil"` 直接映射为 `None`，去除冗余判断，提升解析准确性。

---

### [Bugfix][ROCm][performance] Resolve the performance regression issue of the Qwen3-Next-80B-A3B-Thinking under rocm_atten (#32336)
**SHA**: `e27078e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e27078ea807cf5655c6be3b59608e55c1682756d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `rocm_attn.py` 中将 ROCm 注意力后端支持的块大小从 `[16, 32]` 扩展至 `[16, 32, 544]`，修复 Qwen3‑Next‑80B‑A3B‑Thinking 在 rocm_atten 下的性能回归。

---

