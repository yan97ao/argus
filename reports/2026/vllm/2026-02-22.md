# 每日更新报告（2026-02-22）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-22 18:07:18 | Andreas Karatzas | [ROCm][CI] Fix realtime test timeouts caused by aiter JIT compilation delays (#35052) |
| 2026-02-22 17:03:44 | Andreas Karatzas | [ROCm][CI] Fix flaky embedding chat test by using tolerance-based comparison (#35050) |
| 2026-02-22 15:15:35 | Roger Wang | [Bugfix] Fix Qwen3/Qwen3.5 Reasoning Parser  (#34779) |
| 2026-02-22 13:42:50 | Woosuk Kwon | [Model Runner V2] Enable CUDA graph for Eagle3 (#35040) |
| 2026-02-22 13:11:54 | Xiao Li | Fix apply_top_k_top_p_triton called by non-cuda logits Tensor (#35030) |
| 2026-02-22 12:26:48 | Cyrus Leung | [Benchmark] Use `sns.relplot` for plotting (#35027) |
| 2026-02-22 12:23:41 | Athrael Soju | [New Model] Add ColModernVBERT (#34558) |
| 2026-02-22 12:23:24 | Wentao Ye | [CI] Bump mteb version to `mteb[bm25s]>=2, <3` for pooling model unit tests (#34961) |
| 2026-02-22 12:01:10 | Andreas Karatzas | [CI] Stabilizing ROCm amd-ci signal and minor name fix in upstream (#35008) |
| 2026-02-22 08:42:53 | Woosuk Kwon | [Model Runner V2] Support attention group (#35036) |
| 2026-02-22 08:28:01 | Vadim Gimpelson | [Model Bash][DSR1] Add selective dynamic shape marking for CustomOp (#34900) |
| 2026-02-22 04:55:24 | Woosuk Kwon | [Model Runner V2] Support Eagle3 (no CUDA graph) (#35029) |
| 2026-02-22 03:14:41 | Cyrus Leung | [CI/Build] Fix gRPC version mismatch (#35013) |

### 📊 统计摘要
> 本日共 13 个提交 | 🔴高 1 | 🟡中 6 | 🟢低 6
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (1)](#-🔴-高重要度变更-1)
    - [[New Model] Add ColModernVBERT (#34558)](#970861a)
  - [🟡 中重要度变更 (6)](#-🟡-中重要度变更-6)
    - [[ROCm][CI] Fix realtime test timeouts caused by aiter JIT...](#dd8c3a7)
    - [[Bugfix] Fix Qwen3/Qwen3.5 Reasoning Parser  (#34779)](#40f88d8)
    - [[Model Runner V2] Enable CUDA graph for Eagle3 (#35040)](#2cbf965)
    - [[Model Runner V2] Support attention group (#35036)](#b71fbd0)
    - [[Model Bash][DSR1] Add selective dynamic shape marking fo...](#74d90b1)
    - [[Model Runner V2] Support Eagle3 (no CUDA graph) (#35029)](#a4047d4)
  - [🟢 低重要度变更 (6)](#-🟢-低重要度变更-6)
    - [[ROCm][CI] Fix flaky embedding chat test by using toleran...](#a8a47c1)
    - [Fix apply_top_k_top_p_triton called by non-cuda logits Te...](#30132cd)
    - [[Benchmark] Use `sns.relplot` for plotting (#35027)](#cbd95a2)
    - [[CI] Bump mteb version to `mteb[bm25s]>=2, <3` for poolin...](#d24bdd7)
    - [[CI] Stabilizing ROCm amd-ci signal and minor name fix in...](#d403c1d)
    - [[CI/Build] Fix gRPC version mismatch (#35013)](#965fe45)
#### 🔴 高重要度变更 (1)

### [New Model] Add ColModernVBERT (#34558)
**SHA**: `970861a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/970861ac0cfc93d8ebdeb2c0f5d664289eafb51c)

**🎯 变更类型**：功能增强 / 新模型加入  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：本次提交为 vLLM 引入全新的多模态检索模型 **ColModernVBERT**（`colmodernvbert-merged`），实现了 SigLIP 视觉编码 + ModernBERT 文本编码的 Late‑Interaction 框架，并提供了对应的配置、权重加载、模型注册、文档说明以及示例脚本和完整单元测试。预计能够在文本‑图像检索、重排序、打分等场景中直接使用 vLLM 的高并发推理能力。

**🎯 影响范围**  
- `vllm/model_executor/models/colmodernvbert.py`（核心模型实现）  
- `vllm/model_executor/models/registry.py`（模型注册）  
- `vllm/transformers_utils/config.py`、`configs/colmodernvbert.py`（配置系统）  
- `vllm/transformers_utils/configs/__init__.py`（自动导入）  
- 文档 `docs/models/supported_models.md`（模型列表）  
- 示例 `examples/pooling/score/colmodernvbert_rerank_online.py`  
- 单元测试 `tests/models/multimodal/pooling/test_colmodernvbert.py`  
- 相关注册表 `tests/models/registry.py`（示例模型信息）  

---

### 🔍 技术洞察

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • 通过 `MULTIMODAL_REGISTRY` 注册了全新的 **多模态处理器** 与 **Dummy 输入构建器**，从而在统一的 `vLLM` 运行时实现图像‑文本混合输入的 token‑wise pooling。<br>• 引入 **ColModernVBertConnector**（像素 shuffle + 线性投影），把 Vision 端的高维特征压缩至与文本隐藏维度相匹配，保持了 Late‑Interaction 设计。<br>• 通过 `SupportsMultiModal` 接口把视觉特征并入 `embed_multimodal`，返回 `List[torch.Tensor]` 供后续 token‑wise pooling 使用。<br>• 新增 `ColModernVBertConfig`，在统一的 `transformers_utils.config` 体系中声明模型类型 (`model_type="colmodernvbert"`)，并封装了 Vision、Text、Pixel‑shuffle、Embedding‑dim 等超参数。 |
| **性能影响** | • **计算量**：Vision 端仍使用 SigLIP（与 CLIP 类似），但在像素 shuffle 之后特征序列长度降低了 `factor²`（默认 4²=16），显著降低后续线性投影和交叉相似度计算的 FLOPs。<br>• **内存占用**：加入了视觉特征张量（`image_seq_len`≈(512/16)²/16 = 64 tokens），对每个请求的 KV 缓存需求增加约 64 × `embedding_dim`（128）≈8 KB，属于可接受范围。<br>• **延迟**：首次调用会先执行 Vision 前向（一次卷积/Transformer），随后进入文本侧的完整层数（22 层 ModernBERT）。总体延迟略高于纯文本模型，预计在 1–2 倍左右（视硬件而定），但在批量请求场景下仍能受益于 vLLM 的并行调度。 |
| **安全考虑** | • 代码仅读取本地或从 HuggingFace 拉取的模型权重，未涉及网络交互或执行外部代码，安全风险低。<br>• 新增的 `AutoWeightsLoader` 与手动拼接 `DecoupledEmbedding` 权重的逻辑需要确保不出现未检查的路径遍历或任意文件写入；目前使用的 `default_weight_loader` 仅执行 `param.copy_(tensor)`，安全性符合项目惯例。 |
| **可维护性** | • 代码遵循项目已有的多模态抽象（Processor、DummyInputsBuilder、SupportsMultiModal），便于后续扩展其他多模态模型。<br>• 权重映射表 `hf_to_vllm_mapper` 详尽、顺序明确，便于对齐 HF checkpoint 与 vLLM 参数。<br>• 单元测试覆盖了文本 token‑embed、score 排序、手动 MaxSim 验证以及图像 token‑embed，提供了回归安全网。 |

---

### ⚠️ 潜在风险

1. **权重加载兼容性**  
   - `ColModernVBertForRetrieval.load_weights` 需要先合并 `tok_embeddings` 与 `additional_embedding`，若 HF checkpoint 结构发生变化（如去除 `additional_embedding`），加载会报错。  
   - 映射表中 `"model.connector.modality_projection."` → `"connector."` 与实际实现 `ColModernVBertConnector` 的属性名是否严格对应需验证。

2. **多模态输入边界**  
   - `BaseMultiModalProcessor._call_hf_processor` 中使用 `Idefics3ImageProcessor`，但仅在图像存在时才实例化；若用户传入非‑Idefics3 兼容的图像格式（如 PIL、numpy）可能触发未捕获异常。  
   - `embed_multimodal` 当前只返回图像特征列表，未处理文本‑图像混合的 token‑level 对齐（仍依赖 PromptReplacement），若未来加入更复杂跨模态交互，可能需要额外实现。

3. **性能回归**  
   - Vision 前向在大批量请求时可能成为瓶颈，尤其在 GPU 显存受限的场景下，Vision 模块的缓存（`pixel_values`）与文本缓存共用会加大显存压力。  
   - `pooler_for_token_embed` 仍使用 L2‑norm 投影，若后续改为更复杂的聚合（如 cross‑attention），当前实现需要重构。

4. **文档/示例同步**  
   - 示例脚本默认 `vllm serve ModernVBERT/colmodernvbert-merged`，但模型名称在注册表为 `ColModernVBertForRetrieval`，若用户直接使用模型类而非别名，可能导致 “model not found” 错误。需确保别名映射在 `model_registry` 中保持一致。

---

### 💡 关注建议

1. **回归测试**  
   - 在 CI 中加入对比 **HF 推理结果** 与 vLLM 输出的端到端一致性测试（尤其是 MaxSim 计算），防止未来改动导致分数偏差。  
   - 添加针对 **不同图像尺寸** 与 **不同 patch 大小** 的边界测试，确保 `pixel_shuffle` 计算的 `height/width` 整除安全。

2. **权重映射维护**  
   - 将 `hf_to_vllm_mapper` 与 HF 官方 checkpoint 结构同步到 `vllm/transformers_utils/configs/colmodernvbert.py` 的注释中，便于后续版本升级时快速定位映射变更。  
   - 为 `load_weights` 加入 **日志**（DEBUG 级别）输出每个映射后的参数名称，帮助排查缺失或重复加载情形。

3. **显存优化**  
   - 考虑在 Vision 前向后 **使用 `torch.utils.checkpoint`**（梯度检查点）或 **TorchScript** 编译，以降低峰值显存。  
   - 在 `embed_multimodal` 中对图像特征进行 **半精度 (`torch.float16`)** 处理，确保与 `vllm_config.model_config.head_dtype` 对齐。

4. **错误处理**  
   - 对 `Idefics3ImageProcessor.from_pretrained` 加入异常捕获，给出明确的提示（如 “未安装 `datasets` / `accelerate`，请安装最新 `transformers` 版本”。）  
   - 在 `embed_multimodal` 对 `pixel_values` 的形状进行断言校验，防止意外的 5‑维张量导致 reshape 错误。

5. **文档同步**  
   - 在 `supported_models.md` 中补充 **模型别名**（`ColModernVBertForRetrieval` → `colmodernvbert`）以及 **

---

#### 🟡 中重要度变更 (6)

### [ROCm][CI] Fix realtime test timeouts caused by aiter JIT compilation delays (#35052)
**SHA**: `dd8c3a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dd8c3a7fb2b2448d04bb00934f2cacf43bb14c3b)

**🎯 变更类型**：Bug 修复 / CI 稳定性提升  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 ROCm CI 环境下的实时转写接口测试加入“热身”步骤，先触发 `aiter` 的 JIT 编译并放宽超时，从而避免因首次编译耗时导致的测试超时失败。新增对 `session.updated` 事件的等待与超时警告，所有首次请求的 `receive_event` 超时上调至 360 s。  
**🎯 影响范围**：`tests/entrypoints/openai/test_realtime_validation.py`（仅测试层），间接涉及 `vllm.entrypoints.openai` 的 WebSocket 协议实现。  
**💡 关注建议**：  
1. 确认新增的热身逻辑不会在非 ROCm 环境下引入不必要的延迟，若有必要可通过环境变量或标记条件编译。  
2. 警告信息可能被 CI 抑制，建议在测试报告中显式记录，以帮助定位未实现 `session.updated` 事件的后端实现。  
3. 该修改仅影响 CI 超时，未改动库的功能代码，发布时无需额外的兼容性检查。  

总体来看，改动合理且风险低，只需关注热身时长对整体 CI 时长的影响。

---

### [Bugfix] Fix Qwen3/Qwen3.5 Reasoning Parser  (#34779)
**SHA**: `40f88d8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/40f88d8318aea1792ac7eabfe33241fd26660be7)

**变更类型**：Bugfix（修复 Qwen3/Qwen3.5 推理解析在流式输出和无 \<think\> 场景下的错误）

**核心改动**  
1. **tests/reasoning/test_qwen3_reasoning_parser.py**  
   - 将单模型测试拓展为多模型（`Qwen3‑0.6B`、`Qwen3.5‑397B‑A17B`、`Qwen3‑4B‑Thinking‑2507`），确保解析逻辑兼容不同版本。  
   - 新增 *without‑start‑token*、*complete‑reasoning*、*multi‑token‑delta* 等边界用例，验证在仅出现 `</think>`、`<think>` 与 `</think>` 同时出现、以及一次 delta 包含多个 token 时，解析器不会把 `<think>` 泄露到 reasoning 字段。  
   - 引入 `StreamingReasoningReconstructor` 与 `run_reasoning_extraction_streaming`，对流式解析路径进行完整覆盖。

2. **vllm/entrypoints/openai/chat_completion/serving.py**  
   - 在流式生成的 `chat_completion_stream_generator` 中，提前判断 `prompt_is_reasoning_end_arr[i]`（即模板中已显式关闭思考）并把 `reasoning_end_arr[i]` 置为 `True`，防止首个 delta 被误当作 reasoning。  
   - 当 `prompt_is_reasoning_end_arr[i]` 为真且存在 `reasoning_parser` 时，直接返回 `DeltaMessage(content=delta_text)`，不再走 `extract_reasoning_streaming`。  
   - 相应地在 “only reasoning” 分支中加入同样的提前判断，保证在关闭思考的情况下所有 token 均走内容路径。

3. **vllm/reasoning/qwen3_reasoning_parser.py**  
   - 增加 `Sequence` 导入，提升类型注解。  
   - 对文档进行补充，说明 Qwen3 与 Qwen3.5 在模板层面的差异（Qwen3.5 只在 prompt 放 `<think>`），以及 `enable_thinking=False` 时的处理逻辑。  
   - `extract_reasoning` 现在先去掉生成文本中的 `<think>`（若出现），随后仅检查 `</think>`；若不存在 `</think>`，直接返回 `(None, model_output)`，即全部视为内容。  
   - 实现 `extract_reasoning_streaming`：  
     - 处理 delta 中可能出现的 `<think>`，剔除后再继续解析。  
     - 当 delta 包含 `</think>` 时，拆分为 `reasoning` 与 `content` 两段并返回 `DeltaMessage`。  
     - 若已在前序 token 中出现结束标记，则后续全部返回 `content`；否则返回 `reasoning`。  
   - 通过 `self.start_token_id / self.end_token_id` 与 delta token ids 对齐，保证 token‑level 的可靠性。

**影响范围**  
- **推理解析模块**（`vllm.reasoning`）以及 **OpenAI ChatCompletion 流式服务**。  
- 所有使用 Qwen3 系列模型（包括 Qwen3.5）进行思考/推理的调用都将受益；旧模板（生成 `<think>`）仍保持兼容。  
- 增加的单元测试覆盖了非流、流式、多模型、边界及多 token delta 场景，提升回归安全性。

**建议**  
1. **代码审阅**：确认 `self.start_token_id`、`self.end_token_id` 在 `BaseThinkingReasoningParser` 中已正确定义并对应 tokenizers；若未同步更新，可能出现 `AttributeError`。  
2. **性能**：`extract_reasoning_streaming` 在每个 delta 都进行字符串查找（`find`），在高吞吐量场景下可考虑先用 token‑id 判断结束位置，避免不必要的 `str.find`。  
3. **文档**：在项目 README 或模型使用指南中注明 Qwen3.5 需要在 `enable_thinking=False` 时通过 `prompt_is_reasoning_end` 检测，避免用户误以为仍会输出 `<think>`。  
4. **兼容性回归**：运行全量 CI（包括其它模型的推理解析，如 `gemma`、`llama‑2`）确保改动未破坏统一的 `ReasoningParser` 接口。  
5. **后续扩展**：如果未来新增仅在输出出现 `</think>`（无 `<think>`）的模型，可直接复用此实现，无需额外分支。  

整体来看，此次提交修复了 Qwen3.5 在流式推理下误将 `</think>` 前的文本作为 content 的缺陷，并通过更细致的单元测试确保新老模板均可正确解析，属于中等重要度的 bug 修复。

---

### [Model Runner V2] Enable CUDA graph for Eagle3 (#35040)
**SHA**: `2cbf965` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2cbf9656ce6013f7b531bc5a0909d03b88c14862)

**🎯 变更类型**：功能增强 / 性能优化  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Eagle3（带 auxiliary hidden‑state 输出的模型）在 Model Runner V2 中加入 CUDA‑Graph 支持。`CudaGraphManager` 新增 `use_aux_hidden_state_outputs` 参数，分配并捕获 aux hidden‑state 缓冲区；`run_fullgraph` 现在在需要时返回 `(hidden_states, aux_hidden_states)`。`model_runner.py` 相应传入该标志并适配返回值。

**🎯 影响范围**  
- `vllm/v1/worker/gpu/cudagraph_utils.py`（核心图捕获与执行逻辑）  
- `vllm/v1/worker/gpu/model_runner.py`（构造 `CudaGraphManager` 并调用 `run_fullgraph`）  
- 任何直接使用 `CudaGraphManager.run_fullgraph` 的模块或自定义模型推理路径。  

**💡 关注建议**  
1. **向后兼容**：`use_aux_hidden_state_outputs` 默认应保持 `False`，确保旧模型不受影响。若在其他位置实例化 `CudaGraphManager`，需检查构造参数是否已补全。  
2. **返回值适配**：调用方（如 `StructuredOutputsWorker`、测试脚本）应判断返回值是 `Tensor` 还是 `(Tensor, List[Tensor])`，或使用 `isinstance`/类型检查统一处理。  
3. **内存与性能**：aux hidden‑state 缓冲区会额外占用显存，建议在配置文档中说明其回收逻辑，并在性能基准中对比开启/关闭该特性的吞吐与延迟。  
4. **测试覆盖**：新增单元测试覆盖：① 普通模型（无 aux 输出）仍返回单 `Tensor`；② Eagle3‑style 模型返回 `(Tensor, List[Tensor])` 并校验数据拷贝正确；③ CUDA‑Graph 捕获过程中不会因未初始化 `aux_hidden_states` 抛异常。  
5. **文档更新**：在模型配置章节标明 `use_aux_hidden_state_outputs` 何时应开启，以及对应的 `structured_outputs_worker` 需要的额外处理。  

总体来看，此改动为 Eagle3 引入 CUDA‑Graph 提供了显著加速路径，但新增的返回结构和显存占用需要在代码使用方加以适配和监控。

---

### [Model Runner V2] Support attention group (#35036)
**SHA**: `b71fbd0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b71fbd06e215a1a09600220c947a1bb2d5494de9)

**变更概览**  
本次提交在 V2‑Model‑Runner 中引入 *AttentionGroup* 概念，实现了同一 backend 与相同 KV‑Cache 规格的层的分组管理。核心改动包括：

1. **`init_attn_backend`**：不再返回单一的 `attn_metadata_builders` 列表，而是生成 `attn_groups: list[list[AttentionGroup]]`。通过 `group_map` 合并相同 `(backend, kv_spec)` 的层，统一创建 workspace 并在组内复用 metadata builder。  
2. **`build_attn_metadata`** 与所有调用点（`cudagraph_utils.py、model_runner.py、eagle/*`）同步改为接受 `attn_groups`，遍历每个组并使用其 builder 生成统一的 `AttentionMetadata`.  
3. 引入 `UniformTypeKVCacheSpecs` 的展开逻辑，兼容之前的 uniform‑type 缓存配置。  
4. 在 CudaGraph 捕获路径中加入对 **FULL** 模式下未捕获图的回退（切换到 eager），防止启动时的空指针异常。  
5. 移除旧的 `attn_metadata_builders` 成员，新增 `self.attn_groups`，并完成 spec‑decode、eagle、speculator 等子模块的相应适配。

**影响范围**  
- `vllm/v1/worker/gpu/attn_utils.py`（后端初始化与 metadata 构造）  
- `vllm/v1/worker/gpu/cudagraph_utils.py`（图捕获入口）  
- `vllm/v1/worker/gpu/model_runner.py`（KV‑Cache 初始化、dummy‑metadata、捕获/执行路径）  
- `vllm/v1/worker/gpu/spec_decode/eagle/*`（eagle 方案的 CudaGraph 与 speculator）  
- 相关 utils (`vllm/v1/worker/utils.py`) 中新增/导出 `AttentionGroup`。

**潜在风险 & 建议**  

| 维度 | 风险 | 建议 |
|------|------|------|
| **兼容性** | 外部代码仍可能引用 `attn_metadata_builders` 或依赖其顺序。 | 保留向后兼容包装（如属性别名）或在文档明确标记已废弃。 |
| **workspace 复用** | 不同组共享同一 `workspace`，若某 backend 的 `_get_workspace_buffer` 行为改变，可能导致并发写冲突。 | 在单元测试中加入多组混合 backend（FLASHINFER + XFORMERS）并检查 `set_workspace_buffer` 调用路径。 |
| **Uniform KV‑Cache** | `UniformTypeKVCacheSpecs` 展开后每层可能得到不同 `kv_cache_spec`，但原逻辑仍假设统一。 | 添加断言或日志，确保展开后每层规格匹配对应 backend。 |
| **CudaGraph 回退** | FULL 模式下首次调用可能返回 NONE，导致性能波动。 | 在日志中记录回退次数，监控是否频繁出现，必要时在 `capture_graph` 前强制执行 dummy run。 |
| **性能** | 分组后每组仅创建一次 builder，理论上降低 init 开销；但每次 `build_attn_metadata` 仍循环遍历所有组。 | 对大模型（多 KV‑Cache 组）进行基准，确保没有出现显著的 CPU‑GPU 同步开销。 |
| **测试** | 现有测试大多基于单一 backend，未覆盖多组场景。 | 新增 `test_attention_group`，验证：① 同 backend、不同层共用 builder；② 不同 backend 正常分离；③ FLASHINFER workspace 正确共享。 |

**结论**  
此改动为后续实现更加灵活的多‑backend、混合 KV‑Cache 场景奠定基础，但也引入了类型与运行时兼容性的细微破坏。建议在发布前补齐对应的单元/集成测试，并在文档中说明 `AttentionGroup` 的使用方式与迁移指南。若无需立即使用多组特性，可保留旧接口的兼容层以降低现有用户的升级风险。

---

### [Model Bash][DSR1] Add selective dynamic shape marking for CustomOp (#34900)
**SHA**: `74d90b1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/74d90b1ce49e5984ccf054d6e918c8efbafce3c1)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `CustomOp` 引入 **selective dynamic‑shape 标记**：在编译前可自行指定哪些张量维度需要被标记为动态，以避免全局 `dynamic=True` 带来的不必要重编译。  
- `CustomOp.register` 新增 `dynamic_arg_dims` 参数；`mla_decode_concat_quant_fp8` 在注册时声明 `decode_ql_nope`、`decode_q_pe` 的第 0 维为动态。  
- 编译路径统一使用 `backend` 与 `options` 变量，代码结构更清晰。  

**🎯 影响范围**  
- `vllm/model_executor/custom_op.py`（核心自定义算子编译逻辑）  
- `vllm/model_executor/layers/attention/mla_attention.py`（MLA 注意力实现中使用了新注册方式）  
- 可能波及所有使用 `CustomOp.register` 的自定义算子以及依赖 `torch.compile` 的推理路径。  

**💡 关注建议**  
1. **向后兼容**：`register` 参数已默认 `None`，旧代码仍可工作；但若已有子类在运行时依赖 `self._dynamic_arg_dims`，请确认不会因缺失属性导致 `AttributeError`。  
2. **单元测试**：新增对 `dynamic_arg_dims` 的标记行为进行覆盖，验证在 `torch.compile` 环境下对应维度真正被标记为动态且不触发不必要的重新编译。  
3. **性能回归**：在开启/关闭 `dynamic_arg_dims` 两种模式下跑基准，确保动态标记带来的编译时间与运行时开销符合预期。  
4. **文档/示例**：在 `CustomOp` 与相关算子文档中补充 `dynamic_arg_dims` 用法示例，帮助使用者正确声明维度（负数表示倒数第几维）。  
5. **异常安全**：`wrapper` 中对 `arg` 的类型检查已做，但若参数为 `torch.nn.Parameter` 或子类，仍应保持兼容；考虑使用 `torch.is_tensor(arg)` 替代 `isinstance(..., torch.Tensor)`。  

整体来看，此次改动为 **细粒度动态形状控制** 提供了实用接口，有望提升大模型推理的编译效率，只要做好兼容性与性能验证即可安全上线。

---

### [Model Runner V2] Support Eagle3 (no CUDA graph) (#35029)
**SHA**: `a4047d4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a4047d4ea993fd52038433d87c16e603bee4f214)

**🎯 变更类型**：功能增强（新增 Eagle3 规格化推理）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `ModelRunner` 中加入对 Eagle3（无需 CUDA‑graph）的支持，实现了对辅助 hidden‑state 输出的管道。  
2. 新增 `eagle3_utils`，用于在运行时把配置或模型本身声明的 auxiliary‑layer ID 注入模型的 Eagle3 接口。  
3. 通过 `utils.load_eagle_model` 完成 draft‑model 与 target‑model 参数（embedding、lm_head）共享的细粒度控制，简化了模型加载路径。  
4. 迁移了原来的 `eagle_cudagraph` 实现到 `eagle.cudagraph`，并把 speculator 初始化逻辑中 “是否最后 PP rank” 与 “Eagle3 是否需要 aux hidden states” 的判定分离。  
5. 对 pipeline‑parallel、decode‑context‑parallel 的状态变量做了统一初始化，移除了重复代码。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`（核心运行时逻辑）  
- `vllm/v1/worker/gpu/spec_decode/*`（speculative decode 包括 eagle 相关实现）  
- 新增 `eagle/eagle3_utils.py`、`eagle/utils.py`  
- `vllm/config`（SpeculativeConfig 中对 `draft_model_config.eagle_aux_hidden_state_layer_ids` 的读取）  

**💡 关注建议**  

*开发者*  
1. **兼容性检查**：Eagle3 仅在 **单机单卡或多卡非 pipeline‑parallel** 环境下可用，若用户打开了 `pipeline_parallel_size>1` 会直接抛异常。请在文档或 CLI 参数中提前提醒。  
2. **模型实现**：使用 Eagle3 前需保证模型实现 `SupportsEagle3` 接口（`get_eagle3_aux_hidden_state_layers`/`set_aux_hidden_state_layers`）。若自研模型未实现，运行时会报 `RuntimeError`。  
3. **单元测试**：新增单元测试覆盖 `set_eagle3_aux_hidden_state_layers` 的两条路径（从配置读入 vs 从模型获取），以及 `load_eagle_model` 中的 embed / lm_head 共享逻辑。  
4. **状态同步**：`execute_model_state` 现在保存 `aux_hidden_states`，确保后续调用（如 `sample_tokens` → `propose_draft`）能正确使用。若有自定义 `Sampler` 或 `KVConnector`，请检查是否仍然只依赖 `hidden_states`。  

*用户*  
1. **启用 Eagle3**：在 `speculative_config` 中把 `method` 设为 `"eagle3"`，并在 draft‑model 的 HF 配置里（`eagle_aux_hidden_state_layer_ids`）填写需要的层号，或者让模型自行返回层号。  
2. **避免 Pipeline Parallel**：若开启了 pipeline parallel，请暂时关闭或改用原有的 Eagle（带 CUDA‑graph）实现。  
3. **检查模型兼容性**：对已在 `vllm` 中注册的模型（如 Llama、Mistral）确认是否实现 `SupportsEagle3`；否则需要自行实现或回退到普通 speculative decode。  

总体来看，此次 PR 为 vLLM 引入了更灵活的 Eagle3 推理路径，兼容了不支持 CUDA‑graph 的环境，同时保持了旧的 spec‑decode 代码结构。只要在使用前做好兼容性和并行模式的检查，即可安全迁移。

---

#### 🟢 低重要度变更 (6)

### [ROCm][CI] Fix flaky embedding chat test by using tolerance-based comparison (#35050)
**SHA**: `a8a47c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a8a47c17b68fbd4229a86cc1d4202ebc94bdb9fe)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 环境下为嵌入聊天测试加入确定性参数并使用容差比较，防止浮点非关联导致的随机失效，同时保持模型元数据一致性。

---

### Fix apply_top_k_top_p_triton called by non-cuda logits Tensor (#35030)
**SHA**: `30132cd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/30132cd144af8876e7c0d2aac28cabaea3710254)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `apply_top_k_top_p` 中新增 `logits.is_cuda` 检查，仅在 CUDA 张量且满足条件时调用 Triton 实现，避免非 CUDA 环境触发错误。

---

### [Benchmark] Use `sns.relplot` for plotting (#35027)
**SHA**: `cbd95a2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cbd95a2dd19a5786e8b6572a8e6599c8375c4abf)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `vllm/benchmarks/sweep/plot.py` 中的绘图实现从 `sns.FacetGrid` + `sns.lineplot` 改为统一使用 `sns.relplot`（`kind="line"`），简化代码并统一标题、坐标轴比例等设置。

---

### [CI] Bump mteb version to `mteb[bm25s]>=2, <3` for pooling model unit tests (#34961)
**SHA**: `d24bdd7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d24bdd7c4b3eb61a8a025068b6b3ad4d8041abf7)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 mteb 依赖升级到 2.x（加 bm25s），更新 nightly、rocm、test 等需求文件，并在 pooling 模型的 MTEB 单元测试中加入任务数据加载。

---

### [CI] Stabilizing ROCm amd-ci signal and minor name fix in upstream (#35008)
**SHA**: `d403c1d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d403c1da1cd5c210581a2ed4c08c6b932b45186b)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 配置中取消了 test‑amd 步骤的 `grade: Blocking` 注释，修正了 distributed 测试标签的括号错误，并在 ROCm 测试依赖中新增 `tensorizer==2.10.1` 用于示例测试。

---

### [CI/Build] Fix gRPC version mismatch (#35013)
**SHA**: `965fe45` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/965fe45935473e0a81cc6a0885ae7161b9c8b8cf)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `requirements/rocm.txt`、`requirements/test.in` 与 `requirements/test.txt` 中统一指定 `grpcio`、`grpcio-reflection`、`grpcio-tools` 为同一版本 1.78.0，解决 gRPC 版本不匹配问题。

---

