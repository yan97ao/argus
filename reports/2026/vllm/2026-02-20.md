# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-02-20ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-02-20 23:38:11 | Harry Mellor | Ensure that MkDocs v2 does not get installed (#34958) |
| 2026-02-20 22:21:56 | Huamin Li | [perf] Avoid dtype promotion sync in mamba_get_block_table_tensor (#34870) |
| 2026-02-20 22:20:46 | Flora Feng | [Refactor] Extract Harmony streaming SSE event builders into streaming_events.py (#34909) |
| 2026-02-20 22:19:23 | Cyrus Leung | [V0 Deprecation] Remove unused MM placeholders in request output (#34944) |
| 2026-02-20 21:54:27 | Vadim Gimpelson | [BUGFIX] Fix `_dummy_run` missing `prepare_inputs_event` synchronization (#34866) |
| 2026-02-20 17:34:45 | Xin Yang | [Kernel] Optimize grouped topk kernel (#34206) |
| 2026-02-20 14:25:46 | Kevin McKay | [Bugfix][Hardware][AMD] Fix ROCM_AITER_FA speculative decoding support (#32877) |
| 2026-02-20 14:07:57 | Frank Wang | [Minor] Add logging when using MXFP4 MXFP8 TRTLLM backend (#34916) |
| 2026-02-20 14:07:23 | tianshu-Michael-yu | [Models] LFM2: Support LoRA (#34921) |
| 2026-02-20 13:37:49 | Micah Williamson | [ROCm][CI] Loosen RemoteOpenAIServer Startup Timeout (#34922) |
| 2026-02-20 13:33:25 | æ¨æœ± Â· Kiki | [Misc] Add deprecated environment variable utilities (#33677) |
| 2026-02-20 13:32:40 | rasmith | [CI][AMD][BugFix][P/D] Add default_vllm_config to test_moriio_connector.py so tests pass (#33739) |
| 2026-02-20 13:30:33 | Varun Chawla | Add validation to reject non-text content in system messages (#34072) |
| 2026-02-20 13:29:08 | Elizabeth Thomas | [Model Bash]: Improve FP8 Oracle for Config Specific Kernel Selection (#34260) |
| 2026-02-20 13:27:26 | Matthias Gehre | [Bugfix] Add regression test for MoE quant_config under torch.compile (#34335) |
| 2026-02-20 13:27:14 | Bowen Bao | [Quark] Fix MoE fp8 activation scale handling on mi300 (#34386) |
| 2026-02-20 11:59:15 | Kevin H. Luu | [ci] Use the right tag for CPU arm64 image (#34915) |
| 2026-02-20 11:57:55 | Cyrus Leung | [Refactor] Implement output type check in LLM (#34794) |
| 2026-02-20 09:21:46 | Mark McLoughlin | [Core] Fix state names in pause_scheduler() (#34840) |
| 2026-02-20 09:14:54 | Michael Goin | [CI] Add GPT-OSS Eval job for H100 (#34359) |
| 2026-02-20 08:05:37 | Nick Hill | [Model Runner V2] Minor CPU optimizations (#34856) |
| 2026-02-20 08:01:00 | Mayank Ketkar | [Bugfix] Fix benchmark_fused_collective crash on CustomOp init (#34665) |
| 2026-02-20 07:20:52 | Michael Goin | [UX] More descriptive reasons in is_supported_config for MoE (#34908) |
| 2026-02-20 06:49:07 | Matthew Bonanni | [Bugfix] Fix Basic Models Test (#34818) |
| 2026-02-20 05:34:55 | Roger Wang | [Bugfix] Fix cutlass fp8 kernel on hopper for Qwen3.5 (#34914) |
| 2026-02-20 05:26:53 | Alexei-V-Ivanov-AMD | Change targets for AMD build in the "CI" pipeline (#34918) |
| 2026-02-20 02:23:49 | Wentao Ye | [Refactor] Deprecate `head_first` for `chunk_gated_delta_rule` (#34263) |
| 2026-02-20 01:47:05 | roikoren755 | Revert "[NemotronH] Do not force router to run in fp32 (#34582)" (#34808) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 28 ä¸ªæäº¤ | ğŸ”´é«˜ 0 | ğŸŸ¡ä¸­ 12 | ğŸŸ¢ä½ 16
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (12)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-12)
    - [[Refactor] Extract Harmony streaming SSE event builders i...](#ed31a02)
    - [[BUGFIX] Fix `_dummy_run` missing `prepare_inputs_event` ...](#59965af)
    - [[Kernel] Optimize grouped topk kernel (#34206)](#b1c4f0b)
    - [[Models] LFM2: Support LoRA (#34921)](#ea37530)
    - [[Misc] Add deprecated environment variable utilities (#33...](#07cab21)
    - [Add validation to reject non-text content in system messa...](#676f82a)
    - [[Model Bash]: Improve FP8 Oracle for Config Specific Kern...](#81bfc21)
    - [[Refactor] Implement output type check in LLM (#34794)](#ac900c8)
    - [[Model Runner V2] Minor CPU optimizations (#34856)](#40b2f1c)
    - [[UX] More descriptive reasons in is_supported_config for ...](#f72061a)
    - [[Bugfix] Fix Basic Models Test (#34818)](#662205d)
    - [[Refactor] Deprecate `head_first` for `chunk_gated_delta_...](#c683d11)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (16)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-16)
    - [Ensure that MkDocs v2 does not get installed (#34958)](#6ce80f7)
    - [[perf] Avoid dtype promotion sync in mamba_get_block_tabl...](#1fe4621)
    - [[V0 Deprecation] Remove unused MM placeholders in request...](#f9ac192)
    - [[Bugfix][Hardware][AMD] Fix ROCM_AITER_FA speculative dec...](#8de7c63)
    - [[Minor] Add logging when using MXFP4 MXFP8 TRTLLM backend...](#0597792)
    - [[ROCm][CI] Loosen RemoteOpenAIServer Startup Timeout (#34...](#f5432e3)
    - [[CI][AMD][BugFix][P/D] Add default_vllm_config to test_mo...](#0c1dc42)
    - [[Bugfix] Add regression test for MoE quant_config under t...](#4e2c7ca)
    - [[Quark] Fix MoE fp8 activation scale handling on mi300 (#...](#d9e62c0)
    - [[ci] Use the right tag for CPU arm64 image (#34915)](#a1a2d79)
    - [[Core] Fix state names in pause_scheduler() (#34840)](#76df607)
    - [[CI] Add GPT-OSS Eval job for H100 (#34359)](#16f24e8)
    - [[Bugfix] Fix benchmark_fused_collective crash on CustomOp...](#648951a)
    - [[Bugfix] Fix cutlass fp8 kernel on hopper for Qwen3.5 (#3...](#4fb8bee)
    - [Change targets for AMD build in the "CI" pipeline (#34918)](#304319c)
    - [Revert "[NemotronH] Do not force router to run in fp32 (#...](#3eff45d)
#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (12)

### [Refactor] Extract Harmony streaming SSE event builders into streaming_events.py (#34909)
**SHA**: `ed31a02` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ed31a020ee5e383a069a59750261a307bd8ddde4)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆåŠŸèƒ½æŠ½å–ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- å°†åŸæœ¬ `vllm/entrypoints/openai/responses/serving.py` ä¸­ç”¨äº Harmony æµå¼ SSE çš„å¤§é‡ç§æœ‰æ–¹æ³•ã€`HarmonyStreamingState` ä»¥åŠå·¥å…·åˆ¤å®šé€»è¾‘å…¨éƒ¨æŠ½ç¦»åˆ°æ–°æ–‡ä»¶ `streaming_events.py`ï¼Œå¹¶æ”¹ä¸ºçº¯å‡½æ•°å¼å®ç°ã€‚  
- `serving.py` åªä¿ç•™è°ƒåº¦é€»è¾‘ï¼Œæ”¹ä¸ºè°ƒç”¨ `streaming_events` ä¸­çš„ `emit_*` ç³»åˆ—å‡½æ•°ï¼›ç›¸åº”çš„å·¥å…·åˆ¤æ–­å‡½æ•°ä» `_is_mcp_tool_by_namespace` æ”¹åä¸º `is_mcp_tool_by_namespace`ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **æ ¸å¿ƒæ¨¡å—**ï¼š`vllm.entrypoints.openai.responses.serving`ã€`vllm.entrypoints.openai.responses.streaming_events`ã€‚  
- **å…³è”æ¨¡å—**ï¼š`vllm.entrypoints.mcp.tool_server`ï¼ˆæ–°å¢å‡½æ•°ç­¾å `emit_tool_action_events(..., tool_server)`ï¼‰ï¼Œä»¥åŠæ‰€æœ‰ä¾èµ– `HarmonyStreamingState` çš„å•å…ƒæµ‹è¯•/å¤–éƒ¨è°ƒç”¨ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **ç¡®ä¿å‘åå…¼å®¹**ï¼šå¦‚æœé¡¹ç›®å…¶å®ƒä½ç½®ä»ç›´æ¥å¼•ç”¨ `serving.py` ä¸­çš„ç§æœ‰æ–¹æ³•ï¼ˆå¦‚ `_emit_*`ï¼‰ï¼Œéœ€è¦æ›´æ–°ä¸ºæ–°çš„å…¬å¼€å‡½æ•°æˆ–åœ¨ `__init__` ä¸­æ·»åŠ å…¼å®¹åˆ«åã€‚  
2. **å¯¼å…¥è·¯å¾„**ï¼šç¡®è®¤ `streaming_events.py` å·²è¢«åŠ å…¥ `vllm.entrypoints.openai.responses` åŒ…çš„ `__all__`ï¼ˆæˆ–åœ¨ `__init__.py` ä¸­å¯¼å‡ºï¼‰ï¼Œé˜²æ­¢å¤–éƒ¨ `import` å¤±æ•ˆã€‚  
3. **å·¥å…·åˆ¤å®šç»Ÿä¸€**ï¼šåŸæ¥çš„ `_TOOL_NAME_TO_MCP_SERVER_LABEL` è¢«é‡å‘½åä¸º `TOOL_NAME_TO_MCP_SERVER_LABEL`ï¼Œè¯·æ£€æŸ¥å…¶ä»–æ–‡ä»¶æ˜¯å¦ä»ä½¿ç”¨æ—§å¸¸é‡åã€‚  
4. **å•å…ƒæµ‹è¯•**ï¼šä¸º `streaming_events.py` æ·»åŠ ç‹¬ç«‹æµ‹è¯•ï¼Œè¦†ç›–ï¼š  
   - `emit_content_delta_events` åœ¨ä¸åŒ channel/recipient ç»„åˆä¸‹è¿”å›çš„äº‹ä»¶é¡ºåºä¸ç±»å‹ã€‚  
   - `emit_previous_item_done_events` å¯¹å‡½æ•°è°ƒç”¨ã€MCP è°ƒç”¨ã€analysisã€final å››ç§å‰ç½® item çš„å®Œæ•´é—­ç¯ã€‚  
   - `emit_tool_action_events` åœ¨åŒ…å«æµè§ˆå™¨ã€MCPã€ä»£ç è§£é‡Šå™¨ä¸‰ç±»å·¥å…·æ—¶çš„åˆ†æ”¯è·¯å¾„ã€‚  
5. **æ€§èƒ½ä¸å¯ç»´æŠ¤æ€§**ï¼šæŠ½ç¦»åå‡½æ•°ä¿æŒçº¯ç²¹ï¼Œä¾¿äºå¤ç”¨ä¸ç¼“å­˜ã€‚è‹¥åç»­æ·»åŠ æ–°å·¥å…·ï¼Œåªéœ€åœ¨ `streaming_events.py` ä¸­æ‰©å±•å¯¹åº” `emit_*`ï¼Œä¸å¿…ä¿®æ”¹ä¸»æœåŠ¡é€»è¾‘ã€‚  
6. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨é¡¹ç›®æ–‡æ¡£æˆ–ä»£ç æ³¨é‡Šä¸­æ ‡æ˜ `HarmonyStreamingState` ä¸äº‹ä»¶æ„é€ å·²è¿ç§»è‡³ `streaming_events.py`ï¼Œå¹¶è¯´æ˜ `serving.py` åªè´Ÿè´£æµæ§åˆ¶ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡é‡æ„æ˜¾è‘—æå‡äº†ä»£ç å¯è¯»æ€§ä¸å¯æµ‹è¯•æ€§ï¼Œåªè¦åœ¨è¿ç§»è¿‡ç¨‹ä¸­æ£€æŸ¥æ‰€æœ‰æ—§å¼•ç”¨å¹¶è¡¥å…¨æµ‹è¯•ï¼Œå³å¯å®‰å…¨åˆå¹¶ã€‚

---

### [BUGFIX] Fix `_dummy_run` missing `prepare_inputs_event` synchronization (#34866)
**SHA**: `59965af` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/59965affbd6e652a3c8ed229b66ef34a681e5693)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤ä¿®å¤äº† `GPUModelRunner._dummy_run` ä¸­æœªå¯¹è¾“å…¥å‡†å¤‡è¿›è¡ŒåŒæ­¥çš„é—®é¢˜ã€‚åŸå®ç°ç›´æ¥å†™å…¥å…±äº«çš„ pinnedâ€‘CPU ç¼“å†²åŒºï¼ˆ`seq_lens`ã€`query_start_loc` ç­‰ï¼‰ï¼Œåœ¨å‰ä¸€æ¬¡éé˜»å¡ H2D æ‹·è´å°šæœªå®Œæˆæ—¶å¯èƒ½è¢«è¦†ç›–ï¼Œå¯¼è‡´å›¾æ•è·æˆ–æ¨ç†é˜¶æ®µå‡ºç°æ•°æ®ç«äº‰ã€‚ç°åœ¨åœ¨å‡†å¤‡è¿™äº›ç¼“å†²åŒºå‰ï¼Œä½¿ç”¨ `self.synchronize_input_prep()` è¿›å…¥åŒä¸€äº‹ä»¶åè®®çš„åŒæ­¥ä¸Šä¸‹æ–‡ï¼Œä¿è¯ dummy æ­¥å’ŒçœŸå®æ¨ç†æ­¥ä¹‹é—´çš„ H2D DMA å®Œå…¨åºåˆ—åŒ–ã€‚ä¸æ­¤åŒæ—¶ï¼Œç›¸å…³é€»è¾‘ä¿æŒåŸæœ‰è¡Œä¸ºï¼Œä»…å°†ä»£ç å—æ•´ä½“ç§»å…¥ `with` ä½œç”¨åŸŸã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/worker/gpu_model_runner.py`ï¼ˆ`_dummy_run` æ–¹æ³•ï¼‰  
- ä¾èµ– `GPUModelRunner` çš„æ‰€æœ‰æ¨¡å‹è°ƒåº¦è·¯å¾„ï¼Œå°¤å…¶æ˜¯ **FIï¼ˆFlashâ€‘Inferï¼‰ warmâ€‘up** ä¸ **cudagraph æ•è·** åœºæ™¯ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **åŠŸèƒ½éªŒè¯**ï¼šåœ¨å¼€å¯ `cudagraph_runtime_mode=FULL`ã€`force_attention=True`ã€ä»¥åŠæ··åˆ batchï¼ˆ`create_mixed_batch=True`ï¼‰çš„ç»„åˆä¸‹è·‘å®Œæ•´çš„ warmâ€‘up ä¸å®é™…æ¨ç†ï¼Œç¡®è®¤ä¸å†å‡ºç°ä¸ç¡®å®šçš„ CUDA é”™è¯¯æˆ–æ³¨æ„åŠ›å…ƒæ•°æ®é”™ä½ã€‚  
2. **æ€§èƒ½å›å½’**ï¼šåŒæ­¥ä¸Šä¸‹æ–‡ä¼šåœ¨ CPU ç«¯å¢åŠ ä¸€æ¬¡æµåŒæ­¥å¼€é”€ï¼Œå»ºè®®ä½¿ç”¨å¾®åŸºå‡†å¯¹æ¯” dummyâ€‘run å‰åçš„æ¯å¸§æ—¶å»¶ï¼Œç¡®ä¿å¯¹æ•´ä½“ååå½±å“åœ¨å¯æ¥å—èŒƒå›´ï¼ˆ<1%ï¼‰ã€‚  
3. **ä»£ç å®¡é˜…**ï¼šç¡®è®¤ `self.synchronize_input_prep()` çš„å®ç°å·²æ­£ç¡®åœ¨ `GPUModelRunner` ä¸­åˆ›å»º/è®°å½• `prepare_inputs_event`ï¼Œå¹¶åœ¨åç»­ `execute_model` ä¸­åŒæ­¥å¯¹åº” eventï¼Œé˜²æ­¢å‡ºç° â€œevent æœªæ³¨å†Œâ€ çš„éšè”½é”™è¯¯ã€‚  
4. **æ–‡æ¡£/æ³¨é‡Š**ï¼šåœ¨ `GPUModelRunner` ç±»çš„è¯´æ˜ä¸­è¡¥å…… dummyâ€‘run ä¸çœŸå®è¿è¡Œå…±äº« pinned ç¼“å†²åŒºçš„çº¦æŸï¼Œæé†’åç»­ç»´æŠ¤è€…åœ¨æ–°å¢è·¯å¾„æ—¶ä¹Ÿè¦éµå®ˆåŒæ ·çš„åŒæ­¥åè®®ã€‚  
5. **å›æ»šå‡†å¤‡**ï¼šè‹¥å‡ºç°å¼‚å¸¸ï¼ˆä¾‹å¦‚åœ¨æŸäº›æ—§ç‰ˆæ˜¾å¡ä¸ŠåŒæ­¥å¯¼è‡´æ­»é”ï¼‰ï¼Œä¿ç•™åŸä»£ç çš„è¡¥ä¸æ³¨é‡Šï¼Œä»¥ä¾¿å¿«é€Ÿå›é€€ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤ä¿®å¤æ¶ˆé™¤äº†æ½œåœ¨çš„å†…å­˜ç«äº‰ï¼Œæå‡äº† cudagraph æ•è·ä¸ FIâ€‘warmup çš„é²æ£’æ€§ï¼Œæ”¹åŠ¨å±€éƒ¨ä¸”åå‘å…¼å®¹ï¼Œå»ºè®®åˆå¹¶å¹¶å°½å¿«åœ¨ CI ä¸å®é™…éƒ¨ç½²ç¯å¢ƒä¸­éªŒè¯ã€‚

---

### [Kernel] Optimize grouped topk kernel (#34206)
**SHA**: `b1c4f0b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/b1c4f0b26548d36fca304b298957e4791eafa09b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆGPUâ€¯groupedâ€‘topkâ€¯kernelï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- å¼•å…¥ `moeTopKFuncs.cuh`ï¼Œå®ç°é«˜æ•ˆçš„ warpâ€‘level Topâ€‘K å½’çº¦ä¸æ’åºå·¥å…·ã€‚  
- æ–°å¢ `grouped_topk_fused_small_expert_count_kernel`ï¼Œåœ¨ä¸“å®¶æ•°è¾ƒå°‘ï¼ˆâ‰¤â€¯512ï¼‰æˆ–å•ç»„åœºæ™¯ä¸‹ä½¿ç”¨å›ºå®šâ€‘çº¿ç¨‹æ•°ã€é›¶åŠ¨æ€å…±äº«å†…å­˜çš„è·¯å¾„ï¼Œæ˜¾è‘—é™ä½å¯åŠ¨å¼€é”€å¹¶æå‡ååã€‚  
- `invokeNoAuxTc` é‡æ–°ç»„ç»‡ï¼Œè‡ªåŠ¨æ£€æµ‹å•ç»„/å¤šç»„å°ä¸“å®¶æ•°å¹¶é€‰æ‹©ä¸Šè¿°ç‰¹åŒ– kernelï¼›å…¶ä½™æƒ…å†µå›é€€åˆ°åŸå§‹ `grouped_topk_fused_kernel`ã€‚  
- Python å±‚ `grouped_topk` é€šè¿‡ `LAUNCH_KERNEL_SF` ç»Ÿä¸€è°ƒåº¦ï¼Œå»é™¤åŸæ¥çš„ `scoring_func` æ•´å‹å‚æ•°ï¼Œæ”¹ä¸ºæ¨¡æ¿åŒ– `ScoringFunc`ï¼›åŒæ—¶åœ¨æµ‹è¯•ä¸­å¼ºåˆ¶å¼€å¯ `VLLM_BATCH_INVARIANT` ä»¥è¦†ç›–æ–°ä»£ç è·¯å¾„ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- æ ¸å¿ƒ C++/CUDAï¼š`csrc/moe/grouped_topk_kernels.cu`ã€`csrc/moe/moeTopKFuncs.cuh`ã€æ¨¡æ¿å®ä¾‹åŒ–å®ã€‚  
- Python æ¥å£ï¼š`vllm/model_executor/layers/moe.py`ï¼ˆ`grouped_topk`ï¼‰ï¼Œä»¥åŠå¯¹åº”å•å…ƒæµ‹è¯•ã€‚  
- ç¼–è¯‘é…ç½®ï¼šéœ€è¦æ”¯æŒ CUDAâ€¯â‰¥â€¯9.0ï¼ˆ`cudaGridDependencySynchronize`ï¼‰ä»¥åŠ `-std=c++17`ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **æ­£ç¡®æ€§éªŒè¯**  
   - ç°æœ‰å•å…ƒæµ‹è¯•å·²è¦†ç›– 5 ç»„ä¸åŒçš„ä¸“å®¶æ•°/Topâ€‘K ç»„åˆï¼Œå»ºè®®å†åŠ å…¥è¾¹ç•Œæƒ…å†µï¼ˆå¦‚ `num_experts = 513`ã€`topk = 23`ï¼‰ä»¥åŠæç«¯ `topk_group > num_expert_group` åœºæ™¯ï¼Œé˜²æ­¢ fallback é€»è¾‘é—æ¼ã€‚  
   - æ£€æŸ¥ `apply_scoring<SF>` åœ¨ `SCORING_NONE`ã€`SCORING_SIGMOID` ä¸¤æ¡è·¯å¾„ä¸Šå‡å¾—åˆ°ç›¸åŒæ•°å€¼ï¼ˆå°¤å…¶åœ¨ `renormalize=True` æ—¶çš„å½’ä¸€åŒ–è¯¯å·®ï¼‰ã€‚  

2. **æ€§èƒ½å›å½’**  
   - åœ¨å…¸å‹æ¨¡å‹ï¼ˆNemotronâ€‘66Bã€DeepSeekâ€‘V2ã€Kimiâ€‘K2ï¼‰ä¸Šè·‘åŸºå‡†ï¼Œå¯¹æ¯”æ—§ kernel çš„ `ms/token`ï¼Œç¡®ä¿åœ¨ 1â€‘512 experts èŒƒå›´å†…æå‡ â‰¥â€¯15%ã€‚  
   - å¯¹ `is_multi_group` æ¡ä»¶è¿›è¡Œ stress testï¼Œç¡®è®¤ warpâ€‘å±‚åˆ†å—ä¸ä¼šå‡ºç°å…±äº«å†…å­˜ç«äº‰æˆ– bank conflictã€‚  

3. **å…¼å®¹æ€§**  
   - `invokeNoAuxTc` çš„ç­¾åå»æ‰äº† `scoring_func` å‚æ•°ï¼Œå¤–å±‚ä»é€šè¿‡ `int scoring_func` ä¼ é€’ï¼›ç¡®ä¿æ‰€æœ‰è°ƒç”¨ç‚¹å·²ç›¸åº”æ›´æ–°ï¼Œé¿å…è¿è¡Œæ—¶ `assert`ã€‚  
   - ç¯å¢ƒå˜é‡ `VLLM_USE_FUSED_MOE_GROUPED_TOPK=0` ä»èƒ½å¼ºåˆ¶ä½¿ç”¨è€å®ç°ï¼Œæ–‡æ¡£åº”è¯´æ˜è¯¥å¼€å…³çš„ç”¨é€”ã€‚  

4. **ä»£ç ç»´æŠ¤**  
   - `moeTopKFuncs.cuh` ä¸­å¤§é‡æ¨¡æ¿ä¸å®æ··ç”¨ï¼Œå»ºè®®æ·»åŠ ç®€è¦æ³¨é‡Šè¯´æ˜ `TopKRedType` çš„ä½å‹ç¼©åŸç†ä¸ `kMaxIdx` é™åˆ¶ï¼Œä»¥é™ä½åæœŸç»´æŠ¤é—¨æ§›ã€‚  
   - ç»Ÿä¸€ `constexpr` å‘½åé£æ ¼ï¼ˆå¦‚ `WARP_SIZE` vs `kWARP_SIZE`ï¼‰ï¼Œé¿å…æ··æ·†ã€‚  

5. **éƒ¨ç½²æ³¨æ„**  
   - å¯¹äºæ—§ç‰ˆ GPUï¼ˆSM<90ï¼‰ä»ä¼šèµ°è€ kernelï¼›è‹¥ç”¨æˆ·åœ¨æ–°ç‰ˆ GPU ä¸Šå¼€å¯ `VLLM_BATCH_INVARIANT=True`ï¼Œè¯·ç¡®ä¿ CUDA é©±åŠ¨ä¸åº“å…¼å®¹æ–° kernel çš„ `cudaLaunchAttributeProgrammaticStreamSerialization`ã€‚  

æ€»ä½“æ¥è¯´ï¼Œæ­¤æ¬¡æäº¤é€šè¿‡ç‰¹åŒ–å°ä¸“å®¶æ•°è·¯å¾„å¤§å¹…é™ä½ kernel å¯åŠ¨åŠå…±äº«å†…å­˜å‹åŠ›ï¼Œç¬¦åˆ vLLM å¯¹å¤§è§„æ¨¡ MoE çš„æ€§èƒ½è¯‰æ±‚ã€‚åªè¦åœ¨æµ‹è¯•ä¸æ–‡æ¡£å±‚é¢è¡¥é½è¾¹ç•Œè¦†ç›–ï¼Œå³å¯å®‰å…¨åˆå¹¶ã€‚

---

### [Models] LFM2: Support LoRA (#34921)
**SHA**: `ea37530` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ea37530b474fa738a99a53a8975af4e389b968c7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆLFM2 ç³»åˆ—æ¨¡å‹æ–°å¢ LoRA æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. å°†åŸå…ˆçš„ `w1` å‚æ•°å±‚é‡å‘½åä¸º `w13`ï¼ˆåˆå¹¶ `w1`ã€`w3`ï¼‰ï¼Œå¹¶åœ¨å‰å‘è®¡ç®—ä¸­ä½¿ç”¨ `self.w13`ã€‚  
2. åœ¨æƒé‡åŠ è½½é€»è¾‘ä¸­é€šè¿‡â€œæ®µè¾¹ç•Œâ€( `weight_name + "."` ) åŒ¹é…ï¼Œé˜²æ­¢ `".w1"` å†²çªåˆ° `".w13"`ã€‚  
3. æ–°å¢ `WeightsMapper` ç”¨äºæŠŠ HuggingFace çš„ `.conv.` å‰ç¼€æ˜ å°„ä¸º vLLM çš„ `.short_conv.`ï¼Œé¿å… LoRA æ­£åˆ™åŒ¹é…å†²çªã€‚  
4. å¯¹ MoE ç‰ˆæ¨¡å‹åŒæ­¥åŒæ ·æ”¹åŠ¨ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/model_executor/models/lfm2.py`ã€`lfm2_moe.py`ï¼ˆæ¨¡å‹ç»“æ„ã€æƒé‡åŠ è½½ã€LoRA æ˜ å°„ï¼‰ã€‚  
- ç›¸å…³å·¥å…·ç±» `utils.WeightsMapper`ï¼ˆæ–°åŠ å…¥ï¼‰ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
- **å…¼å®¹æ€§**ï¼šæ—§çš„ checkpoint ä»ä½¿ç”¨ `w1`/`w3` åç§°ï¼Œç°å·²ç»Ÿä¸€ä¸º `w13`ï¼Œè¯·ç¡®è®¤è¿ç§»è„šæœ¬æˆ–æƒé‡è½¬æ¢å·¥å…·å·²æ›´æ–°ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚  
- **LoRA é€‚é…**ï¼šæ–°å¢ `hf_to_vllm_mapper` åªå¤„ç† `.conv.` â†’ `.short_conv.`ï¼Œå¦‚æ¨¡å‹ç»“æ„å‡ºç°å…¶ä»–è‡ªå®šä¹‰å­æ¨¡å—ï¼Œéœ€è¦åœ¨ `WeightsMapper` ä¸­è¡¥å……æ˜ å°„ã€‚  
- **æµ‹è¯•**ï¼šå»ºè®®è¡¥å……å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ `load_weights` åœ¨å«æœ‰é¢„èåˆæƒé‡ï¼ˆ`.w13`ï¼‰å’Œæœªèåˆæƒé‡ï¼ˆåˆ†åˆ« `.w1`ã€`.w3`ï¼‰çš„ä¸¤å¥— checkpoint å‡èƒ½æ­£ç¡®åŠ è½½ã€‚  
- **æ–‡æ¡£**ï¼šæ›´æ–°æ¨¡å‹è¯´æ˜æ–‡æ¡£ï¼Œæ˜ç¡® `w13` ä¸ºåˆå¹¶å±‚å¹¶è§£é‡Š LoRA å‚æ•°çš„å‘½åçº¦å®šï¼Œé¿å…ç”¨æˆ·åœ¨è‡ªå®šä¹‰ LoRA é…ç½®æ—¶äº§ç”Ÿæ··æ·†ã€‚  

æ•´ä½“æ”¹åŠ¨åˆç†ï¼Œæå‡äº†å¯¹ LoRA çš„åŸç”Ÿæ”¯æŒï¼Œåªè¦æ³¨æ„ä¸Šè¿°å…¼å®¹ä¸æ–‡æ¡£åŒæ­¥å³å¯é¡ºåˆ©ä¸Šçº¿ã€‚

---

### [Misc] Add deprecated environment variable utilities (#33677)
**SHA**: `07cab21` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/07cab212f0dcc51cfe4e4f93b58935e8079f26b7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ `vllm/config/utils.py` ä¸­æ–°å¢ä¸¤å¥—å¤„ç†å·²åºŸå¼ƒç¯å¢ƒå˜é‡çš„å·¥å…·å‡½æ•° `get_from_deprecated_env_if_set` ä¸ `set_from_deprecated_env_if_set`ï¼Œç»Ÿä¸€åŠ å…¥æ—¥å¿—ä¸€æ¬¡æ€§è­¦å‘Šå¹¶åœ¨éœ€è¦æ—¶æŠŠå€¼å†™å›é…ç½®å¯¹è±¡ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/config/utils.py`ï¼ˆæ–°å¢å®ç°ï¼‰  
- `vllm/envs.py`ï¼ˆè¢«å¼•ç”¨ä»¥åˆ¤æ–­å˜é‡æ˜¯å¦å·²è®¾ç½®ï¼‰  
- ä»»ä½•ä½¿ç”¨æ—§ç¯å¢ƒå˜é‡çš„æ¨¡å—ï¼ˆå¦‚å¯åŠ¨è„šæœ¬ã€ç¤ºä¾‹é…ç½®ï¼‰å°†å—æ­¤å±‚å°è£…å½±å“ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šæ£€æŸ¥é¡¹ç›®ä¸­æ‰€æœ‰å·²åºŸå¼ƒçš„ envvarï¼ˆå¦‚ `VLLM_XXX`ï¼‰æ˜¯å¦å·²æ”¹ä¸ºå¯¹åº”çš„é…ç½®å­—æ®µï¼Œå¹¶ä½¿ç”¨æ–°å‡½æ•°ç»Ÿä¸€å¤„ç†ï¼Œé˜²æ­¢é—æ¼å¯¼è‡´æ„å¤–è¡Œä¸ºã€‚  
2. **æ—¥å¿—å®ç°**ï¼š`logger.warning_once` å¿…é¡»ä¿è¯åªä¼šè¾“å‡ºä¸€æ¬¡ï¼Œå¦åˆ™ä¼šåœ¨å¤§é‡å¹¶å‘è¯·æ±‚ä¸‹äº§ç”Ÿå™ªå£°ã€‚å»ºè®®åœ¨å•å…ƒæµ‹è¯•é‡ŒéªŒè¯è¯¥å‡½æ•°çš„å¹‚ç­‰æ€§ã€‚  
3. **ç±»å‹è½¬æ¢**ï¼š`to_bool` ä¸ `to_int` äº’æ–¥ï¼Œè°ƒç”¨æ–¹åŠ¡å¿…æ˜ç¡®è½¬æ¢éœ€æ±‚ï¼›è‹¥ä»¥åéœ€è¦å…¶ä»–ç±»å‹ï¼ˆfloatã€listï¼‰ï¼Œå¯æ‰©å±•è¯¥å‡½æ•°ã€‚  
4. **Removal version**ï¼šç¡®ä¿ `removal_version` å‚æ•°ä¿æŒä¸æ–‡æ¡£åŒæ­¥ï¼Œé˜²æ­¢åœ¨å®é™…ç‰ˆæœ¬ä»æ—§æ”¯æŒæ—¶è¯¯æŠ¥ã€‚  
5. **æµ‹è¯•è¦†ç›–**ï¼šæ–°å¢å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ï¼šâ‘  ç¯å¢ƒå˜é‡æœªè®¾ç½®è¿”å› `None`ï¼›â‘¡ è®¾ç½®åäº§ç”Ÿä¸€æ¬¡æ€§è­¦å‘Šå¹¶è¿”å›æ­£ç¡®å€¼ï¼›â‘¢ `set_from_deprecated_env_if_set` èƒ½æ­£ç¡®å†™å…¥ `config` å¹¶å®Œæˆå¸ƒå°”/æ•´æ•°è½¬æ¢ã€‚  
6. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨é…ç½®ç« èŠ‚æ·»åŠ â€œå·²åºŸå¼ƒç¯å¢ƒå˜é‡â€åˆ—è¡¨å’Œè¿ç§»æŒ‡å—ï¼Œæé†’ç”¨æˆ·å°½å¿«æ”¹ç”¨å¯¹åº”çš„é…ç½®å­—æ®µã€‚  

é€šè¿‡ä¸Šè¿°æªæ–½ï¼Œå¯å¹³æ»‘è¿‡æ¸¡æ—§çš„ç¯å¢ƒå˜é‡ä½¿ç”¨æ–¹å¼ï¼Œé™ä½ç”¨æˆ·å‡çº§é£é™©ï¼ŒåŒæ—¶ä¿æŒä»£ç åŸºäºç»Ÿä¸€çš„åºŸå¼ƒå¤„ç†é€»è¾‘ã€‚

---

### Add validation to reject non-text content in system messages (#34072)
**SHA**: `676f82a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/676f82ae8140a512dae73bcae6c6d23907f55e0e)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸ºç³»ç»Ÿæ¶ˆæ¯åŠ å…¥å†…å®¹ç±»å‹æ ¡éªŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `vllm/entrypoints/openai/chat_completion/protocol.py` ä¸­æ–°å¢ `check_system_message_content_type` Pydantic **model validator**ï¼Œåœ¨è¯·æ±‚è§£æå‰éå† `messages`ï¼Œè‹¥å‘ç° role ä¸º `system` ä¸” `content` ä¸­å‡ºç°é™¤ `text` ä»¥å¤–çš„ç±»å‹ï¼ˆimageã€audioã€video ç­‰ï¼‰ï¼Œé€šè¿‡ `logger.warning_once` è¾“å‡ºè­¦å‘Šã€‚  
2. ä¸ºè¯¥æ ¡éªŒè¡¥å……äº† **unit tests**ï¼ˆ`tests/entrypoints/openai/test_chat_error.py`ï¼‰ï¼Œè¦†ç›– imageã€audioã€video ä¸‰ç§éæ–‡æœ¬ content çš„è­¦å‘Šè·¯å¾„ï¼ŒåŒæ—¶éªŒè¯æ™®é€šæ–‡æœ¬åŠæ–‡æœ¬æ•°ç»„ä¸è§¦å‘è­¦å‘Šï¼Œä¸”ç”¨æˆ·æ¶ˆæ¯ä»å¯åŒ…å«å¤šæ¨¡æ€å†…å®¹ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm.entrypoints.openai.chat_completion.protocol.ChatCompletionRequest`ï¼ˆæ ¸å¿ƒè¯·æ±‚æ¨¡å‹ï¼‰  
- `vllm.entrypoints.openai.chat_completion.protocol.logger`ï¼ˆè­¦å‘Šæ—¥å¿—å®ç°ï¼‰  
- ä¸ OpenAI Chat Completion å…¼å®¹å±‚ç›¸å…³çš„æ‰€æœ‰å…¥å£ï¼ˆRESTã€OpenAIâ€‘compatible serverï¼‰  
- å•å…ƒæµ‹è¯•ç›®å½• `tests/entrypoints/openai/`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

| æ–¹å‘ | å»ºè®® |
|------|------|
| **æ—¥å¿—è¡Œä¸º** | ç¡®è®¤ `logger.warning_once` åœ¨é¡¹ç›®ä¸­å®ç°ä¸º **å¹‚ç­‰**ï¼ˆåŒä¸€æ¬¡è¯·æ±‚åªæ‰“å°ä¸€æ¬¡ï¼‰ï¼Œé¿å…åœ¨é«˜å¹¶å‘è¯·æ±‚ä¸‹äº§ç”Ÿå¤§é‡é‡å¤æ—¥å¿—ã€‚è‹¥åç»­å†³å®šå°†æ­¤ç±»è¾“å…¥è§†ä¸ºé”™è¯¯ï¼Œè€ƒè™‘æ”¹ä¸º `raise ValidationError`ï¼Œå¹¶åŒæ­¥æ›´æ–°æ–‡æ¡£ä¸é”™è¯¯ç ã€‚ |
| **æ€§èƒ½å½±å“** | éªŒè¯ä»…åœ¨æ¨¡å‹å®ä¾‹åŒ–å‰æ‰§è¡Œï¼Œéå† `messages` çš„æ—¶é—´å¤æ‚åº¦ä¸º O(N)ï¼ˆN ä¸ºæ¶ˆæ¯æ•°ï¼‰ï¼Œå¯¹å¸¸è§„è¯·æ±‚å‡ ä¹æ²¡æœ‰å¯æ„ŸçŸ¥å¼€é”€ï¼Œæ— éœ€é¢å¤–æ€§èƒ½è¯„å®¡ã€‚ |
| **å…¼å®¹æ€§** | è¯¥æ ¡éªŒä½¿ç”¨ Pydantic `@model_validator(mode="before")`ï¼Œç¡®ä¿å…¼å®¹é¡¹ç›®å½“å‰çš„ Pydantic 2 ç‰ˆæœ¬ï¼›å¦‚æœä»æ”¯æŒ Pydantic 1ï¼Œéœ€æ·»åŠ ç›¸åº”çš„ `@root_validator(pre=True)` æ›¿ä»£å®ç°ã€‚ |
| **æµ‹è¯•è¦†ç›–** | å·²è¦†ç›–æœ€å¸¸è§çš„éæ–‡æœ¬ payloadï¼Œå»ºè®®å†åŠ å…¥ **ç¼ºçœ `type` å­—æ®µä¸”åŒæ—¶åŒ…å« `text` ä¸éæ–‡æœ¬é”®**ï¼ˆå¦‚ `{"image_url": {...}}`ï¼‰çš„ç»„åˆæµ‹è¯•ï¼Œç¡®ä¿è­¦å‘Šè·¯å¾„ä¸å—é”®é¡ºåºå½±å“ã€‚ |
| **æ–‡æ¡£ & ç¤ºä¾‹** | æ›´æ–° OpenAIâ€‘compatible API æ–‡æ¡£ï¼Œæ˜ç¡® â€œç³»ç»Ÿæ¶ˆæ¯åªèƒ½æ˜¯çº¯æ–‡æœ¬â€ï¼Œå¹¶åœ¨ç¤ºä¾‹ä»£ç ä¸­å±•ç¤ºå½“è¯¯ä¼  multimodal å†…å®¹æ—¶ä¼šå¾—åˆ°è­¦å‘Šè€Œéç›´æ¥é”™è¯¯ã€‚ |
| **åç»­æ¼”è¿›** | è‹¥ç¤¾åŒºåé¦ˆæ­¤è­¦å‘Šä¸å¤Ÿä¸¥æ ¼ï¼Œå¯åœ¨æœªæ¥ç‰ˆæœ¬å°†å…¶å‡çº§ä¸ºé”™è¯¯å¹¶è¿”å› `HTTP 400`ï¼ŒåŒæ—¶ä¿æŒå‘åå…¼å®¹çš„ `--allow-nontext-system-message` é…ç½®å¼€å…³ã€‚ |

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨åœ¨ä¸ç ´åç°æœ‰ç”¨æˆ·è¡Œä¸ºçš„å‰æä¸‹ï¼Œæé«˜äº†å¯¹ OpenAI è§„èŒƒçš„éµä»æ€§ï¼Œå¹¶é€šè¿‡å®Œæ•´çš„å•å…ƒæµ‹è¯•ä¿è¯äº†åŠŸèƒ½çš„å¯é æ€§ã€‚åç»­å¯è§†ç¤¾åŒºåé¦ˆå†³å®šæ˜¯å¦æå‡ä¸ºç¡¬æ€§æ ¡éªŒã€‚ç¥å‘å¸ƒé¡ºåˆ©ï¼

---

### [Model Bash]: Improve FP8 Oracle for Config Specific Kernel Selection (#34260)
**SHA**: `81bfc21` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/81bfc21a6ad0cb498dbe5466ccf2987624efbba5)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- ä¸º FP8 MoE Oracle å¼•å…¥ `_get_priority_backends`ï¼Œæ ¹æ®å¹³å°ã€æ˜¾å¡ç‰¹æ€§ä»¥åŠé‡åŒ–é”® (`kFp8Dynamic128Sym`, `kFp8Static128BlockSym`) åŠ¨æ€è°ƒæ•´åç«¯ä¼˜å…ˆçº§ã€‚  
- åœ¨ Hopper GPUï¼ˆSM90ï¼‰ä¸”ä½¿ç”¨ Blockâ€‘Fp8 æ—¶ï¼Œè‹¥ EPâ€¯>â€¯1 åˆ™ä¼˜å…ˆ FlashInferâ€‘CUTLASSï¼›å¦åˆ™ä¼˜å…ˆ Tritonã€‚  
- `select_fp8_moe_backend` ç°åœ¨é€šè¿‡è¯¥å‡½æ•°è·å–æœ‰åºçš„ `AVAILABLE_BACKENDS`ï¼Œå…¶ä½™é€»è¾‘ä¿æŒä¸å˜ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/layers/fused_moe/oracle/fp8.py`ï¼ˆåç«¯é€‰æ‹©é€»è¾‘ï¼‰  
- ä¾èµ– `QuantKey` çš„é‡åŒ–æ¨¡å—ï¼ˆæ–°å¢ `kFp8Dynamic128Sym`ã€`kFp8Static128BlockSym` çš„å¯¼å…¥ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æµ‹è¯•è¦†ç›–**ï¼šä¸º Hopper/SM90 åœºæ™¯å¢åŠ å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿åœ¨ä¸åŒ `ep_size` ä¸‹åç«¯é¡ºåºç¬¦åˆé¢„æœŸï¼Œé˜²æ­¢å›å½’ã€‚  
2. **å¯ç»´æŠ¤æ€§**ï¼š`_AVAILABLE_BACKENDS` åœ¨å‡½æ•°å†…éƒ¨å°±åœ°ä¿®æ”¹ï¼Œè‹¥åç»­åœ¨å…¶ä»–åœ°æ–¹å¤ç”¨è¯¥åˆ—è¡¨å¯èƒ½äº§ç”Ÿå‰¯ä½œç”¨ï¼Œå»ºè®®åœ¨å¤åˆ¶åå† `move_to_front`ã€‚  
3. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤ `kFp8Dynamic128Sym`ã€`kFp8Static128BlockSym` å·²åœ¨ `quant_utils` ä¸­å®šä¹‰å¹¶å‘åå…¼å®¹ï¼›è‹¥æœ‰æ—§ç‰ˆäºŒè¿›åˆ¶ä»ä½¿ç”¨æ—§é”®ï¼Œéœ€ä¿æŒå…¼å®¹è·¯å¾„ã€‚  
4. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨ `vllm` æ–‡æ¡£æˆ–æ³¨é‡Šä¸­è¯´æ˜è¯¥å¹³å°â€‘é…ç½®ç‰¹åŒ–çš„åç«¯é€‰æ‹©è§„åˆ™ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ä¸ºä½•åœ¨ Hopper ä¸Šé»˜è®¤ä½¿ç”¨ Triton/FlashInferã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ”¹åŠ¨æå‡äº† FP8 MoE åœ¨æ–°ç¡¬ä»¶ä¸Šçš„æ€§èƒ½è°ƒåº¦ï¼Œä½†éœ€æ³¨æ„æµ‹è¯•å’Œåˆ—è¡¨çš„å¯å˜æ€§ï¼Œä»¥é¿å…æ½œåœ¨çš„é€‰æ‹©é”™è¯¯æˆ–æ„å¤–å‰¯ä½œç”¨ã€‚

---

### [Refactor] Implement output type check in LLM (#34794)
**SHA**: `ac900c8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ac900c89bba77f69ed42e8a19a5006bd215eeb80)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¸º `LLM` æ¥å£æ–°å¢ `output_type` å‚æ•°ï¼Œå®ç°è¿è¡Œæ—¶è¾“å‡ºç±»å‹æ£€æŸ¥ã€‚  
2. ä½¿ç”¨ `typing.overload` ä¸º `wait_for_completion` æä¾›å¤šç­¾åï¼Œç»Ÿä¸€ `_run_*` ç³»åˆ—å‡½æ•°ç­¾åï¼Œå»æ‰æ—§çš„ `engine_class.validate_outputs` ç»Ÿä¸€è¿”å›ã€‚  
3. åœ¨ `LLMEngine._run_engine` ä¸­åŠ å…¥ `assert isinstance(output, output_type)`ï¼Œå¹¶ç›¸åº”è°ƒæ•´è¿”å›ç±»å‹æ³¨è§£ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/entrypoints/llm.py`ï¼ˆæ ¸å¿ƒ APIï¼š`generateã€chatã€encodeã€scoreã€wait_for_completion` ç­‰ï¼‰  
- `vllm/v1/engine/llm_engine.py`ï¼ˆå»é™¤ `validate_outputs` æ—§å®ç°ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å‘åå…¼å®¹**ï¼šé»˜è®¤ `output_type` ä¸º `(RequestOutput, PoolingRequestOutput)`ï¼Œä½†åœ¨æ—§ç‰ˆä»£ç ä¸­ä»å¯èƒ½ç›´æ¥è°ƒç”¨ `engine.validate_outputs`ï¼Œéœ€ç¡®è®¤æ‰€æœ‰å†…éƒ¨è°ƒç”¨å·²è¿ç§»ï¼Œå¦åˆ™ä¼šè§¦å‘ `AttributeError`ã€‚  
- **æ€§èƒ½**ï¼šåœ¨æ¯ä¸ª step ä¸­è¿›è¡Œ `isinstance` æ£€æŸ¥ï¼Œå¼€é”€å¯å¿½ç•¥ä¸è®¡ï¼›è‹¥åç»­æ”¯æŒå¤šç§è¾“å‡ºç»„åˆï¼ˆtupleï¼‰ï¼Œå»ºè®®æ”¹ä¸º `any(isinstance(...))`ï¼Œé¿å…ä¸å¿…è¦çš„éå†ã€‚  
- **ç±»å‹å®‰å…¨**ï¼š`_O` çš„ `bound` ä¸ `default` åŒæ—¶ä½¿ç”¨ï¼Œç¡®ä¿ `mypy` æ­£å¸¸ï¼›è‹¥ç”¨æˆ·è‡ªè¡Œä¼ å…¥è‡ªå®šä¹‰å­ç±»ï¼Œéœ€è¦åœ¨ `LLMEngine._run_engine` å‰åšå¥½ç±»å‹æ³¨å†Œã€‚  
- **æ–‡æ¡£ & æµ‹è¯•**ï¼šæ›´æ–° README ä¸ API æ–‡æ¡£ï¼Œè¯´æ˜ `output_type` å‚æ•°çš„æ„ä¹‰å’Œé»˜è®¤è¡Œä¸ºï¼›æ–°å¢é’ˆå¯¹ä¸åŒ `output_type`ï¼ˆå¦‚åªè¿”å› `PoolingRequestOutput`ï¼‰çš„å•å…ƒæµ‹è¯•ï¼Œé˜²æ­¢è¿è¡Œæ—¶æ–­è¨€å¤±è´¥ã€‚  
- **é”™è¯¯ä¿¡æ¯**ï¼šå½“å‰ä»…ä½¿ç”¨ `assert`ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒå¯èƒ½è¢«å…³é—­ã€‚å»ºè®®æ”¹ä¸ºæ˜¾å¼æŠ›å‡º `TypeError`ï¼Œå¹¶åœ¨é”™è¯¯ä¿¡æ¯ä¸­æç¤ºæœŸæœ›çš„ç±»å‹ï¼Œæå‡å¯è°ƒè¯•æ€§ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº† API çš„ç±»å‹å¯æ§æ€§ï¼Œå…³é”®æ˜¯ç¡®ä¿æ‰€æœ‰å†…éƒ¨è°ƒç”¨å·²åŒæ­¥åˆ°æ–°ç­¾åå¹¶è¡¥å……ç›¸åº”æµ‹è¯•ï¼Œä»¥é˜²å‡ºç°å…¼å®¹æ€§å›é€€ã€‚

---

### [Model Runner V2] Minor CPU optimizations (#34856)
**SHA**: `40b2f1c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/40b2f1c3d9c1dbcec185e8b6911fd273524f5b88)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆCPU/æµå¤„ç†ï¼‰  

**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨å…±äº«å†…å­˜å¹¿æ’­é‡Œï¼Œå°† `while True` å¾ªç¯çš„ `metadata_buffer` è·å–æå‰åˆ°å¤–å±‚ï¼Œé¿å…åœ¨è‡ªæ—‹æœŸé—´åå¤è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå‡å° CPU cache missã€‚  
2. ä¸º GPU ç«¯å¼‚æ­¥æ‹·è´æ–°å¢è½»é‡çº§ `stream` ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç›´æ¥ä½¿ç”¨ `torch.cuda.set_stream`ï¼Œçœå» `torch.cuda.current_stream()` ä¸è®¾å¤‡æŸ¥è¯¢çš„å¼€é”€ï¼Œå¹¶åœ¨ `ModelRunner` ä¸­ç¼“å­˜é»˜è®¤ `main_stream`ã€‚  
3. `async_copy_to_gpu` ç°åœ¨å§‹ç»ˆå¯¹ CPU å¼ é‡æ‰§è¡Œ `pin_memory()`ï¼ˆåŸæ¥åªåœ¨é pinned æ—¶æ–­è¨€ï¼‰ï¼Œå¹¶ç¡®ä¿è¿”å›çš„ pinned ç¼“å†²åŒºæ˜¯æ–°å¯¹è±¡ï¼Œé˜²æ­¢è¯¯ç”¨åŒä¸€å†…å­˜ã€‚  
4. `copy_to_gpu` é€»è¾‘ç®€åŒ–ï¼Œç»Ÿä¸€èµ° `clone()` åˆ†æ”¯æˆ– `out.copy_`ï¼Œå»æ‰ä¸å¿…è¦çš„åˆ†æ”¯ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/distributed/device_communicators/shm_broadcast.py`ï¼ˆè‡ªæ—‹è¯»å–é€»è¾‘ï¼‰  
- `vllm/v1/worker/gpu/async_utils.py`ï¼ˆæµåŒæ­¥ä¸æ‹·è´ï¼‰  
- `vllm/v1/worker/gpu/buffer_utils.py`ï¼ˆCPUâ†’GPU æ‹·è´ï¼‰  
- `vllm/v1/worker/gpu/model_runner.py`ï¼ˆ`main_stream` ç¼“å­˜ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å¼€å‘è€…**ï¼šç¡®ä¿æ–°åŠ å…¥çš„ `stream` ä¸Šä¸‹æ–‡åœ¨å¼‚å¸¸è·¯å¾„ä»èƒ½æ¢å¤åŸå§‹æµï¼ˆå·²é€šè¿‡ `finally` å®ç°ï¼‰ï¼Œå¹¶åœ¨å•å…ƒæµ‹è¯•ä¸­è¦†ç›– `main_stream` ç¼“å­˜çš„å¤šçº¿ç¨‹/å¤šè¿›ç¨‹åœºæ™¯ã€‚`async_copy_to_gpu` ç°åœ¨å¼ºåˆ¶ `pin_memory`ï¼Œè‹¥æœ‰ç‰¹æ®Šä¸šåŠ¡ä¾èµ–å·² pinned çš„å¼ é‡ï¼Œéœ€è¦ç¡®è®¤è¡Œä¸ºä¸€è‡´ã€‚  
- **ç”¨æˆ·**ï¼šå¯¹å·²ä½¿ç”¨è‡ªå®šä¹‰æ‹·è´æˆ–æ‰‹åŠ¨ç®¡ç† CUDA æµçš„ä»£ç æ— éœ€æ”¹åŠ¨ï¼Œæ–°çš„ `main_stream` åªåœ¨å†…éƒ¨ä½¿ç”¨ï¼Œé€æ˜æå‡ååã€‚è‹¥åœ¨ä½åŠŸè€— CPU ç¯å¢ƒä¸‹ä»å‡ºç°è‡ªæ—‹å¡é¡¿ï¼Œå¯é€šè¿‡è°ƒä½ `n_warning` æˆ–ç›‘æ§ `metadata_buffer` çš„å†™å…¥å»¶è¿Ÿè¿›ä¸€æ­¥å®šä½ã€‚  
- **å›æ»š/å…¼å®¹**ï¼šä¸Šè¿°ä¿®æ”¹å‡ä¸ºå†…éƒ¨å®ç°ç»†èŠ‚ï¼Œæ—  API å˜åŠ¨ï¼Œæ­£å¸¸å‡çº§ä¸ä¼šç ´åç°æœ‰è„šæœ¬ã€‚å»ºè®®åœ¨ upgrade å‰è¿è¡Œå®Œæ•´çš„ benchmarkï¼Œç¡®è®¤ CPU åˆ©ç”¨ç‡å’Œååæå‡ç¬¦åˆé¢„æœŸã€‚

---

### [UX] More descriptive reasons in is_supported_config for MoE (#34908)
**SHA**: `f72061a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f72061a19ae7fbb7f193c31f0abea355fab41892)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆUX æ”¹è¿›ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
ä¸º MoEï¼ˆMixtureâ€‘ofâ€‘Expertsï¼‰ç›¸å…³çš„ `is_supported_config` æ£€æŸ¥æ·»åŠ æ›´ç»†ç²’åº¦çš„é”™è¯¯æè¿°ã€‚ç°åœ¨åœ¨ä¸æ»¡è¶³è®¾å¤‡ã€æ¿€æ´»å‡½æ•°ã€é‡åŒ–æ–¹æ¡ˆã€å¹¶è¡Œé…ç½®ã€è·¯ç”±æ–¹å¼ã€æ¿€æ´»æ ¼å¼ä»¥åŠéšè—ç»´åº¦ç­‰æ¡ä»¶æ—¶ï¼Œä¼šè¿”å›å…·ä½“çš„ â€œcurrent device X / quantization scheme AÃ—B / parallel config Y â€¦â€ ä¿¡æ¯ï¼Œè€Œä¸å†æ˜¯ç¬¼ç»Ÿçš„ â€œcurrent device / quantization schemeâ€ç­‰ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`  
- `vllm/model_executor/layers/fused_moe/modular_kernel.py`  
- `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`  

è¿™äº›æ–‡ä»¶å‡è´Ÿè´£åˆ¤æ–­ MoE æ ¸å¿ƒæ˜¯å¦å¯ç”¨ï¼Œæ¶‰åŠçš„å‡½æ•°å‡åŠ å…¥äº† `current_platform.device_name` çš„å¼•ç”¨ä»¥åŠæ›´ä¸°å¯Œçš„ `reason` æ‹¼æ¥ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤ `current_platform.device_name` åœ¨æ‰€æœ‰å—æ”¯æŒå¹³å°ä¸Šå§‹ç»ˆéç©ºï¼Œå¦åˆ™å¯èƒ½å‡ºç° `None` å‡ºç°åœ¨é”™è¯¯ä¿¡æ¯ä¸­ã€‚è‹¥æœ‰å¹³å°ä¸æä¾›è¯¥å±æ€§ï¼Œè€ƒè™‘å›é€€åˆ°åŸå§‹æç¤ºã€‚  
2. **æµ‹è¯•æ›´æ–°**ï¼šå•å…ƒæµ‹è¯•æˆ–é›†æˆæµ‹è¯•é‡Œå¯èƒ½å¯¹ `is_supported_config` è¿”å›çš„ `reason` è¿›è¡Œæ–­è¨€ï¼Œéœ€åŒæ­¥æ›´æ–°é¢„æœŸå­—ç¬¦ä¸²ã€‚  
3. **æ–‡æ¡£/æ—¥å¿—**ï¼šåœ¨æ–‡æ¡£æˆ–ç”¨æˆ·æŒ‡å—ä¸­åŠ å…¥ç¤ºä¾‹ï¼Œè¯´æ˜å½“å‡ºç° â€œkernel does not support â€¦â€ æ—¶å¦‚ä½•æ ¹æ®å…·ä½“å­—æ®µï¼ˆdeviceã€quantization scheme ç­‰ï¼‰å®šä½é—®é¢˜ã€‚  
4. **é”™è¯¯å¤„ç†**ï¼šä¸Šå±‚ä»£ç ä»ç„¶åªå…³æ³¨è¿”å›çš„å¸ƒå°”å€¼ï¼Œä¿æŒå…¼å®¹ï¼›ä½†è‹¥æœ‰ä¸šåŠ¡é€»è¾‘ç›´æ¥è§£æ `reason`ï¼Œè¯·ç¡®ä¿æ–°æ ¼å¼ä¸ä¼šç ´åç°æœ‰è§£æé€»è¾‘ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ”¹åŠ¨ä»…æ¶‰åŠé”™è¯¯ä¿¡æ¯çš„ç»†åŒ–ï¼Œé£é™©æä½ï¼Œæå‡äº†è°ƒè¯•ä½“éªŒã€‚è¯·åœ¨ CI ä¸­ç¡®è®¤ç›¸å…³æµ‹è¯•å·²è¦†ç›–å¹¶é€šè¿‡ã€‚

---

### [Bugfix] Fix Basic Models Test (#34818)
**SHA**: `662205d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/662205d34eb1bb42228768d7a69a1ac4abf38c89)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / Bug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šæœ¬æ¬¡ PR ç»Ÿä¸€å¹¶å¼ºåŒ–äº† KVâ€‘cache blockâ€‘size çš„æ¨æ–­ã€æ ¡éªŒä¸ä½¿ç”¨è·¯å¾„ã€‚æ ¸å¿ƒæ”¹åŠ¨åŒ…æ‹¬ï¼š  
1. å°†åŸå…ˆåœ¨ `CudaPlatform.check_and_update_config` ä¸­çš„ blockâ€‘size è‡ªåŠ¨æ¨å¯¼è¿ç§»åˆ° `VllmConfig.validate_block_size`ï¼Œå¹¶åœ¨æ¨¡å‹åˆå§‹åŒ–åç»Ÿä¸€è°ƒç”¨ã€‚  
2. æ–°å¢ `VllmConfig.validate_block_size` ä¸ `validate_mamba_block_size`ï¼Œåœ¨ DCPã€Mambaã€Hybrid åœºæ™¯ä¸‹ç»Ÿä¸€æ£€æŸ¥ blockâ€‘size åˆæ³•æ€§ã€‚  
3. ç²¾ç®€ `CudaPlatform.update_block_size_for_backend`ï¼Œä»…åœ¨æœªæ˜¾å¼æŒ‡å®šæ—¶ä¾æ®é¦–ä¸ªæ³¨æ„åŠ›å±‚çš„åç«¯é¦–é€‰å€¼è®¾å®š blockâ€‘sizeï¼›åˆ é™¤äº†åŸå…ˆçš„å¤§æ®µæ‰‹åŠ¨æ ¡éªŒé€»è¾‘ã€‚  
4. å¤šå¤„ executorã€workerã€attention å±‚ä»£ç æ”¹ä¸ºåœ¨è¿è¡Œæ—¶è¯»å– `cache_config.block_size`ï¼ˆé»˜è®¤ 16ï¼‰ï¼Œå¹¶åœ¨ `GpuModelRunner` ä¸­ä½¿ç”¨å ä½å€¼é¿å…ç©ºæŒ‡é’ˆã€‚  
5. æµ‹è¯•ç”¨ä¾‹ç›¸åº”åŠ å…¥ `CacheConfig(block_size=16)`ï¼Œå¹¶åœ¨ `dummy_hf_overrides` ä¸­ä¿®æ­£ MoEâ€‘related å‚æ•°ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/config/*`ï¼ˆCacheConfigã€VllmConfigï¼‰  
- `vllm/platforms/*`ï¼ˆCudaã€Interfaceï¼‰  
- `vllm/v1/executor/*`ï¼ˆå¤šè¿›ç¨‹ã€Rayã€å•è¿›ç¨‹ï¼‰  
- `vllm/v1/engine/core.py`ã€`gpu_model_runner.py`  
- å¤šæ¨¡æ€ä¸ MoE ç›¸å…³çš„å•å…ƒæµ‹è¯•  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å‘åå…¼å®¹æ€§**ï¼šè‹¥ç”¨æˆ·åœ¨å‘½ä»¤è¡Œæˆ–é…ç½®æ–‡ä»¶ä¸­æ‰‹åŠ¨è®¾å®š `--block-size`ï¼Œç°åœ¨ä»ä¿ç•™ï¼Œä½†æ³¨æ„ä»…åœ¨åç«¯æ”¯æŒæ—¶æ‰ç”Ÿæ•ˆï¼Œåç«¯ä¸å…¼å®¹æ—¶ä¼šåœ¨ `validate_block_size` æŠ›å¼‚å¸¸ã€‚å»ºè®®åœ¨æ–‡æ¡£ä¸­æ˜ç¡® â€œè‡ªå®šä¹‰ blockâ€‘size å¿…é¡»æ»¡è¶³åç«¯å…¼å®¹æ€§â€ã€‚  
2. **é»˜è®¤å€¼**ï¼šåœ¨æœªæŒ‡å®šä¸”æ¨¡å‹ä¸º encoderâ€‘only/SSM æ—¶è‡ªåŠ¨ä½¿ç”¨ 16ï¼Œç¡®ä¿ä¸å›  `None` å¯¼è‡´ downstream æŠ¥é”™ã€‚  
3. **æ€§èƒ½å›å½’**ï¼š`validate_block_size` åœ¨æ¯æ¬¡ `VllmConfig` ä½¿ç”¨åéƒ½ä¼šæ–­è¨€ï¼Œç†è®ºä¸Šå¼€é”€æä½ï¼Œä½†å»ºè®®åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼è·‘æ‰¹æ—¶è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥ç¡®è®¤æœªå¼•å…¥æ˜¾è‘—å¯åŠ¨å»¶è¿Ÿã€‚  
4. **ä»£ç å®¡è®¡**ï¼š`attention.select_backend` ç°åœ¨ä¼šåœ¨ `invalid_reasons` ä¸­è®°å½•ä¼˜å…ˆçº§ï¼Œæ—¥å¿—ä¸­ä¼šå‡ºç° â€œexcluded higherâ€‘priority backend(s)â€ è­¦å‘Šï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°çœŸå®åŸå› ã€‚è‹¥æœ‰è‡ªå®šä¹‰åç«¯ï¼Œéœ€è¦ç›¸åº”å®ç° `get_preferred_block_size`ã€‚  
5. **æµ‹è¯•è¦†ç›–**ï¼šå½“å‰å·²æ·»åŠ å¯¹ `CacheConfig(block_size=16)` çš„å•å…ƒè¦†ç›–ï¼Œå»ºè®®å†è¡¥å……ä¸€ä¸ªæ˜¾å¼ `--block-size` ä¸ä¸å…¼å®¹åç«¯çš„è´Ÿæµ‹è¯•ï¼ŒéªŒè¯å¼‚å¸¸ä¿¡æ¯æ¸…æ™°ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº† blockâ€‘size çš„ç»Ÿä¸€ç®¡ç†å’Œé”™è¯¯æç¤ºï¼Œé™ä½äº†å› å¹³å°å·®å¼‚å¯¼è‡´çš„éšå¼é”™è¯¯é£é™©ï¼Œå½±å“èŒƒå›´ä¸»è¦åœ¨é…ç½®ã€å¹³å°æ£€æµ‹åŠæ‰§è¡Œè·¯å¾„ï¼Œå»ºè®®åœ¨æ­£å¼å‘å¸ƒå‰åšä¸€æ¬¡å…¨é“¾è·¯æ€§èƒ½å›å½’ã€‚

---

### [Refactor] Deprecate `head_first` for `chunk_gated_delta_rule` (#34263)
**SHA**: `c683d11` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/c683d11c94655655cd7bf95a27aef7e245325102)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / è¿ç§»ï¼ˆDeprecate `head_first` å‚æ•°ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ `chunk_gated_delta_rule` å®ç°ä¸­åˆ é™¤ `head_first` å‚æ•°åŠå…¶ç›¸å…³çš„å¼ é‡è½¬ç½®é€»è¾‘ï¼Œç»Ÿä¸€æ‰€æœ‰è¾“å…¥å‡é‡‡ç”¨ **[B,â€¯T,â€¯H,â€¯â€¦]**ï¼ˆseqâ€‘firstï¼‰å¸ƒå±€ï¼›ç›¸åº”åœ°åœ¨ `qwen3_next`ã€`llava_onevision` ç­‰æ¨¡å‹å±‚çš„è°ƒâ½¤å¤„å»é™¤è¯¥å‚æ•°ï¼Œå¹¶åˆ å‡äº†ä¸å¿…è¦çš„ `einops.rearrange` å¯¼å…¥ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/layers/fla/ops/chunk.py`ï¼ˆæ ¸å¿ƒ FlashAttentionâ€‘Gatedâ€‘Delta å®ç°ï¼‰  
- `vllm/model_executor/models/qwen3_next.py`ï¼ˆæ‰€æœ‰ `forward_*` åˆ†æ”¯ï¼‰  
- `vllm/model_executor/models/llava_onevision.py`ï¼ˆ`embed_multimodal` çš„å°æ”¹åŠ¨ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **è°ƒç”¨æ–¹æ£€æŸ¥**ï¼šç¡®è®¤é¡¹ç›®ä¸­æ‰€æœ‰è°ƒç”¨ `chunk_gated_delta_rule`ï¼ˆæˆ–å°è£…çš„ `fi_chunk_gated_delta_rule`ã€`fla_chunk_gated_delta_rule`ï¼‰çš„ä»£ç å·²ç§»é™¤ `head_first` å®å‚ï¼Œä¸”ä¼ å…¥çš„å¼ é‡ç»´åº¦ä¸º `[B,â€¯T,â€¯H,â€¯â€¦]`ã€‚  
2. **å…¼å®¹æ€§æµ‹è¯•**ï¼šå°¤å…¶æ˜¯ä½¿ç”¨å˜é‡é•¿åº¦ `cu_seqlens` çš„åœºæ™¯ï¼Œä¹‹å‰ `head_first=True` ä¸å¯å˜é•¿è¾“å…¥ä¸å…¼å®¹ï¼Œç°å·²ç»Ÿä¸€ä¸º `head_first=False`ï¼Œåº”éªŒè¯æ— è¯¯ã€‚  
3. **æ–‡æ¡£åŒæ­¥**ï¼šæ›´æ–°å‡½æ•°æ³¨é‡Šã€README ä¸ API æ–‡æ¡£ï¼Œå»é™¤å…³äº `head_first` çš„è¯´æ˜ï¼Œä¿ç•™å¯¹å½¢çŠ¶ä¸åŒ¹é…çš„è­¦å‘Šä¿¡æ¯ã€‚  
4. **å•å…ƒ/é›†æˆæµ‹è¯•**ï¼šæ·»åŠ æˆ–æ‰©å±•é’ˆå¯¹ seqâ€‘first ä¸ variableâ€‘length è¾“å…¥çš„æµ‹è¯•ï¼Œç¡®ä¿åœ¨ä¸åŒ batch / seq é•¿åº¦ç»„åˆä¸‹ä»å¾—åˆ°æ­£ç¡®çš„ `o` ä¸ `final_state`ã€‚  
5. **ä»£ç è§„èŒƒ**ï¼šå·²åˆ é™¤æœªä½¿ç”¨çš„ `einops` å¯¼å…¥ï¼Œä¿æŒ lint å¹²å‡€ï¼›æ³¨æ„ `warnings.warn` ä»ä¼šåœ¨è¾“å…¥ç»´åº¦å¼‚å¸¸æ—¶è§¦å‘ï¼Œå»ºè®®åœ¨ä¸Šå±‚æå‰æ ¡éªŒä»¥é¿å…è¿è¡Œæ—¶å™ªå£°ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ç®€åŒ–äº† APIï¼Œé™ä½äº†è¯¯ç”¨é£é™©ï¼Œåªè¦ç¡®è®¤å…¨éƒ¨è°ƒç”¨å·²è¿ç§»åˆ°ç»Ÿä¸€å¸ƒå±€å¹¶è¡¥é½ç›¸åº”æµ‹è¯•ï¼Œå³å¯å®‰å…¨åˆå¹¶ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (16)

### Ensure that MkDocs v2 does not get installed (#34958)
**SHA**: `6ce80f7` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/6ce80f7071b009badaa2c473e96ec55a134790d2)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `requirements/docs.txt` ä¸­å°† MkDocs çš„ä¾èµ–é™å®šä¸º `<2.0.0`ï¼Œé˜²æ­¢åœ¨æ„å»ºæ–‡æ¡£æ—¶å®‰è£…ä¸å…¼å®¹çš„ MkDocs 2.x ç‰ˆæœ¬ã€‚

---

### [perf] Avoid dtype promotion sync in mamba_get_block_table_tensor (#34870)
**SHA**: `1fe4621` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/1fe462168c381f604a5ef9d491a230a3dd861d2c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `mamba_get_block_table_tensor` ä¸­å°† `offsets` ç”Ÿæˆæ”¹ä¸º `int32`ï¼Œéšåè½¬ä¸º `int64` ä¾› `gather` ä½¿ç”¨ï¼Œé¿å… dtype æå‡åŒæ­¥å¼€é”€ï¼Œæé«˜æ€§èƒ½ã€‚

---

### [V0 Deprecation] Remove unused MM placeholders in request output (#34944)
**SHA**: `f9ac192` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f9ac19204f0c4c3041d0afbe7d5eb4d63e73f15c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šç§»é™¤ `request output` ä¸­æœªä½¿ç”¨çš„å¤šæ¨¡æ€å ä½ç¬¦ç›¸å…³ä»£ç å’Œå¯¼å…¥ï¼Œç®€åŒ– `RequestOutput` å®šä¹‰ã€‚

---

### [Bugfix][Hardware][AMD] Fix ROCM_AITER_FA speculative decoding support (#32877)
**SHA**: `8de7c63` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8de7c636cc02a8306441af868b9c1d0e6d64799f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `rocm_aiter_fa.py` ä¸­ï¼ŒåŠ å…¥å¯¹ Speculative Decodingï¼ˆå¤š tokenï¼‰åŠæ»‘åŠ¨çª—å£åœºæ™¯çš„ç»Ÿä¸€æ³¨æ„åŠ›å®ç°ï¼›åœ¨ä¸æ”¯æŒ shuffle KV å¸ƒå±€æ—¶è°ƒç”¨ `unified_attention` å¹¶ç›´æ¥è¿”å›ï¼Œä¿®å¤åŸæœ‰å´©æºƒé—®é¢˜ã€‚

---

### [Minor] Add logging when using MXFP4 MXFP8 TRTLLM backend (#34916)
**SHA**: `0597792` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/059779231f158b8b570e71aaa5c66f49b41b2fb1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `vllm/model_executor/layers/quantization/mxfp4.py` ä¸­ä¸ºä½¿ç”¨ FlashInfer MXFP4/MXFP8 TRTLLM åç«¯ï¼ˆSM100ï¼‰æ·»åŠ ä¸€æ¬¡æ€§æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•å’Œè¿è¡Œæ—¶ä¿¡æ¯è¿½è¸ªã€‚

---

### [ROCm][CI] Loosen RemoteOpenAIServer Startup Timeout (#34922)
**SHA**: `f5432e3` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f5432e35a3a4f0bd6e7d49c51a35a0a01bc32452)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `RemoteOpenAIServer` å¯åŠ¨æ—¶å°†é»˜è®¤ç­‰å¾…è¶…æ—¶ä» 240â€¯ç§’æå‡è‡³ 360â€¯ç§’ï¼ˆæµ‹è¯•ä¸­è®¾ä¸º 480â€¯ç§’ï¼‰ï¼Œä»¥é€‚åº” ROCm ç¯å¢ƒä¸‹å¯èƒ½æ›´æ…¢çš„å¯åŠ¨è¿‡ç¨‹ã€‚

---

### [CI][AMD][BugFix][P/D] Add default_vllm_config to test_moriio_connector.py so tests pass (#33739)
**SHA**: `0c1dc42` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0c1dc42748760fc75aef68e973c9ff7a47501337)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `test_moriio_connector.py` ä¸­ä½¿ç”¨ `set_current_vllm_config` ä¸Šä¸‹æ–‡ç®¡ç†å™¨åŒ…è£… `MoRIIOConnector` å®ä¾‹ï¼Œç¡®ä¿æµ‹è¯•æ—¶èƒ½å¤Ÿæ­£ç¡®è·å–å½“å‰ VLLM é…ç½®ï¼Œä¿®å¤æµ‹è¯•å¤±è´¥ã€‚

---

### [Bugfix] Add regression test for MoE quant_config under torch.compile (#34335)
**SHA**: `4e2c7ca` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4e2c7caf2d11444ce6c1e4895bc921c93610bd7c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ–°å¢ `test_w4a16_moe_torch_compile` å›å½’æµ‹è¯•ï¼ŒéªŒè¯åœ¨ `torch.compile` ç¯å¢ƒä¸‹ MoE é‡åŒ–é…ç½®åœ¨ `moe_forward` è‡ªå®šä¹‰ç®—å­ä¸­æ­£ç¡®åˆå§‹åŒ–ï¼Œé˜²æ­¢éšè—ç»´åº¦ä¸åŒ¹é…é”™è¯¯ã€‚

---

### [Quark] Fix MoE fp8 activation scale handling on mi300 (#34386)
**SHA**: `d9e62c0` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d9e62c03eb98e3adcf82a2177f4a8b8f851406e4)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ MoE fp8 æ¿€æ´»å°ºåº¦å¤„ç†é€»è¾‘ä¸­ï¼Œæ–°å¢ `self.input_dtype == "fp8"` æ¡ä»¶ï¼Œå¹¶å°†å½’ä¸€åŒ–æ—¶ä½¿ç”¨çš„å¼ é‡ dtype ä» `float8_e4m3fnuz` ä¿®æ­£ä¸º `float8_e4m3fn`ï¼Œè§£å†³ mi300 ä¸Šçš„ fp8 æ¿€æ´»å°ºåº¦é”™è¯¯ã€‚

---

### [ci] Use the right tag for CPU arm64 image (#34915)
**SHA**: `a1a2d79` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a1a2d79442ed00284e70b829e07cadbb887bdf73)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šCI è„šæœ¬å°† CPU arm64 é•œåƒçš„æ ‡ç­¾ä» `-cpu` æ›´æ­£ä¸º `-arm64-cpu`ï¼Œå¹¶ç›¸åº”æ›´æ–°æ£€æŸ¥ã€æ„å»ºå’Œæ¨é€å‘½ä»¤ã€‚

---

### [Core] Fix state names in pause_scheduler() (#34840)
**SHA**: `76df607` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/76df6072ff4829980ad71764191fc970a873275a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `pause_scheduler()` ä¸­ç»Ÿä¸€ä½¿ç”¨æ–°æš‚åœçŠ¶æ€åç§°ï¼ˆ`PAUSED_NEW`ã€`PAUSED_ALL`ï¼‰ï¼Œå¹¶åŒæ­¥æ›´æ–°ç›¸åº”æ–‡æ¡£è¯´æ˜ï¼Œä¿æŒè¡Œä¸ºä¸å˜ï¼Œä»…ä¿®æ­£å‘½åé”™è¯¯ã€‚

---

### [CI] Add GPT-OSS Eval job for H100 (#34359)
**SHA**: `16f24e8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/16f24e87975ef4cd2c12879425062913ef62f6fd)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ Buildkite CI é…ç½® `misc.yaml` ä¸­æ–°å¢é’ˆå¯¹ H100 GPU çš„ GPTâ€‘OSS è¯„ä¼°ä»»åŠ¡ï¼Œå®‰è£… `gpt-oss[eval]==0.0.5` å¹¶è¿è¡Œå¯¹åº”æ­£ç¡®æ€§æµ‹è¯•ã€‚

---

### [Bugfix] Fix benchmark_fused_collective crash on CustomOp init (#34665)
**SHA**: `648951a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/648951a9c3ab7d8ade25b80edb55eb4018acfd58)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `benchmark_fused_collective.py` ä¸­ï¼Œå°† `VllmFusedAllreduce` çš„å®ä¾‹åŒ–ç§»åŠ¨åˆ°æ¯ä¸ªé…ç½®å—å†…éƒ¨ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒè‡ªå®šä¹‰ç®—å­ï¼ˆRMSNormã€QuantFP8 ç­‰ï¼‰ä¸‹èƒ½å¤Ÿæ­£ç¡®ç»‘å®šå¯¹åº”çš„å‰å‘å®ç°ã€‚åŒæ—¶ä¿®æ­£äº†ç»“æœé”®åçš„æ‹¼æ¥é”™è¯¯ã€‚

---

### [Bugfix] Fix cutlass fp8 kernel on hopper for Qwen3.5 (#34914)
**SHA**: `4fb8bee` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4fb8beefaa8b2c4bd2cd3b336b01ff006dc98bdc)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `flashinfer_utils.py` ä¸­ä¸º `block_quant` çš„ `w13_scale`ã€`w2_scale` åŠ æœ€å°é˜ˆå€¼ `1e-10` clampï¼Œé¿å… Hopperï¼ˆSMâ€¯9.0ï¼‰ä¸Š CUTLASS FP8 kernel äº§ç”Ÿ NaNã€‚

---

### Change targets for AMD build in the "CI" pipeline (#34918)
**SHA**: `304319c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/304319c4edcc1a50317c22715ad6c0111459025d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `.buildkite/hardware_tests/amd.yaml` ä¸­ï¼Œå°† AMD CI æ„å»ºçš„ `ARG_PYTORCH_ROCM_ARCH` å‚æ•°ç”±åŸæ¥çš„ `'gfx90a;gfx942'` æ›´æ–°ä¸º `'gfx942;gfx950'`ï¼Œä»¥é€‚é…æ–°çš„ GPU æ¶æ„ã€‚

---

### Revert "[NemotronH] Do not force router to run in fp32 (#34582)" (#34808)
**SHA**: `3eff45d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/3eff45d793daa976a21d0df5954cf6cc6723335f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ¢å¤ NemotronH æ¨¡å‹ä¸­è·¯ç”±å™¨å¼ºåˆ¶ä½¿ç”¨ FP32 çš„å®ç°ï¼Œæ–°å¢ `router_logits_dtype = torch.float32` å‚æ•°å¹¶åœ¨å‰å‘ä¼ æ’­æ—¶å°†è¾“å…¥æ˜¾å¼ cast ä¸º `torch.float32`ï¼Œç¡®ä¿è·¯ç”±è®¡ç®—çš„æ•°å€¼ç²¾åº¦ã€‚

---

