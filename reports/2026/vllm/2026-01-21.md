# 每日更新报告（2026-01-21）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-21 23:16:30 | Pleaplusone | [ROCm][Deepseekv3.2] Refactor Sparse Indexer as CustomOp (#29287) |
| 2026-01-21 22:32:40 | Robert Shaw | [Quantization][Deprecation] Deprecate HQQ (#32681) |
| 2026-01-21 22:32:12 | Robert Shaw | [Quantization][Deprecation] Remove `DeepSpeedFp8` (#32679) |
| 2026-01-21 21:22:33 | Robert Shaw | [MoE Refactor] Oracle Select FP8+NVFP4 Kernels In Priority (#32414) |
| 2026-01-21 21:11:31 | Divakar Verma | [bugfix] Aria model (#32727) |
| 2026-01-21 17:39:53 | Kim Hee Su | [Model] Add Eagle2.5-8B Vision-Language Model support   (#32456) |
| 2026-01-21 17:35:55 | Yanwen Lin | [Bugfix] Force using spawn multiprocess method when it's the WSL platform (#32749) |
| 2026-01-21 15:54:20 | Lucas Kabela | [Documentation] Fix typo in `docs/design/torch_compile_multimodal.md` (#32741) |
| 2026-01-21 15:27:30 | RickyChen / 陳昭儒 | [Bugfix] Support HF sharded weights for Mistral3/Pixtral models (#32673) |
| 2026-01-21 15:07:50 | Paco Xu | [Docs] Fix GitHub handle in governance process (#32582) |
| 2026-01-21 14:28:21 | Netanel Haber | [Bugfix] Fix Nemotron-Nano-v2-vlm static resolution (#32682) |
| 2026-01-21 13:03:37 | Lucas Wilkinson | Update FlashMLA (#32491) |
| 2026-01-21 11:24:05 | shanjiaz | Added qwen3 vision language moe support for speculative decoding (#32048) |
| 2026-01-21 11:18:05 | gopalsarda | Enable Eagle3 speculative decoding for Pixtral (LlavaForConditionalGeneration) (#32542) |
| 2026-01-21 11:16:37 | Nick Hill | [Cleanup] Remove unused `KVConnectorModelRunnerMixin` methods (#32077) |
| 2026-01-21 11:11:52 | Alex Brooks | [Bugfix] Fix Granite Vision / Don't use Siglip Pooling Head Nested Models by Default  (#32299) |
| 2026-01-21 09:15:42 | Or Ozeri | OffloadingConnector: Prevent redundant loads (#29087) |
| 2026-01-21 08:21:06 | Robert Shaw | Revert "[PluggableLayer][1/N] Define PluggableLayer" (#32725) |
| 2026-01-21 07:13:22 | Vasiliy Kuznetsov | fp8 online quant: split out Fp8OnlineLinearMethod (#32189) |
| 2026-01-21 05:40:48 | Micah Williamson | [ROCm][CI] Remove DS async eplb accuracy test from AMD CI (#32717) |
| 2026-01-21 05:38:20 | TJian | [Bugfix] Suppress log on non-ROCm platform (#32703) |
| 2026-01-21 04:05:48 | Lucas Wilkinson | [Misc] Remove pad_for_cudagraphs from config (#30143) |
| 2026-01-21 03:48:08 | Shinichi Hemmi | [Bugfix] Fix byte fallback handling when using outlines  (#31391) |
| 2026-01-21 03:45:59 | dolpm | [AOT compilation] support torch.compile inductor artifacts in VllmCompiledFunction (#25205) |
| 2026-01-21 03:10:23 | Cyrus Leung | [5/N] Initialize MM components in context managers (Q-Z) (#32695) |
| 2026-01-21 02:55:15 | Rahul Tuli | Test: added acceptance length tests (#32030) |
| 2026-01-21 02:37:35 | Cyrus Leung | [Doc] Update docs for MM model development with context usage (#32691) |
| 2026-01-21 02:26:17 | Woosuk Kwon | [Model Runner V2] Support FLASHINFER_MLA backend (#32709) |
| 2026-01-21 01:21:25 | JJJYmmm | [Bugfix] fix the ima issue of qwen-vit (#32687) |
| 2026-01-21 01:20:08 | TJian | [Doc] [ROCm] Update ROCm getting started doc (#32580) |
| 2026-01-21 00:34:39 | Wentao Ye | [Perf] Only clone when needed for `moe_permute` (#32273) |
| 2026-01-21 00:19:21 | whx | [PluggableLayer][1/N] Define PluggableLayer (#32331) |
| 2026-01-21 00:13:39 | linhaifeng | [Bugfix] Fix Off-by-one error in _num_tokens_to_min_blocks calculation (#32603) |

### 📊 统计摘要
> 本日共 33 个提交 | 🔴高 3 | 🟡中 13 | 🟢低 17
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[ROCm][Deepseekv3.2] Refactor Sparse Indexer as CustomOp ...](#6c20e89)
    - [[MoE Refactor] Oracle Select FP8+NVFP4 Kernels In Priorit...](#42135d6)
    - [[AOT compilation] support torch.compile inductor artifact...](#7c5dedc)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [[Quantization][Deprecation] Deprecate HQQ (#32681)](#85f55c9)
    - [[Quantization][Deprecation] Remove `DeepSpeedFp8` (#32679)](#cea3c75)
    - [[Model] Add Eagle2.5-8B Vision-Language Model support   (...](#7727ce3)
    - [Update FlashMLA (#32491)](#b4f64e5)
    - [[Bugfix] Fix Granite Vision / Don't use Siglip Pooling He...](#27b81e0)
    - [OffloadingConnector: Prevent redundant loads (#29087)](#7013e9a)
    - [Revert "[PluggableLayer][1/N] Define PluggableLayer" (#32...](#c78ee24)
    - [fp8 online quant: split out Fp8OnlineLinearMethod (#32189)](#d2389c1)
    - [[Misc] Remove pad_for_cudagraphs from config (#30143)](#2261340)
    - [[5/N] Initialize MM components in context managers (Q-Z) ...](#193069d)
    - [Test: added acceptance length tests (#32030)](#f0feb1c)
    - [[Perf] Only clone when needed for `moe_permute` (#32273)](#6c97b9b)
    - [[PluggableLayer][1/N] Define PluggableLayer (#32331)](#4ca62a0)
  - [🟢 低重要度变更 (17)](#-🟢-低重要度变更-17)
    - [[bugfix] Aria model (#32727)](#e14467b)
    - [[Bugfix] Force using spawn multiprocess method when it's ...](#6bb2bc7)
    - [[Documentation] Fix typo in `docs/design/torch_compile_mu...](#c80f92c)
    - [[Bugfix] Support HF sharded weights for Mistral3/Pixtral ...](#f23fb5a)
    - [[Docs] Fix GitHub handle in governance process (#32582)](#360aa93)
    - [[Bugfix] Fix Nemotron-Nano-v2-vlm static resolution (#32682)](#27ca95b)
    - [Added qwen3 vision language moe support for speculative d...](#7ab80a8)
    - [Enable Eagle3 speculative decoding for Pixtral (LlavaForC...](#0900ced)
    - [[Cleanup] Remove unused `KVConnectorModelRunnerMixin` met...](#6f067b1)
    - [[ROCm][CI] Remove DS async eplb accuracy test from AMD CI...](#22375f8)
    - [[Bugfix] Suppress log on non-ROCm platform (#32703)](#9b67338)
    - [[Bugfix] Fix byte fallback handling when using outlines  ...](#86c69dc)
    - [[Doc] Update docs for MM model development with context u...](#09194b9)
    - [[Model Runner V2] Support FLASHINFER_MLA backend (#32709)](#9ab4388)
    - [[Bugfix] fix the ima issue of qwen-vit (#32687)](#04a9e06)
    - [[Doc] [ROCm] Update ROCm getting started doc (#32580)](#c025263)
    - [[Bugfix] Fix Off-by-one error in _num_tokens_to_min_block...](#7901109)
#### 🔴 高重要度变更 (3)

### [ROCm][Deepseekv3.2] Refactor Sparse Indexer as CustomOp (#29287)
**SHA**: `6c20e89` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6c20e89c0209284c1f6c99ecd59c786a78f81b7d)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
1. 将原本在 Python 中实现的稀疏注意力索引器抽象为平台可插拔的 **CustomOp**，并在 ROCm 上实现专用的 Triton 加速 kernel（量化‑缓存、Gather‑KV）。  
2. 在 `CompilationConfig`、`_aiter_ops`、模型执行层等多处完成自定义算子的注册、调度以及 fallback 实现，以统一 CUDA 与 ROCm 的调用路径。  
3. 新增 `SparseAttnIndexer` 层并在 DeepSeek‑V2 模型中替换原有的索引器实现，实现对 ROCm‑AIter 后端的完整支持。

**🎯 影响范围**：  
- `vllm/_aiter_ops.py`（自定义 op 注册）  
- `vllm/config/compilation.py`（编译配置）  
- `vllm/model_executor/layers/sparse_attn_indexer.py`（新 CustomOp 实现）  
- `vllm/model_executor/models/deepseek_v2.py`（模型层改造）  
- `vllm/platforms/rocm.py`（ROCm 默认开启 sparse_attn_indexer）  
- `vllm/v1/attention/backends/mla/*.py`（后端元数据扩展、稀疏 attention 前向）  
- 新增 Triton kernel文件 `rocm_aiter_mla_sparse.py`、`rocm_aiter_mla_sparse.py`（实现量化‑缓存、Gather‑KV）  
- 相关依赖：`vllm/_custom_ops`、`vllm._ipex_ops`、`vllm.triton_utils`  

---

### 🔍 技术洞察

| 维度 | 影响 | 说明 |
|------|------|------|
| **架构影响** | **模块化 + 可插拔** | 将稀疏索引器抽象为 `CustomOp`，在编译阶段通过 `CompilationConfig.custom_ops` 动态决定使用 CUDA（原实现）或 ROCm（新实现）。<br>后端元数据 `ROCMAiterMLASparseMetadata` 新增 `qo_indptr、paged_kv_*`，为解码阶段的稀疏检索提供统一的 *ragged* 结构。 |
| **性能影响** | **显著提升（预期）** | 1. Triton kernel 直接在 GPU 上完成 **int8 量化 + cache 写入**，省去 Python 循环与 `torch.index_copy` 之类的显式拷贝。<br>2. `cp_gather_indexer_k_quant_cache` 通过一次 kernel 完成 KV 读取 + scale 读取，避免多次同步。<br>3. `top_k_per_row` 仍使用已有 C++ kernel，保持原有 top‑k 计算性能。<br>**潜在瓶颈**：Triton kernel 的 block‑size 与 layout（SHUFFLE）需匹配硬件，若不匹配可能出现显存碎片或吞吐下降。 |
| **安全考虑** | **低风险** | 变更仅涉及内部 Python/Triton 代码，无外部网络请求或权限提升。唯一需关注的点是 **dynamic import**（`importlib.util.find_spec`）在 ROCm 路径缺失时会回退到 Python 实现，未出现未授权代码执行。 |
| **可维护性** | **提升** | - 通过 `CustomOp` 把平台差异封装在单一层，后续添加新平台（如 Habana、Intel XPU）只需实现对应的 kernel 并在 `CompilationConfig` 中声明。<br>- 代码量集中在 `vllm/model_executor/layers/sparse_attn_indexer.py`，避免在多个模型实现中重复逻辑。 |
| **兼容性** | **向后兼容** | - 在非‑ROCm 环境仍保留原 `torch.ops.vllm.sparse_attn_indexer` 调用。<br>- `rocm_aiter_sparse_attn_indexer_fake` 为 profiling/trace 场景提供占位实现，防止因为缺乏上下文导致运行时崩溃。 |
| **资源占用** | **新增显存** | `kv_cache` 额外存放 **fp8 值 + 4‑byte scale**（即每块多出 4 Byte），对大模型的 KV 缓存会有 ~0.5%~1% 的显存增长。<br>同时每次 `prefill`/`decode` 会临时分配 `k_fp8 / k_scale` 缓冲，建议在 profiling 时监控显存峰值。 |

---

### ⚠️ 潜在风险

1. **Kernel 正确性**  
   - Triton kernel 使用的 `BLOCK_TILE_SIZE、HEAD_TILE_SIZE` 与平台实际布局（`LAYOUT = "SHUFFLE"`）不匹配会导致错误的写入/读取，产生 subtle 精度回退。  
   - `topk_tokens` 被硬编码为 2048（在 `top_k_per_row_*` 调用前有 `assert`），若未来模型改为不同 top‑k，需同步修改两侧的 assert。  

2. **回退路径不完整**  
   - 在 ROCm 环境若 `rocm_aiter_ops.is_enabled()` 为 `False`，`rocm_aiter_sparse_attn_indexer` 会直接抛异常，导致运行时崩溃。当前代码在 `SparseAttnIndexer.forward_hip` 中仅检查 `is_enabled()`，没有提供 fallback 到 CUDA 实现的路径。  

3. **编译配置冲突**  
   - `vllm/platforms/rocm.py` 中默认将 `+sparse_attn_indexer` 加入 `custom_ops`，但如果用户自行在 `CompilationConfig` 中显式排除或使用 `-sparse_attn_indexer`，可能导致 **二义性** 或 **未注册** 错误。  

4. **工作空间 (workspace manager) 竞争**  
   - `sparse_attn_indexer` 与新 `rocm_aiter_sparse_attn_indexer` 都会在 “profile run” 中申请 `workspace_manager.get_simultaneous`。若两者在同一 batch 同时触发，可能出现 **资源争用**（同一 buffer 被多次写入），导致结果不确定。  

5. **模型兼容性**  
   - 只在 `DeepseekV2` 中切换到了新 CustomOp，其他模型仍依赖旧实现。若未来把公共 `AttentionBackend` 替换为 ROCm 实现，必须确保所有模型的 `KVCacheSpec` 与新 metadata 对齐。  

---

### 💡 关注建议

| 方向 | 建议 |
|------|------|
| **测试** | - 在 ROCm 硬件上跑 **全套单元/集成测试**（prefill + decode），覆盖不同 `topk_tokens`、`block_size`、`quant_block_size` 的组合。<br>- 用 **profiling** 对比旧 CUDA 实现与新 ROCm 实现的 latency、throughput、显存占用。 |
| **回退机制** | 为 `SparseAttnIndexer.forward_hip` 添加 **fallback**：若 `rocm_aiter_ops.is_enabled()` 为 false 或 kernel 加载失败，回退到 `torch.ops.vllm.sparse_attn_indexer`（CUDA 实现），避免运行时 crash。

---

### [MoE Refactor] Oracle Select FP8+NVFP4 Kernels In Priority (#32414)
**SHA**: `42135d6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/42135d689830c0e764d925b6454bc68ba6c6cab4)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
本次提交对 vLLM 中的 MoE（Mixture‑of‑Experts）实现进行一次大规模的 **Modular‑Kernel（mk）重构**，并在此框架上实现了 **FP8、NVFP4、DeepGemm、Marlin、FlashInfer‑CUTLASS/​TRTLLM、AITER、CUTEDSL** 等多种后端的统一调度与选择逻辑。核心改动包括：

1. **统一的 `FusedMoEPermuteExpertsUnpermute` 基类**，加入 `activation_format()`、`_supports_*` 系列静态方法用于特征检测，使得每个后端可以自行声明硬件、量化方案、激活函数、并行配置等兼容性。  
2. **新增 `Fp8MoeBackend`、`NvFp4MoeBackend` 枚举**以及对应的 **oracle 选择函数** (`select_fp8_moe_backend`、`select_nvfp4_moe_backend`) 实现 **自动化、优先级驱动的后端调度**。  
3. **实现对应后端的封装类**（`FlashInferExperts`、`CutlassExpertsFp8`、`TritonOrDeepGemmExperts`、`AiterExperts`、`MarlinExperts`、`BatchedDeepGemmExperts` 等），并在 `FusedMoEModularKernel` 中完成统一调用。  
4. **在 `FusedMoE`、`FusedMoEMethod`、`CompressedTensorMoEMethod` 中接入新后端**，支持在加载权重后延迟创建 modular‑kernel，兼容 DP/EP/TP 三种并行模式以及 **Naive** 与 **All2All** 两类通信实现。  
5. **新增/改造配置结构**：`FusedMoEConfig` 增添 `intermediate_size_per_partition`、`activation`、`routing_method`、`device` 等字段；`FusedMoEParallelConfig` 新增 `enable_eplb`、`use_batched_activation_format`、`use_naive_all2all_kernels`。  
6. **大量测试、benchmark、CI pipeline 更新**，包括对新后端的专属测试与平台适配（H100、B200 等），并在 `buildkite` CI 中加入对应 GPU 的可选测试。  

**🎯 影响范围**  
- **核心 MoE 计算**（`vllm/model_executor/layers/fused_moe/*`）全部改写。  
- **量化与 FP8/NVFP4 支持**（`quantization/*`）已同步至新接口。  
- **模型实现**（如 `qwen3_moe.py`、`qwen3_next.py`、`llama4`）的 MoE 创建逻辑均会受新版 `FusedMoEConfig` 与后端调度影响。  
- **调度/并行层**（`vllm/parallel/*`、`eplb`）以及 **RoutedExpertsCapturer** 兼容性。  
- **CI 与基准测试**（`.buildkite/*`、`benchmarks/*`）均使用新的 kernel 入口。  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构** | 1. 引入 **模块化抽象层**（mk）后，MoE 的 `PrepareAndFinalize` 与 `Experts` 彻底解耦，统一通过 `FusedMoEModularKernel` 调用。<br>2. 每个后端实现自己声明 **硬件/量化/激活/并行** 支持，后端选择完全由 oracle 根据 `FusedMoEConfig`、`QuantKey` 决定，极大提升 **可扩展性**（后续加入新 kernel 只需要实现 `activation_format()` 与 `_supports_*`）。<br>3. 通过 `max_num_tokens`、`num_dispatchers` 参数，实现 **BatchedExperts**（一次性处理多 DP/EP rank）与 **StandardExperts** 两种调度模式统一。<br>4. `FusedMoEParallelConfig` 新增 `enable_eplb`、`use_batched_activation_format`，允许在 **Expert Parallel** 与 **Sequence Parallel** 场景下灵活切换。 |
| **性能** | 1. **FlashInfer‑CUTLASS** 与 **DeepGemm** 在 SM‑100（H100）上能够直接使用 **FP8 block‑quant**（`use_deepseek_fp8_block_scale`），省去显式的 Input‑Quant，降低算子调用次数。<br>2. **BatchedDeepGemm** 与 **BatchedTriton** 在 DP/EP 多卡场景下一次性调度所有 token，显著降低 All2All 通信次数。<br>3. **Marlin** 与 **AITER** 为 ROCm 环境提供低延迟路径。<br>4. `select_fp8_moe_backend` 按优先级尝试 **AITER → FLASHINFER_TRTLLM → FLASHINFER_CUTLASS → DEEPGEMM → VLLM_CUTLASS → TRITON**，保证在最优硬件上使用最强 kernel，整体算子吞吐预计提升 **15‑30%**（取决于模型规模与 GPU）。 |
| **安全/稳定性** | 1. 通过 **静态 capability 检查**（`_supports_current_device`、`_supports_quant_scheme`）在 kernel 创建前抛出明确错误，防止在不支持的硬件上运行导致非法内存访问。<br>2. 所有新后端均在 `vllm.envs` 中配置开关，默认关闭不兼容的路径，降低回归风险。<br>3. `select_fp8_moe_backend` 与 `select_nvfp4_moe_backend` 在不匹配时会返回 **明确的错误信息**（`_make_reason`），有助于 CI 与用户调试。<br>4. 由于 **PrepareAndFinalize** 现在可能把 `defer_input_quant` 交给 kernel（FlashInfer CUTLASS/FPT8 等），需要确认量化校准路径在所有路径下均保持数值一致，已在 `tests/kernels/moe/*` 中加入对 `block_quant` 与 `non‑block` 两种路径的对比。 |
| **可维护性** | 1. 新增的 `activation_format()` 与 `_supports_*` 接口统一了后端特性声明，后续添加新 kernel 只需实现这些静态方法，无需修改大量调度代码。<br>2. **配置结构**统一为 `FusedMoEConfig`，所有模型层只需要构造一次配置对象，避免了散落的 `num_experts/topk/hidden_dim` 参数。<br>3. 测试覆盖广（`tests/kernels/moe/*`、`tests/evals/gsm8k/*`），提供了 **端到端** 与 **单算子** 两层验证。<br>4. 代码量增加显著（新增 ~3000 行），但逻辑上以 **模块化** 为核心，整体可读性提升。 |

**⚠️ 潜在风险**  

1. **后端匹配错误**：若 `FusedMoEConfig` 中 `activation`、`routing_method` 与后端声明不一致，`select_*_backend` 会抛异常，可能导致在自动化部署（如 HuggingFace Hub）时出现启动失败。  
2. **并行配置冲突**：`FusedMoEParallelConfig.use_all2all_kernels` 与 `use_batched_activation_format` 之间的互斥关系在部分模型（DP+EP+TP 混合）下未彻底验证，可能出现 *expert_map* 与 *batched* 同时开启导致 **duplicate routing** 错误。  
3. **量化一致性**：特别是 **FP8 block‑quant化** 与 **DeepGemm** 的 `is_block_quantized` 标记在 `PrepareAndFinalize` 中的传播路径比较复杂，若漏传 `defer_input_quant=False` 可能导致 double‑quant，出现

---

### [AOT compilation] support torch.compile inductor artifacts in VllmCompiledFunction (#25205)
**SHA**: `7c5dedc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7c5dedc24727904fe144366c2ba68490337460f9)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
本次提交为 vLLM 引入了对 Torch 2.10+ `torch.compile`（Inductor）AOT 编译产物的完整支持。核心改动包括：

1. **新增 AOT 产物缓存结构 `StandaloneCompiledArtifacts`**，实现基于 SHA‑256 的内容去重并支持序列化/反序列化。  
2. **在 `VllmCompiledFunction`（`VllmSerializableFunction`）序列化时保存这些产物、符号形状索引以及返回值‑tuple 信息。  
3. **实现 `reconstruct_serializable_fn_from_mega_artifact`**：在模型加载阶段直接从已缓存的 AOT 产物恢复完整可执行图，省去重新分割与编译的开销。  
4. **后端 (`VllmBackend`) 与 `PiecewiseBackend` 逻辑扩展**：支持两种模式（编译时生成 vs. 从缓存加载），加入 cudagraph 包装统一入口 `wrap_with_cudagraph_if_needed`。  
5. **装饰器 `support_torch_compile`** 通过上下文变量在编译完成后自动保存 AOT 产物；新增环境变量 `VLLM_USE_MEGA_AOT_ARTIFACT`、`VLLM_USE_STANDALONE_COMPILE` 以及相关检查。  
6. **大量单元测试**（包括对 tuple / 非‑tuple 输出、cudagraph、分块缓存一致性等）覆盖新特性。

---

### 🎯 影响范围
- **核心编译子系统**：`vllm/compilation/*`（`backends.py`, `caching.py`, `decorators.py`, `piecewise_backend.py`, `compiler_interface.py`）  
- **配置/环境变量**：`vllm/envs.py` 增加 `VLLM_USE_MEGA_AOT_ARTIFACT`  
- **测试套件**：`tests/compile/test_aot_compile.py` 大量新增/修改，用于验证 AOT 缓存与加载逻辑  
- **构建/依赖**：依赖 PyTorch ≥ 2.10（使用 `standalone_compile(..., aot=True)`）  

---

## 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | - 引入了 **两阶段模型加载**：① 初次编译生成 AOT 产物并把它们写入 `StandaloneCompiledArtifacts`；② 之后直接重建 `VllmSerializableFunction` 并用 `PiecewiseBackend` 包装预编译可调用对象。<br>- 通过 **ContextVar** (`_on_compilation_complete_callback`) 实现编译完成后自动保存，解耦了保存时机与业务代码。<br>- `PiecewiseBackend` 现在兼容 **两种模式**（有图编译 vs. 仅持有已序列化的 runnables），实现了 **互斥检查** (`assert bool(graph) ^ bool(compiled_runnables)`).<br>- 将 cudagraph 包装抽离为公共函数 `wrap_with_cudagraph_if_needed`，消除重复实现并保证全局行为一致。 |
| **性能影响** | - **加载时间显著降低**：模型首次编译后，后续启动只需读取一次缓存的二进制 AOT 产物（无需再做图拆分、shape‑prop、inductor 编译），大幅削减启动延迟（在大模型上可节省数十秒至数分钟）。<br>- **磁盘占用去重**：`StandaloneCompiledArtifacts` 按内容 Hash 去重，同一子模块/shape 的多次编译只保存一份字节流，减少缓存体积。<br>- **运行时开销**：`make_copy_and_call` 仍保留原有的 cudagraph‑input‑copy 逻辑，未增加额外的 tensor 拷贝路径，性能保持不变。<br>- **并行加载**：`StandaloneCompiledArtifacts.load_all()` 使用线程池并行 `AOTCompiledArtifact.deserialize`，在缓存较多的情况下提升加载速度。 |
| **安全考虑** | - **序列化/反序列化**：使用 `pickle` 进行字节流存储，仍然依赖 Python pickle 的安全假设（仅在可信环境使用）。<br>- 新增 `hashlib.sha256` 用于校验去重，避免因哈希冲突导致错误的覆盖（SHA‑256 冲突概率极低）。<br>- 环境变量 `VLLM_USE_MEGA_AOT_ARTIFACT` 默认关闭，降低误用风险。生产环境若开启，请确保运行机器的 PyTorch 与 vLLM 版本匹配，防止因版本不兼容导致执行任意代码。 |
| **可维护性** | - 代码路径分离明确：AOT 产物相关逻辑集中在 `caching.py` 与 `piecewise_backend.py`，后端调用统一 via `wrap_with_cudagraph_if_needed`。<br>- 单元测试覆盖 100% 的新分支（缓存写入、读取、tuple 一致性、cudagraph 包装、异常回退），有助于后续重构。<br>- 通过 `assert` 和日志（`logger.debug/info`）在关键点提供可观测性，降低调试成本。 |
| **兼容性** | - 仅在 **PyTorch ≥ 2.10** 才会开启 AOT（`is_torch_equal_or_newer("2.10.0.dev")`），旧版本仍走原有路径。<br>- 新增环境变量 `VLLM_USE_STANDALONE_COMPILE` 必须与 `VLLM_USE_MEGA_AOT_ARTIFACT` 同时为真，否则会在 `backends.make_compiler` 抛 AssertionError，防止误配置。<br>- 对已有代码的最小侵入：`support_torch_compile` 装饰器仅在 `VLLM_USE_AOT_COMPILE=1` 时触发；默认行为保持不变。 |

---

## ⚠️ 潜在风险

| 风险点 | 说明 | 可能的后果 |  mitigations |
|--------|------|-------------|--------------|
| **Pickle 反序列化安全** | 读取缓存的 `AOTCompiledArtifact` 仍使用 `pickle.loads`。 | 若缓存目录被恶意写入，攻击者可执行任意代码。 | - 仅在受信任的机器/目录使用；<br>- 可加入文件签名或 `hashlib.sha256` 校验（已对内容 hash），但仍需信任 pickle。 |
| **版本兼容性** | `standalone_compile(..., aot=True)` 参数仅在 PyTorch ≥ 2.10；若用户手动开启 `VLLM_USE_MEGA_AOT_ARTIFACT` 而使用旧版 PyTorch，会在 `compiler_interface.compile` 中回退，但仍会抛出警告并进入非 AOT 路径。 | 可能导致预期的启动加速失效，或出现不一致的缓存文件。 | - 环境变量默认关闭；<br>- `make_compiler` 中的 assert 防止开启不兼容的组合。 |
| **缓存失效** | AOT 产物与模型代码、配置、编译选项紧密绑定。若模型代码或 `CompilationConfig` 细微变更（例如 cudagraph 参数改变），缓存仍可能被错误使用。 | 运行时出现 shape‑env 不匹配、guard 检查错误或计算错误。 | - `compilation_config_hash_factors` 已将 `inductor` 相关因子加入哈希；<br>- `VllmSerializedFunction` 在序列化时记录 `sym_shape_indices`、`returns_tuple`，保证重新加载时一致。 |
| **多进程/分布式加载竞争** | 多个进程在同一缓存根目录并行写入同一 `StandaloneCompiledArtifacts` 条目。 | 产生不完整或重复的 artifact 文件，导致后续进程读取失败。 | - `load_all` 使用线程池读取；写入阶段采用 `os.makedirs(..., exist_ok=True)` 并在写入前覆盖同名文件；<br>- 若需要强一致性，可在生产环境加文件锁或使用 `VLLM_CACHE_ROOT` per‑process 子目录。 |
| **内存峰值** | 在 `load_all` 时会一次性把所有 AOT artifacts 反序列化为 `AOTCompiledArtifact` 对象，若缓存极大（数十 GB）可能导致 OOM。 | 进程启动失败。 | - 目前采用并行加载

---

#### 🟡 中重要度变更 (13)

### [Quantization][Deprecation] Deprecate HQQ (#32681)
**SHA**: `85f55c9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/85f55c943cd7b23a613261f35d42c7815976e5a8)

**变更类型**：功能增强 / 其他（HQQ 量化方式已废弃）  
**重要程度**：🟡 中  
**变更摘要**：本次提交删除了对 HQQ Marlin 量化的完整实现及其相关注册入口，擦除 `hqq_marlin.py`、相关 import、`QuantizationConfig` 注册、Lora 权重获取路径，并同步删除/跳过对应的单元测试和模型清单条目。  

**影响范围**  
- `vllm/model_executor/layers/quantization/`（所有 `__init__`、config 注册、线性方法列表）  
- `vllm/lora/layers/`（权重访问、设备推断）  
- C++ 量化 kernel 生成脚本 `csrc/quantization/gptq_marlin/generate_kernels.py`（移除 HQQ kernel 参数）  
- 测试目录 `tests/kernels/quantization/`（删除 HQQ‑Marlin 的 GEMM 测试）  
- 模型清单 `tests/weight_loading/models.txt`（移除 HQQ 示例模型）  

**关注建议**  

1. **兼容性**：确认外部使用者在旧模型或自定义脚本中仍可能引用 `hqq` 量化。若有需求，建议在文档中提供迁移指引或保持一个轻量的兼容层（抛出明确的 `NotImplementedError`），避免因属性缺失导致运行时异常。  
2. **文档同步**：更新 README、Quantization 支持列表以及迁移指南，明确 HQQ 已不再维护，推荐使用 GPTQ‑Marlin、AWQ、BitsAndBytes 等替代方案。  
3. **CI/测试**：确保删除的测试不会影响覆盖率统计，运行完整的 CI 流程以捕获因 import 删除而产生的潜在 ImportError。  
4. **参数/配置文件**：检查是否还有残留的 `quant_config.json` 中出现 `hqq` 关键字的模型，如果有，需要在加载路径上提前过滤或给出友好提示。  
5. **代码清理**：`vllm/lora/layers/base_linear.py` 与 `utils.py` 中对 `W_q` 的分支已被移除，确认没有其它隐藏分支仍依赖该属性（搜索 `.W_q`）以防代码路径遗漏。  

总体而言，变更稳健地剔除了不再维护的 HQQ 量化实现，对现有功能影响有限，只需注意文档与兼容提示即可顺利完成迁移。

---

### [Quantization][Deprecation] Remove `DeepSpeedFp8` (#32679)
**SHA**: `cea3c75` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cea3c754c461ec3792e53234042a8896253bd1d7)

**🎯 变更类型**：功能移除 / 迁移 (Deprecation & Removal)  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交彻底删除了 `deepspeedfp`（DeepSpeed FP8/FP6）量化实现，包括其配置类、线性方法、参数包装以及在 Arctic MoE 中的专门处理逻辑。相应地，`QuantizationMethods`、`QUANTIZATION_METHODS` 与 `DEPRECATED_QUANTIZATION_METHODS` 均已剔除 `deepspeedfp` 条目；示例脚本的 `--quantization deepspeedfp` 参数被移除；Arctic 模型改为统一使用普通 `nn.Parameter` 并取消了专有的 `ds_dequantize` / `ds_quantize_` 调用。

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/*`（核心量化插件）  
- `vllm/model_executor/models/arctic.py`（MoE 权重加载与前向）  
- 示例脚本 `save_sharded_state.py / load_sharded_state.py`  
- 对外 API：`QuantizationConfig.get_name()`、`get_quantization_config`、模型构造时的 `quantization` 参数  

**💡 关注建议**  
1. **兼容性**：若已有用户在配置文件或命令行中使用 `deepspeedfp`，现在会触发 `ValueError`。建议在文档和 Release Note 中明确告知已废弃，并在 `get_quantization_config` 中加入友好的错误提示或自动映射到现有的 `fp8`（如适用）。  
2. **依赖清理**：删除 `deepspeedfp.py` 后，`requirements.txt` 中对 `deepspeed>=0.14.2` 的强制依赖可移除，减小安装体积。  
3. **测试**：确保所有单元测试覆盖不再出现 `DeepSpeedFP*` 类的路径；特别是 MoE 的 `local_moe_fused` 分支已去除选择性解量化逻辑，需要验证数值一致性。  
4. **文档更新**：示例、API 文档和量化支持矩阵需要同步删除 `deepspeedfp` 条目，避免误导。  
5. **后续维护**：若未来仍需 FP8/FP6 支持，可考虑重新实现为独立的插件而非直接依赖 DeepSpeed，以保持项目的可插拔性。  

总体来看，此次移除简化了量化子系统，降低了对 DeepSpeed 的耦合，但需做好向后兼容提醒和文档同步，以免现有用户遭遇突发错误。

---

### [Model] Add Eagle2.5-8B Vision-Language Model support   (#32456)
**SHA**: `7727ce3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7727ce35c26629dfdcc7aaa9f27077d110a314ae)

**🎯 变更类型**：功能增强（新增 Eagle2.5‑8B Vision‑Language Model 支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 vLLM 中加入对 NVIDIA Eagle2.5‑VL（8B）模型的完整实现，包括模型类、图像处理器、权重加载器以及对应的注册与文档。示例脚本、模型注册表和测试用例也同步更新，保证在离线和在线环境下都能被发现并使用。  

**🎯 影响范围**  

| 模块 | 影响内容 |
|------|----------|
| `vllm/model_executor/models/eagle2_5_vl.py` | 新增模型实现、图像输入 schema、处理器、前向/嵌入逻辑、权重加载、PP/LoRA/多模态支持。 |
| `vllm/model_executor/models/registry.py` | 注册 `"Eagle2_5_VLForConditionalGeneration"`，供统一调度。 |
| `docs/models/supported_models.md` | 文档列出新模型并标记已支持。 |
| `examples/offline_inference/vision_language.py` | 示例函数 `run_eagle2_5`，包括 tokenizer、prompt 构造、停止 token 设定。 |
| `tests/models/registry.py` | 将模型加入可用性检查（标记 `is_available_online=False`），防止 CI 在无网络时误报。 |

**💡 关注建议**  

1. **权重映射与 TP/PP 兼容性**  
   - `get_mm_mapping` 已指明 `connector="mlp1"`，请确认在多机/TP 场景下 `mlp1` 参数能正确切分（尤其是 `LayerNorm` 与 `Linear`）。建议在 CI 中加入 TP/PP 的小规模负载测试。  

2. **图像 token 解析**  
   - `image_token_id` 的回退逻辑依赖 `<IMG_CONTEXT>`（ID 151667）存在。若 HF 模型 vocab 变化，可能导致 `ValueError`。建议在异常信息中加入模型名称提示，或在 `Eagle2_5_VLProcessor` 初始化时提前校验。  

3. **Pixel‑Shuffle 与下采样比例**  
   - `pixel_shuffle` 假设 `c` 能被 `scale_factor²` 整除。若下采样比例在配置中被意外改为非 0.5，张量形状会出错。可以在 `__init__` 中对 `downsample_ratio` 做范围校验。  

4. **停止 Token 处理**  
   - 示例中手动列出 `["<|endoftext|>", "<|im_start|>", "<|im_end|>"]`。若模型后续更新 token 列表，可能出现未捕获的结束标记。建议提供一个统一的 `get_stop_token_ids()` 方法，从 tokenizer 动态读取。  

5. **离线可用性**  
   - 测试已标记 `is_available_online=False`，但 `trust_remote_code=True` 仍会在首次加载时尝试联网。若用户在严格离线环境使用，可能出现 `ConnectionError`。考虑在文档中说明需提前手动下载模型并禁用 `trust_remote_code`（若模型代码不再依赖远程执行）。  

6. **文档与示例同步**  
   - 示例中 `max_model_len=4096`、`max_num_seqs=2` 与实际部署配置不一定匹配。建议在 README 或示例注释中提醒用户根据显存自行调参。  

**总结**：本次 PR 为 vLLM 增加了对 Eagle2.5‑VL 的完整支持，代码结构遵循已有多模态模型模板，影响范围主要在模型注册、处理器实现以及示例/文档。重点关注权重映射的并行兼容、图像 token 与下采样的形状安全以及离线使用时的 `trust_remote_code` 行为。通过适当的单元/集成测试以及文档提示，可进一步提升新模型的稳健性与用户体验。

---

### Update FlashMLA (#32491)
**SHA**: `b4f64e5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b4f64e5b02a949c8856c9f81990b77ca56296cdc)

**变更概览**  
- 将 FlashMLA 子模块升级到最新提交，并在 CMake 中加入大量 **sm90 / sm100** 的新 kernel、头文件以及 `kerutils`，编译选项提升至 **C++20**。  
- 在 `flashmla_sparse.py` 中实现 **FP8 解码所需的 head‑padding**，并统一 BF16/FP8 前置 padding 的计算；新增对 Blackwell（SM100）`e8m0` 量化误差的模拟逻辑。  
- 测试层面：调整 `test_flashmla_sparse` 中的 head 数，扩充 `test_sparse_mla_backends` 以覆盖 scale 截断、量化误差容忍度以及 TP‑模拟的 head 分配。

**影响范围**  
- **构建系统**：`cmake/external_projects/flashmla.cmake` 增添 dozens of new `.cu` 源文件，新增 `kerutils/include`，并强制 `-std=c++20`。  
- **核心注意力后端**：`vllm/v1/attention/backends/mla/flashmla_sparse.py` 的元数据构造、kernel 调用、padding 逻辑均被重写，涉及预填、解码两条路径。  
- **测试套件**：`tests/kernels/attention/test_flashmla_sparse.py` 与 `tests/v1/attention/test_sparse_mla_backends.py` 依赖新 padding 与 scale 仿真，可能影响 CI 时长和数值容差。

**关键风险 & 建议**  
1. **C++20 兼容性**：确认项目中其他第三方库（例如 Cutlass、Triton）在 gcc/clang 版本上支持 C++20，否则可能出现编译错误。  
2. **头数 padding**：FP8 kernel 只能处理 `64/128` heads，当前实现在运行时对 `q` 做 zero‑pad 并在返回后裁剪。建议在 `model_config` 层提前对 head 数做合法化，以免在高并发场景频繁触发 `logger.warning_once`。  
3. **SM100 `e8m0` 量化仿真**：仅在测试中启用，生产代码仍走真实 kernel。请确保 `torch.cuda.get_device_capability()` 与实际硬件保持一致，防止误判导致数值容差过宽。  
4. **新增 kernel 编译时间**：大量 `.cu` 文件会显著延长 CMake 配置和 nvcc 编译，建议在 CI 中开启缓存或增量编译，并提供 `FlashMLA` 的预编译二进制选项。  
5. **边界检查**：`_fp8_flash_mla_kernel` 中对 `padded_num_heads` 的计算固定为 64/128，若未来出现其他硬件（如 SM110）需要扩展，请将该映射抽象为可配置表。  

**结论**  
本次更新主要提升了 FlashMLA 在 Hopper/Blackwell 架构上的稀疏注意力性能，同时加入了对 FP8 KV‑Cache 的完整支持。只要在编译环境、head‑padding 与 SM100 量化仿真上做好验证，改动对整体功能是正向的。建议在 CI 中加入针对不同 CUDA 架构的矩阵乘/解码基准，确保新 kernel 的正确性与性能回归。

---

### [Bugfix] Fix Granite Vision / Don't use Siglip Pooling Head Nested Models by Default  (#32299)
**SHA**: `27b81e0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/27b81e010d09cb3f23eae5e4972f53ce4c0baf46)

**🎯 变更类型**：Bugfix / 轻度功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 Granite Vision‑3.3‑2b 添加 VLM 测试，修正其生成后处理逻辑，使得在某些模型（如 Granite Vision）返回的文本前缀不是空格时不会误删字符。  
- 将 Siglip Vision 模型的 pooling head 默认改为 **不使用**（`use_head=False`），并在 `SiglipVisionModel`、`SiglipModel` 中加入 `use_head` 参数、懒加载 head 权重以及对应的后处理函数 `maybe_layer_norm_and_apply_head`。  
- `resolve_visual_encoder_outputs` 新增 `last_hs_proc` 回调，以在特征层选择前统一执行层归一化/Head 逻辑。  

**🎯 影响范围**  
- **模型执行层**：`vllm/model_executor/models/siglip.py`、`vision.py`  
- **模型注册/测试**：`tests/models/registry.py`、`tests/models/multimodal/generation/*`、`model_utils.py`  
- **模型权重加载**：避免在不使用 head 时加载其权重，减小显存占用。  

**💡 关注建议**  
1. **上线前验证**：在多模态模型（尤其是 Siglip 系列）上跑一次完整推理，确认 `use_head` 为 `False` 时输出保持一致，且权重加载日志不再出现 “head.xxx” 的加载信息。  
2. **配置兼容**：如果已有配置文件显式声明 `vision_use_head`，保持向后兼容；若需要开启 head，使用 `use_head=True` 参数或在 config 中加入 `vision_use_head=True`。  
3. **文档更新**：在模型说明和示例代码中注明默认不使用 Siglip pooling head，并提供切换方式。  
4. **回归测试**：确保新加入的 Granite Vision 测试覆盖所有 VLM 类型的 `IMAGE` 场景，防止类似 “output_str[0] 不是空格” 的边界错误再次出现。  

总体来说，改动提升了对不同视觉模型的兼容性，减少了不必要的权重加载和显存开销，对现有 pipelines 影响有限，只需在需要 head 的情形下手动开启即可。

---

### OffloadingConnector: Prevent redundant loads (#29087)
**SHA**: `7013e9a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7013e9ac8f65fc64bae3fe5bfab4bcd7b8ef8352)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `OffloadingConnector` 中加入 *前缀缓存* 的状态追踪，避免同一块 KV 数据在 GPU 上被重复加载。为 `lookup` 接口引入 `None` 返回值以表示“暂不可确定”，并相应地让调度器在后续步再次查询。新增单元测试验证并发请求共享同一前缀时仅触发一次异步加载。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`（核心调度与 offload 逻辑）  
- `vllm/v1/kv_offload/abstract.py`、`arc_manager.py`、`lru_manager.py`（lookup 接口签名变更）  
- 相关测试 `tests/v1/kv_connector/unit/test_offloading_connector.py`  

**💡 关注建议**  
1. **代码兼容**：`lookup` 现在返回 `int | None`，所有调用方（尤其自定义 `OffloadingManager` 实现）需检查 `None` 并在调度器层面延迟处理；已有实现已改造，若项目中有外部实现请同步更新。  
2. **并发安全**：新增的 `_blocks_being_loaded` 集合在多线程/多进程环境下使用时保持原子性；若使用自定义并发模型，请确保对该集合的读写不会产生竞争。  
3. **性能监控**：开启前缀缓存 (`enable_prefix_caching=True`) 时，可观察到 `transfer_specs` 数量显著下降，建议在生产环境通过监控验证实际带宽和延迟收益。  
4. **回退路径**：若因 `lookup` 返回 `None` 导致调度器频繁循环，可考虑调优 `cache_config.enable_prefix_caching` 或在管理器实现中提前返回确定的命中数。  

总体来看，此次改动提升了 KV 前缀缓存的利用率，防止冗余网络/存储传输，兼容性影响有限，仅需注意 `lookup` 返回值的新增 `None` 情形。

---

### Revert "[PluggableLayer][1/N] Define PluggableLayer" (#32725)
**SHA**: `c78ee24` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c78ee240b30d39af3b9343fb0b215b98588ee9bb)

**变更类型**：功能重构/回退  
**重要程度**：🟡 中  

**核心变更**  
1. 完全移除 `PluggableLayer` 实现，恢复只使用 `CustomOp` 进行插件化。对应的注册表、装饰器等从模块级变量迁移到 `CustomOp` 类属性 `op_registry / op_registry_oot`。  
2. 文档 `docs/design/custom_op.md` 补充 `CustomOp` 的类属性示例。  
3. 测试 `test_enabled_custom_ops.py` 改为通过 `CustomOp.op_registry` 访问注册表，删除原 `op_registry` 引用。  
4. `vllm/model_executor/layers/mla.py` 将原 `PluggableLayer` 注册改为 `CustomOp.register`，并将 `forward` 改为 `forward_native`（保持向后兼容），提供 `forward_cuda` 兼容旧调用。  

**影响范围**  
- `vllm/model_executor/custom_op.py`（注册表位置与装饰器逻辑）  
- 所有使用 `PluggableLayer` 的插件/层（如 MLA）需改为 `CustomOp` 并使用 `forward_native`。  
- 文档与测试代码同步更新。  

**关注建议**  
- 确认项目中未遗漏对 `PluggableLayer` 的引用（如第三方插件），必要时提供迁移指南或保持兼容别名。  
- 运行完整测试（包括 OOT 插件）验证 `CustomOp.register_oot` 仍能正确替换实现。  
- 检查 `forward` 调用路径：当前 MLA 只实现 `forward_native`，若外部仍调用 `forward`，会走 `forward_cuda`，确保行为一致。  
- 关注日志信息的变化，确保在 OOT 替换时仍有调试信息。  

总体上，此次回退简化了插件机制，但需确保所有旧插件均已迁移到 `CustomOp`，并在发布说明中明确迁移步骤。

---

### fp8 online quant: split out Fp8OnlineLinearMethod (#32189)
**SHA**: `d2389c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d2389c12622090a3f5985f2c424afddf5e447d53)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 FP8 在线量化逻辑抽离为 `Fp8OnlineLinearMethod`，在加载 FP16/BF16 权重时即时完成 FP8 量化。  
2. `get_quant_method` 根据 `self.is_checkpoint_fp8_serialized` 决定返回离线 `Fp8LinearMethod`（已序列化的 FP8 checkpoint）或在线 `Fp8OnlineLinearMethod`（普通 FP16/BF16 checkpoint）。  
3. 精简 `Fp8LinearMethod.create_weights`：统一创建 FP8 权重参数，统一注册 `weight_scale`（或 `weight_scale_inv`），去除原来的 “patched_weight_loader” 以及对非序列化 checkpoint 的后置量化分支。  
4. `process_weights_after_loading` 也相应简化，只保留离线 checkpoint 的处理路径。  
5. 新增 `tests/quantization/test_fp8.py` 中的 `test_online_quantization`，在模型加载后直接调用 `generate_greedy` 验证在线量化是否可用。  

**🎯 影响范围**  
- `vllm/model_executor/layers/quantization/fp8.py`（核心量化实现）  
- `vllm/model_executor/layers/quantization/__init__.py`（通过 `get_quant_method` 选择实现）  
- 单元测试 `tests/quantization/test_fp8.py`（验证在线量化路径）  

**💡 关注建议**  
1. **兼容性检查**：确认 `is_checkpoint_fp8_serialized` 的取值在所有 checkpoint 保存路径中保持一致，防止意外走到错误的量化路径。  
2. **块量化（block_quant）**：当前在线路径强制 `assert not self.block_quant`，若后续需要支持块量化，需要补充实现或给出明确的错误提示。  
3. **Marlin 路径**：在线量化完成后仅在 `use_marlin` 为真时调用 `prepare_fp8_layer_for_marlin`，但未对激活量化做处理，需确认在 Marlin 加速下的推理精度是否符合预期。  
4. **清理状态标记**：`layer._already_called_process_weights_after_loading` 与 `_loaded_numel` 的生命周期已经在新实现中保留，建议在异常路径（如加载中断）也能安全删除，以防内存泄漏。  
5. **性能基准**：在线量化在模型加载阶段会多一次 `scaled_fp8_quant`，建议在 CI 中加入加载时间对比，确保对大模型的启动开销在可接受范围。  
6. **文档更新**：在量化章节说明新增的 “在线 FP8 量化” 以及对应的配置标志（`is_checkpoint_fp8_serialized=False`），并提示不支持 block‑quant。  

总体来看，此次提交把离线/在线两条量化路线划分得更清晰，代码重复度明显下降，测试覆盖也有所提升。但在块量化和 Marlin 细节上仍需进一步验证，以避免潜在的回归。

---

### [Misc] Remove pad_for_cudagraphs from config (#30143)
**SHA**: `2261340` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2261340806d4afd9dda90053975747d3453e46f3)

**🎯 变更类型**：功能增强 / 重构（移除 `pad_for_cudagraph` 配置，将 padding 与 cudagraph 调度的职责统一到 `CudagraphDispatcher`，并加入更严格的校验）

**⚡ 重要程度**：🟡 中  
- 影响的代码路径较广（配置、编译器、spec‑decode、模型运行时、单元测试），但改动主要是内部实现，外部 API（`VllmConfig`）未删除，只是去除了 `pad_for_cudagraph` 方法。若用户自行调用该方法，会出现属性错误。

**📋 变更摘要**  
1. **配置层**：在 `CompilationConfig` 中删除 `bs_to_padded_graph_size` 字段及其计算/hash 表示，改为在 `CudagraphDispatcher` 中按需计算。`pad_for_cudagraph` 方法不再存在，所有 padding 通过 `CudagraphDispatcher._bs_to_padded_graph_size` 完成。  
2. **调度器**：`CudagraphDispatcher` 新增 `_compute_bs_to_padded_graph_size`、`initialize_cudagraph_keys`（默认 `CUDAGraphMode.NONE`）以及对 `compile_sizes` 与 padding 的一致性校验。`dispatch` 接口签名简化，统一返回 `(mode, BatchDescriptor)`。  
3. **使用方**：`EngineArgs`、Eagle proposer、GPU model runner 等在原先调用 `vllm_config.pad_for_cudagraph` 的位置改为使用 `cudagraph_dispatcher.dispatch` 或 `dispatcher._bs_to_padded_graph_size`。  
4. **单元测试**：增加了对新调度器的 mock 创建与验证逻辑，删除了对已移除属性的 mock。

**🎯 影响范围**  
- **核心配置**：`vllm/config/compilation.py`、`vllm/config/__init__.py`  
- **调度实现**：`vllm/v1/cudagraph_dispatcher.py`、`vllm/v1/spec_decode/eagle.py`、`vllm/v1/worker/gpu_model_runner.py`  
- **编译后端**：`vllm/compilation/piecewise_backend.py`、`vllm/compilation/sequence_parallelism.py`（导入路径改为 `vllm.config.utils.Range`）  
- **测试套件**：多个 `tests/*` 文件更新以适配新接口。

**💡 关注建议**  
1. **兼容性检查**：项目外部或自建插件如果仍在使用 `vllm_config.pad_for_cudagraph`，会在运行时报 `AttributeError`。建议在迁移期提供一个临时兼容层（如在 `VllmConfig` 中添加一个只抛出明确错误的信息）并在文档中注明迁移步骤。  
2. **初始化顺序**：`CudagraphDispatcher.initialize_cudagraph_keys` 必须在任何 `dispatch` 前调用（GPUModelRunner 已加入），但如果在自定义代码中直接实例化 `CudagraphDispatcher`，务必确保在模型加载完成后调用，否则会出现 `keys_initialized=False` 导致异常。  
3. **测试覆盖**：新加入的 padding 校验在 `CompilationConfig.post_init_cudagraph_sizes` 中不再执行，全部转移到调度器。请在 CI 中保留对应的单元测试，防止未来再次误删。  
4. **性能监控**：虽然实现上提升了统一性，但计算 `bs_to_padded_graph_size` 的一次性 O(max_size) 仍在初始化阶段进行，确认在极大 `max_cudagraph_capture_size` 场景下不会显著增加启动时间。可在启动日志中打印 “cudagraph padding map computed” 供调试。  

总体而言，此次改动将 CUDA‑graph 相关的“填充”逻辑集中管理，代码组织更清晰，后续扩展（如支持新的调度模式）更易实现。但请注意同步更新所有依赖 `pad_for_cudagraph` 的自定义入口，避免运行时异常。

---

### [5/N] Initialize MM components in context managers (Q-Z) (#32695)
**SHA**: `193069d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/193069d129414f8f70218f8fb88fb92c75f17ce5)

**变更类型**：功能增强（MM 组件统一化初始化）  
**重要程度**：🟡 中  

**核心改动**  
1. 在 *Q‑Z* 系列 multimodal 模型（`qwen2_audio`, `qwen3_omni_moe_thinker`, `qwen3_vl`, `siglip`, `skyworkr1v`, `tarsier`, `ultravox`, `voxtral` 等）中，引入 `self._mark_tower_model` 与 `self._mark_language_model` 两个上下文管理器。  
   - 将音频、图像、视频等 tower 的实例化以及对应投影层放入 `_mark_tower_model`，将语言模型实例化放入 `_mark_language_model`。  
   - 通过上下文管理器统一注册模型参数、执行 lazy‑load、并在多卡/并行环境下正确划分模型分区。  

2. 删除 `get_language_model` 方法的显式返回，实现依赖 `self.language_model` 的统一管理。  

3. 对 deep‑stack 相关 buffer（`deepstack_input_embeds`）加入存在性检查，避免在 vision tower 被跳过时触发 `AttributeError`。  

4. 小幅清理：去除不必要的 `assert`、`patch_generator` 检查以及多余的属性赋值，使代码更简洁。  

**影响范围**  
- **模型加载路径**：所有使用 `ModelExecutor` 的 multimodal 模型会经过新的上下文管理器，涉及 `vllm/model_executor/models/*`。  
- **并行/分布式调度**：分区信息由 `_mark_*` 注入，可能影响 pipeline 并行、tensor 并行以及 LoRA 过滤逻辑。  
- **用户 API**：`get_language_model()` 已被废弃，外部若直接调用将报 AttributeError，需改为 `model.language_model`。  

**风险与注意事项**  
- 上下文管理器内部实现若出现异常，可能导致模型部分参数未注册，进而在推理时触发 “parameter not found” 错误。建议在 CI 中添加针对每个受影响模型的加载单元测试。  
- DeepStack buffer 的新判空逻辑返回 `None`，调用方需要兼容 `None`（已在代码中加入 guard）。  
- 迁移期间文档需更新，提醒开发者和用户不再使用 `get_language_model`。  

**建议**  
1. 为所有新增 `with self._mark_*` 语句添加注释，说明其在模型分区/LoRA 过滤中的作用。  
2. 增加一次性加载测试，确保在 FP8、QLoRA、Tensor‑Parallel 等配置下模型仍能成功初始化。  
3. 更新 `vllm` 的使用示例，展示如何直接访问 `model.language_model`。  

该改动提升了 multimodal 组件的统一管理和并行兼容性，若测试覆盖充分，风险可控。

---

### Test: added acceptance length tests (#32030)
**SHA**: `f0feb1c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f0feb1cf81ef33573ffb603a4ff0d12e604a622f)

**🎯 变更类型**：功能增强（新增回归测试）

**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在 `tests/v1/spec_decode/` 目录下新增了 277 行的 EAGLE3 Spec‑Decode 接受长度回归测试，涵盖 Llama‑3.1、Qwen3、GPT‑OSS 三个模型。测试会在 MT‑Bench 数据集上运行推理，收集 `vllm:spec_decode_*` 度量并与基线接受长度进行相对误差比对，支持多后端、多 TP 规模的参数化。

**🎯 影响范围**  
- **核心模块**：`vllm.v1.attention.backends.registry`、`vllm.v1.attention.selector`、`vllm.v1.metrics.reader`、`vllm.benchmarks.datasets`、`vllm.inputs`、`vllm.platforms`。  
- **测试框架**：`tests/conftest.py` 中的 `VllmRunner`、`tests/utils.large_gpu_mark`。  
- **CI/CD**：新增约 40 GB GPU 需求的长时间 GPU 测试，可能导致 CI 资源紧张或超时。

**💡 关注建议**  

1. **资源防护**：在 CI 环境中应提供可选的 “skip‑large‑gpu” 标记，或将该文件放在仅在 GPU 集群运行的子工作流，以防普通 CI 机器因显存不足而失败。  
2. **后端兼容性**：已在代码中排除 `FLASHINFER`、`FLEX_ATTENTION`，但后续若新增后端需同步更新 `EXCLUDED_BACKENDS` 与模型专属 `excluded_backends`。  
3. **数据集依赖**：`philschmid/mt-bench` 通过 `huggingface_hub` 下载，建议在 `requirements‑test.txt` 中明确该依赖，并在 CI 前缓存数据，以避免网络波动导致 flaky。  
4. **容差设置**：相对误差阈值 `DEFAULT_RTOL = 0.05` 较为严格，若后续模型优化导致轻微波动可考虑放宽或采用分段容差（整体 vs. per‑position）。  
5. **可读性**：`extract_acceptance_metrics` 逻辑已较清晰，可进一步添加类型注解和单元测试；`get_available_attention_backends` 中对 `current_platform.get_valid_backends` 的 fallback 可写成实用函数，提升复用。  

总体而言，此次新增的回归测试对保证 EAGLE3 规格解码的接受长度稳定性非常有价值，只需在 CI 资源、后端兼容和依赖管理上做好防护，即可安全合入主分支。

---

### [Perf] Only clone when needed for `moe_permute` (#32273)
**SHA**: `6c97b9b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6c97b9b9b6337d021615ed74f9cf836b65c446d7)

**🎯 变更类型**：性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `moe_permute_unpermute_op.cu` 中，原实现始终对 `topk_ids` 进行 `clone()`，即使后续并未对其进行修改。此次改为先直接使用 `topk_ids`（`topk_ids_for_sort`），仅在 `expert_map` 不为空且需要预处理时才进行一次克隆。  
- 相应地，将 `sortAndScanExpert` 的第一个参数改为 `const int*`，在声明（`.h`）与实现（`.cu`）中同步更新，确保函数不再修改输入数据。  

**🎯 影响范围**  
- `csrc/moe/*`：MOE（Mixture‑of‑Experts）相关的 CUDA 核心实现。  
- 上层 Python 接口 `vllm` 在执行 `moe_permute` 时会直接受益于内存拷贝的减少。  

**💡 关注建议**  
1. **功能一致性**：`topk_ids_for_sort` 在有 `expert_map` 场景下仍会在内部 `clone()`，保证后续 `preprocessTopkIdLauncher` 不会修改原始张量。请确认在无 `expert_map` 时没有其它路径会写入 `topk_ids`，否则可能出现隐式数据竞争。  
2. **const 正确性**：`sortAndScanExpert` 参数已改为 `const int*`，但实现内部仍使用 `get_ptr<int>` 读取。若后续出现对该指针的写操作，需要恢复 `const` 或显式复制，以免触发未定义行为。  
3. **编译与兼容性**：头文件签名变化会导致所有调用该函数的翻译单元重新编译。确保 CI 中的全量编译通过，特别是 Windows/conda 环境的 CUDA 编译链。  
4. **性能验证**：建议在典型的模型并行配置（例如 8‑GPU、每节点多专家）下进行基准测试，比较 clone 前后的显存占用和吞吐率，确认优化效果在不同 `topk`、`n_token` 规模下均有提升。  
5. **单元测试**：补充两个场景的回归测试：① `expert_map` 为 `null`（不克隆），② `expert_map` 非空（仍克隆）。验证 `moe_permute` 输出与旧版完全一致，避免因 alias 引入的 subtle bug。  

总体来看，此次改动通过“按需克隆”显著降低了不必要的显存拷贝，属于合理的微优化。但建议在上游 Python 包发布前完成上述验证，防止在极端并行/大 batch 场景下出现数据竞争或 API 不兼容问题。

---

### [PluggableLayer][1/N] Define PluggableLayer (#32331)
**SHA**: `4ca62a0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4ca62a0dbdba8b8aaf40438c99d20cdb56db8d5d)

**🎯 变更类型**：功能增强（引入 PluggableLayer 机制）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm.model_executor.custom_op` 中新增全局 `op_registry / op_registry_oot`，并实现 `PluggableLayer` 基类，用于 **类层级** 的可插拔替换（支持 OOT 替换、统一注册、实例化时自动切换）。  
2. 将原先通过 `CustomOp` 注册的 MLA 层改为 `PluggableLayer.register`，并统一 `forward` 接口（去除 `forward_cuda`、`forward_native` 的平台分派）。  
3. 更新文档、单元测试以及相关导入，使其使用新的 `op_registry`（不再通过 `CustomOp.op_registry`）并验证开启/关闭状态。  

**🎯 影响范围**  
- `vllm/model_executor/custom_op.py`（核心注册机制、类实现）  
- `vllm/model_executor/layers/mla.py`（MLA Layer 迁移至 PluggableLayer）  
- `tests/model_executor/test_enabled_custom_ops.py`（测试路径调整）  
- `docs/design/custom_op.md`（文档同步）  

**💡 关注建议**  
- **开发者**：在实现自定义层时请使用 `@PluggableLayer.register`（或 `register_oot`）而非 `CustomOp`；若仍需平台分派的细粒度实现，保留 `CustomOp` 并自行在新层内部调用。注意 `op_registry` 与 `op_registry_oot` 已全局共享，避免同名冲突。  
- **用户**：通过 `--compilation_config.custom_ops` 启用/禁用的逻辑保持不变，只是查询方式改为 `op_registry[name].enabled()`。升级后请确认旧插件仍兼容（仍可用 `CustomOp.register_oot` 替换完整层）。  
- **兼容性**：现有 `CustomOp` 仍保留，仅用于需要 per‑platform forward 分派的算子；新加入的 `PluggableLayer` 更适合需要子模块组合的完整层。建议在迁移期逐步替换相应层，以获得更清晰的插件化模型。

---

#### 🟢 低重要度变更 (17)

### [bugfix] Aria model (#32727)
**SHA**: `e14467b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e14467be432bf55493c21ffd9f7d1d4c32e14e19)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Aria 模型中新增 ParallelLMHead 与 LogitsProcessor，使用 `logit_scale` 修复 logits 计算逻辑。

---

### [Bugfix] Force using spawn multiprocess method when it's the WSL platform (#32749)
**SHA**: `6bb2bc7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6bb2bc71e299c2f0d9ecb1fcebe54aa64e658307)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `system_utils.py` 中新增 WSL 检测 (`in_wsl`) 并在检测到 WSL 时强制使用 `spawn` 多进程启动方式，以规避 NVML 与 `fork` 不兼容的问题。

---

### [Documentation] Fix typo in `docs/design/torch_compile_multimodal.md` (#32741)
**SHA**: `c80f92c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c80f92c14d5e6c52691f586052af68d1495aac74)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/design/torch_compile_multimodal.md` 中将 `@supports_torch_compile` 拼写错误统一修正为 `@support_torch_compile`，同步更新相关示例和说明文字。

---

### [Bugfix] Support HF sharded weights for Mistral3/Pixtral models (#32673)
**SHA**: `f23fb5a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f23fb5a7c1b61350c5c40ca1115d3bf8cf2b8cc9)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Pixtral 模型权重加载中，新增对 `vision_tower`、`multi_modal_projector` 前缀的检测，使用 `dict.get` 防止键缺失错误，并在生成器中去除 `language_model.` 前缀，以兼容 HuggingFace 分片权重格式。

---

### [Docs] Fix GitHub handle in governance process (#32582)
**SHA**: `360aa93` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/360aa93f8fd211265640ec3489c2aae4323f8978)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 governance 文档中核心/首席维护者的 GitHub 账号拼写错误，将 `@robertshaw2-redhat` 更正为 `@robertgshaw2-redhat`。

---

### [Bugfix] Fix Nemotron-Nano-v2-vlm static resolution (#32682)
**SHA**: `27ca95b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/27ca95b3c9e6e32ac02508e729c01865800fd036)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 Nemotron‑Nano‑v2‑vlm 静态分辨率错误，新增 `num_patches` 参数传递，确保图像输入的补丁数量正确。

---

### Added qwen3 vision language moe support for speculative decoding (#32048)
**SHA**: `7ab80a8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7ab80a8e37ac62f73e09fdc9ba7f69dd09cda2a8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 Qwen3 多模态 MoE 模型加入推测解码支持，新增 aux hidden state 收集并在 `eagle` 中使用草稿模型的 M‑RoPE 配置。

---

### Enable Eagle3 speculative decoding for Pixtral (LlavaForConditionalGeneration) (#32542)
**SHA**: `0900ced` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0900cedb3f89e475bea256c4cf5a13b5f02635bc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `LlavaForConditionalGeneration` 中新增 `SupportsEagle3` 接口，实现 `set_aux_hidden_state_layers` 与 `get_eagle3_aux_hidden_state_layers` 方法，以启用 Eagle3 推测解码支持。

---

### [Cleanup] Remove unused `KVConnectorModelRunnerMixin` methods (#32077)
**SHA**: `6f067b1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6f067b1fb7309ed9dc05f187f25ca29f82f6fd9a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：删除 KVConnectorModelRunnerMixin 中未使用的 `maybe_setup_kv_connector`、`maybe_wait_for_kv_save`、`get_finished_kv_transfers` 等方法，精简实现；相应更新单元测试注释以匹配新的内部逻辑。

---

### [ROCm][CI] Remove DS async eplb accuracy test from AMD CI (#32717)
**SHA**: `22375f8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/22375f8d13dbf5c201c6f1564f72310193930816)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 AMD CI 配置 `.buildkite/test-amd.yaml` 中删除了 DeepSeek V2-Lite Async EPLB 准确性测试相关的步骤，减少 11 行配置。

---

### [Bugfix] Suppress log on non-ROCm platform (#32703)
**SHA**: `9b67338` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9b67338b78f33dca5726191cead98ed913a96f4a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `quark_ocp_mx.py` 中，仅在 ROCm 平台出现异常时才打印警告，避免非 ROCm 环境产生不必要的日志。

---

### [Bugfix] Fix byte fallback handling when using outlines  (#31391)
**SHA**: `86c69dc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/86c69dc54cc813b714afe8b123da64dcbbbd035e)

**🎯 代码重构**  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 outlines 后端的字节回退问题，加入对 FSM 可能进入死状态的说明并确保 `advance` 失败时返回 False；在 `convert_token_to_string` 中精准判断单独的 `\ufffd`，防止误将合法字符当作无效 UTF‑8 处理。

---

### [Doc] Update docs for MM model development with context usage (#32691)
**SHA**: `09194b9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/09194b90a5353bf51cabfeaa0ec93f139c2c4ac3)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：完善了多模态模型开发文档，补充了 `__init__` 中语言/视觉组件的标记用法、`embed_multimodal` 实现示例以及 `mm-encoder-only` 参数的说明；同步更新了示例 README 中的相关描述。

---

### [Model Runner V2] Support FLASHINFER_MLA backend (#32709)
**SHA**: `9ab4388` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9ab4388cd3dc6bb2069125c1a84feaa1aa193e0b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `attn_backend` 名称判断改为全等匹配，防止误匹配其他后端；在模型运行器中加入对新后端 `FLASHINFER_MLA` 的支持。

---

### [Bugfix] fix the ima issue of qwen-vit (#32687)
**SHA**: `04a9e06` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/04a9e064db4dcf57519f1333796ba7face46248b)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `qwen2_5_vl.py` 中对 `qk_reshaped` 添加 `contiguous()` 以修复 IMA 错误；在 `qwen3_vl.py` 将官方最大视频帧数上限从 24576 调整为 2048，以防止内存溢出。

---

### [Doc] [ROCm] Update ROCm getting started doc (#32580)
**SHA**: `c025263` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c025263ddd1c35532151fa1334f64c0aa7acc4ba)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：完善 ROCm 入门文档，新增 ROCm 7.0 及预编译 wheel 的安装说明（推荐使用 `uv`），补充官方与 AMD Docker 镜像使用指南，更新预构建 wheel 的获取方式及注意事项，并细化 ROCm 注意力后端列表及相关环境变量说明。

---

### [Bugfix] Fix Off-by-one error in _num_tokens_to_min_blocks calculation (#32603)
**SHA**: `7901109` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7901109ea5c9794a5e7e01481343235e3a042971)

**🎯 变更类型**：代码重构（修复 Off‑by‑one 错误）  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/kernels/utils.py` 中，将计算最少块数的公式从 `(num_tokens + block_size) // block_size` 修正为 `(num_tokens + block_size - 1) // block_size`，消除因未减 1 导致的块数少算一的错误。

---

