# 每日更新报告（2026-01-08）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-08 23:28:13 | Chauncey | [Bugfix]: Fix Step3ReasoningParser missing is_reasoning_end_streaming (#31969) |
| 2026-01-08 22:42:57 | yxing-bj | [Model] Support IQuestCoder model (#31575) |
| 2026-01-08 22:04:55 | Chauncey | [Docs]: update claude code url (#31971) |
| 2026-01-08 21:42:01 | TJian | [CI] [Bugfix] Fix unbounded variable in `run-multi-node-test.sh` (#31967) |
| 2026-01-08 21:01:42 | Mary | [OpenAI] Fix tool_choice=required streaming when output has trailing extra data (#31610) |
| 2026-01-08 21:00:57 | Ce Zhao | [Model] Enable LoRA support for Pixtral (#31724) |
| 2026-01-08 21:00:27 | tianshu-Michael-yu | [Model] Add LFM2-VL model support (#31758) |
| 2026-01-08 20:59:48 | Bijaya Dangol | [Model] Add Grok-2  (#31847) |
| 2026-01-08 18:34:19 | Patrick von Platen | [Voxtral] Fix speech transcription api (#31388) |
| 2026-01-08 18:33:48 | Isotr0py | [MM Encoder]: Make MMEncoderAttention's `scale` takes effect properly  (#31950) |
| 2026-01-08 18:33:16 | Cyrus Leung | [Model] Standardize common vision encoders (#31947) |
| 2026-01-08 18:16:21 | Cyrus Leung | [Chore] Further cleanup pooler (#31951) |
| 2026-01-08 18:00:25 | omer-dayan | RayLLM Bugfix - Preserve obj store URL for multi engine_config creation (#30803) |
| 2026-01-08 17:27:50 | BingjiaWang | [Misc] Support qwen3-next lora (#31719) |
| 2026-01-08 17:27:26 | DevByteAI | fix(compile): apply partition wrapper when loading AOT cached functions (#31536) |
| 2026-01-08 17:00:24 | Ryan Rock | [CI/Build] Enable test_kv_cache_events_dp for AMD (#31834) |
| 2026-01-08 17:00:07 | Lumosis | Decouple page_size_bytes calculation in AttentionSpec for TPU/RPA Compatibility. (#31635) |
| 2026-01-08 16:10:15 | Isotr0py | [Models] Allow converting Qwen3-VL into Reranker model (#31890) |
| 2026-01-08 15:47:44 | Zhiwei | [ROCm]Skip test_torchao.py::test_pre_quantized_model on CDNA3 arch (#31905) |
| 2026-01-08 15:47:02 | Shang Wang | [docker] A follow-up patch to fix #30913: `[docker] install cuda13 version of lmcache and nixl` (#31775) |
| 2026-01-08 15:46:07 | Rabi Mishra |  fix(rocm): Add get_supported_kernel_block_sizes() to ROCM_ATTN (#31712) |
| 2026-01-08 15:45:53 | Zyyeric | [Model] Enable LoRA support for tower and connector in GLM4-V (#31652) |
| 2026-01-08 15:45:10 | Andy Liu | [Bugfix] Remove the num_hidden_layers override for glm4_moe (#31745) |
| 2026-01-08 15:31:51 | prashanth058 | [Fix] Enable mm_processor_cache with vision LoRA (#31927) |
| 2026-01-08 15:31:03 | Nick Hill | [BugFix] Fix spec decoding edge case bugs (#31944) |
| 2026-01-08 15:24:46 | Chang Su | [grpc] Support gRPC server entrypoint (#30190) |
| 2026-01-08 15:24:18 | Lucas Wilkinson | [chore] Update FA commit (#30460) |
| 2026-01-08 14:56:44 | Ronald | [platform] add dp_metadata arg to set_additional_forward_context (#31942) |
| 2026-01-08 14:50:16 | ShaanveerS | [Model] Enable LoRA support for tower and connector in DotsOCR (#31825) |
| 2026-01-08 14:37:50 | Andreas Karatzas | [ROCm][CI] v1 cpu offloading attention backend fix (#31833) |
| 2026-01-08 13:50:23 | Michael Goin | [Doc] Add Claude code usage example (#31188) |
| 2026-01-08 13:06:09 | rasmith | [CI][BugFix][AMD] Actually skip tests marked @pytest.mark.skip_v1 (#31873) |
| 2026-01-08 12:36:39 | Charlie Fu | [ROCm][CI] Add rocm support for run-multi-node-test.sh (#31922) |
| 2026-01-08 12:35:25 | Andreas Karatzas | [ROCm][CI] Fix attention backend test flakiness from uninitialized KV cache memory (#31928) |
| 2026-01-08 12:17:56 | Andreas Karatzas | [ROCm][LoRA] Fix MoE accuracy regression by preserving float32 router weight scaling (#31931) |
| 2026-01-08 12:04:58 | Richard Zou | [BugFix] Fix flakiness in test_eagle_dp for PyTorch 2.10 (#31915) |
| 2026-01-08 11:46:27 | Robert Shaw | [MoE Refactor][16/N] Apply Refactor to NVFP4 (#31692) |
| 2026-01-08 10:58:01 | Andreas Karatzas | [CI] Skip Qwen-VL in multimodal processing tests due to flaky external dependency (#31932) |
| 2026-01-08 10:28:07 | Rabi Mishra | fix(rocm): add early return in get_flash_attn_version for ROCm (#31286) |
| 2026-01-08 10:27:09 | Rabi Mishra | feat(moe): Add is_act_and_mul=False support for Triton MoE kernels (#31645) |
| 2026-01-08 09:15:17 | Matthew Bonanni | [0/N][Attention] Fix miscellaneous pre-commit issues (#31924) |
| 2026-01-08 08:42:33 | Robert Shaw | [MoE Refactor][15/N] Apply Refactor to Fp8 (#31415) |
| 2026-01-08 06:47:54 | Elvir Crnčević | Add back missing DeepEP LL params (#31911) |
| 2026-01-08 04:46:42 | Nick Hill | [BugFix] Fix bad words with speculative decoding (#31908) |
| 2026-01-08 04:21:35 | Ilya Markov | [EPLB] Optimize EPLB with numpy (#29499) |
| 2026-01-08 04:16:32 | Xin Yang | [Kernel] Support bias type in grouped_topk kernel (#31781) |
| 2026-01-08 02:37:31 | Ning Xie | [refactor] refactor memory constants usage (#31865) |
| 2026-01-08 02:31:26 | Michael Goin | [Perf] Fuse stride preparation for NVFP4 cutlass_moe (#31837) |
| 2026-01-08 02:06:42 | Festus Ayobami Owumi | [Doc] Fix: Correct vLLM announcing blog post link in docs (#31868) |
| 2026-01-08 01:37:19 | roikoren755 | Enable quantized attention in NemotronH models (#31898) |
| 2026-01-08 01:13:02 | Jee Jee Li | UX: add vLLM env info in '/server_info' (#31899) |
| 2026-01-08 00:59:43 | Kfir Toledo | [KVConnector]: Enable Cross-layers KV cache layout for MultiConnector (#30761) |
| 2026-01-08 00:15:19 | Marko Rosenmueller | [Bugfix]: prevent leaking tokens in crash log (#30751) |
| 2026-01-08 00:07:43 | Cyrus Leung | [Refactor] Clean up pooler modules (#31897) |

### 📊 统计摘要
> 本日共 54 个提交 | 🔴高 8 | 🟡中 17 | 🟢低 29
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (8)](#-🔴-高重要度变更-8)
    - [[Model] Add LFM2-VL model support (#31758)](#03fd76c)
    - [[Model] Add Grok-2  (#31847)](#59d260f)
    - [fix(compile): apply partition wrapper when loading AOT ca...](#1f21429)
    - [[grpc] Support gRPC server entrypoint (#30190)](#791b2fc)
    - [[MoE Refactor][16/N] Apply Refactor to NVFP4 (#31692)](#9f6dcb7)
    - [feat(moe): Add is_act_and_mul=False support for Triton Mo...](#25eef3d)
    - [[MoE Refactor][15/N] Apply Refactor to Fp8 (#31415)](#5dcd7ef)
    - [[EPLB] Optimize EPLB with numpy (#29499)](#6170d47)
  - [🟡 中重要度变更 (17)](#-🟡-中重要度变更-17)
    - [[Model] Support IQuestCoder model (#31575)](#fe86be6)
    - [[Voxtral] Fix speech transcription api (#31388)](#18d4e48)
    - [[MM Encoder]: Make MMEncoderAttention's `scale` takes eff...](#2972a05)
    - [[Model] Standardize common vision encoders (#31947)](#5576227)
    - [[Chore] Further cleanup pooler (#31951)](#d1b6fe0)
    - [RayLLM Bugfix - Preserve obj store URL for multi engine_c...](#04a4966)
    - [Decouple page_size_bytes calculation in AttentionSpec for...](#b634e61)
    - [[Models] Allow converting Qwen3-VL into Reranker model (#...](#eac3b96)
    - [fix(rocm): Add get_supported_kernel_block_sizes() to ROCM...](#107cf8e)
    - [[Fix] Enable mm_processor_cache with vision LoRA (#31927)](#d3235cb)
    - [[BugFix] Fix spec decoding edge case bugs (#31944)](#287b37c)
    - [fix(rocm): add early return in get_flash_attn_version for...](#39d8200)
    - [[0/N][Attention] Fix miscellaneous pre-commit issues (#31...](#0d76674)
    - [[Kernel] Support bias type in grouped_topk kernel (#31781)](#0ada960)
    - [[refactor] refactor memory constants usage (#31865)](#c907d22)
    - [[KVConnector]: Enable Cross-layers KV cache layout for Mu...](#b89443b)
    - [[Refactor] Clean up pooler modules (#31897)](#b7036c8)
  - [🟢 低重要度变更 (29)](#-🟢-低重要度变更-29)
    - [[Bugfix]: Fix Step3ReasoningParser missing is_reasoning_e...](#eaba8ec)
    - [[Docs]: update claude code url (#31971)](#1da3a54)
    - [[CI] [Bugfix] Fix unbounded variable in `run-multi-node-t...](#72c068b)
    - [[OpenAI] Fix tool_choice=required streaming when output h...](#7645bc5)
    - [[Model] Enable LoRA support for Pixtral (#31724)](#1123a87)
    - [[Misc] Support qwen3-next lora (#31719)](#96fcd3c)
    - [[CI/Build] Enable test_kv_cache_events_dp for AMD (#31834)](#8cbdc7e)
    - [[ROCm]Skip test_torchao.py::test_pre_quantized_model on C...](#573a1d1)
    - [[docker] A follow-up patch to fix #30913: `[docker] insta...](#33156f5)
    - [[Model] Enable LoRA support for tower and connector in GL...](#63baa28)
    - [[Bugfix] Remove the num_hidden_layers override for glm4_m...](#e5173d3)
    - [[chore] Update FA commit (#30460)](#be6a81f)
    - [[platform] add dp_metadata arg to set_additional_forward_...](#2ab441b)
    - [[Model] Enable LoRA support for tower and connector in Do...](#9572f74)
    - [[ROCm][CI] v1 cpu offloading attention backend fix (#31833)](#5f2a473)
    - [[Doc] Add Claude code usage example (#31188)](#6b2a672)
    - [[CI][BugFix][AMD] Actually skip tests marked @pytest.mark...](#f1b1bea)
    - [[ROCm][CI] Add rocm support for run-multi-node-test.sh (#...](#cddbc2b)
    - [[ROCm][CI] Fix attention backend test flakiness from unin...](#087a138)
    - [[ROCm][LoRA] Fix MoE accuracy regression by preserving fl...](#c4041f3)
    - [[BugFix] Fix flakiness in test_eagle_dp for PyTorch 2.10 ...](#a79079f)
    - [[CI] Skip Qwen-VL in multimodal processing tests due to f...](#8dd2419)
    - [Add back missing DeepEP LL params (#31911)](#ffc0a27)
    - [[BugFix] Fix bad words with speculative decoding (#31908)](#10ef65e)
    - [[Perf] Fuse stride preparation for NVFP4 cutlass_moe (#31...](#f347ac6)
    - [[Doc] Fix: Correct vLLM announcing blog post link in docs...](#05f47bd)
    - [Enable quantized attention in NemotronH models (#31898)](#bf184a6)
    - [UX: add vLLM env info in '/server_info' (#31899)](#30399cc)
    - [[Bugfix]: prevent leaking tokens in crash log (#30751)](#1d9e9ae)
#### 🔴 高重要度变更 (8)

### [Model] Add LFM2-VL model support (#31758)
**SHA**: `03fd76c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/03fd76c57002747ded9bee5e7a5a699475cb4e9f)

**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**：  
- 为 vLLM 添加对 LFM2‑VL（LiquidAI LFM2‑VL）模型的原生支持，涵盖模型注册、权重映射、视觉塔 (Siglip2) 实现、跨模态投影以及 Prompt 替换逻辑。  
- 同步更新文档、示例脚本、模型注册表与单元测试，以便用户通过 `vllm` 直接进行图像‑文本推理。  

**🎯 影响范围**：  
- `vllm/model_executor/models/lfm2_vl.py`（核心模型实现）  
- `vllm/model_executor/models/siglip2.py`（视觉塔实现）  
- `vllm/model_executor/models/registry.py`（模型注册）  
- `vllm/multimodal`（处理器、占位符、虚拟输入）  
- `docs/models/supported_models.md`、`examples/offline_inference/vision_language.py`、`tests/models/registry.py`  
- 相关配置结构：`Lfm2VLProcessingInfo`、`Lfm2VLMultiModalProcessor`、`Lfm2VLMultiModalProjector`  

---

### 🔍 技术洞察  

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | 1. 引入 **Lfm2VLForConditionalGeneration**，实现了 **IsHybrid、SupportsMultiModal、SupportsLoRA、SupportsPP** 四大接口，完整接入 vLLM 的混合（语言+视觉）执行路径。<br>2. 新增 **Siglip2VisionModel**（`siglip2.py`），作为视觉塔的底层实现，支持 Tensor‑Parallel 与 Data‑Parallel 两种并行模式。<br>3. 通过 **Lfm2VLMultiModalProcessor** 完成 HF `Lfm2VlProcessor` 与 vLLM 多模态框架的桥接，处理图像拆分、smart‑resize、grid‑layout 与占位符拼装。<br>4. 新增 **Lfm2VLMultiModalProjector** 将视觉特征经过 pixel‑unshuffle、可选 LayerNorm、两层线性投影映射到语言隐藏维度。 |
| **性能影响** | - **计算开销**：视觉塔的 ViT‑style 编码 + Mamba‑style 语言模型，使每帧推理的 FLOPs 明显上升（尤其在大尺寸图像或多块拆分时）。<br>- **内存占用**：`pixel_values`、`spatial_shapes`、`num_patches` 等张量会保留在 CPU（keep_on_cpu），但在 `image_pixels_to_features` 中会一次性搬迁至 GPU，若 `max_image_tokens` 较大可能导致 OOM。<br>- **并行优化**：使用 `MMEncoderAttention` 与分块的 `QKVParallelLinear`/`RowParallelLinear`，在 TP 环境下仍保持线性层的切分；`use_data_parallel` 选项提供了另一种更轻量的并行方式。<br>- **智能裁剪**：`smart_resize` 与 `_get_grid_layout` 尽量把图像切分至合理的 token 数，降低无效计算。 |
| **安全考虑** | - 仅增加了对 HuggingFace Hub 的模型下载路径（已有安全审计），未引入新的网络交互或系统调用。<br>- 加入了对 **Transformers ≥ 5.0.0** 的最低版本检测（`tests/models/registry.py`），避免因旧版库的 API 改动导致运行时异常。<br>- 权重加载使用统一的 `AutoWeightsLoader` 与映射表，避免硬编码路径导致的潜在路径遍历。 |
| **可维护性** | - 代码高度模块化：处理器、投影、视觉塔各自独立，便于单独单元测试与替换。<br>- 大量注释和类型标注（`TensorSchema`、`Annotated`）提升可读性。<br>- 通过 `MULTIMODAL_REGISTRY.register_processor` 完成自动注册，保持与其他多模态模型统一的扩展方式。 |
| **兼容性** | - 通过 `registry.py` 将新模型映射到模块名 `lfm2_vl`，对外保持与已有模型相同的 `EngineArgs.model` 接口。<br>- 示例代码使用 `AutoProcessor.from_pretrained` 与 `apply_chat_template`，兼容 HF 统一的 Prompt 体系。 |

---

### ⚠️ 潜在风险  

1. **内存峰值风险**  
   - 大分辨率图像（或大量图像）在 `_get_grid_layout` 中可能产生 **> 1k patches**，在 `image_pixels_to_features` 中一次性拼接成 `bn × d × fd`，可能导致显存瞬间爆炸。  
2. **权重映射错误**  
   - `hf_to_vllm_mapper` 对应的前缀映射较多，若 HF 官方模型结构微调（如 `vision_tower` 子模块命名变化），加载会失败且报错信息不一定直观。  
3. **并行模式冲突**  
   - `use_data_parallel` 与 Tensor‑Parallel 同时开启时，`num_heads % tp_size` 必须成立，若用户误配 `mm_encoder_tp_mode` 可能触发断言。  
4. **Transformer 版本兼容**  
   - 代码直接引用 `transformers.models.lfm2_vl`、`Lfm2VlProcessor`，在低于 5.0 版本的环境会抛 `ImportError`。CI 需加固此依赖。  
5. **占位符拼接错误**  
   - `Lfm2VLMultiModalProcessor._get_prompt_updates` 使用 `image_token` 作为 `target`，若用户自行修改 tokenizer 的特殊 token，可能导致占位符未被替换，引发模型生成异常。  

---

### 💡 关注建议  

| 对象 | 建议 |
|------|------|
| **开发者** | - 在模型初始化时打印 **`max_image_tokens`、`downsample_factor`** 等关键配置，帮助定位 OOM。<br>- 为 `smart_resize`、`_get_grid_layout` 添加单元测试，覆盖极端宽高比与超大图像。<br>- 在 `tests/models/registry.py` 增加 **transformers 版本检查**，确保 CI 在 5.x+ 环境运行。 |
| **使用者** | - 推荐在 `EngineArgs.limit_mm_per_prompt={"image": 1}` 限制单 Prompt 中的图像数量，防止意外的多图堆叠导致显存爆炸。<br>- 对大图像使用 `max_image_tokens` 更低的模型变体（如 450M）或自行预‑resize，以降低 GPU 负载。 |
| **运维 / CI** | - 在 CI 中增加 **GPU 资源监控**，检测 `lfm2_vl` 推理时的显存峰值；设定阈值警报。<br>- 确保 `torch.compile` 与 `torch.backends.cuda.matmul.allow_tf32` 等全局编译选项在有 **Siglip2** 时保持兼容（参考 `support_torch_compile` 装饰器）。 |
| **安全审计** | - 虽然暂无新网络请求，但建议对 **huggingface_hub.snapshot_download** 的下载路径进行白名单校验，防止恶意模型被拉取。 |
| **文档** | - 在 `supported_models.md` 中补充 **“LFM2‑VL 对显存需求”** 与 **“图像尺寸上限”** 的提示，帮助用户快速评估可行性。 |

--- 

**结论**：

---

### [Model] Add Grok-2  (#31847)
**SHA**: `59d260f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/59d260f5e4b07eaffb34c79579959f3c517e606f)

**🎯 变更类型**：功能增强（新增 Grok‑2 模型及其专属 tokenizer）  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 在模型表与注册表中加入 `Grok‑2`（`xai-org/grok-2`），并在文档中说明其 token‑json 要求及可选 MoE router renormalize 参数。  
2. 实现 **Grok2Tokenizer**（基于 tiktoken）并在 tokenizer 解析逻辑中自动检测 `tokenizer.tok.json`，支持自定义 chat template。  
3. 重构 Grok 系列模型代码：抽象出 `GrokBaseForCausalLM`，新增版本检测 (`_get_grok_version`) 与工厂类 `GrokForCausalLM`，并分别实现 `Grok1ForCausalLM` 与 `Grok2ForCausalLM`。  
4. 为 Grok‑2 添加 MoE‑router‑soft‑capping、残差 MoE、专家权重命名映射等新特性；修改 `load_weights` 以兼容不同 checkpoint 命名。  
5. 更新模型注册表、测试用例以及 tokenizer 解析的自动模式选择。

---

### 🎯 影响范围
- **模型层**：`vllm/model_executor/models/grok1.py`（几乎全部），`vllm/model_executor/models/registry.py`。  
- **Tokenizer 层**：新增 `vllm/tokenizers/grok2.py`，`vllm/tokenizers/registry.py`。  
- **文档**：`docs/models/supported_models.md`。  
- **测试**：`tests/models/language/generation/test_grok.py`、`tests/models/registry.py`、`tests/tokenizers_/test_basic.py`。  
- **运行时**：模型加载、权重分配、推理路径均可能走新实现。

---

### 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | - 引入 **工厂模式** `GrokForCausalLM`，在运行时根据 HF 配置自动选择 Grok‑1 或 Grok‑2，实现向后兼容。<br>- 将原本硬编码的专家权重名称抽象为可配置字段 (`ckpt_gate_proj_name` 等) 与映射 (`weight_name_remapping`) ，为后续可能的第三方 Grok 变体留下扩展口。<br>- 新增 `Grok2Tokenizer`，使用 `tiktoken.Encoding` 替代原有基于 HuggingFace 的 tokenizer，实现 **fast‑path**（但在 vLLM 中仍走 Python 代码），并在 `resolve_tokenizer_args` 中加入自动检测逻辑。 |
| **性能影响** | - **MoE Router Soft‑capping**（`router_logit_soft_cap`）默认 30，降低极端路由 logits，可能减少数值不稳定导致的梯度爆炸，但对推理速度影响微乎其微（仅一次 `tanh` 运算）。<br>- 对 Grok‑2‑only 的 **残差 MoE** 路径引入 `self.residual_moe_scale = 1/√2`，增加一次 MLP 前向运算，略微增加 FLOPs（约 5% 额外计算，视 expert 数量而定）。<br>- `MergedColumnParallelLinear` 替换原 `gate_up_proj` 实现，提升张量并行时的通信效率。<br>- 使用 `tiktoken` 的正则表达式分词在大多数情形下比 HuggingFace 的 `BPE` 更快，尤其在长文本场景。 |
| **安全考虑** | - 新增对 **`tiktoken` 包的依赖**，若未安装会在 tokenizer 创建时抛异常，防止 silent fallback。<br>- `resolve_tokenizer_args` 自动下载 `tokenizer.tok.json`，涉及网络 I/O；已通过异常捕获并记录警告，防止未授权下载导致的 supply‑chain 攻击。<br>- 权重加载时增加 **名称 remapping** 检查，避免错误的权重映射导致模型行为异常（潜在的安全风险已通过 `logger.warning_once` 提示）。 |
| **可维护性** | - 通过抽象基类与版本映射表 (`_GROK_VERSIONS`) 把版本差异局部化，后续新增 Grok‑X 只需实现子类并在映射表登记。<br>- 大量硬编码字符串（如 `"linear"`、`"w1"`）已集中在类属性与 `get_weight_name_remapping`，易于统一修改。<br>- 测试覆盖新增模型、tokenizer 与注册表，提升回归安全性。 |

---

### ⚠️ 潜在风险
1. **版本检测误判**：`_get_grok_version` 仅检查 `residual_moe` 或 `moe_intermediate_size`，若未来的 Grok‑1 checkpoint 引入其中任意字段，可能被误判为 Grok‑2，导致加载错误。  
2. **权重映射失配**：`weight_name_remapping` 只处理 `.self_attn.` 与 `.block_sparse_moe.` 前缀，若 checkpoint 使用其他命名（如 `attention.`），仍会报错或静默跳过。  
3. **依赖冲突**：`tiktoken` 与 `torch` 的二进制兼容性在某些 Linux 发行版上可能出现冲突，导致 tokenizer 加载失败。  
4. **残差 MoE 计算路径**：在极端高并发的分布式环境下额外的 MLP 计算可能触发 **CUDA OOM**，尤其在显存紧张的 8‑bit 量化模式下。  
5. **自动 tokenizer 选择**：`resolve_tokenizer_args` 先检查 `tokenizer.tok.json` 再 fallback，若仓库同时包含 `tokenizer_config.json` 与 `tokenizer.tok.json`（非官方），可能产生不一致的 tokenization 行为。  

---

### 💡 关注建议
- **完善版本检测**：在 `_get_grok_version` 中加入显式的 `model_type` 或 `model_name` 检查，防止误判。  
- **权重映射配置化**：将 `weight_name_remapping` 抽离为模型配置字段，便于用户在自定义 checkpoint 时自行声明映射。  
- **CI/CI‑Smoke**：在 CI 中加入对 **tiktoken** 兼容性的多平台测试（Linux/macOS/Windows），并确保 `pip install "tiktoken>=0.7"` 能被满足。  
- **文档更新**：在模型支持页面明确标注 Grok‑2 需要 `tokenizer.tok.json` 与 `tiktoken`，并给出 `pip install tiktoken` 示例。  
- **显存监控**：对使用 Grok‑2 且启用 `residual_moe` 的用户，建议在 `vllm` 启动时添加 `--gpu-memory-utilization` 或开启 **cpu offload**，以防 OOM。  
- **回滚路径**：若用户在生产环境遇到加载错误，可通过环境变量 `VLLM_ALLOW_INSECURE_SERIALIZATION=0` 暂时禁用 Grok‑2 自动检测，强制使用 HF tokenizer 进行对比调试。  

--- 

**结论**：本次 PR 为 vLLM 引入了对 Grok‑2 的完整支持，涉及模型层、tokenizer 层以及注册表的多处改动。架构上实现了干净的版本分离与可扩展的权重映射，性能影响可忽略不计，安全风险主要集中在依赖与权重映射的正确性上。建议在正式部署前完成上述风险缓解措施，并在 CI 中加入对应的回归测试，以确保新模型在各种硬件配置下的稳定运行。

---

### fix(compile): apply partition wrapper when loading AOT cached functions (#31536)
**SHA**: `1f21429` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1f214290d65a6a69b898e058f1408f2d929f8fa7)

**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：修复在 AOT 编译缓存加载时未使用图划分（partition）包装器导致的 2 倍延迟回退。新增单元测试确保在首次加载缓存以及后续调用时均正确开启并关闭 `maybe_use_cudagraph_partition_wrapper`，并在 `decorators.py` 中将调用 AOT 编译函数的路径统一包裹该上下文。  

**🎯 影响范围**：  
- `vllm/compilation/decorators.py`（Runtime 调用 AOT 编译函数的核心入口）  
- `tests/compile/test_aot_compile.py`（新增针对该修复的完整测试）  
- 受 `VLLM_USE_AOT_COMPILE`、`VLLM_FORCE_AOT_LOAD` 以及 `use_inductor_graph_partition` 配置影响的所有模型加载/推理路径  

**🔍 技术洞察**：  
- **架构影响**：  
  - 将图划分包装器的激活从编译阶段迁移到运行时调用阶段，确保在 **AOT 缓存加载** 时也能保持与 `torch._inductor` 的图划分协同。  
  - 引入 `maybe_use_cudagraph_partition_wrapper` 的统一使用，提高了装配层的一致性，降低了因路径分支不同导致的行为差异。  

- **性能影响**：  
  - 解决了 GitHub #31439 报告的 **2× 延迟回退**，在开启 `use_inductor_graph_partition=True` 时恢复原有的 AOT 编译加速效果。  
  - 只在调用 AOT 编译函数时短暂激活包装器，开销极低（基本为上下文管理的函数调用），对整体吞吐量几乎无负面影响。  

- **安全考虑**：  
  - 变更仅涉及内部运行时上下文管理与函数包装，不引入外部依赖或 I/O，未产生安全风险。  
  - 环境变量的读取仍受 `monkeypatch` 控制，未泄露或扩大攻击面。  

**⚠️ 潜在风险**：  
1. **上下文泄漏**：若 `maybe_use_cudagraph_partition_wrapper` 的实现在异常路径未正确清理，可能导致后续调用保持包装器状态，影响 CUDA Graph 捕获的正确性。  
2. **兼容性**：该修复仅在 Torch >= 2.10（包含 dev）下验证，旧版本仍会走原有路径，可能出现不一致的行为。  
3. **线程安全**：AOT 编译函数可能在多进程/多线程环境下并发调用，包装器的全局状态（如果有）需要确认为线程安全。  

**💡 关注建议**：  
- **回归测试**：在 CI 中加入多版本 Torch（包括 2.8、2.9）跑相同测试，确保未因缺失包装器而产生新回归。  
- **异常路径覆盖**：增加对 `maybe_use_cudagraph_partition_wrapper` 抛异常时的单元测试，验证 wrapper 能够在 `except` 块中正确清理。  
- **文档说明**：在 VLLM 配置文档中明确说明：开启 `use_inductor_graph_partition` 时，AOT 缓存加载同样会自动使用图划分包装器，无需额外手动设置。  
- **监控指标**：在生产环境添加 “AOT cache load latency” 与 “partition wrapper active time” 的度量，确保修复效果在真实流量下保持预期。  

---  

---

### [grpc] Support gRPC server entrypoint (#30190)
**SHA**: `791b2fc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/791b2fc30a25cc48e06a6bd7ce4fd62d765ac004)

**🎯 变更类型**：功能增强（新增 gRPC 服务器入口、协议层实现、自动生成 protobuf 代码、对应测试等）

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 为 vLLM 项目新增基于 gRPC 的服务端入口 `vllm.entrypoints.grpc_server`，实现了生成、健康检查、模型/服务器信息、Abort、Embed（占位）等 6 项 RPC。  
- 引入 protobuf 定义 `vllm/grpc/vllm_engine.proto`，并在构建流程中通过 `grpcio-tools` 自动编译生成 `_pb2.py/_pb2_grpc.py/_pb2.pyi`，对应的构建脚本、`.gitignore`、`pyproject.toml`、`requirements` 均已同步更新。  
- 为新功能提供 400+ 行端到端测试 `tests/entrypoints/test_grpc_server.py`，覆盖健康检查、模型信息、流式/非流式生成、采样参数、停止字符串、并发、种子可复现、错误处理、Abort 等场景。  

**🎯 影响范围**  
- **核心模块**：`vllm/grpc/*`（协议层及代码生成）、`vllm/entrypoints/grpc_server.py`（服务实现）  
- **构建/发布**：`setup.py`、`pyproject.toml`、`requirements/*`（新增 `grpcio-tools`、`protobuf`、`grpcio`、`grpcio-reflection`）  
- **文档**：`mkdocs.yaml`（排除 auto‑generated protobuf 文件）  
- **测试**：`tests/entrypoints/test_grpc_server.py`（全链路验证）  
- **工具链**：`vllm/grpc/compile_protos.py`（手动重新编译脚本）  

---

## 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **新服务层**：在原有 OpenAI‑compatible HTTP API 之上，加入 **gRPC** 入口，采用 `grpc.aio` 异步服务器，直接复用 `AsyncLLM` 引擎。<br>- **协议统一**：通过 protobuf 明确定义请求/响应结构，提升跨语言（如 Rust 路由）集成的可维护性。<br>- **模块解耦**：`VllmEngineServicer` 通过依赖注入 `AsyncLLM`，保持业务逻辑（采样、生成）不受传输层影响。<br>- **反射支持**：开启 `grpc_reflection`，便利调试（`grpcurl`）和动态客户端生成。 |
| **性能影响** | - **流式传输**：`Generate` 使用 `RequestOutputKind.DELTA` 仅返回增量 token，降低带宽与序列化开销。<br>- **异步 IO**：基于 `grpc.aio` 与 `uvloop`，保持与现有 HTTP 服务器相似的高并发能力。<br>- **启动成本**：编译 protobuf 在 `setup.py` 的 `build_py` 阶段执行，首次构建略增耗时（≈几秒），但不影响运行时。<br>- **资源占用**：默认监听 `0.0.0.0`，若在生产环境未限制并发请求数，可能导致 CPU/内存峰值增大；建议通过 `max_concurrent_requests`（后续可在 `AsyncEngineArgs` 中配置）加以防护。 |
| **安全考虑** | - **网络暴露**：新增端口 50051（默认）对外开放，如未在防火墙或容器层面限制，会成为未授权访问入口。<br>- **认证/授权**：当前实现未加入 token、TLS、或 mTLS 机制，所有客户端均可调用所有 RPC。<br>- **输入校验**：`Generate` 的采样参数通过 `SamplingParams` 转换时已抛出 `ValueError`，但对 `request_id`、`text` 长度等未做显式限制，可能引发 DoS（超长请求或恶意 token 序列）。<br>- **异常泄露**：异常信息直接通过 `grpc.StatusCode.INTERNAL` 返回，可能泄露内部实现细节。 |
| **可维护性** | - **自动生成**：`compile_grpc_protos` 在 `build_py` 阶段自动运行，确保发行版始终包含最新的 protobuf 代码。<br>- **代码组织**：protobuf 生成文件被显式排除在 lint、type‑check、文档生成之外，避免噪声。<br>- **测试覆盖**：端到端测试覆盖了核心路径（健康、模型信息、生成、错误、Abort），为后期迭代提供安全网。<br>- **文档**：新增 `mkdocs` 过滤规则，防止 auto‑generated文件干扰 API 文档渲染。 |
| **兼容性** | - **依赖冲突**：新增 `grpcio-tools>=1.76.0`、`protobuf>=6.30.0`，对已有使用旧版 `grpcio`/`protobuf` 的下游项目可能造成冲突，需要在 `requirements` 中声明兼容范围或提供额外的 `extras_require`。<br>- **Python 版本**：`grpcio-tools` 与 `protobuf` 在 Python≥3.8 环境下稳定，未影响现有支持的 Python 3.9‑3.12。 |

---

## ⚠️ 潜在风险

1. **未授权访问**：默认 `--host 0.0.0.0` 让 gRPC 服务直接暴露在公网，若部署在不受防火墙约束的环境会导致未授权调用。  
2. **依赖升级冲突**：`grpcio`、`grpcio-tools`、`protobuf` 版本快速迭代，可能与已有用户的 pin 版本冲突，引起安装失败。  
3. **资源耗尽**：并发生成请求未做速率限制，恶意或误配置的高并发请求（尤其在流式模式）可能耗尽 GPU/CPU 资源，导致服务崩溃。  
4. **异常信息泄露**：`context.abort(grpc.StatusCode.INTERNAL, str(e))` 将内部异常文字直接暴露给客户端。  
5. **生成文件未纳入 sdist**：若构建缓存未正确触发 `compile_grpc_protos`，发布的 wheel 可能缺失 `_pb2*` 文件，导致 ImportError。  
6. **不完整实现的 RPC**：`Embed` 仍返回 UNIMPLEMENTED，若外部系统依赖该 RPC 会导致运行时错误。  
7. **信号处理兼容性**：在 Windows 环境下 `loop.add_signal_handler` 会抛出 `NotImplementedError`，当前实现未做平台适配。  

---

## 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | - 为生产环境提供 **TLS / mTLS** 配置选项（`--tls-cert`, `--tls-key`），并在文档中强调默认不安全的 `0.0.0.0` 绑定。<br>- 在 `AsyncEngineArgs` 中加入 **并发/速率限制** 参数（如 `max_concurrent_requests`、`request_rate_limit`），防止 DoS。<br>- 将异常信息抽象为统一错误码，避免直接泄露内部实现细节。<br>- 在 `setup.py` 中确保 `package_data` 包含 `vllm/grpc/*_pb2*.py*`，防止缺失。 |
| **运维 / 部署** | - **防火墙** 或容器网络策略务必限制 gRPC 端口的访问来源。<br>- 监控 `grpc_server` 的指标（连接数、QPS、错误率），建议在 Prometheus / OpenTelemetry 中暴露。 |
| **CI/CD** | - 在

---

### [MoE Refactor][16/N] Apply Refactor to NVFP4 (#31692)
**SHA**: `9f6dcb7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9f6dcb71ae4fd410d144fa3d4bb135ea9ab74185)

**🎯 变更类型**：架构变更 / 重构 / 功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将 NVFP4 MoE 的实现从单独的 `cutlass_moe_fp4` 函数迁移到统一的 **Modular‑Kernel（MK）** 抽象 `FusedMoEModularKernel`。  
- 新增 **NvFp4MoeBackend** Oracle，用于在运行时自动挑选 Cutlass、FlashInfer (CUTLASS / TRT‑LLM) 或 Marlin 作为底层实现。  
- 引入 `nvfp4_w4a16_moe_quant_config`、`use_nvfp4_w4a16` 配置项；完善权重加载、全局 scaling‑factor 处理以及 EP/DP 场景的兼容逻辑。  
- 大幅删减 `cutlass_moe_fp4`（已迁移至 MK），并在多处改写 `CompressedTensorsW4A4Nvfp4MoEMethod`、`ModelOptNvFp4LinearMethod` 以及测试/文档。

---

## 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构** | 1. **统一抽象层**：所有 NVFP4‑MoE 前向路径均走 `FusedMoEModularKernel` → `MoEPrepareAndFinalize` + 专家实现（Cutlass / FlashInfer / Marlin）。  <br>2. **后端选择 Oracle** (`select_nvfp4_moe_backend`) 把硬件/环境检测、环境变量 (`VLLM_USE_FLASHINFER_MOE_FP4`) 与后端优先级统一管理，避免散落在各模块的硬编码判断。  <br>3. **配置中心**：`FusedMoEConfig` 新增 `use_nvfp4_w4a16`、`use_int4_w4a16` 等属性；`FusedMoEQuantConfig` 支持 `nvfp4` 量化路径。  <br>4. **权重序列化**：原始 `cutlass_moe_fp4` 参数名（`w13_weight_packed` 等）在 `process_weights_after_loading` 中被重新命名为 `w13_weight`、`w2_weight`，并通过 `replace_parameter` 维护 `state_dict` 兼容性（但已在注释中提到需要后续清理）。 |
| **性能** | 1. **模块化 Kernel**：一次性完成 **输入量化 + MoE 计算**，和原 `cutlass_moe_fp4` 中 “两段量化 + 组 GEMM” 的拆分相比，理论上可减少 1 次内存搬迁、降低 kernel launch 开销。 <br>2. **多后端优化**：<br>   - **FlashInfer‑CUTLASS** 在支持全局 scaling‑factor 时可共享 `a_scale`，显著降低 per‑expert scaling 开销。 <br>   - **FlashInfer‑TRTLLM** 利用 “static weight layout” 进一步压缩权重并在一次 kernel 中完成 FP4 量化与 GEMM。 <br>   - **Marlin** 仍作为 fallback，保证在不支持原生 FP4（旧 GPU）时仍可运行，但会出现 **FP4‑only weight‐only** 的额外转化成本。 <br>3. **DP/EP 场景**：`is_global_sf_supported_for_nvfp4_backend` 明确哪些后端支持跨 rank 的 global scale，避免在不支持的后端误用导致错误或性能回退。<br>4. **Padding 与块对齐**：在 Cutlass/FlashInfer 需要对 `intermediate_size` 进行块对齐（16‑块），已在 `process_weights_after_loading` 中自动填充，避免运行时崩溃。 |
| **安全** | - **无外部 I/O**：所有改动局限于内部张量变换、kernel 调度，未引入新的网络或文件读写路径，安全风险基本为 **数值安全**（溢出、精度损失）。 <br>- **量化路径**：NVFP4 采用 4‑bit 量化 + 动态 scaling，若 `a1_gscale` / `a2_gscale` 计算错误会导致 **∞/NaN** 输出。新增的检查（`torch.allclose` 对 `w13_weight_scale_2`）已发出警告，仍建议在 CI 中加入 **数值一致性** 对比（cutlass vs modular）。 |
| **可维护性** | - **代码集中**：所有 NVFP4 相关的权重转换、全局 scale 计算集中在 `oracle/nvfp4.py` 与 `utils/flashinfer_fp4_moe.py`，易于后期扩展新后端（例如 cuBLAS‑NVFP4）。 <br>- **去除冗余入口**：`cutlass_moe_fp4` 已删除，降低维护面。 <br>- **遗留兼容性**：因为 `state_dict` 键名已被改动（`w13_weight_packed` → `w13_weight`），旧 checkpoint 仍能通过 `load_state_dict` 自动映射（`replace_parameter`）但**不保证**在未来版本仍保持兼容，建议在文档中提供迁移指南或保留旧别名实现。 |

---

## ⚠️ 潜在风险

| 风险点 | 说明 | 可能后果 | 缓解措施 |
|--------|------|----------|----------|
| **Checkpoint 兼容性** | `process_weights_after_loading` 更改了参数名称并使用 `replace_parameter`。 | 旧的 `torch.save(state_dict)` 仍能加载，但重新保存后会生成新键名，导致 **future‑incompatible**。 | - 在 `vllm/model_executor/layers/fused_moe/__init__.py` 保持向后兼容的别名（如 `cutlass_moe_fp4` 作为薄包装）。 <br>- 在发布日志中明确说明 **“NVFP4 checkpoint migration required”**。 |
| **后端检测错误** | `select_nvfp4_moe_backend` 依赖硬件/库检测 (`cutlass_fp4_supported()`, `is_flashinfer_fp4_*_available()`). | 在某些平台（尤其是自定义 CUDA 发行版）返回错误后端，引发 **RuntimeError**，或错误回退到不支持的后端导致性能骤降。 | - 为每个检测函数提供 **详细日志**（已有 `logger.info_once`），并在失败时给出 **fallback 警告**。 <br>- 在 CI 中加入跨平台矩阵测试（A100, H100, 3090 等）。 |
| **全局 Scale 处理不一致** | `is_global_sf_supported_for_nvfp4_backend` 只覆盖 FlashInfer‑CUTLASS 与 TRT‑LLM。若后端扩展（例如 新版 Cutlass）忘记更新此列表，会出现 **scale mismatch**。 | 量化误差或数值溢出。 | - 将 `global_sf_supported` 迁移至后端实现类的属性或通过 **capability query** 动态返回。 |
| **Marlin 兼容性** | 当硬件不支持原生 FP4，Marlin 会使用 **weight‑only FP4**，但 `input_dtype` 必须是非 8‑bit。若用户自行强制 `params_dtype=torch.int8`，会触发 `RuntimeError`。 | 启

---

### feat(moe): Add is_act_and_mul=False support for Triton MoE kernels (#31645)
**SHA**: `25eef3d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/25eef3dc2ec82ca6ae4a029b247791b72a75ce15)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：为 Triton 实现的 MoE（Mixture‑of‑Experts） kernels 新增 `is_act_and_mul=False` 支持，实现对非融合激活（如 `relu2_no_mul`、`silu_no_mul`、`gelu_no_mul`）的完整路径。修改包括配置结构、工作空间计算、激活实现、平台兼容性检查以及对应单元测试。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/fused_moe/config.py`（新增 `is_act_and_mul` 字段及其传播）  
- `fused_batched_moe.py`、`fused_moe.py`、`layer.py`（工作空间尺寸、激活调用、平台校验）  
- `modular_kernel.py`（实现非融合激活的算子）  
- `unquantized_fused_moe_method.py`（量化配置生成逻辑）  
- 新增测试 `tests/kernels/moe/test_triton_moe_no_act_mul.py`  
- 受影响的 CUDA 与 ROCm（在 ROCm 关闭 AITER 时）运行时路径  

**🔍 技术洞察**：

- **架构影响**  
  - 在 `FusedMoEQuantConfig` 中加入 `is_act_and_mul`，将激活与门乘的融合状态显式化，所有 MoE 相关模块都需读取该标志以决定中间尺寸 (`intermediate_size`)。  
  - 工作空间计算从 `N//2`（SwiGLU 融合）切换为 `N`（非融合）或保持 `N//2`，确保内存分配正确且不产生越界。  
  - `layer._get_quant_method` 的平台检查从 “仅 CUDA” 放宽到 “CUDA 或 ROCm（关闭 AITER）”，提升跨平台可用性。  
  - `modular_kernel.activation` 中加入纯 PyTorch 实现的 `*_no_mul` 分支，避免调用自定义 C++ fused算子，从而在 Triton 路径上完整支持这些激活。  

- **性能影响**  
  - **正向**：在不需要门乘的模型（如 Nemotron‑H）上，省去一次乘法和一次降维/升维的内部拷贝，理论上可削减内存带宽占用。  
  - **负向**：非融合激活使用逐元素 `torch.sigmoid`、`torch.erf`、`torch.clamp` 等普通算子，相比专门的 fused kernel（一次内核完成两步）会产生额外的内核调度和潜在的临时张量（虽已使用 `out=`/in‑place），导致延迟略增。实际影响视 expert 大小、batch规模以及 GPU 型号而定，需通过基准测试确认。  
  - 新增的工作空间计算略微改变内存布局，可能对缓存行为产生轻微影响。  

- **安全考虑**  
  - 变更仅涉及算子实现和内存尺寸计算，无外部输入校验或权限逻辑，未引入新的安全风险。  
  - 仍需关注 `torch.erf` 与 `torch.sqrt` 的数值稳定性，特别是在极端 `dtype`（bfloat16）下的 NaN/Inf 产生，已有单元测试覆盖基本检查。  

**⚠️ 潜在风险**：  
1. **维度不匹配**：`intermediate_size` 计算错误会导致 workspace 维度不匹配，引发运行时 `RuntimeError: expected … but got …`。  
2. **数值误差**：在低精度（bf16、fp16）下手动实现的 Gelu、ReLU² 可能产生略微不同的数值，与原 fused kernel 结果不完全一致。  
3. **平台差异**：ROCm 环境需要显式关闭 AITER；若用户未设置相应 env，仍会触发 `NotImplementedError`。  
4. **性能回退**：对已有模型默认使用 fused 激活的代码路径没有影响，但如果误将 `is_act_and_mul=False` 用于原本设计为 fused 的模型，可能导致不必要的性能下降。  
5. **测试覆盖不足**：当前仅在 CUDA/ROCm（AITER 关闭）上做了功能验证，未覆盖所有量化配置（例如 FP8、NvFp4）下的路径。  

**💡 关注建议**：  
- **代码层**：在所有创建 `FusedMoEQuantConfig` 的地方显式传递 `is_act_and_mul`，防止默认 `True` 引入隐式错误。  
- **文档**：更新 README / API 文档，说明 `is_act_and_mul` 的含义、何时使用以及在 ROCm 环境下的配置方法（`VLLM_ROCM_USE_AITER=0`）。  
- **基准**：在主要的模型（如 Nemotron‑H、Llama‑2）上跑一次性能基准，量化 `fp16/bf16` 与 `fp32` 两种激活路径的吞吐量、显存占用差异，确保性能回退在可接受范围内。  
- **持续集成**：在 CI 中加入对 ROCm（AITER 关闭）平台的模拟测试，或者利用容器化的 ROCm 环境验证 `disable_aiter_on_rocm` fixture 的有效性。  
- **回滚方案**：如果在生产环境观察到显著性能下降，可通过 `quant_config = FusedMoEQuantConfig.make(is_act_and_mul=True)` 快速恢复到 fused 路径，无需代码改动。  

--- 

*以上分析侧重技术细节和风险评估，帮助开发者在引入 `is_act_and_mul=False` 支持后快速定位影响点并制定相应的验证与优化计划。*

---

### [MoE Refactor][15/N] Apply Refactor to Fp8 (#31415)
**SHA**: `5dcd7ef` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5dcd7ef1f219068e6b6be5b614bc43978f028651)

**🎯 变更类型**：架构变更 / 功能增强 / 重构  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 将 FP8 MoE（Mixture‑of‑Experts）实现迁移到全新的 *Modular Kernel* 框架，抽象出 `FusedMoEModularKernel`、`FusedMoEPrepareAndFinalize` 等核心类。  
- 引入 **FP8 MoE Backend 选型**（FlashInfer‑TRTLLM、FlashInfer‑CUTLASS、DeepGEMM、Marlin、TRITON、AITER），并在运行时根据平台特性、Tensor‑Parallel 大小、是否使用 LoRA 等自动决定后端。  
- 大幅删除旧的 `cutlass_moe_fp8`、`flashinfer_cutlass_moe_fp8`、`flashinfer_fused_moe_per_tensor_scale_fp8` 等散落函数，统一为 `mk.FusedMoEModularKernel` 调用。  
- 为 **DP/EP** 场景新增 `FallbackExperts`，在 SM100 上小 batch 时自动回退到 Triton。  
- 更新所有基准、评测、文档以及 `tests/evals/gsm8k` 的配置文件，以使用新的后端切换逻辑。  
- 对权重加载、量化策略（Per‑Tensor vs Per‑Channel）以及块量化（Block‑Quant）做了统一的后处理（如 DeepGEMM‑E8M0、Marlin‑shuffle、FlashInfer‑shuffle）。  
- 移除 `vllm.model_executor.layers.fused_moe.cutlass_moe.cutlass_moe_fp8` 接口，改为在 `cutlass_moe` 中仅保留 `CutlassExpertsFp8` 与 `CutlassBatchedExpertsFp8` 类。  

---

## 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构** | 1. **模块化抽象**：`modular_kernel` 成为 MoE 的统一入口，所有后端实现（CUTLASS、TRITON、DeepGEMM、FlashInfer、Marlin、AITER）只需实现 `FusedMoEPermuteExpertsUnpermute` 接口即可；代码复用度提升，后端切换更灵活。<br>2. **后端选型层**：`select_fp8_moe_backend` 将平台特性、环境变量、TP 大小、LoRA 支持等因素集中判断，避免过去在每个调用点重复硬编码。<br>3. **Fallback 机制**：在 SM100 小 batch 场景自动降级到 Triton，保证兼容性。 |
| **性能** | 1. **高吞吐后端优先**：在 SM90/SM100 上默认走 FlashInfer‑CUTLASS（或 TRTLLM），对吞吐友好；在不支持原生 FP8 的老 GPU（< SM89）自动走 Marlin‑W8A16，仍保持较高算子密度。<br>2. **DeepGEMM 支持**：在 `VLLM_USE_DEEP_GEMM` 且块量化满足对齐要求时，可使用 DeepGEMM‑E8M0，显著降低显存占用并提升 GEMM 带宽利用率。<br>3. **统一工作空间**：通过 `mk.FusedMoEModularKernel` 的 `workspace_shapes` 统一管理 CUDA‑Graph 缓冲区，减少重复分配开销。<br>4. **权重预处理**：对 FP8 权重进行一次性 `deepgemm_post_process_fp8_weight_block`、`prepare_fp8_moe_layer_for_marlin`、`align_fp8_moe_weights_for_fi`，在加载阶段完成布局转换，后续执行无需再做额外拷贝。 |
| **安全** | - 代码路径新增了对环境变量的读取（`VLLM_USE_FLASHINFER_MOE_FP8`、`VLLM_MOE_USE_DEEP_GEMM` 等），若被恶意或误配置为不受支持的组合（如在 SM100 上开启块量化 + FlashInfer‑CUTLASS），会在 `select_fp8_moe_backend` 抛出 `ValueError`，防止产生错误的 kernel 调用。<br>- 所有新实现均走 `torch.ops.vllm.*` 的安全包装，未引入直接的 C/C++ 内存操作。 |
| **可维护性** | - **删除冗余函数**：老的 `cutlass_moe_fp8`、`flashinfer_*` 接口已彻底移除，代码基线更简洁。<br>- **统一量化配置**：`make_fp8_moe_quant_config` 负责根据后端生成 `FusedMoEQuantConfig`，后续若新增后端，仅需在该函数里补全即可。<br>- **测试覆盖**：大幅新增 `gsm8k` 多配置文件，覆盖 DP、EP、不同后端组合，降低回归风险。 |
| **兼容性** | - 现有模型 checkpoint（FP8‑serialized）仍可加载：`process_weights_after_loading` 会在运行时转换为对应后端格式（Marlin shuffle、FlashInfer rotation、DeepGEMM UE8M0 等）。<br>- 通过 `self.fp8_backend` 保存后端选择，后端切换对外层 API（`apply`、`select_experts`）保持透明，用户无需改动推理脚本。 |
| **依赖** | 新增对 `flashinfer`、`deep_gemm`、`rocm_aiter_ops`（ROCm） 的可选依赖检查；如果未安装相应库，`select_fp8_moe_backend` 会安全退化到 Triton/Marlin。 |

---

## ⚠️ 潜在风险

| 风险点 | 说明 | 严重度 |
|--------|------|--------|
| **后端不匹配** | 在某些极端配置（如 TP≥8 且未手动开启 `VLLM_MOE_USE_DEEP_GEMM`），仍会回退到 Triton，可能导致性能下降。| 中 |
| **权重布局转换错误** | `prepare_fp8_moe_layer_for_fi`、`prepare_fp8_moe_layer_for_marlin` 等在稀疏/块量化的边界（非 128×128）可能产生维度不匹配，触发运行时断言。| 中 |
| **环境变量冲突** | 同时开启 `VLLM_USE_FLASHINFER_MOE_FP8=throughput` 与 `VLLM_MOE_USE_DEEP_GEMM=1`，在 SM100 上会报错（当前不支持块量化 + FlashInfer‑CUTLASS）。| 低 |
| **AITER 后端** | 只在 ROCm 平台且 `VLLM_ROCM_USE_AITER_MOE=1` 时可用，尚未在 CI 完全覆盖，可能出现未捕获的 `AttributeError`。| 低 |
| **旧代码残留** | 部分旧 import（如 `cutlass_moe_fp8`）在其它未更新的子模块中仍可能被引用，导致 ImportError。| 低 |
| **GPU 驱动/CUDA 版本** | FlashInfer‑CUTLASS 需要 SM90/SM100 且对应的 CUDA 11.8+；在旧驱动上会降级为 Marlin，降低吞吐。| 中 |

---

## 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | 1. 在新增自定义 MoE 实现时，直接实现 `FusedMoEPermuteExpertsUnpermute` 并在 `select_gemm_impl` 中注册即可，无需手动修改 `apply` 逻辑。<br>2. 对于需要 **块量化** 的模型，确保 `VLLM_MOE_USE_DEEP_GEMM=1` 且平台支持 DeepGEMM，否则会自动回退并产生显存/性能下降。<br>3. 编写新后端（如 NVidia‑TensorRT‑LLM）时，只需提供 `FusedMoEModularKernel` 包装并在 `select_fp8_moe_backend` 中返回对应 `Fp8MoeBackend`。 |
| **测试/CI 维护** | 1. 为每个后端（FlashInfer‑CUTLASS、FlashInfer‑TRTLLM、Marlin、DeepGEMM、TRITON）添加 **smoke‑test**

---

### [EPLB] Optimize EPLB with numpy (#29499)
**SHA**: `6170d47` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6170d47d2220bc464fb561b93caf799a7560e304)

**🎯 变更类型**：功能增强 / 性能优化 / 重构 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 在 EPLB（Expert‑Parallel Load‑Balancing）模块中引入 `numpy` 加速数据处理，重构了专家权重迁移（`move_to_buffer` / `move_from_buffer`）以及调度函数 `transfer_layer`，实现了批量、向量化的专家索引映射与通信计划生成。  
- 新增 `EPLBConfig.log_balancedness_interval` 以及对应校验，支持更细粒度的平衡度日志输出。  
- 新增 `preserve_intragpu_slots` 策略，使同 GPU 上的专家在重新分配时尽量保留原有 slot，减少不必要的权重拷贝。  
- 对异步模式的状态结构体引入 `RecvMetadata`，统一收发元信息，降低 Python 对象创建开销。  
- 多处代码路径改为使用 `numpy`、`torch.cuda.Event` 计时，并对日志、异常检查进行强化。  

**🎯 影响范围**  
- `vllm/distributed/eplb/*`（核心 EPLB 逻辑、策略、状态、异步 worker）  
- `vllm/config/parallel.py`（EPLB 配置扩展）  
- 单元测试 `tests/distributed/*`（新增测试验证 GPU‑slot 保持逻辑）  

**🔍 技术洞察**  

- **架构影响**  
  - **状态模型**：`EplbModelState` 中的布尔列表被 `np.ndarray` 替代，`RecvMetadata` 统一封装 remote‑receive 信息，提升了序列化/进程间共享的可预测性。  
  - **通信计划**：`get_ep_ranks_with_experts_batch` 采用一次性批量查询取代原先逐 expert 循环，显著降低了跨进程调度的 Python 计算开销，符合分布式计算中 “把控制平面搬到 CPU/NumPy” 的最佳实践。  
  - **策略层**：`DefaultEplbPolicy.preserve_intragpu_slots` 在层次化负载均衡后进行后处理，保持专家在同 GPU 上的 slot 不变，减少了 downstream `move_from_buffer` 的拷贝次数，对整体 pipeline 的稳定性有正面作用。  
  - **日志与监控**：新增 `log_balancedness_interval` 与对应验证，使用户可以在不产生额外通信开销的前提下更频繁观察平衡度趋势，提升运维可观测性。  

- **性能影响**  
  - **CPU 端**：大量原本的 `for`/`list`/`dict` 操作被 `numpy` 向量化（如 `np.unique`, `np.isin`, `np.lexsort`），预计在专家数量（数千至数万）时减少 30%–70% 的调度时间。  
  - **GPU 端**：  
    - `move_to_buffer` 只在必要的 slot 上执行拷贝，`eligible_local_buffer_mask` 的向量化判断避免了逐元素的 `if` 检查。  
    - 使用 `torch.cuda.Event` 计时代替 `time.perf_counter()`，提供毫秒级精准度并兼容 CUDA 同步模型。  
    - `rearrange_expert_weights_inplace` 通过一次性分配 `weights_buffer`，而不是每层重新创建，降低显存碎片和分配开销。  
  - **通信**：批量生成的 `P2POp` 列表更紧凑，减少了 `torch.distributed` 调用次数；发送/接收的 rank 划分采用整数除法和向量化切片，降低了网络拓扑计算的延迟。  

- **安全考虑**  
  - 新增的模型校验 `_validate_eplb_config` 检查 `log_balancedness_interval` 正数以及 async 只在默认策略下可用，防止错误配置导致资源炸裂或 dead‑lock。  
  - 所有对外暴露的 `numpy` 数组在转换回 `torch` 前均使用 `.copy_`（非阻塞）进行显式同步，未出现未初始化内存读取的风险。  
  - 代码路径未引入外部文件读写或系统调用，安全面影响有限。  

- **可维护性**  
  - 引入 `RecvMetadata` 明确了远程接收的输入/输出边界，提升了函数签名的可读性。  
  - `preserve_intragpu_slots` 的实现逻辑相对复杂，但已被封装为单独方法，并配有完整单元测试，后续维护只需关注该函数的输入/输出约定。  
  - 迁移到 `numpy` 虽提升性能，但也引入了额外的依赖（`numpy` 已是项目的必装依赖），团队需确保在构建环境中保持兼容的版本（>=1.26）。  

**⚠️ 潜在风险**  

| 风险点 | 描述 | 严重度 | 缓解措施 |
|--------|------|--------|----------|
| **NumPy 与 Torch 同步错误** | `move_to_buffer` 中先进行 `np` 计算再转回 `torch`，若在异步模式下 `cuda_stream` 未同步可能导致数据竞争。 | 中 | 在所有 `torch.cuda.stream` 使用后已经显式 `torch.cuda.synchronize()`，但建议在异步路径加入 `stream.wait_event` 检查。 |
| **GPU‑slot 保持逻辑误判** | `preserve_intragpu_slots` 假设 `num_ranks` 与 `slots_per_gpu` 不变；若在 同步/异步 重排过程中 GPU 数或 slot 数变化（例如弹性缩放），可能导致映射错位。 | 中 | 在调用前加入断言或参数检查，确保 GPU 数与 slot 数未变；如果未来支持弹性，可在策略里做兼容转置。 |
| **远程接收元数据大小** | `RecvMetadata.recv_primary_mask`、`recv_expert_ids` 等使用 `np.ndarray`；在极端模型（上万 experts）下会占用显存/CPU 内存。 | 低 | 目前这些数组规模为 `num_local_experts`（每 GPU 常数），可接受；如需扩展到更大规模，可改为稀疏表示或 bit‑mask压缩。 |
| **异常路径未覆盖** | 新增 `log_balancedness_interval` 校验，若用户在旧版本配置文件中仍使用 `log_balancedness`（已删除），会触发 `ValueError`。 | 低 | 文档已更新；可在 config 读取阶段提供向后兼容警告。 |
| **测试覆盖不足** | `preserve_intragpu_slots` 只在单 GPU/多 GPU 场景做了基础验证，未覆盖异常如 `num_ranks` 与 `num_phy_experts` 不整除的情况。 | 中 | 建议补充 edge‑case 单测，或在函数入口对除余进行 assert。 |

**💡 关注建议**  

1. **监控与基准**  
   - 在生产环境开启 `log_balancedness_interval=1`（或合理的间隔），收集每轮重排的平衡度、GPU‑slot 保持率以及 `move_to_buffer`/`move_from_buffer` 的耗时，确认新实现相对于旧实现提升 ≥30%。  
   - 使用 `torch.cuda.Event` 采集的 GPU 时间对比 CPU `time.perf_counter()`，验证计时精度。  

2. **代码审查点**  
   - 确认所有 `numpy` 计算的 dtype 与 `torch` 端匹配（int64 vs int32），防止隐式截断导致负数索引错误。  
   - 检查 `ep_group` 的 `rank` 与 `global_rank` 对映关系在异构集群（如多节点混合 NVLink/PCIe）下是否仍满足 `global_rank = ep_rank * local_cnt + local_idx` 的假设。  

3. **部署验证**  
   - 在多节点（≥2）环境进行全流程压测，特别关注 `

---

#### 🟡 中重要度变更 (17)

### [Model] Support IQuestCoder model (#31575)
**SHA**: `fe86be6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fe86be66c555af2110f65318d006dc94f2bba2d2)

**🎯 变更类型**：功能增强（新增 IQuest Coder 系列模型）  
**⚡ 重要程度**：🟡 中——为 vLLM 引入全新模型的实现，影响模型注册、权重加载与推理路径，但对现有模型的运行保持向后兼容。  

**📋 变更摘要**  
- 在模型文档中加入 `IQuestCoderForCausalLM` 与 `IQuestLoopCoderForCausalLM` 条目。  
- `tests/models/registry.py` 增加对应的在线可用性检查。  
- `vllm/model_executor/models/registry.py` 注册两条新模型：`IQuestCoderForCausalLM` 暂挂到现有 `LlamaForCausalLM` 实现，`IQuestLoopCoderForCausalLM` 指向新实现 `iquest_loopcoder.IQuestLoopCoderForCausalLM`。  
- 新增 `vllm/model_executor/models/iquest_loopcoder.py`：完整的 Loop‑Coder 推理模型实现，包括循环注意力、门控投影、层结构、权重加载逻辑以及 `torch.compile` 支持。  

**🎯 影响范围**  
- **模型注册系统**：`registry.py`、`tests/models/registry.py`  
- **模型实现**：全新 `iquest_loopcoder.py`（约 600 行）以及对 `IQuestCoderForCausalLM` 的占位注册。  
- **文档**：`docs/models/supported_models.md`  

**💡 关注建议**  

| 关注点 | 说明 | 建议 |
|---|---|---|
| **实现完整性** | `IQuestLoopCoderModel` 采用循环注意力 (`loop_num` 默认 2) 并在第二轮使用门控 `LoopGateProjection` 将全局 KV 与局部 KV 混合。 | • 确认 `config` 中必须包含 `loop_num`、`loop_window_size`、`dual_chunk_attention_config`（可选）并在 `VllmConfig` 的验证阶段做默认填充。<br>• 为防止用户忘记配置，建议在 `__init__` 中加入 `assert hasattr(config, "loop_num")` 类的提示。 |
| **权重映射** | `load_weights` 里使用 `stacked_params_mapping` 处理 q/k/v 合并权重，同时有专门分支处理 `gate_projections`。 | • `gate_projections` 参数的命名在 HF 权重中为 `gate_proj.weight` / `.bias`，但映射中使用 `gate_up_proj` → `gate_proj`（容易混淆）。建议在注释中明确两者对应关系，或直接使用 `gate_proj` 关键字匹配。<br>• `gate_up_proj` 被误加入 `stacked_params_mapping`，但在此模型里并不存在该层，可能导致误加载。 |
| **并行切分** | `LoopGateProjection` 使用 `ColumnParallelLinear`（`gather_output=False`），输入是 `[num_heads, num_tokens, head_dim]` → 输出 `[num_tokens, num_heads * head_dim]`。 | • 检查 `head_dim` 与 `hidden_size // num_attention_heads` 的一致性；在 Tensor‑Parallel 环境下每个 rank 只持有 `total_num_heads / TP` 列，需确保 `ColumnParallelLinear` 的 `output_size` 为 `total_num_heads` 而不是 `num_heads`。 |
| **滑动窗口** | 对于 `loop_idx > 0`，`CacheConfig` 被重新创建为 `sliding_window=self.loop_window_size`。 | • 当 `cache_config.sliding_window` 已经在全局配置中指定且模型本身也提供 `loop_window_size`，两者可能不一致。建议在 `__init__` 中校验一致性或给出优先级说明。 |
| **兼容性** | `IQuestCoderForCausalLM` 仍映射到 `"llama"` 的实现，属于占位。 | • 若未来实现真正的 `IQuestCoder`，需要在 `registry.py` 进行相应更改并更新文档标记（✅）。当前占位可能误导用户，建议在文档中标记为 “未实现 (placeholder)”。 |
| **测试覆盖** | 仅添加了模型注册的在线可用性检查，未验证前向推理或权重加载。 | • 添加单元测试：① 使用随机权重对 `IQuestLoopCoderModel` 进行一次前向（`loop_num=2`）并检查输出形状；② 检查 `gate_projections` 参数是否成功加载；③ 在 `torch.compile` 环境下跑一次确保装饰器不报错。 |
| **性能** | 循环注意力导致每层被执行两次（或更多），计算和显存开销约 `loop_num` 倍。 | • 在文档或 README 中明确说明该模型的显存/算力需求；如有必要，可提供 `--max-model-len` / `--gpu-memory-utilization` 参数的推荐值。 |
| **错误信息** | `LoopGateProjection.forward` 中对 `query` 维度做了多次 reshape，若输入不满足 `[num_heads, num_tokens, head_dim]` 会抛出断言。 | • 将断言信息改为更友好的错误提示，如 `"Expected query shape (num_heads, num_tokens, head_dim) but got {query.shape}"`，方便调试。 |

**总体结论**  
此次 PR 为 vLLM 引入了全新的 Loop‑Coder 推理模型，代码结构清晰、遵循现有层抽象（`Attention`、`RMSNorm`、并行线性层）。关键风险集中在权重映射的细节、并行切分的维度匹配以及缺乏针对新模型的功能测试。完成上述建议后，可在保持现有模型稳定性的前提下安全地将该功能公开给用户。

---

### [Voxtral] Fix speech transcription api (#31388)
**SHA**: `18d4e48` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/18d4e481d0ce8336f4e64da70f7b6076ad804971)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. `SpeechToTextConfig.max_audio_clip_s` 由 `int` 改为 `int|None`，`None` 表示不限制时长并取消自动切块。  
2. OpenAI 接口在生成文字时加入对 `None` 的判定，确保只能返回单块结果并正确计算 `start_time`。  
3. `voxtral` 与 `voxtral_streaming` 中的音频处理逻辑做了细化：  
   - 只在非流式（`TranscriptionFormat.STREAMING`）时进行 padding。  
   - 新增 `get_speech_to_text_config`、`get_generation_prompt`，统一从 tokenizer 获取采样率并构造离线转写请求。  
   - 对 mel‑feature、audio‑embedding 做左侧裁剪，保证维度可被 2 / 4 整除。  
4. `whisper` 编码器新增 `self.is_causal` 成员，统一对因果模型的 padding、位置嵌入和拼接方式的判断。

**🎯 影响范围**  
- `vllm/config/speech_to_text.py`  
- `vllm/entrypoints/openai/speech_to_text.py`  
- `vllm/model_executor/models/voxtral.py`、`voxtral_streaming.py`  
- `vllm/model_executor/models/whisper.py`  

**💡 关注建议**  
- 文档需更新：说明 `max_audio_clip_s=None` 的使用场景及内存风险。  
- 调用方若依赖旧的整数阈值，应检查兼容性或显式传入整数。  
- 新增的 `get_generation_prompt` 使用 `cached_tokenizer_from_config`，确保 tokenizer 能正确返回 `audio_encoder.audio_config`。  
- 测试用例应覆盖：无限时长音频、流式 vs 离线两种 `transcription_format`、因果 Whisper 模型。  
- 监控内存占用，防止超大音频导致 OOM。

---

### [MM Encoder]: Make MMEncoderAttention's `scale` takes effect properly  (#31950)
**SHA**: `2972a05` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2972a05473d64a21c7390a5676bbb07bd8d2c8da)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为 `MMEncoderAttention` 添加 `scale` 参数，使其在 SDPA/FA 路径上能够正确地乘以 `head_dim⁻⁰·⁵` 的缩放因子。相应地，`vit_attn_wrappers.py` 中的 Flash‑Attention、Torch‑SDPA 包装函数均加入 `scale` 参数并向底层调用传递。所有使用该注意力层的多模态模型（Dots OCR、Ernie45‑VL、GLM4、GLM‑ASR、Isaac、MoonViT、PaddleOCR‑VL、Qwen2‑VL/5‑VL）在构造时显式传入 `scale=hidden_size⁻⁰·⁵`。  

**🎯 影响范围**  
- `vllm/attention/layers/mm_encoder_attention.py`（核心注意力实现）  
- `vllm/attention/ops/vit_attn_wrappers.py`（Flash‑Attention、torch‑sdpa 包装）  
- 多模态模型入口文件（8+ 个 `model_executor/models/*.py`）  

**💡 关注建议**  
1. **兼容性**：确认自研的 `torch.ops.vllm.torch_sdpa_wrapper` 已支持新 `scale` 参数；若旧二进制仍被调用，需同步更新或保持向后兼容。  
2. **数值稳定性**：在 FP16/BF16 场景下验证 `scale` 是否导致溢出或精度下降，必要时在 `apply_sdpa` 中加入 `torch.clamp`。  
3. **性能回归**：运行完整的基准测试，比较加入显式缩放前后的吞吐与延迟，确保没有意外的性能损失。  
4. **文档 & 示例**：在模型初始化文档中标明 `scale` 参数的来源（`head_dim⁻⁰·⁵`），并示例如何自定义或关闭（传 `None`）。  
5. **单元测试**：为 `MMEncoderAttention` 的不同后端（SDPA、Flash‑Attention、Fake）添加覆盖 `scale` 参数的测试，防止未来回滚时遗漏。  

通过以上检查，可确保新缩放因子在所有后端生效且不影响模型正确性和运行效率。

---

### [Model] Standardize common vision encoders (#31947)
**SHA**: `5576227` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5576227bc132996d9e4ca279a8a8cf9e7b8a8ca4)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为所有视觉编码器引入统一的 `MultiModalConfig`，在构造函数链路中透传该配置，并依据 `mm_encoder_tp_mode`（“data”/默认）决定是否关闭 TP。相应地对 QKV/Linear、注意力层、MLP、VisionTower 的初始化、位置编码插值等实现做了细粒度的 `disable_tp` 控制，并统一了 `BaseDummyOptions` 的引用方式。  

**🎯 影响范围**：  
- `clip.py、siglip.py、pixtral.py、deepencoder.py、llava*、hyperclovax_vision.py、tarsier.py、phi3v.py、lightonocr.py、deepseek_ocr.py` 等所有使用视觉 encoder 的模型。  
- `MMEncoderAttention`、`QKVParallelLinear`、`Column/RowParallelLinear` 等并行层的 TP 行为。  
- 配置加载路径：`VllmConfig.model_config.multimodal_config`。

**💡 关注建议**  
1. **向后兼容**：`MultiModalConfig` 为可选参数，但多数调用已改为关键字传参，务必在 `VllmConfig` 中提供默认实例或在入口处加入兼容填充，防止未提供时出现 `NoneType` 错误。  
2. **TP 关闭逻辑**：当前在 `multimodal_config.mm_encoder_tp_mode == "data"` 时将 `disable_tp=True`，请确认在分布式数据并行（DDP）下 `tp_size` 被强制设为 1，避免与现有张量并行实现冲突。建议添加单元测试覆盖 `data`/`tensor` 两种模式。  
3. **文档/示例**：新增 `MultiModalConfig` 参数后，需要在模型快速使用教程、配置说明以及 `init_vision_tower_for_*` 文档中标注该参数的意义、默认值以及可选字段（如 `get_limit_per_prompt`）。  
4. **代码风格**：部分改动出现多余的换行或未使用的 import（如 `math`、`BaseDummyOptions`），可在后续清理。  
5. **性能验证**：关闭 TP 会导致显存/计算分布改变，建议在不同规模 GPU 环境下跑一次完整推理基准，确认性能下降在可接受范围内。  

总体而言，此次改动完成了视觉编码器的配置统一，为后续多模态模型的 TP 策略切换奠定基础，只需注意兼容性和测试覆盖即可安全合入。

---

### [Chore] Further cleanup pooler (#31951)
**SHA**: `d1b6fe0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d1b6fe007f1fd5afb5533a25dd188e1363a119ac)

**变更概述**  
本次提交对 `vllm.model_executor.layers.pooler` 进行了一轮清理：去掉了原来的 `PoolingType` Enum，改为直接使用字符串（如 `"CLS"、"MEAN"`）表示池化方式；相应地更新了 `PoolerConfig.pooling_type` 的注释、类型别名以及 `ResolvedPoolingConfig` 的实现。为配合此改动，所有依赖该 Enum 的代码（Bert、ModernBert、StepPooler 等）均改为使用字符串；输出类型也从 `TokensPoolerOutput` 重命名为 `TokenwisePoolerOutput`，并同步到 `vllm/v1/outputs.py`。测试用例相应修改了对 `pooling_type` 的断言。

**影响范围**  
- `vllm/config/pooler.py`、`vllm/model_executor/layers/pooler.py`、`vllm/v1/outputs.py` 的类型定义和文档。  
- 所有模型实现（Bert、ModernBert、可能的其他模型）以及 `StepPooler`、`AllPooler` 等具体池化类。  
- 单元测试 `tests/model_executor/test_model_load_with_params.py`、`tests/test_config.py`。  
- 任何外部代码直接引用 `PoolingType` Enum 将失效。

**关注建议**  
1. **迁移指南**：在发布说明中明确指出 `PoolingType` 已被废除，推荐使用字符串常量或 `vllm.config.pooler.PoolingTypeStr`。  
2. **向后兼容**：如果仍需兼容旧代码，可在 `pooler.py` 中加入一个轻量的别名 Enum（或 `try/except`），但在下一大版本中可以彻底移除。  
3. **文档更新**：更新配置文档、模型说明以及 API 参考，去除对 Enum 的描述，改为字符串示例。  
4. **类型检查**：确保项目的 mypy、pyright 等静态检查能够通过新的别名 `PoolingTypeStr`，并在 CI 中加入相应检查。  
5. **测试覆盖**：运行完整的单元/集成测试，尤其是涉及自定义池化头的模型，确认 `TokenwisePoolerOutput` 与旧 `TokensPoolerOutput` 的兼容性。  
6. **第三方插件**：如果有插件或外部工具使用 `PoolingType`，需要提前通知用户更新依赖或提供适配层。  

总体而言，此次清理提升了代码可读性和配置灵活性，但需要注意向后兼容和文档同步，以免使用旧 Enum 的用户在升级后遭遇 `AttributeError`。

---

### RayLLM Bugfix - Preserve obj store URL for multi engine_config creation (#30803)
**SHA**: `04a4966` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/04a49669d1be26b2a83441c1c5a968cf3131f0e4)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `ModelConfig` 中新增字段 `model_weights`，用于保存原始模型权重的对象存储 URI（如 RunAI），在模型已被拉取到本地后仍保留该信息。  
2. `EngineArgs`、`EngineConfig` 以及模型加载器相应地传递并使用 `model_weights`，实现「多 engine_config」创建时不被覆盖的需求。  
3. 在 `maybe_pull_model_tokenizer_for_runai` 中加入提前返回逻辑，避免已拉取模型再次触发拉取。

**🎯 影响范围**  
- `vllm/config/model.py`（ModelConfig 定义）  
- `vllm/engine/arg_utils.py`（EngineArgs、EngineConfig 参数传播）  
- `vllm/model_executor/model_loader/*`（RunAI 流式加载与分片加载）  

**💡 关注建议**  
- **兼容性**：新增字段默认空串，仍可兼容旧版调用，无需改动现有业务。请确认序列化（如 `json`、`pickle`）及 CLI 参数解析仍能正确处理。  
- **文档**：在官方文档与参数说明中补充 `model_weights` 的用途、取值方式及与 `model` 的关系，避免使用者误以为两者可随意替换。  
- **测试**：新增单元测试覆盖以下场景：① `model_weights` 为对象存储 URI 时，模型只拉取一次；② 多次创建 `EngineConfig` 时 `model_weights` 不被覆盖；③ `model_weights` 为空时仍使用 `model` 正常加载。  
- **错误处理**：若 `model_weights` 指向无效 URI，当前逻辑仍会尝试使用它加载权重，建议在 `load_weights` 前加入有效性检查或友好错误提示。  
- **性能**：本次改动仅在配置传递层面增加少量字符串拷贝，对整体吞吐无显著影响。  

总体而言，本次提交修复了在多 engine_config 场景下对象存储 URI 丢失的问题，改动集中且向后兼容。后续关注文档同步和相应测试覆盖即可。

---

### Decouple page_size_bytes calculation in AttentionSpec for TPU/RPA Compatibility. (#31635)
**SHA**: `b634e61` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b634e619bbcfed0abe4e01d0e2d97fb1fdfdbbd5)

**🎯 变更类型**：功能增强（TPU/RPA 兼容性）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `AttentionSpec` 及其子类新增 `page_size_padded` 字段，拆分 `page_size_bytes` 为 `real_page_size_bytes`（原有计算）和可选的填充尺寸。  
- `page_size_bytes` 现在先返回 `page_size_padded`（若提供），否则返回真实计算值。  
- 所有 `@dataclass` 改为 `kw_only=True`，并在合并逻辑中传播 `page_size_padded`。  
- 单元测试改为使用关键字参数构造 `FullAttentionSpec`/`SlidingWindowSpec`，确保新字段默认 `None`。  

**🎯 影响范围**  
- KV 缓存相关实现：`vllm/v1/kv_cache_interface.py`、`ChunkedLocalAttentionSpec`、`SlidingWindowSpec`、`MLAAttentionSpec`。  
- 依赖这些规格的调度器、前缀缓存、KV 共享等模块的初始化代码（多个测试文件）。  
- 任何直接实例化 `AttentionSpec` 子类且未使用关键字参数的外部代码（例如自定义插件或旧脚本）需要兼容 `kw_only` 调整。  

**💡 关注建议**  
1. **向后兼容**：`kw_only=True` 会导致位置参数调用失败，请在项目文档和示例中明确要求使用关键字参数；若有内部代码仍使用位置参数，需要同步修正。  
2. **默认行为**：`page_size_padded` 为 `None` 时保持原有大小，确保在未显式设置时不影响现有性能。建议在关键路径添加断言或日志，验证返回值等于 `real_page_size_bytes`。  
3. **合并逻辑**：`merge` 已传递 `page_size_padded`，但若不同 spec 的 `page_size_padded` 不一致，当前实现会取第一个的值。考虑在合并前检查一致性并抛出明确错误，以免出现隐藏的尺寸不匹配。  
4. **测试覆盖**：新增 `page_size_padded` 参数的正向与负向测试（如填充值小于真实大小的断言）可以防止未来回归。  
5. **文档更新**：在 KV 缓存、AttentionSpec 文档中说明 `page_size_padded` 的用途、取值约束以及在 TPU/RPA 场景下的推荐配置。  

总体来说，此次改动为 TPU/RPA 的页面大小对齐提供了灵活性，同时保持了既有行为。请重点检查关键路径的调用方式与合并时的尺寸一致性，以确保改动在生产环境的平滑迁移。

---

### [Models] Allow converting Qwen3-VL into Reranker model (#31890)
**SHA**: `eac3b96` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/eac3b96ec04d07a987823504671650a0bcad5a10)

**🎯 变更类型**：功能增强（新增 Qwen3‑VL Reranker 支持）

**⚡ 重要程度**：🟡 中

**📋 变更摘要**  
- 在模型清单与文档中加入 `Qwen3VLForSequenceClassification`，并提供使用 `--hf_overrides` 的示例。  
- 增添示例脚本（vision‑language reranker）和对应 Jinja 模板，实现文本‑图片/视频的离线打分。  
- 在 `tests/models/registry.py`、`vllm/model_executor/models/config.py`、`adapters.py`、`score_utils.py` 中加入对该架构的注册、配置校验与权重加载逻辑，支持 `classifier_from_token` 与 `is_original_qwen3_reranker` 标记的特殊处理。  

**🎯 影响范围**  
- `vllm/entrypoints/score_utils.py`、`vllm/model_executor/models/*`（配置、适配器、权重加载）  
- 文档 `docs/models/supported_models.md`  
- 示例代码 `examples/pooling/vision_language_reranker.py`、`examples/pooling/score/template/qwen3_vl_reranker.jinja`  
- 测试注册 `tests/models/registry.py`

**💡 关注建议**  
1. **配置兼容**：`hf_overrides` 中的 `architectures`、`classifier_from_token`、`is_original_qwen3_reranker` 必须在实际 HF 模型中保持一致，避免因模型版本变化导致加载错误。  
2. **权重加载路径**：`adapters.py` 对 `embed_tokens` 的获取已改为通过 `text_backbone`，请确保所有 Qwen3‑VL 变体的语言子模型都遵循同一接口，防止 `AttributeError`。  
3. **多模态输入**：已新增 `ChatCompletionContentPartVideoParam`，但后端处理仍以图片为例。若正式支持视频，需在 `mm_processor_kwargs` 中加入相应的预处理配置并在测试覆盖。  
4. **测试覆盖**：当前仅在注册表添加了模型信息，建议补充端到端的 `score` 单元测试，验证 `from_2_way_softmax` 与普通 `softmax` 两种 `method` 的分数计算是否正确。  
5. **文档同步**：示例启动命令使用了 `--hf_overrides`，请在 README 或快速开始指南中同步说明，以免用户忽略该必需参数。  

总体而言，新增 Qwen3‑VL Reranker 功能完整，兼容性处理得当。后续关注模型版本迭代时的配置字段变更以及视频模态的完整实现即可。

---

### fix(rocm): Add get_supported_kernel_block_sizes() to ROCM_ATTN (#31712)
**SHA**: `107cf8e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/107cf8e92f88678712e3aa207f054243e56df330)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 `rocm_attn.py` 中为 `RocmAttentionBackend` 新增静态方法 `get_supported_kernel_block_sizes()`，返回 ROCm 分页注意力实现所支持的块大小列表 `[16, 32]`，并在注释中说明受 AMD GPU LDS 限制的原因。

**🎯 影响范围**  
- `vllm/v1/attention/backends/rocm_attn.py`（ROCm 注意力后端）  
- 可能间接影响 `AttentionBackend` 的调度逻辑、模型初始化以及任何依赖 `get_supported_kernel_block_sizes()` 判断块大小的上层代码或文档。

**💡 关注建议**  
1. **代码路径**：检查项目中是否已有对 `AttentionBackend.get_supported_kernel_block_sizes()` 的统一调用。如果没有，需要在 `AttentionBackend` 抽象基类或调度逻辑中加入对 ROCm 后端的统一查询，以避免硬编码块大小导致的运行时错误。  
2. **兼容性**：该改动只限制了 ROCm 注意力的块大小，不影响其他后端。但在多后端切换时，务必在 `vllm` 启动参数或配置中提示用户仅支持 16/32 的块大小，防止因不匹配导致 kernel 启动失败。  
3. **测试**：新增单元测试以验证 `RocmAttentionBackend.get_supported_kernel_block_sizes()` 返回 `[16, 32]`，并在已有的 ROCm 注意力测试中加入块大小校验，确保在不支持的块大小（如 24）时抛出明确错误。  
4. **文档**：在 ROCm 注意力后端的 README 或 API 文档中补充 “Supported kernel block sizes: 16, 32”，并解释 LDS 限制的根源，帮助用户在模型调优时作出正确选择。  
5. **性能评估**：若将来计划支持更多块大小，需要评估 AMD GPU LDS 使用率和共享内存占用，确保新块大小不会触发 kernel 启动失败。  

通过以上检查与补充，可保证该修复在保持向后兼容的同时，提供更明确的使用指引，降低用户在 ROCm 环境下遭遇的运行时错误概率。

---

### [Fix] Enable mm_processor_cache with vision LoRA (#31927)
**SHA**: `d3235cb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d3235cb503e33bff5f3b0ee11ba92dd3337d2a15)

**🎯 变更类型**：功能增强（在多模态处理器缓存中加入 LoRA 共享）  
**⚡ 重要程度**：🟡 中（影响缓存命中率和内存占用，但不改变模型推理核心逻辑）  

**📋 变更摘要**  
1. 为 `MultiModalFeatureSpec` 添加 `mm_hash` 字段，用于在缓存时把不同 LoRA 实例的同一模态数据映射到统一的缓存键。  
2. `vllm/multimodal/cache.py` 改为先以 `mm_hash`（若存在）作为缓存键，再回退到原来的 `identifier`，实现 LoRA 之间的缓存共享。  
3. `vllm/v1/engine/input_processor.py` 在构造 `MultiModalFeatureSpec` 时传递 `base_mm_hash` 并填充 `mm_hash`。  
4. 取消 `arg_utils.create_engine_config` 中对 `enable_tower_connector_lora` 与 `mm_processor_cache_gb` 不兼容的硬性检查。  
5. 新增单元测试 `test_processor_cache_shared_across_loras` 验证缓存共享行为。  

**🎯 影响范围**  
- **vllm.multimodal.inputs**（结构体字段变更）  
- **vllm.multimodal.cache**（缓存键逻辑）  
- **vllm.v1.engine.input_processor**（特征实例化）  
- **vllm.engine.arg_utils**（配置校验移除）  
- **tests.multimodal**（新增测试）  

**💡 关注建议**  
1. **兼容性**：`mm_hash` 为 `None` 时仍保持老行为，确保已有模型/LoRA 不受影响。请检查所有调用 `MultiModalFeatureSpec` 的路径是否已显式传递 `mm_hash`，防止误用 `identifier` 导致键冲突。  
2. **键冲突风险**：如果不同模态共享同一 `mm_hash`（例如误用了通用哈希），可能导致错误的缓存复用。建议在生成 `mm_hash` 时加入模态或模型前缀，以保证唯一性。  
3. **内存回收**：缓存键变化后，旧键的条目在缓存淘汰机制中仍会被触碰吗？确认 `touch_receiver_cache_item` 对新旧键都能正确计数，防止泄漏。  
4. **并发安全**：`touch_receiver_cache_item` 与 `get_and_update_item` 已在多进程/线程环境下使用，新增的 `mm_hash` 逻辑不会改变锁粒度，但建议在高并发压力测试中观察是否出现竞争。  
5. **配置文档**：`mm_processor_cache_gb` 现在可以和 `enable_tower_connector_lora=True` 共存，请在官方文档或示例中同步更新说明。  

总体来看，此次改动在不破坏现有接口的前提下，提升了 LoRA 场景下的缓存复用率，对推理吞吐有积极作用。后续关注上述兼容性与内存回收细节即可。

---

### [BugFix] Fix spec decoding edge case bugs (#31944)
**SHA**: `287b37c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/287b37cda421f5e037e3b31fe330bfc4908faeaf)

**变更类型**：Bug 修复（针对 speculative decoding 边缘情况）  
**重要程度**：🟡 中  

**核心改动**  
1. **scheduler.py**：在请求被抢占后额外 `request.spec_token_ids.clear()`，避免残留的 speculative token 影响后续调度。  
2. **gpu_input_batch.py**：新增 `update_req_spec_token_ids`，统一在 batch 中维护 `spec_token_ids`，并在 async‑schedule 场景下只写占位，实际 token 会在 `_prepare_input_ids` 中覆盖。  
3. **gpu_model_runner.py**：  
   - 读取 `scheduler_output.scheduled_spec_decode_tokens` 并统一交给 `input_batch.update_req_spec_token_ids` 处理。  
   - 在 async 调度下仅在 `prev_num_draft_len > 0` 时更新 `num_computed_tokens`，防止错误计数。  
   - 精简了对 `num_decode_draft_tokens` 的赋值逻辑，只在已有 Prompt 后才记录 draft 长度。  
   - 新增在新增/恢复请求时调用 `update_req_spec_token_ids`，确保批次状态完整。  

**影响范围**  
- 调度器（`v1/core/sched/scheduler.py`）  
- GPU 输入批处理层（`v1/worker/gpu_input_batch.py`）  
- GPU 模型运行器（`v1/worker/gpu_model_runner.py`）  

**关注建议**  
- **功能验证**：在使用结构化输出 + speculative decoding 的 async 场景下跑全链路测试，确认抢占、恢复、以及 spec token 被正确清空/覆盖。  
- **回归风险**：`prev_num_draft_len` 的更新逻辑改变后，需检查普通（非 async）路径的 token 计数是否仍准确。  
- **性能**：`update_req_spec_token_ids` 只在必要时写占位，建议在 profiling 中确认未引入额外拷贝。  
- **文档/注释**：补充对 `spec_token_ids`、`prev_num_draft_len` 在 async 与 sync 场景下语义的说明，降低后续维护成本。  

整体来看，此次修改关闭了因 scheduler 丢弃不符合 schema 的 draft token 导致的 spec token 残留与计数错误，实现较为完整且对现有接口影响有限。建议在 CI 中加入上述 edge‑case 场景的回归测试。

---

### fix(rocm): add early return in get_flash_attn_version for ROCm (#31286)
**SHA**: `39d8200` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/39d82005f7a58fed901526a82bd83ff91a60ea0c)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 `vllm/attention/utils/fa_utils.py` 中为 ROCm 平台添加提前返回：若检测到 ROCm，则直接返回 `None`，避免后续尝试导入 `vllm_flash_attn` 并传递 `fa_version` 参数。  

**🎯 影响范围**  
- `vllm/attention/utils/fa_utils.py`（`get_flash_attn_version`）  
- 所有调用该函数的注意力实现路径（例如 `attention`、`modeling` 中的 FlashAttention 参数构造）  

**💡 关注建议**  
1. **确认调用方容错**：检查所有使用 `get_flash_attn_version` 的代码是否已能安全处理返回 `None`（即不再向后端传 `fa_version`），防止出现 `TypeError`。  
2. **平台判定顺序**：`current_platform.is_xpu()` 与 `is_rocm()` 互斥，目前逻辑先返回 XPU=2 再判 ROCm，建议在文档或注释中说明两者不会同时为真。  
3. **测试覆盖**：在 ROCm 环境（或模拟 `is_rocm=True`）下运行完整的单元/集成测试，确保模型加载、推理均不因缺失 FlashAttention 而崩溃。  
4. **回退行为**：确认在 ROCm 上不启用 FlashAttention 后，仍能使用其他实现（如标准注意力）并保持性能预期。  
5. **文档更新**：在 README/兼容性说明中标注 ROCm 暂不支持 `vllm_flash_attn`，并指明已自动回退。  

该改动简化了 ROCm 环境的启动流程，避免了因缺失 `vllm_flash_attn` 包导致的 ImportError。确保上述建议落实后，可安全合并。

---

### [0/N][Attention] Fix miscellaneous pre-commit issues (#31924)
**SHA**: `0d76674` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0d7667419f738e44ce7f9bf311987e15b01970b0)

**🎯 变更类型**：Bug 修复 / 代码清理（pre‑commit 规范）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交主要针对 Attention 子模块的导入、设备特性判断以及 Triton‑kernel 写法进行细节修正，统一使用 `current_platform.is_device_capability_family` 替代直接索引 `get_device_capability()`，并消除 IDE/linters 报错（如 type‑ignore、未使用的别名）。同时对 `grid`、`VALID_BLOCK_H` 等局部变量做了更明确的命名与简化。

**🎯 影响范围**：  
- `vllm/attention/layers/static_sink_attention.py`（后端创建）  
- `vllm/attention/ops/*`（flashmla、paged_attn、prefix_prefill、triton_*）  
- `vllm/attention/utils/fa_utils.py`（flash‑attn 参数入口与 FP8/MLA 支持判定）  
- 相关平台分支：CUDA、ROCm、XPU。

**💡 关注建议**  
1. **设备特性判断**：确认 `is_device_capability_family(90/100)` 在所有支持的驱动/硬件上返回预期值，避免因平台实现差异导致 FlashMLA/FA‑FP8 判定错误。  
2. **ops 别名**：`_custom_ops` 与 `ipex_ops` 通过 `ops = …` 重新赋值后仍保持原有 API，建议在 CI 中加入 import‑path 检测，防止未来改动破坏兼容性。  
3. **Triton kernel**：`grid_fn` 的命名仅在内部使用，确保外部未依赖原 `grid` 名称；同样 `VALID_BLOCK_H` 的三元表达式已简化，建议运行完整的性能基准以确认无回归。  
4. **类型忽略**：`# type: ignore[arg-type]` 已限于单行，若后续改动触发类型错误，请适时补全类型注解。  

总体而言，此次改动提升了代码可维护性和平台检测的鲁棒性，但应通过针对 Hopper/Blackwell GPU 的单元/集成测试验证功能 gating 与性能保持一致。

---

### [Kernel] Support bias type in grouped_topk kernel (#31781)
**SHA**: `0ada960` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0ada960a20766fa1de9063576979f9d807583785)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 **grouped_topk** CUDA kernel 中引入了 *BiasT* 泛型，实现了 bias 与主数据类型（float、half、bfloat16）解耦。核心实现包括：  
1. 将 `topk_with_k2`、`topk_with_k2_kernel`、`group_idx_and_topk_idx_kernel` 等函数签名从 `T const* bias` 改为 `BiasT const* bias`，并在相加前使用 `static_cast<T>` 完成类型转换。  
2. 扩展 `invokeNoAuxTc`、实例化宏以及 Python 层 `grouped_topk` 的分支，使其能够根据 `bias.dtype` 动态选择对应模板实例（float/half/bfloat16）。  
3. 调整测试用例，使输入张量与 bias 的 dtype 可以独立指定，验证了 bias 为 float 时对 float/half/bfloat16 输入的兼容性。  
4. 在 `fused_moe` 中去掉对 bias 的强制 `to(gating_output.dtype)`，依赖底层 kernel 自动完成类型转换。

**🎯 影响范围**  
- `csrc/moe/grouped_topk_kernels.cu`（核心 MOE top‑k 计算）  
- `vllm/moe/kernels.cu`（封装调用）  
- `vllm/model_executor/layers/fused_moe/fused_moe.py`（Python 接口）  
- 单元测试 `tests/kernels/moe/test_grouped_topk.py`  

**💡 关注建议**  
1. **精度检查**：bias 为 half 时会被提升为主数据类型再相加，若输入为 float，可能出现精度微弱损失，请在数值敏感的模型中做好对比验证。  
2. **兼容性**：老版本代码仍可运行（bias 与 scores 同 dtype），但新编译的二进制将包含更多模板实例，编译时间和二进制体积会略增。  
3. **使用指引**：用户现在可直接传入 `torch.float32`、`torch.float16` 或 `torch.bfloat16` 的 bias，无需手动 `to(dtype)`，但请确保 bias 与 `scores` 位于同一 CUDA 设备上。  
4. **测试覆盖**：建议在不同 GPU 架构（如 < SM90）上跑一遍完整测试，以确认 `static_cast` 在低层硬件上不会触发隐式同步或性能回退。  

总体而言，此改动提升了 API 灵活性，降低了用户在准备 bias 时的显式类型转换工作量，对模型的数值行为影响有限，值得接受。

---

### [refactor] refactor memory constants usage (#31865)
**SHA**: `c907d22` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c907d22158dbf31508e73e89cb2bb272bb59e37e)

**🎯 变更类型**：重构 / 代码维护  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次 PR 将项目中直接使用 `GiB_bytes`（以及 `MiB_bytes`）进行数值除法的方式统一改为使用新工具函数 `format_gib`（或 `format_mib`），并在必要场景下对换算结果做 `math.ceil` 取整。通过引入 `format_gib`，日志输出和错误信息中的内存大小统一为保留两位小数的字符串，提升可读性并避免重复实现除法/四舍五入逻辑。同时在 `vllm/config/cache.py`、`vllm/multimodal/cache.py`、`vllm/platforms/cpu.py`、`vllm/v1/core/kv_cache_utils.py`、`vllm/v1/worker/gpu_*` 等多个核心模块完成了相应修改。

**🎯 影响范围**  
- **配置验证** (`vllm/config/cache.py`) – swap space 换算及日志。  
- **多模态缓存大小统计** (`vllm/multimodal/cache.py`) – 调试日志。  
- **CPU 平台 KV cache 默认空间** (`vllm/platforms/cpu.py`) – 环境变量提示。  
- **KV cache 内存检查** (`vllm/v1/core/kv_cache_utils.py`) – 报错信息、自动调节提示。  
- **GPU 模型加载与 worker 日志** (`vllm/v1/worker/gpu_*`) – 模型加载、sleep、内存划分日志。  
- **公共工具** (`vllm/utils/mem_utils.py`) – 新增 `format_gib`/`format_mib` 实现。  

**💡 关注建议**  
1. **兼容性检查**：`format_gib` 返回 `str`，而部分旧代码仍期待 `float`（例如可能用于数值比较的地方）。请确认所有调用已改为仅用于日志或格式化，避免因类型不匹配导致运行时错误。  
2. **单元测试**：新增或更新对应的测试，确保 `format_gib` 在极端大数（>TB）时仍保持精度，并验证 `math.ceil` 换算不会导致误报内存不足。  
3. **文档同步**：在项目文档和开发者指南中说明 `GiB_bytes` 仍为底层常量，推荐使用 `format_gib`/`format_mib` 进行人类可读的展示，以防后续误用。  
4. **性能影响**：`format_gib` 只在日志/错误路径调用，对关键路径性能无影响；但在高频循环中使用仍需谨慎。  

总体而言，此次重构提升了代码可维护性和日志一致性，风险主要在类型兼容性上，建议通过 CI 检查和适当的单元测试进行验证。

---

### [KVConnector]: Enable Cross-layers KV cache layout for MultiConnector (#30761)
**SHA**: `b89443b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b89443b8d9c75e6e393a320103c1b6fbd2b89c68)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 KVConnector 引入 `prefer_cross_layer_blocks` 抽象属性，默认返回 `False`，并在 `OffloadingConnector` 中实现为 `True`。  
2. `MultiConnector` 通过该属性统一判断所有子连接器是否都偏好跨层 KV 块；新增 `prefer_cross_layer_blocks` 只要所有子连接器返回 `True` 即为 `True`。  
3. 新增 `register_cross_layers_kv_cache` 方法，MultiConnector 会把跨层 KV 缓存注册到每个子连接器。  
4. 单元测试补充了跨层偏好判断的场景，使用 `MockCrossLayerConnector` 验证了全为 `True` 与混合 `False` 的行为。  

**🎯 影响范围**  
- `vllm/distributed/kv_transfer/kv_connector/v1/base.py`（抽象属性实现）  
- `vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py`（属性聚合、跨层缓存注册）  
- `vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py`（实现为 `True`）  
- 相关单元测试 `tests/v1/kv_connector/unit/test_multi_connector.py`  

**💡 关注建议**  
- **自定义连接器**：若项目实现了自己的 KVConnector，需要实现 `prefer_cross_layer_blocks` 属性（返回 `bool`）并可选实现 `register_cross_layers_kv_cache`，否则在 MultiConnector 场景下会被视为不支持跨层布局。  
- **向后兼容**：旧实现仍然可用，因为默认属性返回 `False`，但使用 MultiConnector 时跨层优化将不生效。建议在迁移期间在测试环境验证性能变化。  
- **文档 & 类型**：更新官方文档说明 `prefer_cross_layer_blocks` 的语义及 `register_cross_layers_kv_cache` 的调用时机；确保类型注解使用 `AttentionBackend` 对应的类对象。  
- **性能监控**：在开启跨层 KV 布局后，观察 KV 传输时延和显存占用，防止因层数不匹配导致异常。  

总体来说，此次改动为跨层 KV 缓存布局提供了统一的开关和注册入口，提升了多连接器组合时的灵活性和潜在传输性能。开发者只需在自定义连接器实现相应属性即可享受该特性。

---

### [Refactor] Clean up pooler modules (#31897)
**SHA**: `b7036c8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b7036c87a13bd94fabf9e46436d3c1e67688f729)

**🎯 变更类型**：重构（清理 pooler 模块）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交对 `vllm/model_executor/layers/pooler.py` 及其调用方进行大幅重构，拆分出 **Token** 与 **Tokens** 两类输出类型，使用 `typing.TypeAlias` 明确返回值；抽象出 `TokenPoolerHead` 与 `TokensPoolerHead`，统一 `forward` 接口并去除旧的 `forward_all`；在 `PoolingMetadata` 中新增 `get_pooling_cursor` 辅助方法；相应模型（Bert、GritLM、ModernBert）以及 `v1/outputs.py`、`v1/pool/metadata.py`、`gpu_model_runner.py` 均已迁移至新 API。

**🎯 影响范围**  
- 核心 pooler 体系：`PoolingMethod`、`Pooler`、`*PoolerHead`、`PoolingMetadata`。  
- 使用 pooler 的模型实现：`bert.py、gritlm.py、modernbert.py`。  
- 类型定义与输出包装：`v1/outputs.py`、`v1/pool/metadata.py`。  
- GPU 端 dummy pooler 统计逻辑：`gpu_model_runner.py`。

**💡 关注建议**  

1. **兼容性**：`forward_all` 已删除且 `forward` 参数从 `PoolingCursor` 改为 `PoolingMetadata`，请确认所有外部调用（插件、第三方代码）已同步改为 `pooling_metadata`，防止运行时 `TypeError`。  
2. **空值处理**：`TokensPoolerHead` 现在可能返回 `None`，相关代码（尤其在 `StepPooler`、`DispatchPooler`）需要检查 `None` 并保持原有行为。已在 `gpu_model_runner` 加入 `if o is not None`，但其它路径亦应审查。  
3. **类型检查**：新增 `TypeAlias` 改动会影响 IDE / mypy 等工具，建议在项目根目录更新 `pyproject.toml` 中的 `mypy` 配置，确保 `TokenPoolingMethodOutput`、`TokensPoolingMethodOutput` 被正确识别。  
4. **测试覆盖**：由于抽象基类和返回类型的变化，务必跑通所有 pooler 相关单元测试，特别是 `CLS/MEAN/ALL` 等多 token 场景，验证 `TokensPoolerHead` 的 `None` 分支不导致 `torch.cat` 等操作异常。  
5. **文档同步**：README 与 API 文档中关于 `PoolerOutput`、`PoolerHead` 的说明需要更新为新的 `TokenPoolerOutput` / `TokensPoolerOutput`，防止使用者误解返回结构。  

总体而言，此次重构提升了代码可读性与类型安全，但需仔细验证向后兼容并补全空值分支的防护。

---

#### 🟢 低重要度变更 (29)

### [Bugfix]: Fix Step3ReasoningParser missing is_reasoning_end_streaming (#31969)
**SHA**: `eaba8ec` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/eaba8ece7700cbeba380530748847f1f5a672733)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `Step3ReasoningParser` 中新增 `is_reasoning_end_streaming` 方法，改为在增量 token (`delta_ids`) 中检查结束标记，实现流式推理时的正确结束检测。

---

### [Docs]: update claude code url (#31971)
**SHA**: `1da3a54` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1da3a5441a03d0eb218f859f8a17e82895ffcc94)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `docs/serving/integrations/claude_code.md` 中的 Claude Code 链接从 Anthropic 官方文档 URL（https://docs.anthropic.com/...）更改为新的快速入门 URL（https://code.claude.com/docs/en/quickstart），内容未做其他修改。

---

### [CI] [Bugfix] Fix unbounded variable in `run-multi-node-test.sh` (#31967)
**SHA**: `72c068b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/72c068b8e0f084a6a6257d6736e459495f9f56a8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修复 `run-multi-node-test.sh` 中对未定义 `ROCM_HOME` 的直接引用，改为安全的 `${ROCM_HOME:-}`，并在 CI 流水线 YAML 中加入该脚本路径。

---

### [OpenAI] Fix tool_choice=required streaming when output has trailing extra data (#31610)
**SHA**: `7645bc5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7645bc524b2fec6ae8e75924f99d19225aa779a9)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修复在 `tool_choice=required` 模式下，流式输出结尾带有额外数据（如 “\nDONE”）时的解析错误；新增对部分 JSON 解析的容错处理，并补充相应单元测试。

---

### [Model] Enable LoRA support for Pixtral (#31724)
**SHA**: `1123a87` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1123a878920f3ef35f858558415d43b9ed22701e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：为 `PixtralForConditionalGeneration` 添加 `SupportsLoRA` 接口，实现多模态映射、视觉/连接器 token 计数等函数，完善 LoRA 支持；并在文档 `supported_models.md` 中标记该模型已支持 LoRA。

---

### [Misc] Support qwen3-next lora (#31719)
**SHA**: `96fcd3c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/96fcd3c267a0b16e3f94abd59e05cb3708a67db8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 Qwen3‑Next 模型中 `shared_expert_gate` 的实现由 `torch.nn.Linear` 替换为支持复制和量化配置的 `ReplicatedLinear`，为 LoRA 兼容性提供支持。

---

### [CI/Build] Enable test_kv_cache_events_dp for AMD (#31834)
**SHA**: `8cbdc7e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8cbdc7eb941e8f3085920b57211ca45df5d71c10)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `tests/v1/engine/test_engine_core_client.py` 中，将平台检查从仅支持 CUDA 改为支持所有 CUDA‑like 平台（包括 AMD），相应更新 `pytest.skip` 的原因描述。此修改为 CI/Build 添加对 AMD 环境的测试支持。

---

### [ROCm]Skip test_torchao.py::test_pre_quantized_model on CDNA3 arch (#31905)
**SHA**: `573a1d1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/573a1d1119af85613ff0cb90ac063ab669cbbd7f)

**变更类型**：测试修改  
**重要程度**：🟢低  
**摘要**：在 `tests/quantization/test_torchao.py` 中新增 ROCm 平台判定，针对 CDNA3 架构的 FP8_FNIZ 跳过 `test_pre_quantized_model`，避免在不支持的环境下运行。

---

### [docker] A follow-up patch to fix #30913: `[docker] install cuda13 version of lmcache and nixl` (#31775)
**SHA**: `33156f5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/33156f56e0b1a4e872048669ae625fe3067f9c24)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Dockerfile 中新增 `torch_cuda_arch_list` 参数及 `TORCH_CUDA_ARCH_LIST` 环境变量，确保在 CUDA 13 环境下构建 lmcache 与 nixl 时使用指定的 GPU 架构列表。

---

### [Model] Enable LoRA support for tower and connector in GLM4-V (#31652)
**SHA**: `63baa28` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/63baa28cf5a95788f0a9237721b9d569bce39148)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `glm4_1v.py` 中为 GLM4‑V 模型新增 `get_num_mm_encoder_tokens` 与 `get_num_mm_connector_tokens` 两个方法，实现基于视觉配置的 spatial_merge_size 计算多模态编码器和连接器所需的 token 数量，提升 LoRA 支持的灵活性。

---

### [Bugfix] Remove the num_hidden_layers override for glm4_moe (#31745)
**SHA**: `e5173d3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e5173d3bac054edc1b556005d804a5acf94eaf15)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/config/speculative.py` 中删除了对 `glm4_moe` 模型的 `num_hidden_layers` 强制设为 0 的覆盖，避免不必要的层数重写。

---

### [chore] Update FA commit (#30460)
**SHA**: `be6a81f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/be6a81f31b04539aceccaa9355cb4ec8a1169f1b)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `cmake/external_projects/vllm_flash_attn.cmake` 中的 Flash-Attention 子模块 Git Tag 从 `86f8f157cf82aa2342743752b97788922dd7de43` 更新为 `188be16520ceefdc625fdf71365585d2ee348fe2`，以使用新版代码。

---

### [platform] add dp_metadata arg to set_additional_forward_context (#31942)
**SHA**: `2ab441b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2ab441befef05b3c5ffdb23f981f13d035f4cd3c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vllm/forward_context.py` 的 `set_forward_context` 函数签名中新增 `dp_metadata` 参数，以支持跨数据并行的元数据信息传递。无需其他代码修改。

---

### [Model] Enable LoRA support for tower and connector in DotsOCR (#31825)
**SHA**: `9572f74` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9572f74f1509e138988bf8288291f1f217f101ae)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `DotsOCRForCausalLM` 中新增 `get_num_mm_encoder_tokens` 与 `get_num_mm_connector_tokens` 两个计算方法，完善 LoRA 支持；同步更新文档表格，标记该模型已支持 LoRA。

---

### [ROCm][CI] v1 cpu offloading attention backend fix (#31833)
**SHA**: `5f2a473` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5f2a473ff3245c0db065c62568a515b72007ac57)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/v1/kv_offload/test_cpu_offloading.py` 中调整 `ATTN_BACKENDS` 配置：CUDA 环境保留 FLASH、FLASHINFER、TRITON 后端，ROCm 环境仅使用 `TRITON_ATTN`，以修复 CPU offloading attention 后端在 ROCm 上的兼容性问题。

---

### [Doc] Add Claude code usage example (#31188)
**SHA**: `6b2a672` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6b2a672e47f15854c2df5d61c8d57ca25547e50c)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `claude_code.md` 文档与示例截图，详细说明在 vLLM 服务器上使用 Claude Code 的步骤、环境变量配置及常见问题。

---

### [CI][BugFix][AMD] Actually skip tests marked @pytest.mark.skip_v1 (#31873)
**SHA**: `f1b1bea` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f1b1bea5c36d48561e7ef4cf4d76ffb2529e3212)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Buildkite 的 AMD 测试流水线中，将 `samplers` 测试命令改为仅运行未标记 `@pytest.mark.skip_v1` 的测试，避免执行已跳过的 V1 相关用例。

---

### [ROCm][CI] Add rocm support for run-multi-node-test.sh (#31922)
**SHA**: `cddbc2b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cddbc2b4b2547c681d1bdb876fdd6a7b8e0ec58d)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `run-multi-node-test.sh` 中新增对 ROCm 环境的检测，并在启动多节点 Docker 容器时依据 ROCm 或 CUDA 选择相应的设备参数，实现对 ROCm 的支持。

---

### [ROCm][CI] Fix attention backend test flakiness from uninitialized KV cache memory (#31928)
**SHA**: `087a138` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/087a138963192749043e306a0b2552c6ba465d8e)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 KV 缓存创建从 `torch.empty` 改为 `torch.zeros`，消除未初始化内存导致的注意力后端测试不稳定。

---

### [ROCm][LoRA] Fix MoE accuracy regression by preserving float32 router weight scaling (#31931)
**SHA**: `c4041f3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c4041f37a4a4b70c8821c35df9c79298ee5fcb7b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 fused_moe_kernel 中提前以 float32 完成路由权重乘法以提升数值稳定性，并将 bias 加法移动到反量化后，确保不受量化因子影响。

---

### [BugFix] Fix flakiness in test_eagle_dp for PyTorch 2.10 (#31915)
**SHA**: `a79079f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a79079feef63d7a8124782cf2eb1cd68cbde99ee)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `test_eagle_dp` 中将期望的 token 数从 100 降至 20，并添加 flaky 问题说明，以提升在 PyTorch 2.10 下的测试稳定性。

---

### [CI] Skip Qwen-VL in multimodal processing tests due to flaky external dependency (#31932)
**SHA**: `8dd2419` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8dd2419fa96488bc862ad60f7fc7ffffa39728e7)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/models/multimodal/processing/test_common.py` 中新增对模型 `Qwen/Qwen-VL` 与 `Qwen/Qwen-VL-Chat` 的跳过逻辑，避免因下载字体文件失败导致 CI 不稳定。

---

### Add back missing DeepEP LL params (#31911)
**SHA**: `ffc0a27` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ffc0a2798b118f7ceb21645df59d2bfdfc461d42)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `deepep_ll_prepare_finalize.py` 中为 `prepare_async` 调用恢复缺失的 DeepEP LL 参数，新增 `round_scale` 与 `use_ue8m0` 两个关键字参数。

---

### [BugFix] Fix bad words with speculative decoding (#31908)
**SHA**: `10ef65e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/10ef65eded8187df92c370d6ffd7fd2b8a3c1d3c)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 speculative decoding 下 bad words 索引错误；重写 `apply_bad_words_with_drafts` 循环以正确跳过没有 bad words 的请求；新增非连续请求的测试保证日志对应准确。

---

### [Perf] Fuse stride preparation for NVFP4 cutlass_moe (#31837)
**SHA**: `f347ac6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f347ac6c346f8e84a75ec975618c5aa79c1b9db5)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 NVFP4 MoE kernel 中将 stride 的初始化合并进 `__get_group_gemm_starts`，改为一次性传入 stride 数组和常量值，去掉原先的单独填充步骤，提升准备阶段性能。

---

### [Doc] Fix: Correct vLLM announcing blog post link in docs (#31868)
**SHA**: `05f47bd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/05f47bd8d2f3ccbe926af05d953c0f429b87b5b6)

**变更类型**：文档更新  
**重要程度**：🟢低  
**摘要**：将 README 中的 vLLM 宣告博客链接从 `https://vllm.ai` 修正为正式博客地址 `https://blog.vllm.ai/2023/06/20/vllm.html`。

---

### Enable quantized attention in NemotronH models (#31898)
**SHA**: `bf184a6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bf184a66218beae5355c1e9784074c21b896fe0f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 NemotronH 的 kv_scale 重映射规则，并在 NemotronH 模型初始化中加入 `quant_config` 参数，实现量化注意力的支持。

---

### UX: add vLLM env info in '/server_info' (#31899)
**SHA**: `30399cc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/30399cc725306deff10169ae395864e70b8ab2ba)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `/server_info` 接口中新增 `_get_vllm_env_vars`，返回所有 `VLLM_` 环境变量（排除 KEY），并在返回的 JSON 中加入 `vllm_env` 字段。代码结构略作优化，保持原有配置展示逻辑。

---

### [Bugfix]: prevent leaking tokens in crash log (#30751)
**SHA**: `1d9e9ae` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1d9e9ae8a4498782de0dd51627ab1fddac4692ef)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `NewRequestData` 与 `CachedRequestData` 添加匿名 `repr`，仅打印 token 列表长度，防止崩溃日志泄露实际 token 内容。

---

