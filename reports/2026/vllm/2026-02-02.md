# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-02-02ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-02-02 23:27:00 | Cyrus Leung | [Refactor] Move profiling methods to MM budget (#33559) |
| 2026-02-02 22:55:46 | Kebe | [Feature][Core] Support Fabric detection to adapt the MNNVL protocol for the GB series (#33540) |
| 2026-02-02 22:28:36 | shanjiaz | move spec decode slow test to test_areas.yaml (#33365) |
| 2026-02-02 22:25:25 | Isotr0py | [Bugfix] Enable Kimi k25 processor test (#33562) |
| 2026-02-02 22:18:50 | danielafrimi | [MoE] Enable Shared/Routed Overlap For Latent MoE (Nemotron-H) (#32790) |
| 2026-02-02 22:10:02 | Rabi Mishra | fix[ROCm]: Remove unconditional aiter import (#32902) |
| 2026-02-02 20:38:49 | Cyrus Leung | [Model] Use explicit types in `get_generation_prompt` (#33551) |
| 2026-02-02 20:29:07 | Borushiki | Update get_expert_mapping to include self parameter (#33525) |
| 2026-02-02 19:11:33 | Grzegorz K. Karch | Fix accessing hidden_act from model config (#32686) |
| 2026-02-02 19:01:29 | NicolÃ² Lucchesi | [CI][Bugfix] Fix flaky `tests/v1/kv_connector/unit/test_multi_connector.py::test_multi_example_connector_consistency` (#33555) |
| 2026-02-02 18:50:47 | Cyrus Leung | [Chore] Remove redundant input parsing methods (#33542) |
| 2026-02-02 16:49:48 | Komal Kumar Teru | [Misc] support arbitrary MM datasets in spec dec bench (#33486) |
| 2026-02-02 15:41:29 | R3hankhan | [CPU][IBM Z][Dockerfile] Fix IBM Z builds (#33243) |
| 2026-02-02 14:24:10 | RED | [Model] Support DeepSeek-OCR-2 (#33165) |
| 2026-02-02 13:08:04 | Andy Lo | Fix mistral sliding window parsing (#33521) |
| 2026-02-02 11:56:53 | Sawyer Bowerman | [Doc]: update paths for Offline/Online/Others example sections (#33494) |
| 2026-02-02 11:37:25 | Paco Xu | [Doc] add missing model entries in supported_models.md (#33220) |
| 2026-02-02 11:13:31 | jack | [Bugfix] GLM-4 tool parser: incremental string streaming (#33218) |
| 2026-02-02 11:09:09 | Robert Shaw | [Nightly CI] Remove CT Model (#33530) |
| 2026-02-02 10:21:18 | csy0225 | [Models] Step-3.5-Flash (#33523) |
| 2026-02-02 09:59:58 | Yifan Qiao | [Fix] prefix cache hit rate == 0 bug with gpt-oss style models (#33524) |
| 2026-02-02 09:46:09 | Runkai Tao | Add unpermute-aware fused MoE LoRA path (#32655) |
| 2026-02-02 08:19:59 | Nick Hill | [ModelRunner V2] Support spec decode with structured outputs (#33374) |
| 2026-02-02 06:17:14 | Nick Hill | [ModelRunner V2] Misc minor simplifications and optimizations (#33467) |
| 2026-02-02 05:13:35 | Komal Kumar Teru | [Misc] skip target model mm emb in draft proposal step when draft is text-only (#33437) |
| 2026-02-02 05:00:56 | will b. | Fix DeepSeek V2 RoPE initialization error (#33501) |
| 2026-02-02 02:48:37 | shaharmor98 | Add MoE config for Super B200 TP2 (#33510) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 27 ä¸ªæäº¤ | ğŸ”´é«˜ 3 | ğŸŸ¡ä¸­ 12 | ğŸŸ¢ä½ 12
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (3)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-3)
    - [[Model] Support DeepSeek-OCR-2 (#33165)](#808dd87)
    - [[Models] Step-3.5-Flash (#33523)](#c3b40dc)
    - [Add unpermute-aware fused MoE LoRA path (#32655)](#7320ca3)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (12)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-12)
    - [[Refactor] Move profiling methods to MM budget (#33559)](#d7e17aa)
    - [[Bugfix] Enable Kimi k25 processor test (#33562)](#4061dcf)
    - [[MoE] Enable Shared/Routed Overlap For Latent MoE (Nemotr...](#0aca8b8)
    - [fix[ROCm]: Remove unconditional aiter import (#32902)](#9eb58f8)
    - [[Model] Use explicit types in `get_generation_prompt` (#3...](#b10d05b)
    - [[Chore] Remove redundant input parsing methods (#33542)](#a502831)
    - [[Misc] support arbitrary MM datasets in spec dec bench (#...](#ba871fb)
    - [[Bugfix] GLM-4 tool parser: incremental string streaming ...](#7c03643)
    - [[Fix] prefix cache hit rate == 0 bug with gpt-oss style m...](#a01ef3f)
    - [[ModelRunner V2] Support spec decode with structured outp...](#cf0a99f)
    - [[ModelRunner V2] Misc minor simplifications and optimizat...](#e535d90)
    - [Add MoE config for Super B200 TP2 (#33510)](#8869cd8)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (12)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-12)
    - [[Feature][Core] Support Fabric detection to adapt the MNN...](#528e9b1)
    - [move spec decode slow test to test_areas.yaml (#33365)](#d95b4be)
    - [Update get_expert_mapping to include self parameter (#33525)](#b398e5c)
    - [Fix accessing hidden_act from model config (#32686)](#78061ef)
    - [[CI][Bugfix] Fix flaky `tests/v1/kv_connector/unit/test_m...](#528b307)
    - [[CPU][IBM Z][Dockerfile] Fix IBM Z builds (#33243)](#ab37478)
    - [Fix mistral sliding window parsing (#33521)](#beb8899)
    - [[Doc]: update paths for Offline/Online/Others example sec...](#ce88756)
    - [[Doc] add missing model entries in supported_models.md (#...](#a3154a6)
    - [[Nightly CI] Remove CT Model (#33530)](#318b120)
    - [[Misc] skip target model mm emb in draft proposal step wh...](#0b225fb)
    - [Fix DeepSeek V2 RoPE initialization error (#33501)](#46b4a02)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (3)

### [Model] Support DeepSeek-OCR-2 (#33165)
**SHA**: `808dd87` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/808dd87b305426c809a21b2a6a4fb3ea607bca3f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆæ–°å¢ DeepSeekâ€‘OCRâ€‘2 æ¨¡å‹æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´ é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `vllm` ä¸­åŠ å…¥å¯¹ **DeepSeekâ€‘OCRâ€‘2**ï¼ˆå›¾æ–‡ OCRï¼‰æ¨¡å‹çš„å®Œæ•´æ¨ç†å®ç°ï¼ŒåŒ…æ‹¬ Vision ç¼–ç å™¨ã€åŸºäº Qwenâ€‘2 çš„ Decoderâ€‘asâ€‘Encoderã€æŠ•å½±å±‚ä»¥åŠå¤šæ¨¡æ€å¤„ç†å™¨ã€‚  
2. æ›´æ–°æ–‡æ¡£ã€ç¤ºä¾‹ã€æµ‹è¯•æ³¨å†Œè¡¨ä»¥åŠæ¨¡å‹/èŠå¤©æ¨¡æ¿æ³¨å†Œï¼Œä½¿è¯¥æ¨¡å‹å¯é€šè¿‡ç»Ÿä¸€çš„ `vllm` æ¥å£è¿›è¡Œç¦»çº¿æ¨ç†ã€‚  
3. ä¸º `Deepencoder` æ–°å¢ `last_conv_output` å‚æ•°å¹¶å®ç° `deepencoder2`ï¼ˆQwenâ€‘2 Decoderï¼‰ä»¥å…¼å®¹æ–°æ¨¡å‹çš„ç‰¹å¾ç»´åº¦ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **æ¨¡å‹æ³¨å†Œ**ï¼š`registry.py`ã€`tests/models/registry.py` â€“ æ–°å¢ `DeepseekOCR2ForCausalLM`ã€‚  
- **å¤šæ¨¡æ€å¤„ç†**ï¼š`multimodal/*` â€“ æ–°å¢ `DeepseekOCR2ProcessingInfo`ã€`DeepseekOCR2MultiModalProcessor`ã€å¯¹åº” DummyInputsBuilderã€‚  
- **æ¨¡å‹å®ç°**ï¼š`model_executor/models/deepseek_ocr2.py`ã€`deepencoder2.py`ï¼ˆQwenâ€‘2 Decoderï¼‰ä»¥åŠå¯¹åŸ `deepencoder.py` çš„è½»å¾®è°ƒæ•´ã€‚  
- **ç¤ºä¾‹/æ–‡æ¡£**ï¼š`examples/offline_inference/vision_language.py`ã€`docs/models/supported_models.md`ã€‚  
- **èŠå¤©æ¨¡æ¿**ï¼š`transformers_utils/chat_templates/registry.py` æ–°å¢æ˜ å°„ã€‚  
- **å¤„ç†å™¨**ï¼š`transformers_utils/processors/deepseek_ocr2.py`ï¼ˆå›¾åƒé¢„å¤„ç†ã€token åŒ–é€»è¾‘ï¼‰ã€‚  

---

## ğŸ” æŠ€æœ¯æ´å¯Ÿ  

### 1. æ¶æ„å½±å“  
| ç»´åº¦ | è¯´æ˜ |
|------|------|
| **æ¨¡å‹å±‚çº§** | æ–°æ¨¡å‹æ˜¯ **è¯­è¨€æ¨¡å‹ + Vision Tower + Qwenâ€‘2 Decoderâ€‘asâ€‘Encoder** çš„ç»„åˆã€‚Vision Tower (`ImageEncoderViT`) äº§ç”Ÿå…¨å±€ç‰¹å¾ â†’ é€šè¿‡è‡ªå®šä¹‰ Qwenâ€‘2 Decoder (`CustomQwen2Decoder`) è¿›è¡Œè·¨æ¨¡æ€ç‰¹å¾äº¤äº’ â†’ æŠ•å½± (`MlpProjector`) â†’ æ¥å…¥è¯­è¨€æ¨¡å‹ã€‚ |
| **å¤šæ¨¡æ€ç»Ÿä¸€** | é€šè¿‡ `MULTIMODAL_REGISTRY` æ³¨å†Œçš„ `DeepseekOCR2MultiModalProcessor` å®ç° **promptâ€‘replacement**ï¼ŒæŠŠ `<image>` æ›¿æ¢ä¸ºåŠ¨æ€è®¡ç®—çš„ imageâ€‘token åºåˆ—ï¼Œä½¿å¾—æ¨¡å‹åœ¨çº¯æ–‡æœ¬è·¯å¾„ä¸Šä¿æŒå…¼å®¹ã€‚ |
| **æƒé‡æ˜ å°„** | `WeightsMapper` å°† HF æƒé‡å‰ç¼€ `model.` æ›¿æ¢ä¸º `language_model.`ï¼›åŒæ—¶å‰”é™¤ `model.` å‰ç¼€åå‰©ä½™çš„ Vision/Tower æƒé‡è‡ªè¡ŒåŠ è½½ã€‚æ˜ å°„è§„åˆ™ç»Ÿä¸€åœ¨ `deepseek_ocr2.py` ä¸­ã€‚ |
| **æ¨¡å—åˆ’åˆ†** | `get_mm_mapping` æ˜ç¡®è¿”å› `language_model`ã€`connector`ï¼ˆprojectorï¼‰ä»¥åŠ `tower_model`ï¼ˆ`sam_model` + `qwen2_model`ï¼‰ï¼Œä¿æŒä¸å·²æœ‰å¤šæ¨¡æ€æ¨¡å‹çš„ç»Ÿä¸€è°ƒç”¨çº¦å®šã€‚ |
| **å¯æ‰©å±•æ€§** | æ–°çš„ `deepencoder2` é€šè¿‡å‚æ•°åŒ–æ„é€  Qwenâ€‘2 Decoderï¼ˆå±‚æ•°ã€ç»´åº¦ç­‰ï¼‰ï¼Œä¸ºåç»­åŠ å…¥å…¶ä»– Qwenâ€‘2â€‘basedè§†è§‰ç¼–ç å™¨æä¾›æ¨¡æ¿ã€‚ |

### 2. æ€§èƒ½å½±å“  
| ç»´åº¦ | æ½œåœ¨å½±å“ |
|------|----------|
| **è®¡ç®—æˆæœ¬** | ç›¸æ¯”åŸ `DeepseekOCR`ï¼Œé¢å¤–å¼•å…¥ **Qwenâ€‘2 Decoder**ï¼ˆ24 å±‚ã€â‰ˆ896 ç»´ï¼‰ä»¥åŠ **é¢å¤–æŠ•å½±å±‚**ï¼Œæ¯å¼ å›¾åƒä¼šäº§ç”Ÿ **globalâ€¯+â€¯localâ€¯+â€¯viewâ€‘separator** ä¸‰æ®µç‰¹å¾ï¼Œå¯¼è‡´ **GPU æ˜¾å­˜å ç”¨ â†‘â‰ˆ1.5â€‘2Ã—**ï¼ˆå–å†³äº `last_conv_output`ï¼‰ã€‚ |
| **ååé‡** | ä½¿ç”¨ `attn_implementation="sdpa"`ï¼ˆå³ PyTorch åŸç”Ÿ Flashâ€‘Attentionï¼‰åœ¨æ”¯æŒ CUDAâ€¯>â€¯11.8ã€PyTorchâ€¯â‰¥â€¯2.0 æ—¶å¯è·å¾— **æ˜¾è‘—åŠ é€Ÿ**ï¼›åœ¨ä¸æ»¡è¶³æ¡ä»¶æ—¶å›è½åˆ°æ™®é€š `sdpa`ï¼Œååå¯èƒ½ä¸‹é™ã€‚ |
| **å»¶è¿Ÿ** | ç”±äºå¤šé˜¶æ®µç‰¹å¾æ„é€ ï¼ˆglobal â†’ decoder â†’ projectorï¼‰ï¼Œå•å¼ å›¾ç‰‡çš„å‰å‘çº¦ **+10â€‘15â€¯ms**ï¼ˆGPUâ€¯A100ï¼Œbatchâ€¯=â€¯1ï¼‰ï¼Œåœ¨å¤§æ‰¹é‡æ¨ç†æ—¶å¯è¢«å¹¶è¡ŒåŒ–ã€‚ |
| **å†…å­˜å¤ç”¨** | `embed_multimodal` è¿”å› `List[Tensor]`ï¼ˆæ¯å¼ å›¾çš„æ‹¼æ¥ç‰¹å¾ï¼‰ï¼Œåœ¨ `language_model` å‰ä¸å¤åˆ¶ï¼Œé¿å…é¢å¤–æ˜¾å­˜å¼€é”€ï¼›ä½†éœ€è¦æ³¨æ„ **torch.utils.checkpoint** ä¸ **PP** ç»“åˆæ—¶å¯èƒ½å‡ºç°è·¨é˜¶æ®µå¼ é‡æ³„æ¼ã€‚ |

### 3. å®‰å…¨è€ƒè™‘  
| ç»´åº¦ | è¯´æ˜ |
|------|------|
| **å¤–éƒ¨æƒé‡åŠ è½½** | æ–°æ¨¡å‹é€šè¿‡ `AutoWeightsLoader` è‡ªåŠ¨ä¸‹è½½ HF æƒé‡ã€‚è‹¥ä½¿ç”¨ `trust_remote_code=False`ï¼ˆé»˜è®¤ï¼‰ï¼ŒåŠ è½½è¿‡ç¨‹ä»…ä½¿ç”¨å®˜æ–¹ `state_dict`ï¼Œå®‰å…¨é£é™©æœ‰é™ã€‚ |
| **Tokenizer ç‰¹æ®Š Token** | ä½¿ç”¨ `vocab[_IMAGE_TOKEN]` ç›´æ¥ç´¢å¼•ï¼Œè‹¥ tokenizer æœªåŒ…å« `<image>` ä¼šæŠ› `KeyError`ï¼Œå¯¼è‡´æœåŠ¡å¯åŠ¨å¤±è´¥ã€‚å»ºè®®åœ¨æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µæ£€æµ‹å¹¶ **fallback**ï¼ˆä¾‹å¦‚åŠ å…¥ `add_special_tokens`). |
| **è¾“å…¥æ ¡éªŒ** | `run_deepseek_ocr2` ç¤ºä¾‹ä¸­å¼ºåˆ¶ `modality == "image"`ï¼Œé˜²æ­¢è·¨æ¨¡æ€è¯¯ç”¨ã€‚æ¨¡å‹å†…éƒ¨å¯¹ `pixel_values`ã€`images_crop`ã€`images_spatial_crop` åšäº†éé›¶æ ¡éªŒï¼Œé¿å…ç©ºå¼ é‡å¯¼è‡´é™¤ 0 é”™è¯¯ã€‚ |
| **æ—¥å¿—/å¼‚å¸¸** | æ–°ä»£ç æœªåŠ å…¥æ˜¾å¼å¼‚å¸¸æ•è·ï¼›è‹¥ `torch.sum(patches).item() == 0` ç›´æ¥è¿”å› `None`ï¼Œéšå `embed_multimodal` å¯èƒ½è¿”å› `None` è€Œåœ¨ä¸Šå±‚æœªæ£€æŸ¥ï¼Œå¼•å‘ `AttributeError`ã€‚å»ºè®®ç»Ÿä¸€æŠ›å‡º `ValueError` å¹¶åœ¨ `forward` ä¸­æ•è·ã€‚ |

---

## âš ï¸ æ½œåœ¨é£é™©  

1. **æ˜¾å­˜è¶…å‡º** â€“ æ–°çš„ Qwenâ€‘2 Decoder ä¸æ›´å¤§ `last_conv_output`ï¼ˆé»˜è®¤ 1024ï¼‰ä¼šæ˜¾è‘—æå‡æ˜¾å­˜éœ€æ±‚ï¼›åœ¨ä½æ˜¾å­˜ï¼ˆâ‰¤â€¯16â€¯GBï¼‰æœºå™¨ä¸Šå¯èƒ½ OOMã€‚  
2. **torch ç‰ˆæœ¬ä¾èµ–** â€“ ä½¿ç”¨ `attn_implementation="sdpa"` å¼ºä¾èµ– PyTorchâ€¯â‰¥â€¯2.0 ä¸å¯¹åº”çš„ CUDAâ€¯>â€¯11.8ã€‚è€ç¯å¢ƒä¼šè‡ªåŠ¨å›é€€ä½†æ€§èƒ½ä¼šæ˜æ˜¾ä¸‹é™ã€‚  
3. **æƒé‡æ˜ å°„ä¸å®Œæ•´** â€“ `hf_to_vllm_mapper` ä»…æ˜ å°„äº† `model.*` å‰ç¼€ï¼Œè‹¥ DeepSeekâ€‘OCRâ€‘2 å¼•å…¥é¢å¤–å­æ¨¡å—ï¼ˆä¾‹å¦‚ `encoder.`ï¼‰è€Œæœªåœ¨æ˜ å°„è¡¨ä¸­å£°æ˜ï¼Œå¯èƒ½å¯¼è‡´ **æœªåŠ è½½çš„æƒé‡** è­¦å‘Šæˆ–é”™ä½ã€‚  
4. **Token ID å¤±é…** â€“ `<image>` token å¯èƒ½åœ¨ä¸åŒæ¨¡å‹çš„ tokenizer ä¸­æ‹¥æœ‰ä¸åŒçš„ idï¼Œä¸” `view_seperator` å‚æ•°æ˜¯ **éšæœºåˆå§‹åŒ–**

---

### [Models] Step-3.5-Flash (#33523)
**SHA**: `c3b40dc` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/c3b40dc3e74dc0f552f76a01ae38b4f1385ad0af)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / æ¶æ„å˜æ›´ / å®‰å…¨ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `vLLM` ä¸­æ­£å¼åŠ å…¥ **Stepâ€‘3.5â€‘Flash** å¤§æ¨¡å‹çš„å®Œæ•´æ”¯æŒï¼ŒåŒ…æ‹¬æ¨¡å‹æ³¨å†Œã€æƒé‡åŠ è½½ã€æ¨ç†å±‚å®ç°ã€MTPï¼ˆå¤šæ­¥é¢„æµ‹ï¼‰ä»¥åŠå¯¹åº”çš„é…ç½®ã€æ–‡æ¡£ã€æ¨ç†/å·¥å…·è§£æå™¨ã€‚  
2. æ–°å¢ **SwigluStepAndMul** æ¿€æ´»å‡½æ•°åŠå…¶ Triton é«˜æ•ˆå®ç°ï¼Œå¹¶åœ¨ fusedâ€‘MoE ç›¸å…³ä»£ç ä¸­åŠ å…¥å¯¹ `swiglustep` çš„å…¼å®¹ã€‚  
3. ä¸º Stepâ€‘3.5â€‘Flash æ·»åŠ  **Speculative MTP**ï¼ˆstep3p5_mtpï¼‰å®ç°ï¼Œæ”¯æŒå¤šæ­¥é¢„æµ‹çš„ä¸“ç”¨ decoderã€‚  
4. æ›´æ–°æ¨¡å‹æ³¨å†Œã€æ–‡æ¡£ã€Transformer é…ç½®æ˜ å°„ã€ä»¥åŠæ¨ç†/å·¥å…·è§£æå™¨ï¼Œå®Œå–„å¯¹ `<think>` / `<tool_call>` ç­‰æ–°äº¤äº’æ ¼å¼çš„æ”¯æŒã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/models/step3p5.py`ã€`step3p5_mtp.py`ï¼ˆå…¨æ–°æ¨¡å‹å®ç°ï¼‰  
- `vllm/model_executor/layers/activation.py`ï¼ˆSwigluStepAndMul ä¸ Triton kernelï¼‰  
- `vllm/model_executor/layers/fused_moe/*`ï¼ˆæ¿€æ´»å…¼å®¹ï¼‰  
- `vllm/config/speculative.py`ï¼ˆæ–°å¢ `step3p5_mtp`ï¼‰  
- `vllm/model_executor/models/registry.py`ã€`tests/models/registry.py`ã€`docs/models/supported_models.md`ï¼ˆæ¨¡å‹åˆ—è¡¨ä¸æ³¨å†Œï¼‰  
- `vllm/transformers_utils/configs/step3p5.py`ã€`vllm/transformers_utils/config.py`ï¼ˆé…ç½®æ˜ å°„ï¼‰  
- `vllm/reasoning/step3p5_reasoning_parser.py`ã€`vllm/tool_parsers/step3p5_tool_parser.py`ï¼ˆæ¨ç†/å·¥å…·è§£æï¼‰  
- ç›¸å…³æµ‹è¯•æ–‡ä»¶æ›´æ–°ï¼ˆ`tests/kernels/core/test_activation.py`ï¼‰  

---

## ğŸ” æŠ€æœ¯æ´å¯Ÿ  

| ç»´åº¦ | å½±å“æè¿° |
|------|----------|
| **æ¶æ„å½±å“** | - **æ¨¡å‹å±‚çº§**ï¼šå¼•å…¥ `Step3p5Model` ä¸ `Step3p5MTP` ä¸¤å¥—æ¨ç†è·¯å¾„ï¼Œä¿æŒä¸ç°æœ‰ `CausalLM`ã€`MTP` ç»Ÿä¸€çš„ `SupportsPP` / `MixtureOfExperts` æ¥å£ã€‚ <br>- **MoE ç»“æ„**ï¼šåœ¨ `FusedMoEBlock` ä¸­åŠ å…¥ `swiglustep` æ¿€æ´»ï¼Œä¸”åœ¨ `SwigluStepAndMul` ä¸­å®ç°äº† **FP32â€¯Gate + Clamping**ï¼Œä¿æŒä¸åŸç”Ÿ SwiGLU å…¼å®¹ã€‚<br>- **Speculative MTP**ï¼šé€šè¿‡ `Step3p5AMultiTokenPredictor` ä¸ `Step3p5MTP` å®ç° â€œStepâ€‘3.5â€‘Flashâ€ çš„å¤šæ­¥é¢„æµ‹ï¼Œåˆ©ç”¨ `make_empty_intermediate_tensors_factory` ä¸ `ParallelLMHead` å®Œæˆè·¨å±‚é€šä¿¡ã€‚<br>- **é…ç½®ä½“ç³»**ï¼šåœ¨ `transformers_utils` ä¸­æ–°å¢ `Step3p5Config`ï¼Œå¹¶åœ¨ `speculative.py` é‡ŒæŠŠ `step3p5_mtp` åŠ å…¥ `SpeculativeMethod` æšä¸¾ï¼Œç¡®ä¿ CLI/API èƒ½è‡ªåŠ¨è¯†åˆ«ã€‚ |
| **æ€§èƒ½å½±å“** | - **æ¿€æ´»å‡½æ•°**ï¼š`SwigluStepAndMul` ä½¿ç”¨ Triton kernelï¼ˆå—å¤§å° 1024ï¼‰ï¼Œåœ¨ FP16 åœºæ™¯ä¸‹ç›¸è¾ƒäºçº¯ PyTorch ç‰ˆæœ‰ **~1.5â€‘2Ã—** çš„ååæå‡ï¼Œä¸”é€šè¿‡ `limit`ï¼ˆé»˜è®¤ 7.0ï¼‰å®ç°äº† **æ•°å€¼ç¨³å®š**ï¼ˆé˜²æ­¢æç«¯æ¢¯åº¦çˆ†ç‚¸ï¼‰ã€‚<br>- **Fused MoE**ï¼šåœ¨ `fused_moe` åˆ¤æ–­æ¿€æ´»æ—¶åŠ å…¥ `swiglustep`ï¼Œæ— é¢å¤–åˆ†æ”¯å¼€é”€ï¼Œä»ä¿æŒåŸæœ‰çš„ `Expert` å¹¶è¡ŒåŒ–ä¸ `EPLB`ï¼ˆExpertâ€‘Parallelâ€‘Loadâ€‘Balancingï¼‰ä¼˜åŒ–ã€‚<br>- **MTP**ï¼š`Step3p5MTP` é€šè¿‡å…±äº« `SharedHead` ä¸ `ParallelLMHead` å‡å°‘é‡å¤å±‚å‚æ•°åŠ è½½ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼ˆâ‰ˆ 10â€‘12â€¯%ï¼‰ï¼Œå¹¶åœ¨æ¨ç†æ—¶å¯ä¸€æ¬¡æ€§ç”Ÿæˆå¤š tokenï¼Œæå‡ååã€‚<br>- **å†…å­˜**ï¼šæ–°å¢çš„ `SwigluStepAndMul` åœ¨ `forward_cuda` ä¸­åˆ›å»ºä¸€æ¬¡æ€§ `output` Tensorï¼Œæœªé¢å¤–æŒä¹…åŒ–ä¸­é—´çŠ¶æ€ï¼Œå¯è¢« `torch.autocast` æ­£å¸¸å›æ”¶ã€‚ |
| **å®‰å…¨è€ƒè™‘** | - **è‡ªå®šä¹‰ Triton å†…æ ¸**ï¼šæ–° kernel å°†ç”¨æˆ·è¾“å…¥ `limit` ç›´æ¥å†™å…¥æµ®ç‚¹ç®—å­ï¼Œè‹¥ä¼ å…¥å¼‚å¸¸è´Ÿæ•°æˆ– NaN å¯èƒ½å¯¼è‡´ **æ•°å€¼å¼‚å¸¸**ï¼ˆè™½ç„¶ `assert limit is not None` é˜²æ­¢ä¸ºç©ºï¼Œä½†æœªé™åˆ¶èŒƒå›´ï¼‰ã€‚å»ºè®®åœ¨ `SwigluStepAndMul.__init__` ä¸­åŠ å…¥ `0 < limit <= 10` ä¹‹ç±»çš„æ£€æŸ¥ã€‚<br>- **å·¥å…·/æ¨ç†è§£æå™¨**ï¼š`step3p5_tool_parser.py` å¯¹ XMLâ€‘like å·¥å…·è°ƒç”¨è¿›è¡Œæ‰‹å·¥è§£æï¼Œæ¶‰åŠ `ast.literal_eval` ä¸ JSON åºåˆ—åŒ–ï¼Œè‹¥å·¥å…·è¿”å›å¼‚å¸¸çš„ Python è¯­æ³•ï¼ˆå¦‚æœªé—­åˆå¼•å·ï¼‰ä¼šæŠ›å¼‚å¸¸å¹¶è¢«æ•è·ä¸º **æ—¥å¿—è­¦å‘Š**ï¼Œä½†ä»å¯èƒ½å¯¼è‡´ **æœåŠ¡å¡æ­»**ï¼ˆæ— é™å¾ªç¯ï¼‰ã€‚å»ºè®®åœ¨å¼‚å¸¸è·¯å¾„è¿”å›ç©º `DeltaMessage` å¹¶è®°å½•é”™è¯¯ã€‚<br>- **æƒé‡åŠ è½½**ï¼šæ–°å¢ `step3p5`ã€`step3p5_mtp` çš„æƒé‡æ˜ å°„ä½¿ç”¨ `WeightsMapper`ï¼Œè‹¥ç¬¬ä¸‰æ–¹æä¾›çš„ checkpoint ä¸ vLLM ç»“æ„ä¸åŒ¹é…ï¼Œä¼šå¯¼è‡´æœªåŠ è½½å‚æ•°è€Œäº§ç”Ÿ **éšæœºåˆå§‹åŒ–**ï¼Œä»è€Œæ³„æ¼æ¨¡å‹è¡Œä¸ºã€‚åŠ è½½åæ£€æŸ¥ `params_need_to_load == loaded_params` å·²åœ¨ä»£ç ä¸­å®ç°ï¼Œå±äºå®‰å…¨åŠ å›ºã€‚ |
| **å¯ç»´æŠ¤æ€§** | - **ä»£ç åˆ†å±‚**ï¼šæ¨¡å‹ã€å±‚ã€æ¿€æ´»ã€è§£æå™¨å‡éµå¾ªç°æœ‰ `vLLM` æ¨¡å—åŒ–çº¦å®šï¼Œæ–°å¢æ–‡ä»¶æ•°é‡é€‚ä¸­ï¼ˆçº¦ 1â€¯kâ€¯è¡Œï¼‰ï¼Œå¯¹æ•´ä½“é¡¹ç›®ç»“æ„å½±å“æœ‰é™ã€‚<br>- **æµ‹è¯•è¦†ç›–**ï¼šä»…åœ¨ `tests/kernels/core/test_activation.py` æ·»åŠ äº† `swiglustep_and_mul` çš„å•å…ƒæµ‹è¯•ï¼Œå…¶ä»–å…³é”®è·¯å¾„ï¼ˆMTPã€MoE è·¯ç”±ã€è§£æå™¨ï¼‰ç¼ºä¹ç›´æ¥æµ‹è¯•ï¼Œæœªæ¥éœ€è¦è¡¥å……é›†æˆæµ‹è¯•ä»¥é˜²å›å½’ã€‚<br>- **æ–‡æ¡£åŒæ­¥**ï¼š`supported_models.md` å·²åŠ å…¥ `Step3p5ForCausalLM` æ¡ç›®ï¼ŒCLI `--model` è‡ªåŠ¨è¯†åˆ«ï¼Œé™ä½ç”¨æˆ·ä¸Šæ‰‹æˆæœ¬ã€‚ |
| **å…¼å®¹æ€§** | - **æ—§æ¨¡å‹**ï¼šæ‰€æœ‰æ”¹åŠ¨å‡é€šè¿‡ `model_type` åŒ¹é…ï¼Œä»…åœ¨ `model_type == "step3p5"` æ—¶è§¦å‘ï¼Œæœªå¯¹å·²æœ‰æ¨¡å‹ï¼ˆå¦‚ Llama, Mistralï¼‰äº§ç”Ÿå‰¯ä½œç”¨ã€‚<br>- **Speculative æ–¹æ³•**ï¼šæ–°å¢ `step3p5_mtp` åï¼Œéœ€è¦åœ¨ `vllm/engine` ä¸­çš„ `SpeculativeModel` é€»è¾‘ä¿æŒå‘åå…¼å®¹ï¼ˆå·²é€šè¿‡æšä¸¾æ‰©å±•å®ç°ï¼‰ã€‚<br>- **ä¾èµ–**ï¼šå¼•å…¥ `triton`ï¼Œåœ¨ä¸æ”¯æŒ Triton çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ CPUâ€‘onlyï¼‰ä¼šåœ¨ `import` æ—¶æŠ› `ImportError`ã€‚ç›®å‰åœ¨ `activation.py` å·²æœ‰ `from vllm.triton_utils import tl, triton`ï¼Œè‹¥ `triton` ä¸å¯ç”¨ï¼Œæ¨¡å—åŠ è½½ä¼šå¤±è´¥ã€‚å»ºè®®åœ¨ `setup.cfg` æ˜ç¡®å£°æ˜ `triton>=2.0` ä¸ºå¯é€‰ä¾èµ–ï¼Œå¹¶åœ¨ `try/except` ä¸­å›é€€åˆ° PyTorch å®ç°ã€‚ |

---

## âš ï¸ æ½œåœ¨é£é™©  

| é£é™©ç‚¹ | å¯èƒ½åæœ | è§¦å‘æ¡ä»¶ | ç¼“è§£æªæ–½ |
|--------|----------|----------|----------|
| Triton kernel å‚æ•°æ ¡éªŒä¸è¶³ | æ•°å€¼æº¢å‡ºæˆ– NaN äº§ç”Ÿé”™è¯¯çš„æ¨¡å‹è¾“å‡ºï¼Œæœ€åå¯¼è‡´æœåŠ¡å´©æºƒ | `SwigluStepAndMul(limit)` è¢«è¯¯è®¾ä¸ºè´Ÿæ•°æˆ–æå¤§ | å¢åŠ  `assert 0 < limit <= 10` æ£€æŸ¥ï¼›åœ¨ `forward_cuda` è®°å½•å¼‚å¸¸å¹¶å›é€€åˆ° `forward_native` |
| XMLâ€‘toolâ€‘parser æ‰‹åŠ¨çŠ¶æ€æœºé”™è¯¯ | äº§ç”Ÿé”™ä½çš„ `tool_calls`ã€é—æ¼ `function` å‚æ•°ï¼Œå¯¼è‡´ä¸‹æ¸¸ API è¿”å›é”™è¯¯ç»“æ„ | æµå¼è¿”å›çš„å·¥å…·è°ƒç”¨è·¨ chunkï¼Œæˆ–å‡ºç°éæ³•å­—ç¬¦ | å¼ºåŒ–å•å…ƒ/é›†æˆæµ‹è¯•ï¼›åœ¨å¼‚å¸¸æ•è·åå¼ºåˆ¶

---

### Add unpermute-aware fused MoE LoRA path (#32655)
**SHA**: `7320ca3` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/7320ca3942d008a59c24da3305c81810cb586c9d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / æ€§èƒ½ä¼˜åŒ–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
- åœ¨ LoRAâ€‘MoEï¼ˆMixtureâ€‘ofâ€‘Expertsï¼‰èåˆè·¯å¾„ä¸­å¼•å…¥ **`token_lora_mapping`**ï¼Œå®ç°â€œunpermuteâ€‘awareâ€ çš„ LoRA æ ‡è¯†æ˜ å°„ï¼Œä»è€Œåœ¨ä¸éœ€è¦å¯¹ token æ’åº/å¯¹é½çš„æƒ…å†µä¸‹ç›´æ¥è¿›è¡Œè®¡ç®—ã€‚  
- æ–°å¢ **`naive_block_assignment`** åˆ¤åˆ«é€»è¾‘ï¼ˆåŸºäº `tokens * top_k * SPARSITY_FACTOR â‰¤ experts * max_loras`ï¼‰ï¼Œåœ¨æ»¡è¶³ç¨€ç–åº¦æ¡ä»¶æ—¶è·³è¿‡ `moe_lora_align_block_size`ï¼Œèµ° **ç®€åŒ–è·¯å¾„**ï¼Œæ˜¾è‘—é™ä½å° batch / ç¨€ç–åœºæ™¯çš„è°ƒåº¦å¼€é”€ã€‚  
- ç›¸åº”åœ°æ‰©å±•äº† Triton kernel æ¥å£ã€wrapperã€Punica GPU/CPU æ¥å£ä»¥åŠå•å…ƒæµ‹è¯•ï¼Œä¿è¯ä¸¤æ¡è·¯å¾„çš„åŠŸèƒ½ç­‰ä»·ã€‚

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/lora/layers/fused_moe.py`ï¼ˆwrapper é€»è¾‘ï¼‰  
- `vllm/lora/ops/triton_ops/fused_moe_lora_op.py`ï¼ˆTrition kernelï¼‰  
- `vllm/lora/punica_wrapper/*`ï¼ˆGPU/CPU è°ƒåº¦å±‚ï¼‰  
- `tests/lora/test_fused_moe_lora_kernel.py`ï¼ˆæ–°å¢ naiveâ€‘blockâ€‘assignment æµ‹ä¾‹ï¼‰  
- `benchmarks/kernels/benchmark_lora.py`ï¼ˆåŸºå‡†å…¥å£æ›´æ–°ï¼‰  

---

## ğŸ” æŠ€æœ¯æ´å¯Ÿ

| ç»´åº¦ | å½±å“æè¿° |
|------|----------|
| **æ¶æ„å½±å“** | <ul><li>åœ¨ LoRAâ€‘MoE æµç¨‹ä¸­åŠ å…¥ **`token_lora_mapping`**ï¼ˆæ¯ä¸ª token å¯¹åº”çš„ LoRA idï¼‰ï¼Œå¹¶å°†å…¶ä» **`token_mapping_meta`** ä¸­è§£è€¦ï¼Œä½¿å¾—åç«¯ kernel èƒ½å¤Ÿç›´æ¥æ ¹æ® token ç´¢å¼•è·å– LoRA idï¼Œé¿å…åŸæœ‰çš„ **`sorted_token_ids / expert_ids`** å¯¹é½æ­¥éª¤ã€‚</li><li>å¢åŠ äº† **`naive_block_assignment`** åˆ¤åˆ«ä½ï¼Œå½¢æˆä¸¤æ¡äº’æ–¥çš„æ‰§è¡Œè·¯å¾„ï¼š<br>â‘  ç»å…¸è·¯å¾„ï¼šå…ˆæ’åº â†’ å¯¹é½ â†’ æ‰§è¡Œ fused kernelã€‚<br>â‘¡ ç®€åŒ–è·¯å¾„ï¼šç›´æ¥ä½¿ç”¨å±•å¹³çš„ `topk_ids` ä½œä¸º `expert_ids`ï¼Œ`sorted_token_ids` ä¸º `None`ï¼Œè·³è¿‡å¯¹é½ã€‚</li></ul> |
| **æ€§èƒ½å½±å“** | <ul><li>**å‡å°‘å†…å­˜æ‹·è´ & æ’åºå¼€é”€**ï¼šåœ¨ç¨€ç–åœºæ™¯ï¼ˆtoken æ•°é‡ä¸ä¸“å®¶/LoRA æ•°é‡æ¯”ä¾‹å¾ˆå°ï¼‰ä¸‹ï¼Œ`moe_lora_align_block_size` ä¼šå æ®æ˜¾è‘—æ—¶é—´ã€‚æ–°è·¯å¾„æŠŠè¿™ä¸€æ­¥çœæ‰ï¼Œç†è®ºä¸Šå¯é™ä½ 30%â€‘70% çš„å‰ç½®å¼€é”€ï¼Œå°¤å…¶åœ¨æ¨ç† batchâ€‘size=1/2ã€topâ€‘k=1â€‘2 çš„åœºæ™¯ã€‚</li><li>**Kernel å‚æ•°ç®€åŒ–**ï¼š`_adjust_kernel_inputs` ä¸º `sorted_token_ids=None` æ—¶æŠŠ `grid_lora_dim` è®¾ä¸º 1ï¼Œé¿å…åœ¨ grid ä¸­å‡ºç°æ— æ„ä¹‰çš„ `max_loras+1` ç»´åº¦ã€‚</li><li>**Heuristic é€‰è·¯**ï¼š`SPARSITY_FACTOR=8` ä¸ºç»éªŒå€¼ï¼Œè‹¥ **è¯¯åˆ¤**ï¼ˆå³ç¨€ç–åº¦ä¸å¤Ÿå´èµ° naive è·¯å¾„ï¼‰å¯èƒ½å¯¼è‡´ **é¢å¤–çš„åˆ†æ”¯å¤±æ•ˆ**ï¼Œä½†å¯¹æ­£ç¡®æ€§çš„å½±å“å·²åœ¨ kernel ä¸­é€šè¿‡è¾¹ç•Œæ£€æŸ¥ï¼ˆ`lora_id >= max_loras`ã€`expert_id == -1`ï¼‰è¿›è¡Œé˜²æŠ¤ã€‚</li></ul> |
| **å®‰å…¨è€ƒè™‘** | <ul><li>æ‰€æœ‰æ–°åŠ å…¥çš„æŒ‡é’ˆè¯»å–å‡ä½¿ç”¨ **`tl.load(..., mask, other)`** è¿›è¡Œè¾¹ç•Œä¿æŠ¤ï¼›è‹¥ `token_lora_mapping` é•¿åº¦ä¸è¶³ï¼Œå°†è¿”å› `-1` å¹¶æå‰é€€å‡ºï¼Œé¿å…è¶Šç•Œå†™å…¥ã€‚</li><li>æ— å¤–éƒ¨æ¥å£æš´éœ²ï¼Œæ”¹åŠ¨ä»…é™å†…éƒ¨æ•°æ®æµã€‚</li></ul> |
| **å¯ç»´æŠ¤æ€§** | <ul><li>å‡½æ•°ç­¾åå¤§å¹…æ‰©å±•ï¼ˆ`sorted_token_ids` å˜ä¸ºå¯é€‰ï¼Œæ–°å¢ `token_lora_mapping` å‚æ•°ï¼‰ï¼Œå¯¹è°ƒç”¨æ–¹å…¼å®¹æ€§å·²é€šè¿‡é»˜è®¤ `None` å¤„ç†ï¼Œç°æœ‰ä»£ç ä»å¯ä¸æ”¹åŠ¨ä½¿ç”¨ã€‚</li><li>ä»£ç åˆ†æ”¯è¾ƒå¤šï¼ˆclassic vs naiveï¼‰ï¼Œå»ºè®®åœ¨æ–‡æ¡£ä¸­æ˜ç¡® **ä½•æ—¶ä¼šè§¦å‘ naive è·¯å¾„**ï¼Œå¹¶æä¾›é…ç½®å¼€å…³ï¼ˆå¦‚ç¯å¢ƒå˜é‡ï¼‰ä¾›è°ƒè¯•ã€‚</li></ul> |
| **æµ‹è¯•è¦†ç›–** | <ul><li>æ–°å¢ `test_fused_moe_lora_kernel_naive_block_assignment`ï¼Œè¦†ç›–äº†ä¸åŒ `num_tokensã€top_kã€num_expertsã€max_lorasã€block_size` çš„ç»„åˆï¼ŒéªŒè¯äº† naive è·¯å¾„ä¸å‚è€ƒå®ç°çš„æ•°å€¼ç­‰ä»·ã€‚</li><li>ç°æœ‰ `test_fused_moe_lora_kernel` ä»è¦†ç›– classic è·¯å¾„ï¼Œæ•´ä½“å•å…ƒæµ‹è¯•è¦†ç›–ç‡æå‡ã€‚</li></ul> |

---

## âš ï¸ æ½œåœ¨é£é™©

1. **Heuristic å¤±è¯¯**  
   - `SPARSITY_FACTOR=8` æ˜¯å›ºå®šç»éªŒå€¼ï¼Œè‹¥æ¨¡å‹æˆ–ç¡¬ä»¶ç‰¹æ€§å˜åŒ–ï¼ˆå¦‚æ›´å¤§ `block_size`ã€ä¸åŒ `NUM_WARPS`ï¼‰ï¼Œè¯¯åˆ¤ç‡å¯èƒ½ä¸Šå‡ï¼Œå¯¼è‡´ä¸æ°å½“åœ°èµ° naive è·¯å¾„ï¼Œå‡ºç° **æ€§èƒ½å›é€€**ï¼ˆå°¤å…¶åœ¨ä¸­ç­‰ç¨€ç–åº¦åœºæ™¯ï¼‰ã€‚  
2. **`token_lora_mapping` é•¿åº¦ä¸åŒ¹é…**  
   - è‹¥å¤–éƒ¨å±‚åœ¨æ„é€  `token_lora_mapping` æ—¶å‡ºç°é”™è¯¯ï¼ˆå¦‚å°‘äº†å‡ ä¸ª tokenï¼‰ï¼Œkernel ä¼šè¯»å–åˆ° `-1` å¹¶æå‰è¿”å›ï¼Œå¯¼è‡´ **è¾“å‡ºç¼ºå¤±**ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒéš¾ä»¥å®šä½ã€‚  
3. **åŒè·¯å¾„ä»£ç åˆ†å‰**  
   - ç»´æŠ¤æˆæœ¬ä¸Šå‡ï¼šä¸¤å¥—å¯¹é½/ç´¢å¼•é€»è¾‘éœ€è¦åŒæ­¥æ›´æ–°ï¼Œæœªæ¥è‹¥å¯¹ `sorted_token_ids` ç»“æ„åšæ”¹åŠ¨ï¼Œå®¹æ˜“é—æ¼ naive åˆ†æ”¯çš„ç›¸åº”ä¿®æ”¹ã€‚  
4. **ç¼–è¯‘/éƒ¨ç½²å…¼å®¹**  
   - Triton kernel å‚æ•°å˜åŒ–ï¼ˆæ–°å¢ `naive_block_assignment` å¸¸é‡ï¼‰è¦æ±‚é‡æ–°ç¼–è¯‘ `.so`ã€‚è‹¥æ—§äºŒè¿›åˆ¶ä»åœ¨ä½¿ç”¨ï¼Œå¯èƒ½å‡ºç° **ç­¾åä¸åŒ¹é…** é”™è¯¯ã€‚  
5. **å†…å­˜å ç”¨**  
   - åœ¨ classic è·¯å¾„ä»ç„¶ä¼šåˆ›å»º `sorted_ids`ã€`expert_ids`ã€`num_tokens_post_pad`ï¼Œå½“ `max_loras`ã€`max_num_tokens_padded` è¾ƒå¤§æ—¶ä»ä¼šäº§ç”Ÿè¾ƒé«˜çš„ä¸´æ—¶å†…å­˜ï¼Œè™½ç„¶ä¸å½±å“ naive è·¯å¾„ï¼Œä½†æ•´ä½“è¿›ç¨‹å³°å€¼ä»å—é™ã€‚

---

## ğŸ’¡ å…³æ³¨å»ºè®®

| å»ºè®® | è¯´æ˜ |
|------|------|
| **1ï¸âƒ£ æ–‡æ¡£åŒ– Heuristic** | åœ¨ `README` æˆ– `doc/api` ä¸­æ³¨æ˜ `

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (12)

### [Refactor] Move profiling methods to MM budget (#33559)
**SHA**: `d7e17aa` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d7e17aaacd5ed1b4b4be6bcfef3a1b7cbc84fc9a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šå°†åŸæœ¬æ•£è½åœ¨ `vllm/v1/worker/utils.py`ã€`encoder_cache_manager.py` ç­‰æ–‡ä»¶ä¸­çš„å¤šæ¨¡æ€é¢„ç®—è®¡ç®—ã€profiling é€»è¾‘æŠ½å–åˆ°æ–°æ¨¡å— `vllm/multimodal/budget.py`ï¼Œå¹¶ç»Ÿä¸€é€šè¿‡ `MultiModalBudget` ä¾›è°ƒåº¦å™¨ã€è¾“å…¥å¤„ç†å™¨ã€æ¨¡å‹ Runner ç­‰ä½¿ç”¨ã€‚ä¸æ­¤åŒæ—¶ï¼Œ`MultiModalRegistry` ä¸­ä¸é¢„ç®—ç›¸å…³çš„æ—§æ¥å£ (`get_max_tokens_per_item_by_modality`ã€`get_mm_limits_per_prompt`) è¢«åˆ é™¤æˆ–è¿ç§»ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/multimodal/budget.py`ï¼ˆæ–°å®ç°ï¼‰  
- `vllm/multimodal/registry.py`ï¼ˆåˆ å‡â€¯â‰ˆâ€¯90 è¡Œï¼‰  
- `vllm/v1/core/encoder_cache_manager.py`ï¼ˆåˆ é™¤ `compute_encoder_budget`ï¼Œæ”¹ä¸ºç›´æ¥ä½¿ç”¨ `compute_mm_encoder_budget`ï¼‰  
- `vllm/v1/core/sched/scheduler.py`ã€`vllm/v1/engine/input_processor.py`ã€`vllm/v1/worker/gpu_model_runner.py`ã€`vllm/v1/worker/utils.py`ï¼ˆå¤§é‡æ”¹ä¸ºä½¿ç”¨ `MultiModalBudget`ï¼‰  
- ç›¸å…³ import ä¸å±æ€§åçš„è°ƒæ•´ï¼ˆå¦‚ `mm_max_items_per_prompt`ã€`mm_max_items_per_batch`ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¬å…± API å…¼å®¹æ€§**ï¼š`MultiModalRegistry.get_max_tokens_per_item_by_modality` ç­‰å·²åˆ å‡çš„å‡½æ•°ä¹‹å‰å¯èƒ½è¢«å¤–éƒ¨æ’ä»¶æˆ–è„šæœ¬è°ƒç”¨ã€‚å»ºè®®åœ¨ `registry.py` ä¸­ä¿ç•™å…¼å®¹å±‚ï¼ˆæŠ›å‡º deprecation è­¦å‘Šï¼‰ï¼Œæˆ–åœ¨æ–‡æ¡£ä¸­æ˜ç¡®è¿ç§»è·¯å¾„ã€‚  
2. **é¢„ç®—è®¡ç®—ä¸€è‡´æ€§**ï¼šæ–°çš„ `MultiModalBudget` ä¸æ—§å®ç°çš„æ•°å€¼åº”ä¿æŒä¸€è‡´ã€‚å»ºè®®æ–°å¢å•å…ƒæµ‹è¯•ï¼Œå¯¹æ¯”ä¸¤ç§å®ç°ä¸‹ `encoder_compute_budget`ã€`encoder_cache_size`ã€`mm_max_items_per_prompt` ç­‰å…³é”®å­—æ®µçš„ç»“æœã€‚  
3. **å¾ªç¯ä¾èµ–**ï¼š`budget.py` ç›´æ¥å¼•ç”¨ `vllm.multimodal.processing` ä¸ `vllm.multimodal.registry`ï¼Œè€Œåè€…åˆåœ¨æ–‡ä»¶åº•éƒ¨ä½¿ç”¨ `MultiModalBudget`ï¼ˆé—´æ¥ï¼‰ã€‚è¯·ç¡®è®¤ import é¡ºåºä¸äº§ç”Ÿå¾ªç¯å¯¼å…¥é—®é¢˜ï¼Œæˆ–ä½¿ç”¨å±€éƒ¨ importã€‚  
4. **ç¼“å­˜é‡ç½®**ï¼š`InputProcessor` ç°åœ¨åœ¨æ„é€ æ—¶è°ƒç”¨ `mm_budget.reset_cache()`ï¼Œä½†éšåå¹¶æœªå†ä½¿ç”¨è¯¥ç¼“å­˜ã€‚ç¡®è®¤è¿™ä¸€æ­¥ä¸ä¼šå¯¼è‡´é‡å¤åˆ›å»º/æ¸…ç†å¯¼è‡´çš„æ€§èƒ½æ³¢åŠ¨ï¼Œæˆ–åœ¨éœ€è¦æ—¶æ˜¾å¼ç®¡ç†ç¼“å­˜ç”Ÿå‘½å‘¨æœŸã€‚  
5. **æ–‡æ¡£ä¸ç¤ºä¾‹æ›´æ–°**ï¼šå¤šæ¨¡æ€æ¨¡å‹çš„ä½¿ç”¨è¯´æ˜ã€è°ƒåº¦å™¨å‚æ•°è§£é‡Šç­‰åº”åŒæ­¥æ”¹ä¸ºå±•ç¤º `MultiModalBudget` çš„é…ç½®å…¥å£ï¼ˆå¦‚ `scheduler_config.max_num_seqs`ã€`model_config.max_model_len` ä¸ `limit_mm_per_prompt` çš„å…³ç³»ï¼‰ã€‚  
6. **å›å½’æµ‹è¯•**ï¼šé‡ç‚¹è·‘åŒ…å«è§†è§‰ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€æ¨¡å‹çš„å…¨é“¾è·¯æµ‹è¯•ï¼ŒéªŒè¯ï¼šâ‘  encoderâ€‘cache æ­£å¸¸åˆ›å»ºã€â‘¡ `profile_run` ä¸­çš„ `max_mm_items_per_batch` è®¡ç®—ä¸å†å‡ºç° KeyErrorï¼Œâ‘¢ å…¼å®¹ â€œtextâ€‘onlyâ€ æ¨¡å‹çš„æ—§è·¯å¾„ï¼ˆ`supports_mm_inputs` ä¸º False æ—¶ä»èƒ½æ­£å¸¸å·¥ä½œï¼‰ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡é‡æ„æŠŠé¢„ç®—/profiling é€»è¾‘ç»Ÿä¸€æŠ½è±¡ï¼Œä»£ç ç»“æ„æ›´æ¸…æ™°ï¼Œå‡å°‘é‡å¤å®ç°ã€‚åªè¦æ³¨æ„å…¼å®¹æ€§ã€ç¼“å­˜ç®¡ç†å’Œå……åˆ†çš„å›å½’æµ‹è¯•ï¼Œå°±èƒ½å¹³æ»‘è¿ç§»åˆ°æ–°å®ç°ã€‚

---

### [Bugfix] Enable Kimi k25 processor test (#33562)
**SHA**: `4061dcf` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/4061dcf4c51ae33aee4bc73096f82da29b580c57)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šä¸º Kimiâ€‘K2.5ï¼ˆmoonshotai/Kimiâ€‘K2.5ï¼‰æ¨¡å‹åœ¨ç¦»çº¿æ¨ç†æ—¶åŠ å…¥å¯¹ **vision_chunk**ï¼ˆå›¾åƒå—ï¼‰è¾“å…¥çš„å®Œæ•´æ”¯æŒã€‚åŒ…æ‹¬ï¼š  
1. åœ¨ `examples/offline_inference/vision_language.py` ä¸­æ–°å¢ `run_kimi_k25` ç¤ºä¾‹å¹¶åœ¨å‚æ•°æ˜ å°„ã€é»˜è®¤é™åˆ¶ä¸­åŠ å…¥ `vision_chunk`ã€‚  
2. `vllm/model_executor/models/kimi_k25.py` é‡æ„å¤„ç†å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥å—å¯é€‰çš„ `media_token_id`ï¼Œåœ¨ `__call__` æ—¶æŠŠå ä½ token æ›¿æ¢ä¸ºå®é™…çš„åª’ä½“ token é•¿åº¦ï¼Œå¹¶å…¼å®¹ `text` æ—¢å¯ä»¥æ˜¯å­—ç¬¦ä¸²ä¹Ÿå¯ä»¥æ˜¯ token åˆ—è¡¨ã€‚  
3. `vllm/multimodal/parse.py` æ”¾å®½å¯¹ `vision_chunk` æ•°æ®çš„è§£æï¼Œæ¥å—å•ä¸ª dict å¹¶è½¬æˆåˆ—è¡¨ã€‚  
4. æµ‹è¯•å±‚é¢æ–°å¢ `random_vision_chunk` ç”Ÿæˆå™¨ã€åœ¨ `test_common.py` ä¸­æ³¨å†Œ `vision_chunk` ç¤ºä¾‹å¹¶å–æ¶ˆå¯¹ Kimiâ€‘K2.5 çš„è·³è¿‡ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `examples/offline_inference/vision_language.py`ï¼ˆæ–°å¢ç¤ºä¾‹ï¼‰  
- `vllm/model_executor/models/kimi_k25.py`ï¼ˆæ ¸å¿ƒå¤„ç†å™¨é€»è¾‘ï¼‰  
- `vllm/multimodal/parse.py`ï¼ˆå¤šæ¨¡æ€è§£æï¼‰  
- `tests/models/multimodal/processing/test_common.py`ï¼ˆæµ‹è¯•ç”Ÿæˆä¸éªŒè¯ï¼‰  
- ç›¸å…³ CLI å‚æ•° (`--modality`) ä»¥åŠé»˜è®¤ `limit_mm_per_prompt` é…ç½®  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å¼€å‘è€…**ï¼šç¡®è®¤ `media_token_id` åœ¨æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­æ­£ç¡®å¡«å…¥ï¼›è‹¥è‡ªè¡Œå®ç°è‡ªå®šä¹‰ `media_processor`ï¼Œéœ€ä¿è¯ `media_tokens_calculator` ä¸å ä½ token æ•°ä¿æŒä¸€è‡´ã€‚  
- **ç”¨æˆ·**ï¼šä½¿ç”¨ `vision_chunk` æ—¶è¯·åœ¨ `EngineArgs.limit_mm_per_prompt` ä¸­æ˜¾å¼è®¾ç½® `vision_chunk` çš„ä¸Šé™ï¼›å¦‚å¼€å¯å¤šå¡å¹¶è¡Œï¼ˆtensor_parallelï¼‰ï¼Œä¿æŒ `tensor_parallel_size` ä¸æ¨¡å‹æ¨èçš„ 4 åŒ¹é…ã€‚  
- **CI/æµ‹è¯•**ï¼šæ–°åŠ å…¥çš„ `random_vision_chunk` ä»…äº§ç”Ÿå•å¸§å—ï¼Œè‹¥åç»­éœ€è¦å¤šå¸§è§†é¢‘å—ï¼Œè¯·ç›¸åº”æ‰©å±• `min_frames/max_frames` å‚æ•°å¹¶æ›´æ–°å¯¹åº”æ–­è¨€ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æäº¤æ¢å¤äº† Kimiâ€‘K2.5 å¯¹ vision_chunk çš„ç¦»çº¿æ¨ç†èƒ½åŠ›ï¼Œä»£ç è·¯å¾„æ¸…æ™°ï¼Œå½±å“èŒƒå›´ä¸»è¦é™å®šåœ¨å¤šæ¨¡æ€è§£æä¸æ¨¡å‹å¤„ç†å™¨ï¼Œé£é™©è¾ƒä½ã€‚å»ºè®®åœ¨ä¸‹ä¸€æ¬¡å‘å¸ƒå‰æ‰§è¡Œå®Œæ•´çš„å¤šæ¨¡æ€å›å½’æµ‹è¯•ï¼Œä»¥ç¡®ä¿æ–° token æ›¿æ¢é€»è¾‘æœªå¯¹å…¶å®ƒæ¨¡å‹äº§ç”Ÿå‰¯ä½œç”¨ã€‚

---

### [MoE] Enable Shared/Routed Overlap For Latent MoE (Nemotron-H) (#32790)
**SHA**: `0aca8b8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0aca8b8c628e9a73ab8758d78c9c721bc703ee66)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šä¸º Nemotronâ€‘H ä¸­çš„ latentâ€¯MoE å¼•å…¥äº† â€œsharedâ€¯/â€¯routed overlapâ€ æœºåˆ¶ã€‚æ–°å¢ `routed_input_transform`ï¼ˆå³ `fc1_latent_proj`ï¼‰åœ¨è¿›å…¥è·¯ç”±ä¸“å®¶å‰è¿›è¡ŒæŠ•å½±ï¼ŒåŒæ—¶ä¿æŒåŸå§‹ hiddenâ€¯states ä¾›å…±äº«ä¸“å®¶ä½¿ç”¨ï¼›é€šè¿‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®‰å…¨ä¿å­˜/æ¢å¤è¯¥åŸå§‹å¼ é‡ã€‚é…å¥—å•å…ƒæµ‹è¯•éªŒè¯äº†â€œå†…éƒ¨å˜æ¢â€å’Œâ€œå¤–éƒ¨æ‰‹åŠ¨å˜æ¢â€å¾—åˆ°çš„è·¯ç”±ä¸å…±äº«è¾“å‡ºä¸€è‡´ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/model_executor/layers/fused_moe/layer.py`ï¼ˆæ ¸å¿ƒ MoE è°ƒåº¦é€»è¾‘ï¼‰  
- `vllm/model_executor/layers/fused_moe/shared_fused_moe.py`ï¼ˆæ–°å¢ `routed_input_transform` æ”¯æŒï¼‰  
- `vllm/model_executor/models/nemotron_h.py`ï¼ˆæ¨¡å‹æ„é€ æ—¶æŒ‚è½½æŠ•å½±å±‚å¹¶ä¼ é€’åˆ° `SharedFusedMoE`ï¼‰  
- æ–°å¢ `tests/kernels/moe/test_shared_fused_moe_routed_transform.py`ï¼ˆåŠŸèƒ½éªŒè¯ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æ¥å£å…¼å®¹**ï¼š`apply_routed_input_transform` é»˜è®¤è¿”å›åŸå¼ é‡ï¼Œä¿æŒå‘åå…¼å®¹ï¼›ä½†å¤–éƒ¨è°ƒç”¨è€…ï¼ˆå¦‚è‡ªå®šä¹‰ MoE å­ç±»ï¼‰è‹¥è¦†ç›–æ­¤æ–¹æ³•ï¼Œéœ€è¦ç¡®ä¿ `self._shared_experts_input` æ­£ç¡®è®¾ç½®ã€‚  
2. **å†…å­˜ä¸åŒæ­¥**ï¼š`_set_shared_experts_input` åœ¨è‡ªå®šä¹‰ C++/CUDA op `moe_forward_shared` ä¸­ä½¿ç”¨ï¼Œç¡®ä¿åœ¨å¤šæµå¹¶è¡Œæ—¶ä¸ä¼šå‡ºç°æ—§å¼ é‡è¢«è¦†ç›–çš„ race æ¡ä»¶ï¼›å»ºè®®åœ¨ CI ä¸­åŠ å…¥å¤š GPUã€ä¸åŒ `tp/pp` é…ç½®çš„å‹åŠ›æµ‹è¯•ã€‚  
3. **é‡åŒ–è·¯å¾„**ï¼š`apply_routed_input_transform` å¯èƒ½è¿”å› `(output, extra_bias)`ï¼ˆReplicatedLinear çš„æ—§è¡Œä¸ºï¼‰ï¼Œç›®å‰å·²å‰¥ç¦» `extra_bias`ï¼›è‹¥åç»­åœ¨é‡åŒ–æˆ– FP8 åœºæ™¯ä¸‹éœ€è¦é¢å¤–ä¿¡æ¯ï¼Œè¯·åŒæ­¥æ›´æ–°æ­¤å¤„ã€‚  
4. **å ä½ä¸ pad é€»è¾‘**ï¼š`forward_native` ç°åœ¨åŸºäº **å˜æ¢å** çš„ç»´åº¦è¿›è¡Œ padding ä¸åˆ‡ç‰‡ï¼ˆ`transformed_hidden_dim`ï¼‰ï¼Œç¡®è®¤æ‰€æœ‰æ—§æ¨¡å‹åœ¨ä¸ä½¿ç”¨ `routed_input_transform` æ—¶ä»ä¿æŒåŸè¡Œä¸ºã€‚  
5. **æµ‹è¯•è¦†ç›–**ï¼šç°æœ‰å•æµ‹è¦†ç›–äº†ä¸åŒ token æ•°ã€hidden/latent å¤§å°ã€bfloat16 ç­‰ç»„åˆï¼Œå»ºè®®å†è¡¥å…… FP16ã€int8 ä»¥åŠ TP/PP å¹¶è¡Œçš„ç»„åˆï¼Œé˜²æ­¢éšè—çš„ dtype/parallel bugã€‚  

æ€»ä½“æ¥è¯´ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸º latentâ€¯MoE æä¾›äº†æ›´é«˜æ•ˆçš„è®¡ç®—è·¯å¾„ï¼Œä»£ç ç»“æ„æ¸…æ™°ã€å‘åå…¼å®¹æ€§è‰¯å¥½ã€‚åªéœ€å…³æ³¨å¤šæµåŒæ­¥å’Œé‡åŒ–è·¯å¾„çš„ç»†èŠ‚å³å¯å®‰å…¨æŠ•å…¥ä¸»åˆ†æ”¯ã€‚

---

### fix[ROCm]: Remove unconditional aiter import (#32902)
**SHA**: `9eb58f8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9eb58f8cf123dde43a97216cd7fa0f9c7af81043)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. åœ¨ ROCm ç¯å¢ƒä¸‹ï¼ŒåŠ å…¥å¯¹ `VLLM_ROCM_USE_AITER` ç¯å¢ƒå˜é‡çš„æ£€æµ‹ï¼Œä»…åœ¨è¯¥å˜é‡ä¸ºçœŸä¸”è®¾å¤‡ä¸º gfx9ã€`aiter` åº“å¯ç”¨æ—¶æ‰å¯ç”¨ AITERï¼›  
2. ç§»é™¤æ— æ¡ä»¶çš„ `import aiter`ï¼Œæ”¹ä¸ºæ¡ä»¶å¯¼å…¥å¹¶åœ¨æœªå¯ç”¨æ—¶æä¾›å ä½ `AIER_FP8_DTYPE`ï¼›  
3. ç›¸åº”åœ°åœ¨ `rocm_aiter_fa` åç«¯å’Œ `eagle` è§„æ ¼è§£ç ä¸­åŠ å…¥ `rocm_aiter_ops.is_enabled()` åˆ¤æ–­ï¼Œé˜²æ­¢åœ¨æ²¡æœ‰ AITER çš„æœºå™¨ä¸ŠåŠ è½½å¤±è´¥ã€‚

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/_aiter_ops.py`ï¼ˆå¹³å°æ£€æµ‹ã€è£…é¥°å™¨ã€å ä½å˜é‡ï¼‰  
- `vllm/v1/attention/backends/rocm_aiter_fa.py`ï¼ˆåç«¯å®ç°çš„æ¡ä»¶å¯¼å…¥ï¼‰  
- `vllm/v1/spec_decode/eagle.py`ï¼ˆè§„æ ¼è§£ç æ—¶çš„åç«¯å¯ç”¨æ€§æ£€æŸ¥ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
- **æ–‡æ¡£**ï¼šåœ¨ ROCm éƒ¨ç½²æŒ‡å—ä¸­è¯´æ˜ `VLLM_ROCM_USE_AITER=1` çš„ä½œç”¨åŠæ”¯æŒçš„ GPU æ¶æ„ï¼ˆgfx9ï¼‰ã€‚  
- **æµ‹è¯•**ï¼šCI ä¸­åŠ å…¥ä¸¤å¥— ROCm åœºæ™¯ï¼šâ‘  `VLLM_ROCM_USE_AITER` æœªè®¾æˆ–è®¾ä¸º 0ï¼Œç¡®ä¿åº“èƒ½å¤Ÿæ­£å¸¸å¯åŠ¨ä¸”ä¸è§¦å‘ `ImportError`ï¼›â‘¡ ç¯å¢ƒå˜é‡å¼€å¯ä¸” `aiter` å¯ç”¨ï¼ŒéªŒè¯ç›¸å…³ FA åç«¯ä»ç„¶å·¥ä½œã€‚  
- **ä»£ç **ï¼šç¡®è®¤ `rocm_aiter_ops.is_enabled()` å·²æ­£ç¡®å®šä¹‰ä¸º `is_aiter_found_and_supported()` çš„åˆ«åï¼Œé¿å…å‡ºç°æœªå®ç°å‡½æ•°å¯¼è‡´çš„è¿è¡Œæ—¶é”™è¯¯ã€‚  
- **å…¼å®¹æ€§**ï¼šå¦‚æœç”¨æˆ·åœ¨é ROCm ç¯å¢ƒæˆ–ä¸æ”¯æŒ gfx9 çš„ GPU ä¸Šè®¾ç½®äº†è¯¥ç¯å¢ƒå˜é‡ï¼Œåº“ä»ä¼šå®‰å…¨å›é€€ï¼Œä¸ä¼šå°è¯•å¯¼å…¥ `aiter`ã€‚  

æ•´ä½“æ¥è¯´ï¼Œæ­¤æ¬¡ä¿®æ”¹æå‡äº† ROCm ç¯å¢ƒçš„é²æ£’æ€§ï¼Œé¿å…äº†å› ç¼ºå¤± `aiter` è€Œå¯¼è‡´çš„å¯åŠ¨å¤±è´¥ï¼Œåªè¦éµå¾ªæ–‡æ¡£è¯´æ˜å³å¯å¹³æ»‘ä½¿ç”¨ã€‚

---

### [Model] Use explicit types in `get_generation_prompt` (#33551)
**SHA**: `b10d05b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/b10d05b8a8aec29ae507405f0a08bf7facbb0551)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / ç±»å‹å®‰å…¨æå‡  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨å¤šä¸ª STTï¼ˆspeechâ€‘toâ€‘textï¼‰æ¨¡å‹ä»¥åŠ OpenAI ç¿»è¯‘å…¥å£ä¸­ï¼Œå¼•å…¥äº†æ˜¾å¼çš„ Prompt ç±»å‹ï¼ˆ`TextPrompt`ã€`TokensPrompt`ã€`ExplicitEncoderDecoderPrompt`ï¼‰ï¼Œå¹¶ä½¿ç”¨ `is_explicit_encoder_decoder_prompt` è¿›è¡Œæ ¡éªŒã€‚åŸæ¥é€šè¿‡ `cast(PromptType, dict)` çš„â€œå­—å…¸æ¨¡æ‹Ÿâ€å®ç°è¢«ç»Ÿä¸€ä¸ºç»“æ„åŒ– dataclass/TypedDictï¼Œæå‡äº†ä»£ç å¯è¯»æ€§å’Œç±»å‹æ£€æŸ¥èƒ½åŠ›ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm.inputs.data`ï¼ˆæ–°å¢æ˜¾å¼ Prompt ç±»å‹ï¼‰  
- `vllm.inputs.parse`ï¼ˆæ–°å¢ `is_explicit_encoder_decoder_prompt`ï¼‰  
- æ‰€æœ‰ä½¿ç”¨ `get_generation_prompt` çš„æ¨¡å‹å®ç°ï¼š`gemma3n_mm.pyã€glmasr.pyã€granite_speech.pyã€qwen3_asr.pyã€voxtral.pyã€voxtral_realtime.pyã€whisper.py`  
- OpenAI ç¿»è¯‘å…¥å£ `vllm/entrypoints/openai/translations/speech_to_text.py`ï¼ˆæ”¹ä¸ºæ˜¾å¼æ ¡éªŒå¹¶ç»Ÿä¸€å¤„ç† verboseâ€‘json çš„ decoder_promptï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§**ï¼šè™½ç„¶å†…éƒ¨å·²æ”¹ä¸ºç»“æ„åŒ– Promptï¼Œå¤–éƒ¨ä»é€šè¿‡ `PromptType` æ¥å£ä½¿ç”¨ã€‚è¯·ç¡®è®¤é¡¹ç›®å…¶å®ƒä½ç½®ï¼ˆå¦‚è°ƒåº¦å™¨ã€ç¼“å­˜ã€æ—¥å¿—ç­‰ï¼‰å¯¹ Prompt çš„è®¿é—®æ–¹å¼å·²é€‚é… `ExplicitEncoderDecoderPrompt`/`TextPrompt`/`TokensPrompt`ï¼Œé˜²æ­¢å‡ºç° `KeyError`ã€‚  
2. **æµ‹è¯•**ï¼šæ–°å¢å•å…ƒæµ‹è¯•è¦†ç›–ä»¥ä¸‹åœºæ™¯ï¼š  
   - `verbose_json` è¯·æ±‚çš„ decoder_prompt æ›¿æ¢é€»è¾‘ï¼ˆ`<|notimestamps|>` â†’ `<|0.00|>`ï¼‰  
   - `ExplicitEncoderDecoderPrompt` çš„ encoder/decoder å‡ä¸º `TextPrompt` çš„æƒ…å½¢ï¼ˆå¦‚ Whisperï¼‰  
   - `TokensPrompt` åœ¨ä¸åŒæ¨¡å‹è¿”å›çš„ `prompt_token_ids` ä¸ `multi_modal_data` ç»„åˆæ˜¯å¦æ­£ç¡®ã€‚  
3. **æ–‡æ¡£/ç¤ºä¾‹**ï¼šæ›´æ–° Prompt ç›¸å…³çš„ç”¨æˆ·æ–‡æ¡£ï¼Œè¯´æ˜ç°åœ¨æ¨èä½¿ç”¨ `TextPrompt`/`TokensPrompt` æ„é€ è¯·æ±‚ï¼Œæ—§çš„æ‰‹å†™ dict æ–¹æ³•ä»å¯å·¥ä½œä½†ä¸æ¨èã€‚  
4. **ç±»å‹æ£€æŸ¥**ï¼šç¡®ä¿é¡¹ç›® CI ä¸­çš„ `mypy` / `pyright` é…ç½®å·²åŒ…å«æ–°å¯¼å…¥çš„ç±»å‹ï¼Œé¿å…å› æœªå¯¼å…¥ `ExplicitEncoderDecoderPrompt` è€Œå¯¼è‡´çš„éšå¼ Any è­¦å‘Šã€‚  
5. **æ€§èƒ½**ï¼šç±»å‹åŒ…è£…æœ¬èº«å¼€é”€æå°ï¼Œä½†å› åœ¨å¤šä¸ªæ¨¡å‹ä¸­é¢å¤–åˆ›å»ºå¯¹è±¡ï¼Œå»ºè®®åœ¨é«˜å¹¶å‘æ¨ç†è·¯å¾„ä¸Šè¿è¡ŒåŸºå‡†ï¼Œç¡®è®¤æ²¡æœ‰æ˜¾è‘—å»¶è¿Ÿã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨æ˜¾è‘—æå‡äº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œé™æ€å®‰å…¨æ€§ï¼Œåªè¦åšå¥½å…¼å®¹æ€§æ£€æŸ¥ä¸æµ‹è¯•è¦†ç›–ï¼Œé£é™©å¯æ§ã€‚

---

### [Chore] Remove redundant input parsing methods (#33542)
**SHA**: `a502831` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a502831d36426ed3e0eee99ba4bb03490734381b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–ï¼ˆä»£ç æ¸…ç† / åŠŸèƒ½æŠ½è±¡ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤åˆ é™¤äº† `vllm.inputs.parse.get_prompt_components` ä¹‹å¤–çš„å†—ä½™è§£æå®ç°ã€‚  
* `is_embeds_prompt / is_tokens_prompt` ä¸¤ä¸ª Typeâ€‘guard è¢«å½»åº•ç§»é™¤ï¼Œæ‰€æœ‰å…¥å£æ”¹ä¸ºç›´æ¥ä½¿ç”¨ `get_prompt_components`ï¼ˆæˆ–å…¶åŒ…è£… `get_prompt_text`ï¼‰è·å– prompt æ–‡æœ¬ã€‚  
* `EmbedsPrompt` å¢åŠ äº†å¯é€‰ `prompt` å­—æ®µï¼Œä»¥å…¼å®¹ä»éœ€ä¿ç•™åŸå§‹æ–‡æœ¬çš„åœºæ™¯ã€‚  
* ç›¸åº”çš„å•å…ƒæµ‹è¯•ã€OpenAI æ¥å£å®ç°ã€å†…éƒ¨ `llm_engine` ä¸å·¥å…·å‡½æ•°ä¹ŸåŒæ­¥æ”¹å†™ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm.inputs.data`ï¼ˆPrompt æ•°æ®æ¨¡å‹ï¼‰  
- `vllm.inputs.parse`ï¼ˆç»Ÿä¸€çš„ `get_prompt_components`ï¼‰  
- OpenAI å…¼å®¹å±‚ï¼š`entrypoints/openai/completion/serving.py`ã€`chat_completion/serving.py`ã€`engine/serving.py`  
- V1 å¼•æ“ï¼š`v1/engine/llm_engine.py`ã€`v1/engine/utils.py`  
- æµ‹è¯•ï¼š`tests/renderers/test_completions.py`

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šå¤–éƒ¨æ’ä»¶æˆ–è‡ªå®šä¹‰ IOâ€‘processor ä»å¯èƒ½ä¾èµ–å·²åˆ é™¤çš„ `is_*_prompt`ï¼Œè¯·ç¡®è®¤å®ƒä»¬å·²æ”¹ä¸ºä½¿ç”¨ `get_prompt_components` æˆ–æ£€æŸ¥ `prompt` æ˜¯å¦å­˜åœ¨ã€‚  
2. **æ•°æ®æ¨¡å‹**ï¼š`EmbedsPrompt` ç°åœ¨å¯æºå¸¦ `prompt`ï¼Œè‹¥ä¸šåŠ¡éœ€è¦å›ä¼ åŸå§‹æ–‡æœ¬ï¼Œè¯·åœ¨æ„é€ æ—¶æ˜¾å¼åŠ å…¥è¯¥é”®ï¼Œä»¥å…å‡ºç° `None`ã€‚  
3. **æ€§èƒ½**ï¼šç»Ÿä¸€è§£æè·¯å¾„æ¶ˆé™¤äº†é‡å¤ `isinstance` æ£€æŸ¥ï¼Œç†è®ºä¸Šç•¥æœ‰æå‡ï¼Œä½†ä¸»è¦æ˜¯ä»£ç å¯ç»´æŠ¤æ€§æ”¹å–„ã€‚  
4. **æµ‹è¯•**ï¼šå·²æ›´æ–°çš„æµ‹è¯•è¦†ç›–äº†åµŒå…¥â€‘prompt ä¸ tokenâ€‘prompt ä¸¤ç§æƒ…å†µï¼Œå»ºè®®åœ¨æœ¬åœ° CI ä¸­å…¨è·‘ä¸€éï¼Œç¡®ä¿æœªé—æ¼å…¶ä»–æ¨¡å—çš„ `is_*_prompt` è°ƒç”¨ã€‚  
5. **æ–‡æ¡£**ï¼šæ›´æ–°ç›¸å…³æ–‡æ¡£ï¼ˆå¦‚ Prompt ç±»å‹è¯´æ˜ï¼‰ä»¥åæ˜ æ–°çš„å¯é€‰ `prompt` å­—æ®µå’Œå·²åºŸå¼ƒçš„è¾…åŠ©å‡½æ•°ã€‚  

æ•´ä½“è€Œè¨€ï¼Œæ­¤æ¬¡å˜æ›´æå‡äº†è¾“å…¥è§£æçš„ä¸€è‡´æ€§ï¼Œé™ä½äº†é‡å¤å®ç°çš„ç»´æŠ¤æˆæœ¬ï¼Œé£é™©ä¸»è¦é›†ä¸­åœ¨å¤–éƒ¨ä¾èµ–çš„å…¼å®¹æ€§ä¸Šã€‚åšå¥½ä¸Šè¿°æ£€æŸ¥å³å¯å®‰å…¨å‡çº§ã€‚

---

### [Misc] support arbitrary MM datasets in spec dec bench (#33486)
**SHA**: `ba871fb` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ba871fb788f5036b8f55f61ffc7c05e367dacc5a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ benchmark æ¡†æ¶ä¸­æ–°å¢ `CustomMMDataset`ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡æœ¬åœ° `.jsonl` æ–‡ä»¶ç›´æ¥å¯¹ä»»æ„å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆåŒ…å« `prompt` ä¸ `image_files`ï¼‰è¿›è¡Œæµ‹è¯„ã€‚CLI æ–‡æ¡£ã€`spec_decode.py` ç¤ºä¾‹ä»¥åŠæ•°æ®é›†è§£æé€»è¾‘åŒæ­¥æ›´æ–°ï¼ŒåŠ å…¥ `--backend openai-chat`ã€`--allowed-local-media-path`ã€`--enable-multimodal-chat` ç­‰å‚æ•°ï¼Œä»¥æ”¯æŒ OpenAIâ€‘Chat å¤šæ¨¡æ€å…¥å£å¹¶ç»Ÿä¸€å›¾ç‰‡è·¯å¾„é™åˆ¶ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/benchmarks/datasets.py`ï¼ˆæ ¸å¿ƒæ•°æ®æŠ½å–ä¸ `CustomMMDataset` å®ç°ï¼‰  
- `examples/offline_inference/spec_decode.py`ï¼ˆç¤ºä¾‹è„šæœ¬åŠ å…¥å¤šæ¨¡æ€å‚æ•°ã€è°ƒæ•´ç”Ÿæˆè·¯å¾„ï¼‰  
- `docs/benchmarking/cli.md`ï¼ˆæ–‡æ¡£è¯´æ˜æ–°å¢ â€œCustom multimodal datasetâ€ï¼‰  
- ç›¸å…³çš„ CLI å‚æ•°è§£æ (`add_dataset_parser`)  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **è·¯å¾„å®‰å…¨**ï¼š`allowed_local_media_path` å¿…é¡»åœ¨æœåŠ¡ç«¯æ˜¾å¼æ ¡éªŒï¼Œé˜²æ­¢ä»»æ„æ–‡ä»¶è¯»å–ï¼›å»ºè®®åœ¨ `LLMEngine` åˆå§‹åŒ–æ—¶åŠ å…¥è·¯å¾„ç™½åå•æ£€æŸ¥ã€‚  
2. **å…¼å®¹æ€§**ï¼šé»˜è®¤ä»èµ° `generate` æµç¨‹ï¼Œåªæœ‰ `backend=openaiâ€‘chat` æ—¶æ‰èµ° `chat`ï¼Œç¡®ä¿å·²æœ‰éå¤šæ¨¡æ€åŸºå‡†ä¸å—å½±å“ã€‚  
3. **å¤šæ¨¡æ€ Chat å˜æ¢**ï¼š`enable_multimodal_chat` åªåœ¨ `CustomMMDataset` ä¸éƒ¨åˆ†å·²æœ‰æ•°æ®é›†ï¼ˆå¦‚ `random_mm`ï¼‰ä¸­ä½¿ç”¨ï¼Œéœ€ç¡®è®¤åœ¨ `apply_multimodal_chat_transformation` ä¸­å¯¹ `prompt_len` çš„é‡æ–°è®¡æ•°ä¸å½±å“è°ƒåº¦å™¨çš„ `prompt_len` ç»Ÿè®¡ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šæ–°å¢å•å…ƒæµ‹è¯•æ£€æŸ¥ï¼šâ‘  å•å¼ å›¾ç‰‡æ­£å¸¸åŠ è½½ï¼›â‘¡ å¤šå¼ å›¾ç‰‡ä»…å–ç¬¬ä¸€å¼ å¹¶ç»™å‡ºè­¦å‘Šï¼›â‘¢ `--allowed-local-media-path` è¶…å‡ºé™åˆ¶æ—¶æŠ›é”™ã€‚  
5. **æ–‡æ¡£åŒæ­¥**ï¼šç¤ºä¾‹ä¸­ `vllm bench serve--save-result` å°‘äº†ç©ºæ ¼ï¼Œéœ€è¦åœ¨æ–‡æ¡£æˆ–è„šæœ¬ä¸­ç»Ÿä¸€ä¸º `vllm bench serve --save-result`ã€‚  

æ€»ä½“æ¥è¯´ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸ºå¤šæ¨¡æ€åŸºå‡†æä¾›äº†çµæ´»å…¥å£ï¼Œè‹¥åœ¨éƒ¨ç½²ç¯å¢ƒä¸­ä¸¥æ ¼æ§åˆ¶æœ¬åœ°åª’ä½“è·¯å¾„å¹¶è¡¥å…¨ç›¸åº”æµ‹è¯•ï¼Œå¯å®‰å…¨ä¸Šçº¿ã€‚

---

### [Bugfix] GLM-4 tool parser: incremental string streaming (#33218)
**SHA**: `7c03643` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/7c036432fc3e545487540460bec42aef58a8874d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º GLMâ€‘4 å·¥å…·è§£æå™¨æ–°å¢å¢é‡å­—ç¬¦ä¸²æµå¼æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
- åœ¨ `vllm/tool_parsers/glm4_moe_tool_parser.py` ä¸­å®ç°äº†å…¨æ–°æµå¼çŠ¶æ€æœºï¼Œèƒ½å¤Ÿåœ¨ `<arg_value>` æ ‡ç­¾å†…éƒ¨å¯¹å­—ç¬¦ä¸²å‚æ•°é€å­—ç¬¦è¾“å‡ºï¼Œè§£å†³ Issueâ€¯#32829 ä¸­é•¿å­—ç¬¦ä¸²ï¼ˆå¦‚ä»£ç ï¼‰å¯¼è‡´çš„å‡ ç§’å¡é¡¿ã€‚  
- æ–°å¢ `_is_string_typeã€_deserializeã€_json_escape_string_content` ç­‰å·¥å…·å‡½æ•°ï¼Œå¹¶å¼•å…¥ `make_tool_call_id` ç”Ÿæˆå”¯ä¸€ toolâ€‘call idã€‚  
- è°ƒæ•´ `extract_tool_calls_streaming` å®ç°ï¼šæ£€æµ‹å·¥å…·æ˜¯å¦å¯ç”¨ã€é€æ­¥è§£æ `<tool_call>`ã€å·¥å…·åç§°ã€é”®å€¼å¯¹ã€å¢é‡å­—ç¬¦ä¸²ã€ç»“æŸæ ‡ç­¾ï¼Œå¹¶åœ¨é€‚å½“æ—¶è¿”å› `DeltaMessage`ã€‚  
- ä¸ºæ–°è¡Œä¸ºæ·»åŠ äº†å¤§é‡å•å…ƒæµ‹è¯•ï¼ˆå¢é‡å­—ç¬¦ä¸²ã€ç©º toolã€å¤šä¸ª toolã€JSON è½¬ä¹‰ã€æ•°å€¼ååºåˆ—åŒ–ã€é•¿å†…å®¹åˆ†ç‰‡ç­‰ï¼‰ï¼Œæå‡è¦†ç›–ç‡ã€‚

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- **æ ¸å¿ƒè§£æå™¨**ï¼š`Glm4MoeModelToolParser` çš„å†…éƒ¨çŠ¶æ€ã€æµå¼é€»è¾‘å…¨éƒ¨æ”¹å†™ï¼Œæ¶‰åŠ `self._bufferã€_in_tool_callã€_streaming_string_value` ç­‰æ–°æˆå‘˜ã€‚  
- **è¯·æ±‚é€‚é…**ï¼šåŠ å…¥ `adjust_request`ã€`_tools_enabled` åˆ¤æ–­ï¼Œå¯èƒ½å½±å“æ‰€æœ‰ä½¿ç”¨ OpenAIâ€‘compatible API çš„å…¥å£ã€‚  
- **æµ‹è¯•å¥—ä»¶**ï¼š`tests/tool_parsers/test_glm4_moe_tool_parser.py` å¤§å¹…æ‰©å±•ï¼Œæ–°å¢ 10+ æµ‹ä¾‹ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
1. **çŠ¶æ€å¤ä½**ï¼š`_reset_streaming_state` åœ¨æµ‹è¯•é‡Œæ‰‹åŠ¨è°ƒç”¨ï¼Œç”Ÿäº§ç¯å¢ƒéœ€ç¡®ä¿æ¯æ¬¡æ–°è¯·æ±‚éƒ½è°ƒç”¨ `adjust_request` æˆ–åœ¨ `extract_tool_calls_streaming` å¼€å¤´æ£€æµ‹å¹¶æ¸…ç†æ®‹ç•™çŠ¶æ€ï¼Œé˜²æ­¢è·¨è¯·æ±‚æ±¡æŸ“ã€‚  
2. **çº¿ç¨‹å®‰å…¨**ï¼šParser å®ä¾‹åœ¨å¤šçº¿ç¨‹/å¹¶å‘æ¨ç†æ—¶ä¼šå…±äº«ä¸Šè¿°å¯å˜å±æ€§ï¼›å»ºè®®åœ¨ `Engine` å±‚ä¸ºæ¯ä¸ªä¼šè¯åˆ›å»ºç‹¬ç«‹ parser å®ä¾‹æˆ–åœ¨æ–¹æ³•å…¥å£å¤åˆ¶å±€éƒ¨çŠ¶æ€ã€‚  
3. **æ€§èƒ½è°ƒä¼˜**ï¼šå¢é‡å­—ç¬¦ä¸²æ¯æ¬¡éƒ½ä¼š `json.dumps`/`json_escape`ï¼Œå¯¹è¶…é•¿å†…å®¹ä»ä¼šæœ‰ä¸€å®š CPU å¼€é”€ã€‚å¯ä»¥è€ƒè™‘ç¼“å­˜ `json.dumps(key)`ã€ä¸€æ¬¡æ€§æ‹¼æ¥ `escaped` å†å†™å…¥ï¼Œæˆ–åœ¨å­—ç¬¦æµæé•¿æ—¶é‡‡ç”¨ LRU ç¼“å­˜ã€‚  
4. **å‘åå…¼å®¹**ï¼šæ—§ç‰ˆ `extract_tool_calls` ä»ä¿ç•™ï¼Œä½†å†…éƒ¨ä»ä¾èµ–æ–° `_is_string_type` ç­‰å·¥å…·ï¼Œç¡®ä¿æœªå¼€å¯ `tools`ï¼ˆå¦‚æ™®é€šèŠå¤©ï¼‰æ—¶ä¸äº§ç”Ÿé¢å¤– `self._buffer` æ¶ˆè€—ã€‚  
5. **å¼‚å¸¸å¤„ç†**ï¼š`_deserialize` å’Œ JSON è§£æå¯èƒ½æŠ› `JSONDecodeError`ï¼Œå½“å‰å·²æ•è·å¹¶è®°å½• `warning`ï¼Œå»ºè®®åœ¨å¼‚å¸¸è·¯å¾„è¿”å›æ˜ç¡®çš„ `DeltaMessage(content=â€¦)`ï¼Œé¿å…ä¸Šå±‚å›  `None` äº§ç”Ÿç©ºæŒ‡é’ˆã€‚  
6. **æ–‡æ¡£ä¸ç¤ºä¾‹**ï¼šåœ¨ `docsite` æˆ– README ä¸­è¡¥å…… â€œæµå¼å·¥å…·è°ƒç”¨â€ ä½¿ç”¨è¯´æ˜ï¼Œå°¤å…¶æ˜¯å¦‚ä½•åœ¨ `ChatCompletionRequest` ä¸­å£°æ˜ `string` ç±»å‹å‚æ•°ä»¥è·å¾—å¢é‡æ•ˆæœã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨æ˜¾è‘—æå‡äº† GLMâ€‘4 åœ¨é•¿å­—ç¬¦ä¸²å‚æ•°ï¼ˆå¦‚ä»£ç ã€æ–‡ä»¶å†…å®¹ï¼‰ä¸‹çš„å“åº”å®æ—¶æ€§ï¼Œæµ‹è¯•è¦†ç›–å……åˆ†ã€‚è‹¥åœ¨å¤šç”¨æˆ·/å¤šçº¿ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œè¯·é‡ç‚¹æ£€æŸ¥å®ä¾‹åŒ–æ–¹å¼ä¸çŠ¶æ€æ¸…ç†ã€‚

---

### [Fix] prefix cache hit rate == 0 bug with gpt-oss style models (#33524)
**SHA**: `a01ef3f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a01ef3fa51a0312914ca60ecbab2bc45aa43e32c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¸ºæ··åˆæ¨¡å‹ï¼ˆfullâ€¯+â€¯slidingâ€‘windowï¼‰æ–°å¢ `sliding_window_blocks` å‚æ•°ï¼Œä½¿æ»‘åŠ¨çª—å£çš„å¤§å°å¯é…ç½®ï¼›ç›¸åº”æµ‹è¯•å¼ºåŒ–äº†å—å‘½ä¸­é•¿åº¦çš„æ ¡éªŒã€‚  
2. åœ¨ `tests/v1/core/test_prefix_caching.py` ä¸­æŠ½å– `_test_partial_request_hit`ï¼Œå¹¶è¡¥å……äº†é’ˆå¯¹ Eagle æ¨¡å¼çš„å®Œæ•´æ··åˆæ¨¡å‹ç”¨ä¾‹ï¼ŒéªŒè¯åœ¨ä¸åŒå—è¢«é©±é€æ—¶çš„å‘½ä¸­è¡Œä¸ºã€‚  
3. `vllm/v1/core/kv_cache_coordinator.py` ä¸­ `_get_block_hashes` æ·»åŠ äº† **simpleâ€‘hybrid** åˆ¤å®šï¼Œé¦–æ¬¡å¾ªç¯åå³æˆªæ–­ fullâ€‘attention å—ï¼Œä»¥é¿å… EAGLE ä¸¢å—åœ¨éâ€‘fullâ€‘attention ç»„è¢«é‡å¤åº”ç”¨ï¼Œä¿®æ­£äº† â€œprefix cache hit rate ==â€¯0â€ çš„é”™è¯¯ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `KVCacheManager`ã€`KVCacheCoordinator`ï¼ˆå‰ç¼€ç¼“å­˜é€»è¾‘ï¼‰  
- Hybrid KVCache é…ç½® (`make_kv_cache_config_hybrid_model`)  
- `use_eagle` å¼€å…³ç›¸å…³çš„ç¼“å­˜å‘½ä¸­ç»Ÿè®¡  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- å·²ä¿®å¤çš„ bug åªé’ˆå¯¹ **1 fullâ€¯+â€¯1 other** çš„ç®€å•æ··åˆæ¨¡å‹ï¼Œå¤æ‚å¤šç»„ hybridï¼ˆissueâ€¯#32802ï¼‰ä»å¯èƒ½å‡ºç°ä¸ä¸€è‡´ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é‡ç‚¹æµ‹è¯•ï¼›  
- å¦‚åœ¨è‡ªå®šä¹‰æ¨¡å‹ä¸­ä½¿ç”¨ `sliding_window_blocks`ï¼ŒåŠ¡å¿…åŒæ­¥æ›´æ–°å¯¹åº” `block_size` ä¸ `num_blocks`ï¼Œé˜²æ­¢æ»‘åŠ¨çª—å£è¶…å‡ºåˆ†é…ã€‚  
- è¿ç§»åˆ°æ–°ç‰ˆæœ¬æ—¶ï¼Œä¿ç•™æ—§æµ‹è¯•è„šæœ¬çš„å…¼å®¹æ€§ï¼ˆ`make_kv_cache_config_hybrid_model` å‚æ•°å˜æ›´ï¼‰ï¼Œå¹¶å…³æ³¨ `use_eagle=True` çš„è¡Œä¸ºå˜åŒ–ã€‚  

---  

æ­¤æäº¤é€šè¿‡æ›´ç²¾å‡†çš„å—æˆªæ–­ä¸æµ‹è¯•è¦†ç›–ï¼Œæ¢å¤äº† GPTâ€‘OSS ç³»åˆ—æ¨¡å‹åœ¨å‰ç¼€ç¼“å­˜ä¸­çš„å‘½ä¸­ç‡ï¼Œæå‡äº†æ··åˆæ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

---

### [ModelRunner V2] Support spec decode with structured outputs (#33374)
**SHA**: `cf0a99f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/cf0a99f84db5385722e78dc4d7ba76075545a858)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆåœ¨ ModelRunner V2 ä¸­ä¸º specâ€‘decode å¼•å…¥ç»“æ„åŒ–è¾“å‡ºçš„æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- ä¸º `InputBatch` å¢åŠ å¸ƒå°”å­—æ®µ `has_structured_output_reqs`ï¼Œç”¨äºæ ‡è®°æ‰¹æ¬¡ä¸­æ˜¯å¦åŒ…å«éœ€è¦ç»“æ„åŒ–è¾“å‡ºçš„è¯·æ±‚ã€‚  
- åœ¨ `ModelRunner` ä¸­å¼•å…¥ `DraftTokensHandler`ï¼Œåœ¨é‡‡æ ·ç»“æŸåæŠŠè‰ç¨¿ tokenï¼ˆä»…å½“æ‰¹æ¬¡å«ç»“æ„åŒ–è¾“å‡ºæ—¶ï¼‰å¼‚æ­¥æ‹·è´å› CPU å¹¶åŒ…è£…ä¸º `DraftTokenIds` ä¾›è°ƒåº¦å™¨åš grammar éªŒè¯ã€‚  
- æ–°å¢ `vllm/v1/worker/gpu/spec_decode/utils.py` å®ç°ä¸Šè¿°æ‹·è´é€»è¾‘ï¼Œä½¿ç”¨ç‹¬ç«‹ CUDA stream / event ä»¥é¿å…é˜»å¡ä¸»è®¡ç®—æµã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/worker/gpu/input_batch.py`  
- `vllm/v1/worker/gpu/model_runner.py`  
- æ–°å¢ `vllm/v1/worker/gpu/spec_decode/utils.py`ï¼ˆDraftTokensHandlerï¼‰  
- ç›¸å…³è°ƒåº¦å™¨ã€è¾“å‡ºç»“æ„ `ModelRunnerOutput`ã€`DraftTokenIds` çš„ä½¿ç”¨è·¯å¾„  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å‘åå…¼å®¹**ï¼š`InputBatch.make_dummy` å·²è¡¥é»˜è®¤ `False`ï¼Œä½†ä»»ä½•æ‰‹å†™ `InputBatch` è°ƒç”¨å¤„éœ€æ˜¾å¼ä¼ å…¥ `has_structured_output_reqs`ï¼Œé˜²æ­¢é—æ¼å¯¼è‡´è‰ç¨¿ token ä¸è¢«è¿”å›ã€‚  
2. **åŒæ­¥å¼€é”€**ï¼š`DraftTokensHandler.get_draft_tokens` åœ¨ `copy_event.synchronize()` æ—¶ä¼šé˜»å¡ï¼Œå»ºè®®åœ¨è°ƒåº¦å™¨ä¾§ä»…åœ¨éœ€è¦ grammar éªŒè¯æ—¶è°ƒç”¨ï¼Œé¿å…å¯¹çº¯æ–‡æœ¬ç”Ÿæˆä»»åŠ¡äº§ç”Ÿé¢å¤–å»¶è¿Ÿã€‚  
3. **è®¾å¤‡å®¹é”™**ï¼š`DraftTokensHandler.__init__` æ¥æ”¶ `device=None`ï¼Œä½†å†…éƒ¨æ‰€æœ‰ CUDA æ“ä½œå‡ä½¿ç”¨ `self.device`ï¼Œè¯·ç¡®è®¤åœ¨ CPUâ€‘onlyè·¯å¾„ä¸‹ä¸ä¼šè§¦å‘ `torch.cuda` è°ƒç”¨ï¼ˆå¯åœ¨ `__init__` ä¸­åŠ å…¥ guardï¼‰ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šå¢åŠ å«ç»“æ„åŒ–è¾“å‡ºçš„ specâ€‘decode åœºæ™¯å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ `has_structured_output_reqs=True` æ—¶ `draft_tokens_np` æ­£å¸¸è¿”å›ï¼Œ`False` æ—¶è¿”å› `None`ï¼Œå¹¶æ£€æŸ¥ `req_ids` ä¸è¾“å…¥æ‰¹æ¬¡ä¿æŒå¯¹åº”ã€‚  
5. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨æ¨¡å‹æ¨ç†ã€specâ€‘decode ä¸ç»“æ„åŒ–è¾“å‡ºçš„ä½¿ç”¨è¯´æ˜ä¸­è¡¥å…… `has_structured_output_reqs` å­—æ®µåŠ `DraftTokensHandler` çš„ä½œç”¨ï¼Œæé†’ç”¨æˆ·åœ¨å¤šè¯·æ±‚æ‰¹æ¬¡ä¸­åˆç†æ··åˆç»“æ„åŒ–/éç»“æ„åŒ–è¯·æ±‚ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ›´æ”¹åœ¨ä¿æŒåŸæœ‰è·¯å¾„ä¸å˜çš„å‰æä¸‹ä¸ºç»“æ„åŒ–è¾“å‡ºæä¾›äº†å¿…è¦çš„è‰ç¨¿ token å›ä¼ æœºåˆ¶ï¼Œæ ¸å¿ƒå½±å“é›†ä¸­åœ¨ GPU worker ä¸è°ƒåº¦å±‚ã€‚å…³æ³¨å¼‚æ­¥æ‹·è´çš„åŒæ­¥æˆæœ¬å’Œè®¾å¤‡å¯ç”¨æ€§ï¼Œå¯æå‡æ–°ç‰¹æ€§çš„ç¨³å¥æ€§ã€‚

---

### [ModelRunner V2] Misc minor simplifications and optimizations (#33467)
**SHA**: `e535d90` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/e535d90debb39e8462cd39004fb7e0d9ded10740)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç ç®€åŒ– & å°å¹…è¡Œä¸ºä¼˜åŒ–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šæœ¬æ¬¡æäº¤ä¸»è¦æŠŠå¤§é‡å‡½æ•°å‚æ•°ã€è°ƒç”¨å’Œå†…éƒ¨å˜é‡çš„æ¢è¡Œå†™æ³•å‹ç¼©ä¸ºå•è¡Œå½¢å¼ï¼Œå»é™¤å†—ä½™çš„ `if None` åˆ†æ”¯ï¼Œå¹¶åœ¨ `ModelRunner`ã€`CudaGraphManager`ã€`Eagle` ç­‰æ ¸å¿ƒæ¨¡å—åŠ å…¥å¯¹ `supports_mm_inputs` ä¸ `cudagraph_mode` çš„æ˜¾å¼æ£€æŸ¥ï¼Œå…¼å®¹ç©ºé…ç½®çš„æƒ…å†µã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š`vllm/v1/worker/gpu/*`ï¼ˆåŒ…æ‹¬ `async_utils.pyã€attn_utils.pyã€block_table.pyã€buffer_utils.pyã€cudagraph_utils.pyã€dp_utils.pyã€kv_connector.pyã€lora_utils.pyã€mm/encoder_runner.pyã€mm/mrope_utils.pyã€model_runner.pyã€sample/*ã€spec_decode/* ç­‰ï¼‰ï¼Œä»¥åŠç›¸å…³çš„å†…éƒ¨æ•°æ®ç»“æ„å’Œè°ƒåº¦è·¯å¾„ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **åŠŸèƒ½ä¸€è‡´æ€§**ï¼šè™½ç„¶å¤§å¤šæ•°æ”¹åŠ¨æ˜¯æ ¼å¼åŒ–ï¼Œä½†æ–°å¢çš„ `if self.supports_mm_inputs:` åˆ†æ”¯ä¼šåœ¨æœªå¼€å¯å¤šæ¨¡æ€æ—¶ç»•è¿‡ç¼“å­˜æ¸…ç†ï¼Œéœ€ç¡®è®¤åŸå…ˆä¾èµ–è¿™äº›æ¸…ç†çš„æµ‹è¯•/æ—¥å¿—ä»ç„¶é€šè¿‡ã€‚  
2. **å‰ç½®/åå¤„ç†**ï¼š`ModelRunner.finish_requests` ç°åœ¨å…ˆæŠŠ `preempted_req_ids` å–å‡ºå†åˆå¹¶ï¼Œé€»è¾‘ä¿æŒä¸å˜ï¼Œä½†å»ºè®®æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›– preemptâ€‘+â€‘finish ä¸¤ç§ç»„åˆï¼Œä»¥é˜²æ­¢é—æ¼ã€‚  
3. **é»˜è®¤å€¼å¤„ç†**ï¼š`states.py` å°† `top_k`ã€`seed`ã€`logprobs` çš„é»˜è®¤é€»è¾‘åˆå¹¶ä¸ºä¸€æ¬¡èµ‹å€¼ï¼Œç¡®ä¿ `top_k` ä¸ºè´Ÿæˆ–è¶…å‡º vocab æ—¶ä»è¿”å› `vocab_size`ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æœ‰éšå¼ä¾èµ–æ—§çš„ `if 0 < â€¦` æ¡ä»¶ã€‚  
4. **CUDAâ€‘Graph é…ç½®**ï¼š`CudaGraphManager` ä¸ `EagleCudaGraphManager` ç›´æ¥ä½¿ç”¨ `compilation_config.cudagraph_mode`ï¼Œå¹¶åœ¨ `FULL` æ—¶é™çº§ä¸º `FULL_DECODE_ONLY`ã€‚è¯·éªŒè¯åœ¨ `FULL` æ¨¡å¼ä¸‹å…¶ä»–é Eagle è·¯å¾„ä»ä¿æŒåŸæœ‰è¡Œä¸ºã€‚  
5. **ä»£ç å¯è¯»æ€§**ï¼šå‹ç¼©æ¢è¡Œæå‡æ–‡ä»¶é•¿åº¦çš„ç´§å‡‘åº¦ï¼Œä½†å¯è¯»æ€§ä¸‹é™ã€‚å»ºè®®åœ¨å…³é”®å‡½æ•°ï¼ˆå¦‚ `init_attn_backend`ã€`prepare_inputs`ï¼‰ä¿æŒå‚æ•°å¯¹é½æˆ–åŠ æ³¨é‡Šï¼Œä»¥å…åç»­ç»´æŠ¤è¯¯è¯»ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æäº¤å±äº**å®‰å…¨çš„ç»“æ„æ€§ä¼˜åŒ–**ï¼Œå¯¹åŠŸèƒ½å½±å“æœ‰é™ï¼Œä½†å»ºè®®é€šè¿‡ CI æ·»åŠ é’ˆå¯¹ä¸Šè¿°åˆ†æ”¯çš„å›å½’æµ‹è¯•ï¼Œä»¥ç¡®ä¿åœ¨å…³é—­å¤šæ¨¡æ€ã€CUDAâ€‘Graph æˆ– preempt åœºæ™¯ä¸‹ä»ä¿æŒåŸæœ‰è¡Œä¸ºã€‚

---

### Add MoE config for Super B200 TP2 (#33510)
**SHA**: `8869cd8` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/8869cd8ec1b2586df96e6556f2d2e3b3760bfb93)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šä¸º NVIDIAâ€¯Superâ€¯B200ï¼ˆTP2ï¼‰GPU æ·»åŠ äº†ä¸“ç”¨çš„ MoEï¼ˆMixtureâ€‘ofâ€‘Expertsï¼‰è°ƒåº¦é…ç½®æ–‡ä»¶ `E=512,N=1344,device_name=NVIDIA_B200.json`ã€‚è¯¥ JSON åˆ—ä¸¾äº†ä¸åŒ Expert æ•°é‡ï¼ˆé”® 1â€‘1536ï¼‰å¯¹åº”çš„ Triton ç¼–è¯‘å‚æ•°ï¼ˆBLOCK_SIZE_M/N/Kã€GROUP_SIZE_Mã€num_warpsã€num_stagesï¼‰ï¼Œä»¥æœŸåœ¨è¯¥ç¡¬ä»¶ä¸Šè·å¾—æ›´ä¼˜çš„ç®—å­è°ƒåº¦å’Œååã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- **vllm/model_executor/layers/fused_moe/configs/**ï¼šæ–°å¢é…ç½®æ–‡ä»¶ã€‚  
- **MoE è°ƒåº¦å™¨**ï¼ˆ`fused_moe`ï¼‰åœ¨åŠ è½½é…ç½®æ—¶ä¼šæ ¹æ®æ£€æµ‹åˆ°çš„ GPU åç§°ï¼ˆ`NVIDIA_B200`ï¼‰è‡ªåŠ¨é€‰å–è¯¥æ–‡ä»¶ã€‚  
- ä»»ä½•ä½¿ç”¨ MoE çš„æ¨¡å‹éƒ¨ç½²è„šæœ¬ï¼Œå¦‚æœåœ¨è¯¥ GPU ä¸Šè¿è¡Œï¼Œå°†ç›´æ¥å—æ­¤é…ç½®å½±å“ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **éªŒè¯å…¼å®¹æ€§**ï¼šåœ¨é B200 GPU ä¸Šç¡®è®¤ä»ä¼šå›é€€åˆ°é»˜è®¤é…ç½®ï¼Œé¿å…è¯¯ç”¨å¯¼è‡´æ€§èƒ½å›é€€æˆ–é”™è¯¯ã€‚  
2. **é…ç½®è¦†ç›–**ï¼šè‹¥ç”¨æˆ·è‡ªè¡Œæä¾›è‡ªå®šä¹‰é…ç½®ï¼Œç¡®ä¿æ–‡ä»¶åæˆ–è·¯å¾„ä¼˜å…ˆçº§é«˜äºæ­¤å†…ç½®æ–‡ä»¶ï¼Œä»¥å…è¦†ç›–æ„å¤–ç”Ÿæ•ˆã€‚  
3. **æµ‹è¯•åŸºå‡†**ï¼šå»ºè®®åœ¨ B200 å®æœºä¸Šè·‘ä¸€æ¬¡ `benchmark`ï¼ˆå¦‚ `benchmark_moe.py`ï¼‰ï¼Œå¯¹æ¯”å…³é”®å‚æ•°ï¼ˆååã€æ˜¾å­˜å ç”¨ï¼‰æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚  
4. **æ–‡æ¡£åŒæ­¥**ï¼šåœ¨ Release Notes æˆ–é…ç½®æ–‡æ¡£ä¸­è¡¥å……è¯¥æ–‡ä»¶çš„ä½¿ç”¨è¯´æ˜ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®šä½å¯¹åº”ç¡¬ä»¶ã€‚  

æ­¤æ–°å¢æ–‡ä»¶æœ¬èº«ä¸æ”¹åŠ¨ä»£ç é€»è¾‘ï¼Œé£é™©ä¸»è¦åœ¨é…ç½®é€‰æ‹©é”™è¯¯å¯¼è‡´æ€§èƒ½ä¸ä½³ï¼ŒæŒ‰ä¸Šè¿°æ£€æŸ¥å³å¯å¹³ç¨³ä¸Šçº¿ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (12)

### [Feature][Core] Support Fabric detection to adapt the MNNVL protocol for the GB series (#33540)
**SHA**: `528e9b1` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/528e9b14900fc8a012f2599172e2a4576caafe1a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ CUdevice å¥æŸ„åˆ›å»ºå‰æ£€æµ‹ Fabric æ”¯æŒå¹¶è¯·æ±‚ FABRIC å¥æŸ„ï¼›è‹¥ fabric åˆ†é…å› æƒé™æˆ–ç¡¬ä»¶ä¸æ”¯æŒå¤±è´¥ï¼Œåˆ™å›é€€ä½¿ç”¨ POSIX æ–‡ä»¶æè¿°ç¬¦ã€‚

---

### move spec decode slow test to test_areas.yaml (#33365)
**SHA**: `d95b4be` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d95b4be47a3b07d38bccd34b282321f479915de2)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `v1/spec_decode` çš„æ…¢é€Ÿæµ‹è¯•è¿ç§»è‡³ `test_areas.yaml` ä¸­çš„å•ç‹¬å¯é€‰ GPUâ€¯H100 æ­¥éª¤ï¼Œä½¿ç”¨ `-m 'not slow_test'` æ’é™¤æ…¢æµ‹ï¼›æ–°å¢ Acceptance Length æµ‹è¯•å¹¶è®¾ç½® `VLLM_ALLOW_INSECURE_SERIALIZATION=1`ï¼›æ›´æ–° `test_acceptance_length.py` æ³¨é‡Šã€‚

---

### Update get_expert_mapping to include self parameter (#33525)
**SHA**: `b398e5c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/b398e5c819a6fab7d7c4a4482dfa9893935261a4)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `glm4_moe_lite.py` ä¸­çš„ `get_expert_mapping` è°ƒç”¨ `SharedFusedMoE.make_expert_params_mapping` æ—¶ï¼Œæ–°å¢ `self` å‚æ•°ï¼Œä»¥ç¡®ä¿å®ä¾‹ä¸Šä¸‹æ–‡æ­£ç¡®ä¼ é€’ã€‚

---

### Fix accessing hidden_act from model config (#32686)
**SHA**: `78061ef` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/78061ef584ad1bc124b10f7515edaf879464f6ff)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `nemotron_nas.py` ä¸­æ–°å¢å¯¹ `block_config.ffn.hidden_act` çš„æ£€æŸ¥ï¼Œè‹¥ä¸å­˜åœ¨åˆ™å›é€€ä½¿ç”¨å…¨å±€ `config.hidden_act`ï¼Œç¡®ä¿æ¨¡å‹åˆå§‹åŒ–æ—¶æ­£ç¡®è·å–æ¿€æ´»å‡½æ•°ã€‚

---

### [CI][Bugfix] Fix flaky `tests/v1/kv_connector/unit/test_multi_connector.py::test_multi_example_connector_consistency` (#33555)
**SHA**: `528b307` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/528b3076afaac5ed47da85ee28966f9fd845dd78)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°†å•æ¬¡ç”Ÿæˆè°ƒç”¨æ”¹ä¸ºä½¿ç”¨å•ä¸ª promptï¼ˆ`PROMPTS[0]`ã€`PROMPTS[1]`ï¼‰ä»¥é¿å…è°ƒåº¦é¡ºåºå¯¼è‡´çš„ç«äº‰ï¼Œä»è€Œä¿®å¤ `test_multi_example_connector_consistency` çš„ä¸ç¡®å®šæ€§ã€‚

---

### [CPU][IBM Z][Dockerfile] Fix IBM Z builds (#33243)
**SHA**: `ab37478` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ab374786c7ebc7e3b6af1778404094261338e4ca)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ›´æ–° IBMâ€¯Z Dockerfileï¼Œå‡çº§ UBI åŸºç¡€é•œåƒå¹¶ä½¿ç”¨ GCCâ€‘toolsetâ€‘14ï¼Œå‡çº§ PyTorch/torchvisionï¼Œæ–°å¢ OpenCV ç¼–è¯‘æ­¥éª¤ï¼Œè®¾ç½® pip/uv é•œåƒæºï¼Œå¹¶ä¿®æ­£é root ç”¨æˆ·åˆ›å»ºå‘½ä»¤ã€‚

---

### Fix mistral sliding window parsing (#33521)
**SHA**: `beb8899` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/beb889948276843a8df5e74fc704118a6f54b2b8)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† Mistral é…ç½®ä¸­ `sliding_window` çš„åˆ—è¡¨å½¢å¼ç»Ÿä¸€è½¬æ¢ä¸º HF æ‰€éœ€çš„æ•´æ•° `sliding_window` ä¸ `layer_types` åˆ—è¡¨ï¼Œå®ç°å…¼å®¹å¹¶æå‡è§£æå¯é æ€§ã€‚

---

### [Doc]: update paths for Offline/Online/Others example sections (#33494)
**SHA**: `ce88756` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ce88756b967c2c5006746a424c15dd59a284ed8c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼š æ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼š ğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼š å°† `docs/examples/README.md` ä¸­çš„ç¤ºä¾‹ç« èŠ‚æè¿°æ”¹ä¸ºæŒ‡å‘å®é™…ç›¸å¯¹è·¯å¾„çš„ Markdown é“¾æ¥ï¼Œæå‡æ–‡æ¡£å¯ç‚¹å‡»æ€§ã€‚

---

### [Doc] add missing model entries in supported_models.md (#33220)
**SHA**: `a3154a6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a3154a609261e876350072043f1c4cc28ec9f45c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `supported_models.md` ä¸­è¡¥å…¨ç¼ºå¤±æ¨¡å‹æ¡ç›®ã€çº æ­£æ¨¡å‹åç§°æ‹¼å†™ï¼ˆå¦‚ `ExaoneMoEForCausalLM`ã€`Olmo*` ç³»åˆ—ï¼‰ï¼Œå¹¶æ–°å¢ `CwmForCausalLMã€Glm4MoeLiteForCausalLMã€LongcatFlashForCausalLMã€MiniMaxForCausalLMã€RWForCausalLMã€StableLMEpochForCausalLMã€TeleChatã€HCXVisionForCausalLM` ç­‰æ¨¡å‹ä¿¡æ¯ã€‚

---

### [Nightly CI] Remove CT Model (#33530)
**SHA**: `318b120` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/318b120766ad54e68a2eff02598f336772b3abda)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `tests/weight_loading/models.txt` ä¸­åˆ é™¤äº† `nm-testing/TinyLlama-1.1B-Chat-v1.0-actorder-group` æ¡ç›®ã€‚

---

### [Misc] skip target model mm emb in draft proposal step when draft is text-only (#33437)
**SHA**: `0b225fb` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0b225fb7b22f8ae1f5fc8ee618640ae0983c76de)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `gpu_model_runner.py` ä¸­çš„ `propose_draft_token_ids`ï¼Œæ–°å¢å¯¹ `self.drafter.supports_mm_inputs` çš„æ£€æŸ¥ï¼Œä»…å½“ä¸»æ¨¡å‹å’Œè‰ç¨¿æ¨¡å‹å‡æ”¯æŒå¤šæ¨¡æ€è¾“å…¥æ—¶æ‰æ”¶é›† MM åµŒå…¥ï¼Œé˜²æ­¢åœ¨çº¯æ–‡æœ¬è‰ç¨¿é˜¶æ®µå‡ºç°ä¸å¿…è¦çš„ MM å¤„ç†ã€‚

---

### Fix DeepSeek V2 RoPE initialization error (#33501)
**SHA**: `46b4a02` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/46b4a02794223a75987d07b2ceaaa1c1010dfbf0)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¿®å¤ DeepSeek V2 RoPE åˆå§‹åŒ–é”™è¯¯ï¼Œå»é™¤ `device=current_platform.device_type` å‚æ•°ä»¥é¿å…æ— æ•ˆè®¾å¤‡è®¾ç½®ã€‚

---

