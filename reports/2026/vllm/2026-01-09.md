# 每日更新报告（2026-01-09）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-09 23:30:40 | Kevin Šuc | Rename --exclude-log-deltas to --enable-log-deltas (#32020) |
| 2026-01-09 22:44:52 | Isotr0py | [Doc] Remove hardcoded Whisper in example openai translation client (#32027) |
| 2026-01-09 22:40:33 | Michael Goin | [Perf][Kernel] Fused SiLU+Mul+Quant kernel for NVFP4 cutlass_moe (#31832) |
| 2026-01-09 22:14:46 | R3hankhan | [CPU] Add head sizes 80 and 112 with vec16 fallback (#31968) |
| 2026-01-09 22:12:44 | maang | [Model] Remove redundant None check in DeepSeekOCR image input processing (#32016) |
| 2026-01-09 22:03:32 | Adolfo Victoria | Fix type error (#31999) |
| 2026-01-09 22:01:57 | inkcherry | [ROCm][PD] add moriio kv connector. (#29304) |
| 2026-01-09 21:20:59 | Roger Wang | [Misc] Skip hashing kwargs if value is `None` (#32025) |
| 2026-01-09 20:48:32 | Andreas Karatzas | [ROCm][CI][V1] Fix `nixl_connector` test failure and achieve CUDA parity in `test_async_scheduling` (#32000) |
| 2026-01-09 20:40:59 | Sophie du Couédic | [Feature][Benchmarks] Custom dataset: read output length from dataset (#31881) |
| 2026-01-09 20:13:17 | Bofeng Xue | fix: remove duplicate engine_id check in nixl_connector (#31948) |
| 2026-01-09 19:46:59 | Xin Yang | [Bugfix] Fix Triton FusedMoE LoRA (#30585) |
| 2026-01-09 19:28:02 | vllmellm | [Bugfix][ROCm]Fix Qwen3-Next-80B-A3B-Thinking inference and optimize non-standard block size (544) support under rocm_atten (#31380) |
| 2026-01-09 19:02:14 | Cyrus Leung | [Model] Reorganize pooling layers (#31973) |
| 2026-01-09 18:56:20 | Andreas Karatzas | [Bugfix] Fix OpenAPI schema test failures (#31921) |
| 2026-01-09 18:28:43 | Alex Brooks | [Bugfix] Fix Var Length Batched Padding in Granite Speech (#31906) |
| 2026-01-09 14:43:25 | gnovack | fix lora moe sharding when rank < max_lora_rank (#31994) |
| 2026-01-09 13:54:05 | Xin Yang | [Bugfix] Fix FusedMoE LoRA w2_output_size (#31949) |
| 2026-01-09 13:44:18 | Nick Hill | [Cleanup] Remove obsolete spec decoding compatibility logic (#32003) |
| 2026-01-09 12:54:20 | TJian | [CI] [ROCm] Fix `tests/entrypoints/test_grpc_server.py` on ROCm (#31970) |
| 2026-01-09 12:04:33 | Divakar Verma | [ROCm][CI] Fix test_token_classification.py::test_bert_models (#31993) |
| 2026-01-09 11:59:34 | RioS | [Bugfix] missing tokens occur in harmony streaming (#30437) |
| 2026-01-09 10:46:05 | Xin Yang | [Bugfix] Fix typo in FusedMoE LoRA reshape comment (#31992) |
| 2026-01-09 10:31:50 | zhrrr | [Async][Feat] support apply penalty or bad_words for async + spec (#30495) |
| 2026-01-09 09:19:34 | daniel-salib | [Frontend] Add MCP tool streaming support to Responses API (#31761) |
| 2026-01-09 08:18:50 | Robert Shaw | [Bugfix] Fix Typo from NVFP4 Refactor (#31977) |
| 2026-01-09 08:13:39 | Max Hu | [Feature] Add iteration level logging and enhance nvtx marker (#31193) |
| 2026-01-09 08:08:21 | Nick Hill | [BugFix] Add spec-decode-incompatible request param validation (#31982) |
| 2026-01-09 08:07:45 | Robert Shaw | [Quantization] Deprecate Long Tail of Schemes (#31688) |
| 2026-01-09 08:01:30 | Yongye Zhu | [MoE Refactoring][Bugfix]Wrap WNA16 Triton kernel into mk and change compressed tensor kernel selection (#31752) |
| 2026-01-09 07:20:49 | Lucas Wilkinson | [Misc] Fix `Current vLLM config is not set.` warnings, assert to avoid issues in the future (#31747) |
| 2026-01-09 06:45:17 | Dipika Sikka | [Compressed-Tensors] Simplify NVFP4 Conditions, enable marlin support for NVFP4A16 MoEs (#30881) |
| 2026-01-09 04:52:55 | bnellnm | [Misc][Refactor] Add FusedMoERouter object (#30519) |
| 2026-01-09 04:07:03 | Cyrus Leung | [Frontend] Improve error message (#31987) |
| 2026-01-09 03:33:24 | Lucas Kabela | [Documentation][torch.compile] Add documentation for torch.compile + multimodal encoders (#31627) |
| 2026-01-09 03:31:53 | Michael Goin | Revert "feat(moe): Add is_act_and_mul=False support for Triton MoE kernels" (#31978) |
| 2026-01-09 02:24:26 | Woosuk Kwon | [Model Runner V2] Simplify BlockTables with UVA (#31965) |
| 2026-01-09 01:34:43 | Nicolò Lucchesi | [CI][ROCm] Fix NIXL tests on ROCm (#31728) |
| 2026-01-09 01:12:33 | Nishidha Panpaliya | Fix ijson build for Power. (#31702) |
| 2026-01-09 01:10:07 | Nick Hill | [Misc] Tidy up some spec decode logic in GPUModelRunner (#31591) |
| 2026-01-09 00:55:22 | Jee Jee Li | [Doc] Improve MM models LoRA notes (#31979) |
| 2026-01-09 00:08:37 | danisereb | [Bugfix] Fix vllm serve failure with Nemotron Nano V3 FP8 (#31960) |

### 📊 统计摘要
> 本日共 42 个提交 | 🔴高 3 | 🟡中 20 | 🟢低 19
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[ROCm][PD] add moriio kv connector. (#29304)](#4505849)
    - [[Model] Reorganize pooling layers (#31973)](#c8ed39b)
    - [[Misc] Fix `Current vLLM config is not set.` warnings, as...](#6cdf015)
  - [🟡 中重要度变更 (20)](#-🟡-中重要度变更-20)
    - [Rename --exclude-log-deltas to --enable-log-deltas (#32020)](#ac9f933)
    - [[Perf][Kernel] Fused SiLU+Mul+Quant kernel for NVFP4 cutl...](#34cd32f)
    - [[CPU] Add head sizes 80 and 112 with vec16 fallback (#31968)](#8e27663)
    - [[ROCm][CI][V1] Fix `nixl_connector` test failure and achi...](#e02706d)
    - [fix: remove duplicate engine_id check in nixl_connector (...](#55212c1)
    - [[Bugfix] Fix Triton FusedMoE LoRA (#30585)](#e7b68f4)
    - [[Bugfix][ROCm]Fix Qwen3-Next-80B-A3B-Thinking inference a...](#1a19e9c)
    - [[Cleanup] Remove obsolete spec decoding compatibility log...](#29ce482)
    - [[Async][Feat] support apply penalty or bad_words for asyn...](#8ff4a99)
    - [[Frontend] Add MCP tool streaming support to Responses AP...](#a4ec0c5)
    - [[Bugfix] Fix Typo from NVFP4 Refactor (#31977)](#0fa8dd2)
    - [[Feature] Add iteration level logging and enhance nvtx ma...](#6ebe34d)
    - [[Quantization] Deprecate Long Tail of Schemes (#31688)](#5825bbc)
    - [[MoE Refactoring][Bugfix]Wrap WNA16 Triton kernel into mk...](#d62cfe5)
    - [[Compressed-Tensors] Simplify NVFP4 Conditions, enable ma...](#5d3b609)
    - [[Misc][Refactor] Add FusedMoERouter object (#30519)](#e74698c)
    - [[Frontend] Improve error message (#31987)](#aa125ec)
    - [Revert "feat(moe): Add is_act_and_mul=False support for T...](#87e07a6)
    - [[Model Runner V2] Simplify BlockTables with UVA (#31965)](#7508243)
    - [[Misc] Tidy up some spec decode logic in GPUModelRunner (...](#a3d909a)
  - [🟢 低重要度变更 (19)](#-🟢-低重要度变更-19)
    - [[Doc] Remove hardcoded Whisper in example openai translat...](#2d0c5b6)
    - [[Model] Remove redundant None check in DeepSeekOCR image ...](#7cdf7e2)
    - [Fix type error (#31999)](#bbf80ed)
    - [[Misc] Skip hashing kwargs if value is `None` (#32025)](#db07433)
    - [[Feature][Benchmarks] Custom dataset: read output length ...](#b474782)
    - [[Bugfix] Fix OpenAPI schema test failures (#31921)](#0207328)
    - [[Bugfix] Fix Var Length Batched Padding in Granite Speech...](#dc77cb7)
    - [fix lora moe sharding when rank < max_lora_rank (#31994)](#bde38c1)
    - [[Bugfix] Fix FusedMoE LoRA w2_output_size (#31949)](#707b240)
    - [[CI] [ROCm] Fix `tests/entrypoints/test_grpc_server.py` o...](#7a05d2d)
    - [[ROCm][CI] Fix test_token_classification.py::test_bert_mo...](#a1648c4)
    - [[Bugfix] missing tokens occur in harmony streaming (#30437)](#e2d49ec)
    - [[Bugfix] Fix typo in FusedMoE LoRA reshape comment (#31992)](#8413868)
    - [[BugFix] Add spec-decode-incompatible request param valid...](#11cec29)
    - [[Documentation][torch.compile] Add documentation for torc...](#f16bfbe)
    - [[CI][ROCm] Fix NIXL tests on ROCm (#31728)](#83e1c76)
    - [Fix ijson build for Power. (#31702)](#a563866)
    - [[Doc] Improve MM models LoRA notes (#31979)](#49568d5)
    - [[Bugfix] Fix vllm serve failure with Nemotron Nano V3 FP8...](#b8112c1)
#### 🔴 高重要度变更 (3)

### [ROCm][PD] add moriio kv connector. (#29304)
**SHA**: `4505849` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4505849b309b5f665a16778a4f0070d73c79e06d)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 在 ROCm 镜像中新增对 **MORIIO**（AMD mori IO）库的构建与安装，暴露 `MORI_GPU_ARCHS`、`handshake_port`、`notify_port` 等端口配置。  
2. 在 vLLM 的 KV‑Transfer 框架中实现 **MoRIIOConnector**（包括 Scheduler、Worker、Writer、Engine、Common 等模块），实现基于 RDMA 的 KV‑Cache 读写分离（Prefill ↔ Decode）以及“read / write”两种工作模式，并将其注册到 `KVConnectorFactory`。  
3. 新增示例 `moriio_toy_proxy_server.py` 与完整的单元测试 `test_moriio_connector.py`，验证 KV‑Cache 的远程保存与加载流程。  
4. 为该功能提供环境变量 (`VLLM_MORIIO_*`) 及文档说明。  

**🎯 影响范围**  
- **docker/Dockerfile.rocm_base**：ROCm 镜像构建时会额外拉取、编译 `mori` 项目。  
- **vllm/distributed/kv_transfer/kv_connector/**：新增 `v1/moriio/` 包，涉及 Scheduler、Worker、Writer、Engine、Common、注册逻辑。  
- **vllm/envs.py**：新增四个 MORIIO 相关配置项。  
- **docs/getting_started/installation/gpu.rocm.inc.md**：更新安装指南，加入 MORIIO 编译与使用步骤。  
- **examples/online_serving/disaggregated_serving/**：提供演示脚本。  
- **tests/v1/kv_connector/unit/**：新增单元测试，覆盖写模式、读模式以及 handshake 流程。  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 引入 **MoRIIOConnector**，在 KV‑Transfer 中形成 **Producer（Prefill） ↔ Consumer（Decode）** 双向 RDMA 通道。<br>- 通过 ZMQ 实现 **handshake / ping / notify**，并使用 `MoRIIOWrapper` 把 MORIIO IOEngine 与 vLLM 业务解耦。<br>- 新增 `ROLE`（PRODUCER / CONSUMER）全局单例管理，所有 Scheduler/Worker 代码通过 `get_role()`/`set_role()` 读取。<br>- 当 `kv_role = kv_producer` 时，Scheduler 负责把本地 KV‑Cache **写** 到远端；`kv_consumer` 时负责 **读** 远端 KV‑Cache。<br>- 通过 `MoRIIOConnectorMetadata` 将需要 **保存**、**读取**、**通知** 的请求信息统一传递。 |
| **性能影响** | - **读/写模式切换**：在 **READ**（prefill → decode）模式下，prefill 只负责 **发送请求**，实际 KV 数据在 decode 端通过 RDMA 拉取，能够显著降低网络 RTT 与 CPU → GPU 数据拷贝开销。<br>- **WRITE**（decode → prefill）模式下，decode 完成后直接将 KV‑Cache 写回 prefill，避免额外的 CPU → GPU 路径，并利用 `MoRIIOWrapper.write_remote_data` 批量写入，可实现 **近乎零拷贝**（取决于 RDMA 硬件与 MORIIO 实现）。<br>- 引入 **WriteTask** 队列和后台写线程，允许 **异步调度**，在 GPU 计算与网络传输之间实现流水线化。<br>- 仍依赖 **ZMQ** 进行请求/通知的控制信道，若消息频率极高可能成为小瓶颈；但一般仅在 block‑notify 与完成通知层面使用，负载极低。 |
| **安全考虑** | - 新增 **handshake_port / notify_port / http_port** 等对外可达端口，默认在容器内部绑定 `0.0.0.0`（Dockerfile 中未限制），在多租户环境下可能被外部恶意请求攻击。<br>- Handshake 与通知均未进行身份校验或加密，使用明文 **msgpack**，在不受信任的网络中存在 **MITM / 假冒** 风险。<br>- `mori.io` 本身对 RDMA 通信没有TLS层，需要在部署层面通过内部网络或 VPN 隔离。 |
| **可维护性** | - 代码体量大（>1500 行），跨多个文件，涉及多线程、ZMQ、RDMA 调用，学习曲线陡峭。<br>- `MoRIIOWrapper`、`MoRIIOWriter`、`MoRIIOConnectorScheduler`、`MoRIIOConnectorWorker` 各自维护独立状态，仍通过全局 `ROLE` 进行切换，若后续引入更多角色（如 **proxy**）可能导致状态不一致。<br>- 单元测试覆盖了核心路径，但未覆盖异常路径（handshake 失败、RDMA 传输错误、ZMQ 超时），在真实集群中可能出现未捕获的异常。 |
| **兼容性** | - 仅在 **ROCm** 环境下可用（`mori` 只能在 AMD GPU 上编译），对 **CUDA**、**CPU‑only** 暂不支持。<br>- 添加 `KVConnectorFactory.register_connector("MoRIIOConnector", …)`，若用户自行配置 `kv_connector = "MoRIIOConnector"` 而未部署 MORIIO，会在运行时抛出 `RuntimeError`。<br>- 新增的环境变量默认关闭（`READ_MODE=False`），不影响现有使用者；但若误打开可能导致调度逻辑不匹配（写模式下仍走 prefetch‑read 流程），产生错误。 |

---

### ⚠️ 潜在风险

1. **依赖缺失 / 编译失败**  
   - `docker/Dockerfile.rocm_base` 需要能成功 `git clone https://github.com/ROCm/mori.git` 并编译，若镜像网络或源码分支失效，整个 ROCm 镜像将无法构建。  
2. **RDMA /网络配置错误**  
   - MORIIO 需要底层 RDMA（IB/roce）硬件和驱动支持，缺失或配置不当会导致 `MoRIIOWrapper` 在 `register_remote_engine`、`write_remote_data` 等调用时抛异常，进而导致请求卡死。  
3. **端口冲突 / 防火墙**  
   - `handshake_port`、`notify_port`、`http_port` 在多实例部署时可能被占用；默认采用基于 `dp_rank / tp_rank` 的偏移，但若手动覆盖配置或容器端口映射错误，会导致 ZMQ 连接失败。  
4. **多线程死锁**  
   - `MoRIIOWriter` 在写任务被 defer 多次且 `remote_ready` 条件长期不满足（如远端未完成 handshake），会导致任务队列永久增长，占满内存。  
5. **安全泄露**  
   - Handshake/notify 使用明文 msgpack，无身份验证，外部恶意节点可伪造 `register` 消息，导致非法引擎注册或误触写/读操作。  
6. **回滚困难**  
   - 该功能对 Scheduler/Worker 接口的实现做了大量改动，若后续决定移除，需要同步撤回 factory 注册、env 变量、Dockerfile 改动以及所有新引用的类，否则会留下残余的 `MoRIIOConnector` 入口导致启动时报错。  

---

### 💡 关注建议

| 建议 | 目的 |
|------|------|
| **部署文档强化**：在 README 与部署手册中明确 **RDMA 前置条件**（驱动版本、网络、端口放通）以及 **MORIIO 库的兼容性矩阵**（ROCm 7.x、GPU 架构）。 | 防止因硬件/网络不符导致运行时卡死。 |
| **安全

---

### [Model] Reorganize pooling layers (#31973)
**SHA**: `c8ed39b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c8ed39b9dd8efad4bed9547c52df7999c1735114)

**🎯 变更类型**：架构变更 / 重构  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 将原本 845 行的单体 `vllm/model_executor/layers/pooler.py` 拆分为 **抽象层、公共工具、序列池化、令牌池化、特殊调度** 五个子包，实现功能模块化。  
2. 引入 `DispatchPooler` 的工厂方法 `for_embedding` 与 `for_seq_cls`，取代原先在模型中硬编码的 `Pooler.for_*` 调用。  
3. 替换 `DummyPooler` 为更具语义的 `IdentityPooler`，并统一 `PoolerOutput` 类型定义。  
4. 在 `vllm/config/pooler.py` 新增 `get_pooling_type` 辅助方法，确保 `PoolerConfig` 在运行时始终能返回已解析的 `pooling_type`。  
5. 大量模型实现（BERT、RoBERTa、GPT‑2、CLIP、Jamba、InternLM2 等）及对应测试文件同步改为新 API 与新包路径。  

**🎯 影响范围**  
- `vllm/model_executor/layers/pooler/*`（全新子包及实现）  
- 所有使用池化层的模型实现（约 30+）  
- 相关单元测试（`tests/model_executor/*`）  
- 配置层 (`vllm/config/pooler.py`)  
- `vllm/v1/outputs.py` 类型别名  

**🔍 技术洞察**  

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | - **模块化**：原来的巨型文件被拆解，职责更加单一（抽象接口、参数更新、激活函数、序列/令牌两类池化实现、调度）。 <br> - **可扩展性**：新增池化方式（如自定义序列/令牌池化）仅需在对应子包实现并在 `DispatchPooler` 中注册，无需修改核心模型代码。 <br> - **向后兼容**：对外暴露的 `DispatchPooler` 接口保持不变，但旧的 `Pooler.for_*` 静态工厂已被移除，外部直接导入 `vllm.model_executor.layers.pooler` 将导致 `ImportError`。 |
| **性能影响** | - 功能保持 1:1，核心计算（CLS、LAST、MEAN、ALL、STEP）未改动，运行时额外的函数层包装（如 `pooler_for_*`）几乎可以忽略不计。 <br> - 新的 `IdentityPooler` 替代 `DummyPooler`，实现更轻量（直接返回 `hidden_states`），在插件/插件‑only 场景略有加速。 |
| **安全考虑** | - 激活函数加载仍限制在 `torch.nn.modules.*`（`activations.py` 中的断言），安全策略未改变。 <br> - 新增 `__all__` 明确导出，避免意外暴露内部实现。 |
| **可维护性** | - 每类池化的实现均集中在对应子目录，代码阅读与定位更直接。 <br> - 公共类型（`PoolingParamsUpdate`、`ClassifierFn`）统一放在 `common.py`，减少重复定义。 <br> - `DispatchPooler` 统一校验子池化器对任务的支持，提前在初始化时报错，提升调试友好度。 |
| **兼容性风险** | - **导入路径变更**：外部项目（插件、自定义模型）若仍使用 `from vllm.model_executor.layers.pooler import Pooler` 等旧路径，将失效。 <br> - **序列化/Pickle**：已训练的模型状态字典中包含的 `Pooler` 实例类名已改变（如 `AllPooler` → `TokenPooler`），在不重新保存权重的情况下加载旧模型可能报错。 <br> - **类型别名**：`PoolerOutput` 现在统一为 `torch.Tensor | list[torch.Tensor] | list[torch.Tensor | None]`，若用户在类型检查或手动 `isinstance` 判断，需同步更新。 |
| **测试与文档** | - 单元测试已同步改为新导入路径，确保覆盖 `CLSPool、MeanPool、AllPool、StepPool` 等方法。 <br> - `CODEOWNERS` 与文档（`security.md`）更新确保新包受审。 |

**⚠️ 潜在风险**  
1. **向后兼容破坏**：第三方插件仍依赖旧的 `Pooler` 静态工厂或 `DummyPooler`，升级后会出现 `ImportError` 或 `AttributeError`。  
2. **模型序列化不兼容**：使用 `torch.save(model.state_dict())` 并在新版本中 `torch.load()`，若模型内部仍引用已删除类路径（如 `vllm.model_executor.layers.pooler.pooler.AllPooler`），会触发 `ModuleNotFoundError`。  
3. **并发/Chunked Prefill**：`AllPool` 与 `StepPool` 仍依赖 `vllm_config.scheduler_config.enable_chunked_prefill`，若调度配置在新版本被调整或未同步，可能导致隐藏状态缓存异常。  
4. **配置解析**：`PoolerConfig.get_pooling_type()` 假设 `pooling_type` 已经被 `ModelConfig` 填充；若用户自行构造 `PoolerConfig` 而未走 `ModelConfig` 初始化路径，可能触发断言。  

**💡 关注建议**  

| 受众 | 建议 |
|------|------|
| **库维护者** | - 在下一个次要版本（`v0.x.y+1`) 添加 **兼容层**：提供 `from vllm.model_executor.layers.pooler import Pooler` 的别名实现，内部调用 `DispatchPooler`，给旧插件留出迁移窗口。 <br> - 在发布说明中明确 **模型序列化兼容性**，建议用户在升级前 `torch.save(model)` 并在新版本使用 `torch.load(..., map_location='cpu')` 进行一次全量加载，以触发迁移。 |
| **插件/第三方开发者** | - 将所有 `Pooler` 相关导入改为 `from vllm.model_executor.layers.pooler import DispatchPooler, IdentityPooler`（或对应子包），并使用 `DispatchPooler.for_embedding` / `for_seq_cls` 替代旧 `Pooler.for_*`。 <br> - 若保存了模型状态，迁移时在新版环境下 **重新保存一次**（即 `state_dict` → `torch.save` → `torch.load`），以更新内部类路径。 |
| **使用者（模型部署）** | - 检查模型配置文件（`json`/`yaml`）中的 `pooler_config.pooling_type` 是否仍然使用老的枚举值（`LAST、CLS、MEAN、ALL、STEP`），这些仍受支持。 <br> - 若开启 **Chunked Prefill**，确保 `scheduler_config.enable_chunked_prefill` 在新版中保持开启，否则 `AllPool`/`StepPool` 的缓存行为将改变。 |
| **安全审计** | - 验证新 `activations.py` 中的 `resolve_obj_by_qualname` 仍受相同的安全断言限制，防止任意加载外部模块。 <br> - 关注 `IdentityPooler` 是否在插件场景下意外泄露内部张量（不做任何后处理），如果插件期望对输出做自定义过滤，需要自行包装。 |

**总结**  
本次提交通过 **拆分与重构** 极大提升了 `vLLM` 池化层的可读性、可扩展性与模块化程度，同时

---

### [Misc] Fix `Current vLLM config is not set.` warnings, assert to avoid issues in the future (#31747)
**SHA**: `6cdf015` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6cdf015c3cd8ee600dadf17ecd72fc7f44099d03)

**🎯 变更类型**：  
功能增强 / 重构 / 安全修复（将 “Current vLLM config is not set.” 警告改为显式断言，防止隐式默认配置导致潜在错误；同时对大量测试与底层实现进行重构以强制在使用全局配置前显式设置。）

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**：  
1. 在 `vllm/config/vllm.py` 中，`get_current_vllm_config()` 当全局配置未设置时不再默认创建 `VllmConfig`，而是抛出 `AssertionError` 并给出明确提示。  
2. 新增安全的读取函数 `get_current_vllm_config_or_none()`，所有原先直接调用 `get_current_vllm_config()` 的位置改为使用此函数并自行判空。  
3. 在 `tests/conftest.py` 中提供 `default_vllm_config` fixture，使用 `set_current_vllm_config(VllmConfig())` 为需要全局配置的单元测试提供默认上下文。  
4. 大量测试文件加入 `default_vllm_config` 参数，保证在不依赖完整 engine 环境时仍能正常运行。  
5. 对 `vllm/model_executor/layers/fused_moe/cpu_fused_moe.py` 实现 **惰性激活字典**，避免在模块导入时实例化 CustomOp（从而触发 `get_current_vllm_config()`），降低 import‑time副作用。  
6. 相关模块（分布式通信、设备 communicator、parallel_state、kv‑transfer、gpu_worker 等）改用 `get_current_vllm_config_or_none()` 并在有配置时才读取；在需要配置的代码路径显式使用 `set_current_vllm_config` 上下文管理器。  
7. 对部分 Rotary‑Embedding 接口注入 `ApplyRotaryEmb` 实例，避免在函数内部即时创建导致同样的配置读取问题。  

**🎯 影响范围**：  
- `vllm/config/*`（全局配置管理）  
- `vllm/distributed/*`（分布式初始化、parallel_state、device_communicator）  
- `vllm/model_executor/layers/fused_moe/*`（CPU MoE 实现）  
- `vllm/model_executor/models/keye.py`、`vllm/model_executor/models/siglip2navit.py`（Rotary‑Embedding）  
- `vllm/v1/worker/gpu_worker.py`（模型加载）  
- `tests/*`（约 150+ 测试文件，几乎全部）  
- 其他依赖 `get_current_vllm_config` 的内部工具函数  

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | 通过强制显式配置，消除了“隐式全局单例”在模块导入时的副作用，提升了配置生命周期的可控性。引入 `default_vllm_config` fixture，使测试与实际运行时的配置分离，降低了因测试环境缺失配置导致的隐藏错误。 |
| **性能影响** | - 惰性激活字典延迟实例化激活函数，仅在 MoE 前向时创建，稍微降低启动时的 import 开销；<br>- 对 `get_current_vllm_config_or_none` 的使用在运行时几乎无开销（一次函数调用 + 判空），不影响高频路径。<br>- 代码路径中加入 `with set_current_vllm_config(...)` 的上下文开销可以忽略不计（只在模型加载阶段使用）。 |
| **安全/稳健性** | 将未设置配置的隐式默认行为改为显式异常，防止在生产环境因忘记 `set_current_vllm_config` 而产生错配、资源泄露或错误的编译选项。异常信息清晰，利于快速定位。 |
| **可维护性** | 新增的 `*_or_none` 接口统一了“可选配置”读取模式，降低了后续新增模块忘记配置检查的风险。`default_vllm_config` fixture 为所有单元测试提供统一入口，避免各测试自行构造重复代码。 |
| **兼容性** | 只在测试层面做了大量参数注入，不会影响已有生产代码的行为。唯一的兼容性破坏是如果外部代码直接调用 `get_current_vllm_config()` 而未在上下文中设置配置，将立即抛出 `AssertionError`（原来仅产生警告），需要相应地显式设置配置或改用 `*_or_none`。 |

**⚠️ 潜在风险**  

1. **向后兼容性**：第三方插件或自定义 ops 仍可能在模块导入时直接调用 `get_current_vllm_config()`，在未包装 `set_current_vllm_config` 的情况下会触发 AssertionError，导致加载失败。  
2. **测试环境误用**：如果在新增测试中忘记添加 `default_vllm_config` 参数，或在 CI 中未启用该 fixture，相关测试会直接报错。  
3. **上下文泄漏**：`set_current_vllm_config` 采用上下文管理器，若在异常路径中未正常退出（如 `SystemExit`），可能导致全局配置残留，影响后续测试或进程。  
4. **惰性激活字典**：首次访问不存在的激活键会抛出 `KeyError`，而不是原来的 `AssertionError`；若有拼写错误可能导致不易发现的运行时错误。  

**💡 关注建议**  

- **对外文档**：在项目 README 与 `vllm.config` 文档中明确说明 `get_current_vllm_config()` 只能在已经设置配置的上下文中使用；推荐使用 `get_current_vllm_config_or_none()` 进行安全读取。  
- **插件指引**：为第三方插件提供模板或示例，展示如何在插件加载时使用 `with set_current_vllm_config(...):` 包装自定义 ops 的初始化。  
- **CI 检查**：添加一个简单的 lint 步骤，搜索代码库中直接调用 `get_current_vllm_config(`（未加 `_or_none`）的地方，保证它们都位于 “with set_current_vllm_config” 块内或改为安全读取。  
- **异常处理**：在 `set_current_vllm_config` 的实现里确保在 `__exit__` 中即使出现异常也会恢复 `_current_vllm_config` 为原始值，防止跨测试污染。  
- **测试模板**：在 `tests/conftest.py` 中继续维护 `default_vllm_config`，并在项目模板中提供 `@pytest.fixture(autouse=True)` 的可选包装，以免遗漏。  
- **激活字典**：对 `_LazyActivationDict` 添加 `__repr__` 或 `__str__`，便于调试时快速查看已实例化的激活函数。  

总体而言，此次变更显著提升了 **配置安全性** 与 **测试可靠性**，对生产运行的负面影响极小，只需在使用自定义 ops 的外部代码中适配新的配置获取方式即可。

---

#### 🟡 中重要度变更 (20)

### Rename --exclude-log-deltas to --enable-log-deltas (#32020)
**SHA**: `ac9f933` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ac9f9330e662261242f31f4e0efc9cb9603e4eae)

**🎯 变更类型**：功能增强（CLI 参数语义重构）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原 `--exclude-log-deltas` 参数改名为 `--enable‑log‑deltas`，并将默认值改为 `True`（即默认记录增量）。  
- 相应地在 `api_server.py`、`serving_chat.py`、`cli_args.py` 中更新属性名、构造函数签名以及使用处。  
- 移除对 `exclude_log_deltas` 必须配合 `--enable-log-outputs` 的校验，保留 `--enable-log-outputs` 必须配合 `--enable-log-requests` 的检查。  

**🎯 影响范围**  
- `vllm.entrypoints.openai` 包下的 CLI 与服务启动入口。  
- `ChatCompletionStreamer` 中增量日志的分支判断。  
- 文档、帮助信息以及任何外部脚本仍在使用旧参数的兼容性。

**💡 关注建议**  
1. **兼容性**：若需要平滑迁移，可在下一个版本保留隐藏的 `--exclude-log-deltas` 别名并在日志中给出弃用提示；当前改动已直接移除，使用旧参数会报错。  
2. **文档&帮助**：更新 `--help` 文本、README 与 API 文档，说明新参数的含义（默认记录增量，若设 `False` 则仅在流结束后记录整体输出）。  
3. **测试**：补充单元测试验证 `enable_log_deltas=False` 时增量不被记录，同时确认默认行为仍然记录增量，防止回归。  
4. **监控**：开启增量日志会增加 I/O，用户在高吞吐场景下可酌情关闭（`--enable-log-deltas=False`），关注潜在的性能影响。  

总体上，此次改动提升了参数表达的直观性，并保持了原有默认行为，只需同步文档和测试即可安全发布。

---

### [Perf][Kernel] Fused SiLU+Mul+Quant kernel for NVFP4 cutlass_moe (#31832)
**SHA**: `34cd32f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/34cd32fe302f63270ac90c3ac8ec68b561aac3d7)

**🟢 变更类型**：性能优化 / 新特性（SiLU + Mul + NVFP4 量化融合）

**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 在 `nvfp4_experts_quant` 中加入 **FUSE_SILU_MUL** 编译开关，实现在专家路由阶段直接完成 SiLU(gate) × up 的融合量化；  
- 新增 C++/CUDA 接口 `silu_and_mul_scaled_fp4_experts_quant` 并在 Python 层提供 `silu_and_mul_scaled_fp4_experts_quant`；  
- 对 `nvfp4_utils.cuh` 引入 `compute_silu_mul` 与 `silu` 实现，删除旧的独立 SiLU kernel；  
- 修改 MoE 调度代码，去除对临时 `c2` workspace 的需求，直接在 `c1` 上完成融合量化。

**🎯 影响范围**  
- **核心算子**：`csrc/ops.h`、`csrc/quantization/fp4/*.cu/.cuh`、`csrc/torch_bindings.cpp`；  
- **模型执行层**：`vllm/model_executor/layers/fused_moe/cutlass_moe.py`；  
- **Python API**：`vllm/_custom_ops.py`；  
- 仅在 **SM100 / SM120**（NVFP4）设备上生效，其他 GPU 保持原路径。

**💡 关注建议**  

1. **功能验证**  
   - 为融合路径添加单元测试：比较 `scaled_fp4_experts_quant` 与 `silu_and_mul_scaled_fp4_experts_quant` 在相同输入下的数值误差（应仅有 SiLU+Mul 的浮点误差）。  
   - 检查 `gate||up` 布局的宽度必须为偶数，防止隐藏的维度错误。  

2. **性能基准**  
   - 在典型 MoE 场景（如 8‑bit/FP4‑MoE）上对比 **前后**的 `cutlass_moe_fp4` 运行时，记录 TT‑FP、显存占用和 `c2` workspace 的去除带来的收益。  

3. **兼容性**  
   - 保留原 `scaled_fp4_experts_quant` 实现，确保在未编译 NVFP4 或 `FUSE_SILU_MUL=false` 时仍可运行。  
   - 文档需明确：使用新 API 时 **输入必须是 gate 与 up 按列拼接** 的 2×宽张量；`topk` 与 `MAX_TOKENS_PER_EXPERT_FP4_MOE` 仍需满足原限制。  

4. **错误处理**  
   - 当前在非 NVFP4 设备上会抛出 `TORCH_CHECK_NOT_IMPLEMENTED`，考虑在 Python 层提前检测 `torch.cuda.get_device_capability`，给出更友好的错误信息。  

5. **代码维护**  
   - 将 `compute_silu_mul` 与 `silu` 抽取到公共 `nvfp4_utils.cuh`（已完成），避免在不同 kernel 中重复实现。  
   - `quant_impl` 现在接受模板参数 `FUSE_SILU_MUL`，请在任何新增的量化路径中显式传递该标志，以免出现意外的默认行为。  

**结论**：此提交通过在专家量化阶段一次性完成 SiLU + Mul，显著降低了中间存储与拷贝开销，预计在大规模 MoE 推理中可提升 10%‑20% 的吞吐。只要做好兼容性检测和数值回归测试，即可安全上线。祝调优顺利!

---

### [CPU] Add head sizes 80 and 112 with vec16 fallback (#31968)
**SHA**: `8e27663` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8e27663b6a2ba9d4d0c35e592c118aaf7b50fd13)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 CPU Attention 新增 head‑size 80、112，并在 head‑size 不是 32 的整数倍但是 16 的整数倍时提供 “vec16” 回退实现。相应地扩展了 `cpu_attn.cpp` 的分发表，放宽了 NEON/AMX 的编译期对齐断言，并在 Python 层 `cpu_attn.py` 中加入这两个新尺寸以及 `vec16` 判定逻辑。  

**🎯 影响范围**：  
- `csrc/cpu/`（`cpu_attn.cpp`, `cpu_attn_amx.hpp`, `cpu_attn_neon.hpp`）  
- `vllm/v1/attention/backends/cpu_attn.py`  

**💡 关注建议**：  
1. **ISA 选择**：`_get_attn_isa` 现在在 head‑size%32≠0且%16=0 时直接返回 `"vec16"`，但 NEON 分发仍会被编译进来（注释提醒需 32 对齐），可能导致运行时选择不兼容的实现。建议在分发前加层过滤，或在 `cpu_attn.cpp` 中只为 `vec16` 提供对应实现。  
2. **对齐断言**：移除的 `static_assert` 取消了编译期保护，若后续有人误用非 16‑倍对齐的 head‑size，可能触发未知崩溃或性能回退。建议保留断言或改为运行时检查并抛出友好错误。  
3. **测试与基准**：为新增的 80、112、以及 vec16 路径补齐单元测试和性能基准，确保在不同 CPU（ARM‑Neon、x86‑AMX）上不会出现回退错误或显著退化。  
4. **文档更新**：在 CPU Attention 支持的 head‑size 列表及 ISA 说明中注明“vec16 fallback”以及 NEON 对齐限制，避免使用者产生误解。  

总体而言，本次改动增加了灵活性，但需注意 ISA 兼容性检查和保持对齐约束的可验证性。

---

### [ROCm][CI][V1] Fix `nixl_connector` test failure and achieve CUDA parity in `test_async_scheduling` (#32000)
**SHA**: `e02706d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e02706d2d27c9af429adf89e7dec2b37e3ec39c1)

**🎯 变更类型**：功能增强 / CI 兼容性  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. **`test_async_scheduling`** 统一使用 `FLEX_ATTENTION`，去除 ROCm‑专用后端切换及 log‑prob 容差放宽；实现 CUDA 与 ROCm 在该测试上的行为一致。  
2. **`test_nixl_connector`** 对模拟包创建做平台抽象，额外生成 `rixl` 包（ROCm 对应）并复用同一 `FakeNixlWrapper`，保证 ROCm CI 能成功导入。  
3. **`eagle.py`** 在 ROCm 类型列表中加入 `FlexAttentionMetadata`，让 ROCm 环境也支持 FlexAttention 后端的 spec‑decode。

**🎯 影响范围**  
- **测试层**：e2e 异步调度、kv‑connector 单元测试，跨平台 CI（CUDA & ROCm）均受影响。  
- **运行时**：`vllm.v1.spec_decode.eagle` 在 ROCm 环境下可选用 FlexAttention，潜在影响 spec‑decode 的后端选择逻辑。  

**💡 关注建议**  
1. **代码路径检查**：确认 `vllm.v1.attention.backends.flex_attention.FlexAttentionMetadata` 在所有受支持的 ROCm 发行版中可导入，避免因缺失导致运行时 ImportError。  
2. **数值容差**：虽然已统一容差为 `1e-3/1e-6`，但 ROCm 仍使用 float16，仍可能出现轻微数值漂移。建议在 CI 中保持 `--maxfail=1`，若出现间歇性失败，可考虑在未来恢复平台特定容差或加入 `pytest.approx`。  
3. **测试可靠性**：删除了对 ROCm 特定后端的跳过逻辑后，若后续出现平台差异，需及时在测试中重新加入条件分支，避免掩盖真实 bug。  
4. **文档/发布说明**：在 Release Note 中注明 ROCm 已实现与 CUDA 相同的 async‑scheduling 行为，并支持 FlexAttention spec‑decode，以提醒使用者更新依赖。  

总体而言，此次改动主要为提升 ROCm CI 通过率并实现平台一致性，风险集中在新加入的 `FlexAttentionMetadata` 导入和数值差异的潜在回归，建议在完整的矩阵测试后再推广至生产环境。

---

### fix: remove duplicate engine_id check in nixl_connector (#31948)
**SHA**: `55212c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/55212c140461736b0797ff7a9470f9f9150e5b9a)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `nixl_connector.py` 中发现对 `engine_id` 的匹配检查被重复执行，导致在异常路径下产生冗余的 RuntimeError。此次提交删除了多余的检查，使代码逻辑更加简洁，避免不必要的异常抛出。  
**🎯 影响范围**：`vllm.distributed.kv_transfer.kv_connector.v1.nixl_connector` 模块的握手流程；间接影响使用 NIXL KV 传输的分布式推理节点初始化。  
**💡 关注建议**：  
1. **回归测试**：在多节点部署环境下重新运行启动握手的集成测试，确认删除重复检查后仍能正确捕获 engine_id 不匹配的错误。  
2. **日志检查**：确认异常日志仍保持清晰的“Remote NIXL agent engine ID mismatch”信息，便于故障排查。  
3. **代码风格**：建议在关键检查后添加注释，说明此处仅保留单一次检查，以防将来误删或再次引入重复。  

整体来看，此改动仅涉及少量行数，风险低，但建议在 CI 中加入针对 `nixl_connector` 的单元测试，防止未来的改动意外恢复重复校验。

---

### [Bugfix] Fix Triton FusedMoE LoRA (#30585)
**SHA**: `e7b68f4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e7b68f4d6c7e4b71820994566b7317f24e613b2e)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `gpt_oss_triton_kernels_moe.py` 中修正了 LoRA 触发的 FusedMoE Triton kernel 索引错误：  
   - 对 `intermediate_cache1` 先做 `view(-1, N)` 再按 `gather_indx.dst_indx` 采样，确保路由后特征对齐。  
   - 对 `matmul_ogs` 的输入加上 `gather_indx.src_indx`，避免了原实现中对所有 expert 的无差别累加。  
2. 测试层面加入 `VLLM_MXFP4_USE_MARLIN` 环境变量的参数化，覆盖了 True/False 两种配置，并在单机与双卡 TP 场景下验证 LoRA 的正确性。  
3. `pyproject.toml` 新增 typo 词典 `indx`，防止 CI 报错。  

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/`（Triton MoE 实现）  
- LoRA 相关功能（`enable_lora=True`）  
- 受 MXFP4/Marlin 量化开关影响的执行路径  
- 单元测试 `tests/lora/`  

**💡 关注建议**  
- **代码层面**：确认 `gather_indx.dst_indx`、`src_indx` 在所有路由策略（Top‑K、Top‑2 等）下均为有效的 1‑D 索引，避免越界。若后续加入稀疏路由或自定义专家数，需相应扩展该索引处理。  
- **性能**：加入索引切片会产生额外的内存视图，建议在高并发/大批量推理时测量是否对吞吐有可感知影响；如有必要，可在 Triton kernel 内部完成 gather，进一步降低 Python 端开销。  
- **测试**：当前已覆盖 MXFP4 开关的两态，建议再添加不同 `max_lora_rank`、`max_loras` 组合以及 FP8、FP16 量化路径的交叉验证，确保修复不引入新回归。  
- **用户**：如使用 MXFP4 量化，请确认环境变量 `VLLM_MXFP4_USE_MARLIN` 与模型兼容；在升级后观察生成文本的前缀是否仍符合预期。  

总体而言，此次修复恢复了 LoRA 在 Triton FusedMoE 场景下的正确路由与矩阵计算，风险主要集中在索引合法性和潜在的轻微性能波动，建议在发布前进行完整的跨硬件（A100、H100、RTX 4090）回归。

---

### [Bugfix][ROCm]Fix Qwen3-Next-80B-A3B-Thinking inference and optimize non-standard block size (544) support under rocm_atten (#31380)
**SHA**: `1a19e9c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1a19e9cd87b6bb091fc9c530e6446601db52b58e)

**核心变更概览**  
- 在 ROCm 环境下为 Qwen 3‑Next‑80B‑A3B‑Thinking（块大小 544）补齐了 KV‑Cache 的写入、查询与前缀预填逻辑。  
- 通过 `PHYSICAL_BLOCK_SIZE` 参数把逻辑块映射到实际物理块，支持非 2 的幂块大小的跨块访问。  
- 为此新增 `test_qwen3_nonstandard_block_size`，并在 `chunked_prefill_paged_decode`、`prefix_prefill`、`triton_reshape_and_cache_flash` 中加入相应的分支与地址计算。  
- `rocm_attn` 在写缓存时先检查 `block_size` 是否为 2 的幂，若不是则转向改写的 Triton 实现；同时提供 `is_block_table_ptr` 选项，以物理地址方式解析块表。

**影响范围**  
- 关键模块：`vllm/attention/ops/*`（triton kernel 与调度层）以及 `vllm/v1/attention/backends/rocm_attn.py`。  
- 新增的测试文件覆盖了非标准块大小的前向路径，确保在 ROCm 上不回退到错误的实现。  
- 由于加入了 `eviction_policy="evict_last"`、显式的 `L`、`alpha` 零值保护以及 `+1e‑10` 防 NaN，数值稳定性提升。

**潜在风险 & 建议**  
1. **兼容性**：`PHYSICAL_BLOCK_SIZE` 只在 ROCm 路径使用，若未来在 CUDA 上出现非标准块大小，需要同步相同逻辑。建议在统一的抽象层抽出跨平台块映射函数。  
2. **性能**：强制使用 32‑tile 的 Triton 路径（`TRITON_BLOCK_SIZE=32`）在 544‑块模型上可能出现额外的内部填充，导致带宽利用率下降。可在实验中对比 `BLOCK_SIZE=544` 的自行实现，判断是否仍有提升空间。  
3. **内存地址计算**：`is_block_table_ptr` 逻辑依赖 `key_cache.stride(0) * element_size`，若后续改为不同的布局（如 head‑major）需要同步更新。建议在 `attention/ops` 中统一提供 `to_physical_block_table` 辅助函数。  
4. **数值保护**：在 `kernel_paged_attention_2d` 与 `prefix_prefill` 中对 `M`、`L`、`alpha` 的 `float("-inf")` 判断改为显式 `where`，已避免 NaN，但仍应在 CI 中加入极端序列长度的回归测试，以防稀疏 `mask` 触发未预期的 `-inf` 传播。  

**总体评估**  
本次 PR 为 ROCm 上的 Qwen 3‑Next‑80B 引入了关键的非标准块大小支持，改动集中在底层 triton kernel 与缓存写入分支，结构清晰且已有对应单元测例。只要在多平台 CI 中验证上述兼容性与性能基准，即可安全合入。

---

### [Cleanup] Remove obsolete spec decoding compatibility logic (#32003)
**SHA**: `29ce482` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/29ce48221c327d4d10f18dfa8fd06a6cecd358fd)

**🎯 变更类型**：代码清理 / 功能简化  
**⚡ 重要程度**：🟡 中（不影响现有功能，但影响 spec‑decode 相关模块的内部实现）  
**📋 变更摘要**：梳理并移除 vLLM 中针对 speculative decoding（猜测解码）不兼容请求的检测与过滤逻辑。删除 `is_spec_decode_unsupported`、`spec_decode_unsupported_reqs` 集合以及相关参数传递；相应地修改 `ngram_proposer`、`suffix_decoding`、`gpu_input_batch`、`gpu_model_runner` 等调用方，使其仅使用 `num_tokens_no_spec` 与 `token_ids_cpu`。测试用例相应更新为不再提供 `req_ids` 与 `spec_decode_unsupported_reqs` 参数。

**🎯 影响范围**  
- `vllm/v1/spec_decode/*`（ngram、suffix、utils）  
- `vllm/v1/worker/gpu_input_batch.py`、`gpu_model_runner.py`  
- 基准脚本 `benchmarks/benchmark_ngram_proposer.py`  
- 相关单元测试 `tests/v1/sample/test_logprobs.py`、`tests/v1/spec_decode/test_ngram.py`

**💡 关注建议**  
1. **兼容性检查**：确认上层 API（如 `LLM(..., speculative_config=...)`）在没有 `spec_decode_unsupported_reqs` 时仍能正确拒绝不支持的采样参数。若有用户自行使用旧字段，需在文档或迁移指南中说明已移除。  
2. **性能验证**：删除的过滤逻辑本身开销极低，但确保移除后 `propose` 流程仍保持预期的 early‑exit（如 `num_tokens_no_spec` 已达上限）。建议在 CI 中加入对极端 `max_model_len` 场景的回归测试。  
3. **测试覆盖**：当前修改已更新大部分测试，但仍建议补充一个针对 `SamplingParams` 中非法组合（如 `frequency_penalty!=0`）的负向测试，确保新实现不再意外接受这些请求。  
4. **文档同步**：在 spec‑decode 章节说明 “不再对采样参数进行兼容性检测，使用者需自行保证参数合法”。  

整体来看，此次清理简化了代码路径，降低维护成本，风险主要在于外部调用者可能仍期待旧字段的存在。做好文档与迁移提示即可安全发布。

---

### [Async][Feat] support apply penalty or bad_words for async + spec (#30495)
**SHA**: `8ff4a99` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8ff4a9956693450a37843bde88934ebcea440402)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为异步调度 + 结构化解码（spec decode）场景加入对 `presence_penalty`、`frequency_penalty`、`repetition_penalty` 与 `bad_words` 的支持。移除原先的限制检查，并在 GPU 端实现 **output token ids** 与 **spec token ids** 的异步更新逻辑，使惩罚/违禁词能够在下一轮采样前正确计算。  
**🎯 影响范围**：  
- `vllm/v1/engine/input_processor.py`（调参校验）  
- `vllm/v1/worker/gpu_input_batch.py`（新增 `update_async_output_token_ids`、`update_async_spec_token_ids`）  
- `vllm/v1/worker/gpu_model_runner.py`（采样前调用更新、草稿 token 拷贝逻辑调优）  
- 测试 `tests/v1/e2e/test_async_scheduling.py`（覆盖新参数组合）  

**💡 关注建议**  
1. **占位 token 替换逻辑**：`update_async_output_token_ids` 通过 `-1` 标记占位并截断 `new_ids`，需确认在极端情况下（如草稿 token 被截断或出现多余 `-1`）不会出现索引越界或遗漏。建议增加对 `num_placeholders == 0`、`new_ids` 长度异常的断言或日志。  
2. **spec token 同步时机**：仅在 `output_token_ids` 需要时才更新 `spec_token_ids`，但其它特性（如结构化输出）也可能依赖真实 draft token。请确保 `use_async_scheduling` 与 `has_structured_output_requests` 的组合覆盖所有路径，防止遗漏导致不一致。  
3. **同步事件安全**：`_copy_draft_token_ids_to_cpu` 现在在异步调度下仅在必要时复制。若后续增加新的依赖（如 `logprobs`），需审查是否仍满足需求。  
4. **回退兼容**：原先的 `ValueError` 已被去除，外部调用若仍假设不支持惩罚会得到不同的行为。建议在文档或 changelog 中明确 “async + spec 现在支持 penalties & bad_words”。  
5. **性能监测**：新增的 CPU ↔ GPU 同步会在高并发 async 场景下产生额外拷贝，建议在基准测试中对比 `async + spec` 与纯 async 的吞吐与 latency，若出现回退可考虑延迟复制或批量化处理。  

总体而言，此次改动解锁了重要的采样功能，代码路径已加入必要的占位替换与草稿 token 同步，建议补齐边界测试并监控性能回归。

---

### [Frontend] Add MCP tool streaming support to Responses API (#31761)
**SHA**: `a4ec0c5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a4ec0c559521c055519eeabddd8279c83eb4e936)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Responses API 增加了 MCP（Multi‑Channel‑Tool）工具的流式事件支持。实现了对 MCP 工具（code_interpreter、container、web_search_preview 等）在 streaming 场景下的 `output_item`、`mcp_call`、`arguments delta`、`completed` 等细粒度事件的生成；引入 `HarmonyStreamingState` 数据类统一管理流式状态；新增环境变量 `VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS` 用于声明可用的 MCP 标签；相应测试覆盖 MCP 工具开启/关闭、通配符 allowed_tools、代码解释器与函数调用互斥等场景。

**🎯 影响范围**  
- `vllm/entrypoints/openai/serving_responses.py`：核心流式事件处理逻辑大幅改写，加入大量 `_emit_*` 方法。  
- `openai/types/responses/*`：新增 `ResponseMcpCall*`、`ResponseMcpCallInProgressEvent` 等类型。  
- 测试目录：`tests/entrypoints/openai/` 中大量 E2E 流式与非流式 MCP 场景测试。  
- 环境变量配置、工具标签映射 (`_TOOL_NAME_TO_MCP_SERVER_LABEL`)。

**💡 关注建议**  

1. **兼容性检查**：确保在未开启 `VLLM_ENABLE_RESPONSES_API_STORE` 或未设置 MCP 标签时，旧的非 MCP 流式行为保持不变；建议在 `responses_full_generator` 入口处加入明确的分支保护。  
2. **文档与示例**：补充 MCP 工具使用说明（环境变量、`allowed_tools` 语法、`server_label` 对应关系），避免用户因标签不匹配导致工具不可用。  
3. **性能评估**：流式事件数量显著提升，建议在高并发场景做基准测试，关注 `HarmonyStreamingState` 的对象创建及 GC 开销。  
4. **错误处理**：当前 `_emit_*` 方法多处直接抛异常（如未知 MCP 前缀），建议统一包装为 `ResponseError`，并在日志中提供完整上下文，防止单个异常导致整个流被中断。  
5. **测试覆盖**：现有测试已覆盖主要路径，建议再加入异常分支（如非法 `server_label`、空 `allowed_tools`）以及并发调用的稳定性验证。  

总体而言，此次改动为 MCP 工具提供了完整的流式交互能力，功能提升显著，但代码体量大幅增长，务必通过文档、兼容性保障以及性能回归确保对现有用户的平滑迁移。

---

### [Bugfix] Fix Typo from NVFP4 Refactor (#31977)
**SHA**: `0fa8dd2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0fa8dd24d281f71c0b0f6fb5b6033ca30d97f932)

**🎯 变更类型**：Bug 修复 / 小幅功能扩展  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py` 中，修正了 `NvFp4MoeBackend` 枚举列表的笔误：原先出现两次 `FLASHINFER_TRTLLM`，导致缺失 `FLASHINFER_CUTEDSL` 后端的注册。  
2. 新增多套 GSM8K 评估配置 YAML（涉及 NVFP4‑CT、ModelOpt、Cutlass、DeepEP 等组合），并更新 `config-b200.txt` 与 `config-test.txt` 以列出新配置。

**🎯 影响范围**  
- **模型执行层**：`flashinfer_fp4_moe.py` 中的后端选择逻辑会直接受影响，新增的 `FLASHINFER_CUTEDSL` 现在能够被识别并调用对应的 CUDA‑kernel。  
- **评估脚本**：`tests/evals/gsm8k/configs/moe-refactor*` 目录下的配置文件被加入，可能影响 CI/Eval 流程和文档示例。  
- **环境变量**：`VLLM_FLASHINFER_MOE_BACKEND` 可接受的新取值 `masked_gemm`（在配置中已使用）。

**💡 关注建议**  
1. **后端实现检查**：确认 `NvFp4MoeBackend.FLASHINFER_CUTEDSL` 在代码其他位置（如注册表或 C++ 插件）已有对应实现，否则仍会在运行时抛错。  
2. **测试覆盖**：为新后端补充单元/集成测试，尤其是 `prepare_nvfp4_moe_layer_for_fi_or_cutlass` 的分支走向。  
3. **文档同步**：在 README 或部署手册中补充 `FLASHINFER_CUTEDSL` 的说明、适用场景以及推荐的 `server_args`。  
4. **向后兼容**：此次更改仅是删除重复条目并加入缺失项，理论上不会破坏已有后端的行为，但建议在升级前运行 `vllm --check-backend` 类的健康检查。  

总体而言，此次提交纠正了关键枚举的拼写错误，确保 NVFP4‑MoE 在 FlashInfer‑CutEDSL 后端能够被正确调度，同时通过新增评估配置为后续功能验证提供了便利。后续需关注对应 C++ 实现的可用性及文档的同步更新。

---

### [Feature] Add iteration level logging and enhance nvtx marker (#31193)
**SHA**: `6ebe34d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6ebe34d6fab6ab8e6db6e37dcfd3117da1125d11)

**变更类型**：功能增强  
**重要程度**：🟡 中  

**核心变更**  
1. **配置层**：在 `ObservabilityConfig` 中新增 `enable_logging_iteration_details`，并通过 `EngineArgs`、CLI 参数 `--enable-logging-iteration-details` 暴露。  
2. **调度数据**：`CachedRequestData` 增加 `_req_id_to_num_output_tokens` 缓存属性及 `is_context_phase` 判定，用于快速判断请求是否仍在 *context*（prefill）阶段。  
3. **日志与 NVTX**：  
   - `EngineCore.step` 里加入 `log_iteration_details` 上下文管理器，在每轮调度结束后打印 “Iteration(i): …” 统计信息（context/generation 请求数、token 数及耗时）。  
   - `GPUWorker.annotate_profile` 也使用同一统计，生成更直观的 NVTX 标注。  
4. **工具函数**：新增 `IterationDetails` 数据类和 `compute_iteration_details`，在 `vllm/v1/utils.py` 中实现统计逻辑，供日志和 profiler 共用。  

**影响范围**  
- 配置/CLI（`config/observability.py`、`engine/arg_utils.py`）  
- 调度输出结构（`v1/core/sched/output.py`）  
- 引擎核心执行流（`v1/engine/core.py`）  
- GPU worker 的 profiler 注释（`v1/worker/gpu_worker.py`）  
- 公共工具库（`v1/utils.py`）  

**潜在影响**  
- **性能**：开启后每轮会额外遍历 `scheduler_output.num_scheduled_tokens` 并做缓存查表，开销极小（O(N)），但在高并发场景下仍建议通过配置关闭。  
- **兼容性**：默认关闭，老版脚本不受影响；新增 CLI 参数需更新文档和 CI 测例。  
- **日志噪声**：在长链推理中日志行数会迅速增长，用户需要自行过滤或禁用。  

**建议**  
- 对外文档补全 `--enable-logging-iteration-details` 的使用说明及典型场景（性能调优、排障）。  
- 在单元测试中加入 flag 为真/假的两条路径，确保 `log_iteration_details` 不会因异常导致调度中断。  
- 若后续需要更细粒度（如 per‑token 时间），可在 `IterationDetails` 中预留扩展字段。  

该功能为观察与调优提供了明确的迭代层级指标，使用时请注意日志量和轻微的统计开销。

---

### [Quantization] Deprecate Long Tail of Schemes (#31688)
**SHA**: `5825bbc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5825bbc1f792d3fe041f845148a9e9ad837eef30)

**🎯 变更类型**：功能增强 + 兼容性升级（新增 `allow_deprecated_quantization` 标记并将一批旧方案设为已废弃）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `ModelConfig` 中加入 `allow_deprecated_quantization`（默认 `False`）并在量化校验 `_verify_quantization` 中对 `DEPRECATED_QUANTIZATION_METHODS` 做限制；  
- `EngineArgs`、CLI 参数以及 `create_model_config` 同步新增对应选项 `--allow-deprecated-quantization`；  
- `vllm/model_executor/layers/quantization/__init__.py` 列出所有已废弃的量化方法；  
- 测试用例全部加上 `allow_deprecated_quantization=True` 以适配新行为。  

**🎯 影响范围**  
- `vllm/config/model.py`、`vllm/engine/arg_utils.py`、`vllm/model_executor/layers/quantization/__init__.py`（核心配置/参数解析）  
- CLI 调用路径、`VLLMRunner`（间接通过 `EngineArgs`）  
- 所有使用已废弃量化方案的测试和用户脚本。  

**💡 关注建议**  
1. **文档与提示**：在官方文档、`--help` 输出中强调该标志的用途及即将移除的量化方案，避免用户因默认 `False` 而意外报错。  
2. **兼容性验证**：增设单元测试覆盖不带标志的路径，确保抛出明确的 `ValueError`；同时验证开启标志时仅打印一次 `warning`。  
3. **配置序列化**：确认 `ModelConfig` 在 `json`、`yaml` 等持久化方式中能够正确保存/加载 `allow_deprecated_quantization`。  
4. **迁移指南**：提供迁移脚本或示例，帮助用户将旧方案迁移到新版（如 `gptq_marlin_24` → 推荐的替代方案）。  
5. **内部使用**：检查 `vllm/model_executor` 中是否还有对已废弃方法的硬编码路径，必要时统一走 `quantization` 注册表，以免出现隐藏的未检测路径。  

总体来看，改动合理，但需完善文档、测试和迁移说明，以降低因废弃导致的使用中断风险。

---

### [MoE Refactoring][Bugfix]Wrap WNA16 Triton kernel into mk and change compressed tensor kernel selection (#31752)
**SHA**: `d62cfe5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d62cfe546dc587925b59eebf5aa736ab70d47a1f)

**🎯 变更类型**：功能增强 / Bugfix（MoE WNA16 量化核的封装与调用）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 新增 `TritonWNA16Experts`，在原有 `TritonExperts` 基础上封装了 WNA16（权重量化为 W8A16）专用的 Triton kernel。  
- `invoke_fused_moe_wna16_triton_kernel` 的 `block_shape` 参数由可选改为必填，并在内部通过 `assert block_shape is not None and block_shape[0] == 0` 强制检查。  
- `__init__.py` 与 `get_config()` 中把新类加入导出列表，使其可被外部模块发现。  
- 在压缩张量 MoE 的实现 `compressed_tensors_moe.py` 中，将原先的 `TritonExperts` 替换为 `TritonWNA16Experts`，确保在开启 Triton 且使用压缩张量时走 WNA16 路径。  

**🎯 影响范围**  
- **核心模块**：`vllm.model_executor.layers.fused_moe`（新增类、配置导出、Kernel 调用）  
- **压缩张量实现**：`vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors_moe`（选择实现逻辑）  
- **配置系统**：`vllm.model_executor.layers.fused_moe.__init__`（新增名字）  

**💡 关注建议**  
1. **兼容性检查**：调用 `modular_triton_fused_moe` 或其他入口的代码必须确保 `block_shape` 已正确传入（即不再接受 `None`），否则会触发断言导致运行时崩溃。  
2. **回退路径**：如果用户仍需使用原始 `TritonExperts`（例如不想使用 WNA16 量化），需要在配置或代码中显式选择 `TritonExperts`，避免被自动替换。  
3. **单元/集成测试**：新增 `TritonWNA16Experts` 的功能覆盖率应加入 CI，特别是:
   - 不同 `dtype`（fp32/fp16/bf16/float8）下的 compute_type 映射；
   - `use_int8_w8a16` 与 `use_int4_w4a16` 两种量化模式的路径；
   - `block_shape` 为 `[0, …]` 与非法值的断言行为。  
4. **文档更新**：在 `MoE` 与 `compressed tensors` 章节说明新类的使用场景、参数要求（尤其是 `block_shape`），并标注仅在 Triton 可用时生效。  
5. **性能关注**：WNA16 kernel 预期提升 INT8/INT4 量化的吞吐，建议在不同硬件上（GPU A100、H100 等）跑基准，确认突破点与旧 kernel 对齐。  

总体来看，此次改动为 WNA16 量化提供了专属 Triton 实现，提升了模型并行 MoE 的推理效率，但需要注意 `block_shape` 参数的强制要求以及保持对旧实现的兼容路径。做好测试与文档同步后，可安全合并到主分支。

---

### [Compressed-Tensors] Simplify NVFP4 Conditions, enable marlin support for NVFP4A16 MoEs (#30881)
**SHA**: `5d3b609` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5d3b6097ad6f8ddb156b92525cce19e08e5c7b89)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 将原先针对 NVFP4 的 “fp4a4 / fp4a16” 双参数检查合并为统一的 `_is_nvfp4_format`，简化逻辑并去除冗余 `_is_fp4a16_nvfp4` 实现。  
2. 为 NVFP4‑A16（权重量化 4‑bit，激活不量化）MoE 添加 Marlin 后端支持，并在 `CompressedTensorsW4A4Nvfp4MoEMethod` 中加入 `use_marlin` 开关及相应的验证逻辑。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py`（量化格式判定）  
- `vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py`（MoE 方法选择、后端切换）  
- 新增对 `vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py` 的引用（Marlin 支持检测）  

**💡 关注建议**：  
1. **兼容性验证**：Marlin 仅在特定 GPU/驱动上可用，建议在 CI 中加入平台检测，确保在不支持的环境下仍能回退到默认后端而不抛异常。  
2. **输入校验**：新增 `_is_nvfp4_format` 会对 `QuantizationArgs` 为 `None` 返回 `False`，但在 `CompressedTensorsW4A4Nvfp4MoEMethod` 中仍假设 `weight_quant` 非空。请在调用前加入显式校验或文档说明。  
3. **文档和示例**：更新 README / API 文档，说明 `use_marlin=True` 只能在 `input_quant is None`（即 NVFP4‑A16）时使用，并给出开启 Marlin 的示例代码。  
4. **单元测试**：补充以下场景的测试：  
   - `weight_quant` 为 NVFP4、`input_quant` 为 None → 使用 Marlin。  
   - `weight_quant` 为 NVFP4、`input_quant` 为 NVFP4 → 不使用 Marlin，走常规路径。  
   - 平台不支持 Marlin 时抛出友好错误。  
5. **性能回归**：由于后端切换可能影响内存布局和吞吐，建议在典型模型（如 Llama‑2‑7B）上跑一次基准，确认 Marlin‑FP4 MoE 的速度提升符合预期。  

总体而言，此次改动提升了 NVFP4 判定的可维护性，并为 NVFP4‑A16 MoE 引入了更高效的 Marlin 实现，只要做好平台兼容检查和测试，即可安全发布。

---

### [Misc][Refactor] Add FusedMoERouter object (#30519)
**SHA**: `e74698c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e74698c27ad5a66bb0f1b131a9e5582a0fbf00d1)

**🎯 变更类型**：🛠️ 重构 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `vllm/model_executor/layers/fused_moe` 体系中新增抽象类 **`FusedMoERouter`**，并实现 `FusedMoERouterImpl` 负责统一的专家路由 （`select_experts`）调用。所有原先直接在 `FusedMoE` 上的 `select_experts` 调用被改为通过 `router` 对象传递，相关量化、Marlin、Compressed‑Tensors、IPEX 等 30+ 处实现全部改为接受 `router: FusedMoERouter` 参数。测试文件亦相应改为 `fused_moe.router.select_experts`。

**🎯 影响范围**  
- `vllm/model_executor/layers/fused_moe/*`（核心 MoE 层、配置、量化实现）  
- 所有使用 `FusedMoE.select_experts` 的量化插件（awq, bitsandbytes, fp8, gptq 等）  
- `tests/test_routing_simulator.py`（测试路径）  
- `__init__.py` 导出 `FusedMoERouter`  

**💡 关注建议**  

1. **实例化与属性保持**：`FusedMoE` 现在在 `__init__` 中创建 `self.router = FusedMoERouterImpl(self)`，请确认外部代码仍然可以通过 `layer.router` 访问；若有自定义子类直接调用 `layer.select_experts`，需改为 `layer.router.select_experts`。  

2. **向后兼容**：虽然 `FusedMoERouter` 只包装了原有私有 `_select_experts`，但 API 已变更。建议在主分支保留一个向后兼容的包装方法（如在 `FusedMoE` 上保留 `def select_experts(...): return self.router.select_experts(...)`），防止第三方插件在未同步更新时报错。  

3. **类型检查**：许多 `apply` 方法的签名已加入 `router: FusedMoERouter`，确保所有自定义 `FusedMoEMethodBase` 子类实现同样的参数，否则会在运行时触发 `TypeError`。  

4. **性能验证**：路由抽象本身为轻量包装，但大量调用路径的函数签名变更可能导致 JIT/torch.compile 缓存失效。建议在关键路径跑一次基准测试，确认吞吐量未受负面影响。  

5. **文档更新**：README、开发文档以及示例代码中需要说明 `FusedMoE.router` 的使用方式，以及在自定义量化实现中如何接收 `router` 参数。  

6. **测试覆盖**：新增的 `FusedMoERouterImpl` 已在 `tests/test_routing_simulator.py` 覆盖，但建议补充对 `FusedMoE` 直接调用 `select_experts`（如果保留兼容层）以及在不同量化后端（e.g., awq, bitsandbytes）下的路由结果一致性测试。  

总体而言，此次抽象有助于解耦路由逻辑与 MoE 核心实现，为后续扩展（如自定义路由策略）提供统一入口。但因改动范围广，务必在升级文档中明确迁移步骤，防止用户在旧代码基上出现 `AttributeError` 或签名不匹配的问题。

---

### [Frontend] Improve error message (#31987)
**SHA**: `aa125ec` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aa125ecf0edb9cd67656553d11d643aeb444ff9e)

**🎯 变更类型**：其他（前端错误信息改进）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `vllm.entrypoints.utils` 中新增 `sanitize_message`，使用正则剔除对象 `repr` 中的内存地址（如 `0x7a95e299e750`），并在 OpenAI API 服务器的异常处理路径中统一调用该函数，以防泄露底层实现细节。新增对应单元测试 `tests/entrypoints/test_utils.py`。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/api_server.py`：HTTPException 与 RequestValidationError 的错误返回 now 经过 `sanitize_message` 处理。  
- `vllm/entrypoints/utils.py`：新增正则依赖 `regex`，提供公共工具函数。  
- 测试套件：新增 `test_sanitize_message`。  

**💡 关注建议**  
1. **依赖安全**：`regex` 并非标准库，需在 `requirements.txt` 或 `pyproject.toml` 中声明，以免部署时缺失。  
2. **函数健壮性**：`sanitize_message` 目前仅接受 `str`，但部分异常 `detail` 可能为非字符串（如 `dict`、`list`），建议在函数入口做 `str(message)` 或提前检测，防止 `TypeError`。  
3. **正则覆盖**：当前正则仅匹配小写十六进制，若对象 repr 使用大写 `0x7A95E299E750` 将不会被替换，可改为 `r" at 0x[0-9a-fA-F]+>"` 提升兼容性。  
4. **日志与监控**：错误信息在日志中仍会保留原始 `detail`，若日志直接输出异常对象，仍可能泄露地址。考虑在日志层也统一使用 `sanitize_message`。  
5. **文档更新**：在项目的错误处理或 API 使用文档中标注“错误信息已脱敏”，帮助使用者了解返回格式的变化。  

总体而言，此次改动提升了对外错误信息的安全性和可读性，影响范围局限于 API 错误返回，风险可控。建议在 CI 中加入对非字符串异常的额外单元测试，以确保函数在所有路径上安全运行。

---

### Revert "feat(moe): Add is_act_and_mul=False support for Triton MoE kernels" (#31978)
**SHA**: `87e07a6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/87e07a6b46fbd85725cd19bfa14b1387e90003cf)

**🎯 变更类型**：重构 / 代码回滚  
**⚡ 重要程度**：🟡 中（撤销新特性，避免潜在不兼容）  
**📋 变更摘要**：本次提交撤销了 “is_act_and_mul=False” 对 Triton MoE 内核的支持。具体操作包括：  
1. 删除了对应的测试文件 `tests/kernels/moe/test_triton_moe_no_act_mul.py`。  
2. 从 `FusedMoEQuantConfig` 中移除 `is_act_and_mul` 字段及其在 `make`、`biased_moe_quant_config` 等工厂函数中的参数。  
3. 在 `fused_batched_moe.py`、`fused_moe.py`、`layer.py` 中删除所有基于该标志的分支，统一使用 `N // 2`（即 SwiGLU 融合激活）计算中间尺寸。  
4. `modular_kernel.py` 的激活实现恢复为仅支持融合激活；非融合路径的实现被删去。  
5. 更新了 `unquantized_fused_moe_method.py`，不再在 `is_act_and_mul=False` 场景下生成专门的配置。

**🎯 影响范围**  
- **核心 MoE 计算路径**：`vllm/model_executor/layers/fused_moe/*`（配置、工作空间、执行、激活）全部受到修改。  
- **量化配置 API**：`FusedMoEQuantConfig.make` 及相关包装函数签名变化，外部代码若显式传入 `is_act_and_mul` 将报错。  
- **平台兼容性逻辑**：`layer.py` 中对 ROCm/AITER 的限制被简化，仅保留 CUDA 支持。  
- **测试套件**：原有的非融合激活测试被移除，CI 运行时间略有下降。

**💡 关注建议**  
1. **代码升级**：检查项目中任何自定义 MoE 实例（尤其是使用 `FusedMoEQuantConfig` 的地方），确保不再传递 `is_act_and_mul` 参数；若需要非融合激活，请改用已实现的融合激活或自行实现自定义 kernel。  
2. **平台部署**：当前仅在 CUDA 上保证功能，若在 ROCm 环境运行，请确认不使用 `is_act_and_mul=False` 相关特性，或等待后续补丁。  
3. **文档同步**：更新文档、示例以及 README 中关于 “is_act_and_mul” 参数的说明，避免产生误导。  
4. **回滚风险**：如果业务已依赖此特性（如 Nemotron‑H），请评估回滚对性能或数值的影响，并考虑在本地保持旧分支或自行实现对应激活。  
5. **后续计划**：关注后续 PR 是否会重新加入该特性，以及是否会提供更稳健的 ROCm 实现。  

总体而言，此次回滚提高了代码库的稳定性，但也意味着已有的非融合激活路径暂时不可用，使用方需相应调整配置与平台。

---

### [Model Runner V2] Simplify BlockTables with UVA (#31965)
**SHA**: `7508243` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/750824324903f6dfc289d633ff5c513f16304f40)

**🎯 变更类型**：功能增强（引入统一的 UVA Buffer，简化 BlockTables 逻辑）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `UvaBuffer`（CPU → GPU 零拷贝视图），统一使用 UVA （Unified Virtual Addressing）而不再自行管理 `CpuGpuBuffer`。  
2. `BlockTables` 彻底改写：去掉原先的 `pin_memory` 参数与 `CpuGpuBuffer`，改为内部维护 `UvaBuffer`；`append_block_ids` 直接在 CPU numpy 数组上写入，再通过已有的 GPU 视图访问，删除了原本的 Triton kernel。  
3. `ModelRunner` 相应删掉 `pin_memory` 参数的传递，改用新的 `append_block_ids` 接口，简化调度路径。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/block_table.py`（核心数据结构、GPU → CPU 同步）  
- `vllm/v1/worker/gpu/model_runner.py`（调度层调用）  
- 新增 `vllm/utils/platform_utils.is_uva_available` 与 `vllm/utils/torch_utils.get_cuda_view_from_cpu_tensor`（平台检测与视图创建）  

**💡 关注建议**  

1. **UVA 环境检测**：`BlockTables.__init__` 在没有 UVA 时直接抛异常，建议在上层（如 `ModelRunner.initialize_kv_cache`）捕获并提供回退方案，防止在 CPU‑only机器上启动失败。  
2. **同步语义**：`UvaBuffer` 依赖 `torch.cuda.synchronize()` 或显式的 `cudaMemcpyAsync` 保证 CPU 写入可见。检查调用路径是否已隐式同步；若无，可能出现 stale data 的 subtle bug。  
3. **并发安全**：`append_block_ids` 仍在 CPU numpy 数组上原地写入，若同一 `BlockTables` 被多线程/多进程共享，需要加锁或使用线程局部实例防止 race。  
4. **性能评估**：虽然去掉 Triton kernel 减少 kernel 启动开销，但大量小规模的 CPU numpy 写入仍会产生 PCIe 带宽压力。建议在高并发场景下对 `append_block_ids` 的批量写入做基准，必要时考虑一次性合并多请求的写入。  
5. **Pin‑memory 强制**：`_make_ptr_tensor` 仍硬编码 `pin_memory=True`，若未来引入非‑UVA 后端，这里可能需要恢复可配置化。  
6. **测试覆盖**：确保所有原有的 block‑table 单元测试、prefill/decoding 流程以及 Scheduler‑->‑ModelRunner 的交互在无 UVA 环境下也能快速报错，而在有 UVA 环境下保持行为一致。  

总体来看，此次改动极大简化了 BlockTables 的实现路径，降低了 Triton‑kernel 维护成本。但在正式发布前，需要补充对非‑UVA 环境的容错、同步与并发安全检查，以避免在生产环境出现不可预期的失效。

---

### [Misc] Tidy up some spec decode logic in GPUModelRunner (#31591)
**SHA**: `a3d909a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a3d909ad2b2936333af88f308ae2147e1dd7a6d8)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为 `GPUModelRunner` 与 `GPUWorker` 新增 `update_max_model_len` 接口，统一在运行时更新 `max_model_len` 与关联的 `req_states`、`effective_drafter_max_model_len`。  
2. 重构 speculative‑decode 路径：引入 `effective_drafter_max_model_len`，简化 Eagle 与其他 draft‑model 的输入判定逻辑，避免重复计算 `max_model_len`。  
3. 调整 `propose_draft_token_ids` 的调用时机，使用 `propose_drafts_after_bookkeeping` 标记，使 Eagle 能在采样后直接生成 draft，而 n‑gram 等在 bookkeeping 完成后再生成。

**🎯 影响范围**：  
- `vllm/v1/worker/gpu/model_runner.py`、`vllm/v1/worker/gpu_model_runner.py`、`vllm/v1/worker/gpu_worker.py`（GPU 端模型运行与调度）。  
- 相关的 speculative decoding（Eagle、ngram、draft模型）流程。  

**💡 关注建议**：  
1. **兼容性**：确认旧代码（如自定义 `GPUModelRunner` 子类）仍能通过属性 `max_model_len` 访问，必要时在基类保留该属性的 setter。  
2. **同步更新**：在 `update_max_model_len` 中同步 `effective_drafter_max_model_len`，防止在运行期间出现 draft‑model 长度不匹配的异常。  
3. **单元测试**：新增覆盖 `effective_drafter_max_model_len` 变化（包括有/无 `draft_model_config.max_model_len`）的测试；专门校验 Eagle 分支在 `input_fits_in_drafter` 为 false 时仍能回退到 padded‑batch 逻辑。  
4. **性能监控**：该改动改写了分支判断，建议在 CI 中加入基准测试，确保没有因额外条件判断导致显著的推理延迟。  
5. **文档**：在 `SpeculativeConfig` 与 `GPUWorker.update_max_model_len` 的使用说明中补充 “在运行时更新模型最大序列长度” 的注意事项，防止用户误以为仅在启动前可配置。  

总体来看，此次改动提升了运行时调参的灵活性并纠正了 draft‑model 长度判定的 Bug，风险主要集中在未同步更新 `effective_drafter_max_model_len` 的老代码路径，建议通过上述测试和文档补全来降低回归风险。

---

#### 🟢 低重要度变更 (19)

### [Doc] Remove hardcoded Whisper in example openai translation client (#32027)
**SHA**: `2d0c5b6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2d0c5b630e87cd48675fbdd833fe24785a57b1a2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将示例 OpenAI 翻译客户端中硬编码的 Whisper 模型参数抽象为可配置参数，分别在同步、流式调用以及 `main` 中传入模型 ID，实现模型选择的灵活性。

---

### [Model] Remove redundant None check in DeepSeekOCR image input processing (#32016)
**SHA**: `7cdf7e2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7cdf7e2fe0d406724659d84e0a17fde97f855a89)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：移除 DeepSeekOCR 图像处理中的冗余 `None` 检查，简化返回逻辑，删除不可达的异常分支。

---

### Fix type error (#31999)
**SHA**: `bbf80ed` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bbf80ede4325ae8bed35dfb1c04be63e6b0ce372)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `responses_utils.py` 中为 `ResponseFunctionToolCall` 添加 `item.id` 非空检查，防止 `None` 调用 `startswith` 导致类型错误。

---

### [Misc] Skip hashing kwargs if value is `None` (#32025)
**SHA**: `db07433` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/db07433ce58f2f6ac9aa0e2fad61181ff5a1442b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `iter_item_to_bytes` 中新增对 `None` 的检查，直接返回键的字节，以避免对 `None` 值进行哈希。

---

### [Feature][Benchmarks] Custom dataset: read output length from dataset (#31881)
**SHA**: `b474782` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b474782ad776d4517675a6eae6f424a6d3ddf7ca)

**🎯 变更类型**：代码重构/功能更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：为 CustomDataset 增加 `output_tokens` 字段支持；当 `--custom-output-len` 为 -1 或未指定时，可直接从数据集读取每条记录的输出长度。若缺失则抛出明确错误。

---

### [Bugfix] Fix OpenAPI schema test failures (#31921)
**SHA**: `0207328` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/020732800caf3ba1eae62098c4264dac7ef35611)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 OpenAPI schema 测试失败，强化 `file` 类型检查，给 `/v1/completions` 接口添加长超时，并在 `truncate_prompt_tokens` 上加入模型最大令牌数上限。

---

### [Bugfix] Fix Var Length Batched Padding in Granite Speech (#31906)
**SHA**: `dc77cb7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dc77cb7129b9efa1c74a2a05f6359dde977f7c8b)

**变更类型**：其他  
**重要程度**：🟢低  
**摘要**：在 Granite Speech 模型的音频输入解析中，新增对二维特征张量的 `unsqueeze` 处理，以正确支持可变长批量填充；并将相关注释更新为 3D 张量说明。

---

### fix lora moe sharding when rank < max_lora_rank (#31994)
**SHA**: `bde38c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bde38c11df0ea066a740efe9b77fff5418be45df)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 LoRA MoE 在 rank 小于 max_lora_rank 时的分片错误，改为使用实际 shard 大小进行切片，并在测试中移除 max_lora_rank 参数。

---

### [Bugfix] Fix FusedMoE LoRA w2_output_size (#31949)
**SHA**: `707b240` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/707b240d7e7523c652b8a5b075ffc1ee9e1120ae)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `vllm/lora/layers/fused_moe.py` 中，`w2_output_size` 方法由返回 LoRA 参数维度改为返回基层的 `hidden_size`，修复了 FusedMoE LoRA 的输出尺寸错误。

---

### [CI] [ROCm] Fix `tests/entrypoints/test_grpc_server.py` on ROCm (#31970)
**SHA**: `7a05d2d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7a05d2dc65065da3d6fd54c67a670ab08ffaf600)

**🎯 变更类型**：代码重构 / 配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 环境下修复 `tests/entrypoints/test_grpc_server.py` 超时问题；新增 `grpcio-tools` 依赖并在 `setup.py` 中为 `develop` 命令加入生成 gRPC 代码的逻辑；同步更新相关 requirements 文件。

---

### [ROCm][CI] Fix test_token_classification.py::test_bert_models (#31993)
**SHA**: `a1648c4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a1648c4045f2aba8894efa0e607c5940f6337d85)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_token_classification.py` 中新增 ROCm 平台检测，若为 ROCm 则在 HF 模型构造时使用 `attn_implementation="eager"` 以规避 FlashAttention 精度问题；改用 `torch.testing.assert_close` 并放宽容差；删除多余的 `current_platform` 导入。

---

### [Bugfix] missing tokens occur in harmony streaming (#30437)
**SHA**: `e2d49ec` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e2d49ec2a4818186b8bafda43f97c298ff91a35e)

**🎯 变更类型**：代码重构 / Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `HarmonyStreamingContext` 中累计每个 token 的增量文本，新增 `last_content_delta` 保存完整增量；相应地在所有 delta 事件中改用该字段，修复流式返回时出现的缺失 token 问题。

---

### [Bugfix] Fix typo in FusedMoE LoRA reshape comment (#31992)
**SHA**: `8413868` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8413868dab334b23cf8684671eccda3fdd96cc70)

**🎯 变更类型**：代码重构（注释纠正）  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修正 `vllm/lora/model_manager.py` 中 FusedMoE LoRA 权重重塑的注释文字错误，将 “(output_size,num_experts,rank)” 改为正确的 “(output_size,rank,num_experts)”。代码逻辑未变，仅提升可读性。

---

### [BugFix] Add spec-decode-incompatible request param validation (#31982)
**SHA**: `11cec29` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/11cec296dd47bbb25c3bb4e40bcc11341d6a2fe2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `input_processor` 中新增对 speculative decoding 不兼容的采样参数（`min_tokens`、`min_p`、`logit_bias`）的校验；相应测试删除 `min_tokens` 参数以符合新限制。

---

### [Documentation][torch.compile] Add documentation for torch.compile + multimodal encoders (#31627)
**SHA**: `f16bfbe` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f16bfbe5bc8f9045e7bb38d72ab35764bab22e44)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：新增 `torch.compile` 在多模态编码器（如视觉语言模型）上的使用说明，包括原理、配置方法、注意事项及故障排查。

---

### [CI][ROCm] Fix NIXL tests on ROCm (#31728)
**SHA**: `83e1c76` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/83e1c76dbe07e30b7f4e6dbe17ba580f4afc98f0)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修正 ROCm 环境 NIXL 测试脚本路径，更新超时设置，新增 DP‑EP 分布式准确性测试步骤。

---

### Fix ijson build for Power. (#31702)
**SHA**: `a563866` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a563866b48d6c6c7c7a0ca128dc35d4e64e33f42)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docker/Dockerfile.ppc64le` 中更新 CentOS 镜像 URL 与 GPG 包版本，并新增 `yajl-devel` 依赖，以修复 Power 架构下 ijson 的编译问题。

---

### [Doc] Improve MM models LoRA notes (#31979)
**SHA**: `49568d5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/49568d5cf984f9ea0f96dc636036075ef6ddec45)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `supported_models.md` 中精简 LoRA 说明，去除合并示例代码，改为简要提示 vLLM 已支持在大多数多模态模型的语言骨干添加 LoRA，并实验性支持塔和连接模块，新增指向 LoRA 特性页的链接。

---

### [Bugfix] Fix vllm serve failure with Nemotron Nano V3 FP8 (#31960)
**SHA**: `b8112c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b8112c1d85442b060e37df90e9db343cbc7b000c)

**🎯 变更类型**：代码重构 / Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `process_fp8_weight_tensor_strategy_moe` 中，修正了对 w1（非 w13）权重量化尺度的处理逻辑，明确每个 expert 只保留一个尺度，返回 `max_scales`，避免原先仅返回单一标量导致的服务崩溃。  

**简要分析**：原代码在非 w13 场景直接返回 `weight_scales.max()`，忽略了多 expert 的尺度维度；新实现先取每个 expert 的最大尺度并做形状校验，保证返回 `(weight, max_scales)`，从而修复 Nemotron Nano V3 FP8 推理时的崩溃问题。

---

