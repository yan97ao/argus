# 每日更新报告（2026-01-29）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-29 23:56:05 | Isotr0py | [Bugfix] Fix broken GLM-OCR initialization (#33350) |
| 2026-01-29 21:32:04 | Cyrus Leung | [Multimodal] Simplify MM input definitions (#33331) |
| 2026-01-29 20:29:17 | sthWrong | [Backport] [Kimi-K2.5] Replace torch.cuda with current_platform for d… (#33320) |
| 2026-01-29 20:26:52 | Kunshang Ji | [Intel GPU] refine xpu worker (#32894) |
| 2026-01-29 19:27:15 | Roger Wang | [Models] Qwen3-ASR (#33312) |
| 2026-01-29 19:26:58 | Li, Jiang | [Bugfix][CPU] Fix thread num for shared memory communication (#33317) |
| 2026-01-29 19:22:49 | Patrick von Platen | [Voxtral] Streaming example (#33042) |
| 2026-01-29 18:44:58 | zofia | [Quantization][Refactor]  use platform dict to choose kernel (#33130) |
| 2026-01-29 18:42:59 | andrii.pasternak | [Bug Fix] Handle variable-length tensors in MultiModalFlatField batching (#31751) |
| 2026-01-29 18:31:40 | Ilya Markov | [BugFix] Async Eplb fix potential race condition (#32881) |
| 2026-01-29 18:25:58 | daniel-salib | [fix] tesdt mcp_tool_calling_streaming with a more complex math question (#32769) |
| 2026-01-29 18:20:52 | Isotr0py | [Chore] Remove `use_data_parallel` kwargs from ViT implementation  (#33310) |
| 2026-01-29 17:46:02 | Isotr0py | [Misc] Cleanup Kimi-K2.5's vision chunk modality entrypoints (#33157) |
| 2026-01-29 17:36:34 | amirkl94 | Bugfix: Pass router logits dtype in nemotron shared experts (#32669) |
| 2026-01-29 17:12:26 | Harry Mellor | Make `mypy` opt-out instead of opt-in (#33205) |
| 2026-01-29 17:12:05 | Lucas Wilkinson | [Misc] Remove missed `pad_for_cudagraph` (#33283) |
| 2026-01-29 16:56:06 | graftim | [Doc] Update outdated link to Ray documentation (#32660) |
| 2026-01-29 16:54:02 | shanjiaz | Adding optional speculator tests for larger models (#32943) |
| 2026-01-29 16:53:15 | whx | [PluggableLayer][2/N] Apply PluggableLayer to linear layers (#33152) |
| 2026-01-29 16:52:39 | cmunley1 | support returning tokenids in responses api (#33212) |
| 2026-01-29 16:52:11 | Ilya Markov | [BugFix] Fix EPLB fail for MoeFP4 model with Marlin backend (#33262) |
| 2026-01-29 16:52:03 | Didier Durand | [Doc]: fixing multiple typos in diverse files (#33256) |
| 2026-01-29 16:42:53 | wang.yuqi | [Bugfix] Fix Qwen3-VL-Reranker load. (#33298) |
| 2026-01-29 16:19:05 | Pengchao Wang | [CI/Build][BugFix] fix cuda/compat loading order issue in docker build (#33116) |
| 2026-01-29 15:35:51 | TJian | [Release] [ROCm] Remove old build step (#33316) |
| 2026-01-29 14:55:50 | Kiersten Stokes | [Misc][Build] Lazy load cv2 in nemotron_parse.py (#33189) |
| 2026-01-29 14:45:42 | TJian | [Release] [CI] Optim release pipeline (#33156) |
| 2026-01-29 13:57:09 | wangln19 | Fix tool call indexing double-counting (#33141) |
| 2026-01-29 13:55:17 | Cyrus Leung | [Refactor] Define MM data parser in processing info instead of processor itself (#33260) |
| 2026-01-29 13:28:46 | Angela Yi | [ez] Delete more torch version checks <= 2.8 (#33288) |
| 2026-01-29 12:24:20 | Or Ozeri | [Misc] Add orozery to CODEOWNERS (core, kv_transfer, kv_offload) (#33227) |
| 2026-01-29 10:40:59 | Michael Goin | [Bugfix] Register fp8 cutlass_group_gemm as supported for only SM90+SM100 (#33285) |
| 2026-01-29 08:09:30 | Michael Goin | [UX] Remove noisy CT UnquantizedLinearMethod warn (#33273) |
| 2026-01-29 06:41:23 | Nick Hill | [ModelRunner V2] Misc code simplification and cleanup (#33266) |
| 2026-01-29 06:20:22 | Matthew Bonanni | [7/N][Attention][Docs] Add documentation for attention backends (#32477) |
| 2026-01-29 05:54:25 | Michael Goin | [UX] Enable nested configs in config yaml files (#33193) |
| 2026-01-29 05:25:07 | Gregory Shtrasberg | [Bugfix] Add missing encoder only guard for do_kv_cache_update (#33269) |
| 2026-01-29 05:03:56 | Angela Yi | [ez] Remove checks for torch version <= 2.8 (#33209) |
| 2026-01-29 04:47:47 | Rohan Potdar | Use aiter triton fused_add_rmsnorm_pad for gpt-oss (#30976) |
| 2026-01-29 04:30:32 | Wentao Ye | [Feature] Fully support for async scheduling + PP, 30.8% E2E throughput improvement, 31.8% TPOT improvement (#32618) |
| 2026-01-29 03:14:29 | Kevin H. Luu | [CI] Change GPU key to device key for B200 test (#33275) |
| 2026-01-29 02:15:24 | Wentao Ye | [Perf] Optimize `moe_permute` for CUTLASS FP8 (#32892) |
| 2026-01-29 01:36:56 | Nicolò Lucchesi | [CI] Whisper tests `enforce_eager=False` (#33098) |
| 2026-01-29 00:22:45 | cwazai | [lora/moe] Avoid extra intermediate buffer & Python slicing in expand phase when split_k == 1 (#32774) |
| 2026-01-29 00:03:07 | Bin Bao | [Benchmark] Add startup benchmarking to buildkite run (#33183) |

### 📊 统计摘要
> 本日共 45 个提交 | 🔴高 3 | 🟡中 20 | 🟢低 22
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[Models] Qwen3-ASR (#33312)](#8b3f0a9)
    - [[Misc] Cleanup Kimi-K2.5's vision chunk modality entrypoi...](#3a92c6f)
    - [[Refactor] Define MM data parser in processing info inste...](#5155017)
  - [🟡 中重要度变更 (20)](#-🟡-中重要度变更-20)
    - [[Multimodal] Simplify MM input definitions (#33331)](#c6e7404)
    - [[Intel GPU] refine xpu worker (#32894)](#8bb6271)
    - [[Bugfix][CPU] Fix thread num for shared memory communicat...](#8311f08)
    - [[Voxtral] Streaming example (#33042)](#40c3503)
    - [[Bug Fix] Handle variable-length tensors in MultiModalFla...](#615e803)
    - [[Chore] Remove `use_data_parallel` kwargs from ViT implem...](#5400014)
    - [Make `mypy` opt-out instead of opt-in (#33205)](#fb946a7)
    - [support returning tokenids in responses api (#33212)](#3bba2ed)
    - [[Doc]: fixing multiple typos in diverse files (#33256)](#31b25f6)
    - [[Bugfix] Fix Qwen3-VL-Reranker load. (#33298)](#abb34ac)
    - [[Release] [CI] Optim release pipeline (#33156)](#f9d0359)
    - [[ez] Delete more torch version checks <= 2.8 (#33288)](#07ea184)
    - [[ModelRunner V2] Misc code simplification and cleanup (#3...](#6bf3b46)
    - [[7/N][Attention][Docs] Add documentation for attention ba...](#77c4f45)
    - [[UX] Enable nested configs in config yaml files (#33193)](#ca19691)
    - [[ez] Remove checks for torch version <= 2.8 (#33209)](#4197168)
    - [Use aiter triton fused_add_rmsnorm_pad for gpt-oss (#30976)](#59bcc5b)
    - [[Feature] Fully support for async scheduling + PP, 30.8% ...](#3e44078)
    - [[Perf] Optimize `moe_permute` for CUTLASS FP8 (#32892)](#c4e744d)
    - [[Benchmark] Add startup benchmarking to buildkite run (#3...](#392c5af)
  - [🟢 低重要度变更 (22)](#-🟢-低重要度变更-22)
    - [[Bugfix] Fix broken GLM-OCR initialization (#33350)](#5e73e49)
    - [[Backport] [Kimi-K2.5] Replace torch.cuda with current_pl...](#17b17c0)
    - [[Quantization][Refactor]  use platform dict to choose ker...](#a5aa4d5)
    - [[BugFix] Async Eplb fix potential race condition (#32881)](#d09135f)
    - [[fix] tesdt mcp_tool_calling_streaming with a more comple...](#8688c3d)
    - [Bugfix: Pass router logits dtype in nemotron shared exper...](#e01ff5c)
    - [[Misc] Remove missed `pad_for_cudagraph` (#33283)](#a650ad1)
    - [[Doc] Update outdated link to Ray documentation (#32660)](#d697581)
    - [Adding optional speculator tests for larger models (#32943)](#5eeba80)
    - [[PluggableLayer][2/N] Apply PluggableLayer to linear laye...](#08b1195)
    - [[BugFix] Fix EPLB fail for MoeFP4 model with Marlin backe...](#53fc166)
    - [[CI/Build][BugFix] fix cuda/compat loading order issue in...](#2515bbd)
    - [[Release] [ROCm] Remove old build step (#33316)](#c487a8e)
    - [[Misc][Build] Lazy load cv2 in nemotron_parse.py (#33189)](#9e138cb)
    - [Fix tool call indexing double-counting (#33141)](#39037d2)
    - [[Misc] Add orozery to CODEOWNERS (core, kv_transfer, kv_o...](#a663b21)
    - [[Bugfix] Register fp8 cutlass_group_gemm as supported for...](#1bd47d6)
    - [[UX] Remove noisy CT UnquantizedLinearMethod warn (#33273)](#141cd43)
    - [[Bugfix] Add missing encoder only guard for do_kv_cache_u...](#ab597c8)
    - [[CI] Change GPU key to device key for B200 test (#33275)](#8bdd397)
    - [[CI] Whisper tests `enforce_eager=False` (#33098)](#8ebf372)
    - [[lora/moe] Avoid extra intermediate buffer & Python slici...](#f210f0b)
#### 🔴 高重要度变更 (3)

### [Models] Qwen3-ASR (#33312)
**SHA**: `8b3f0a9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8b3f0a99dd501447e3f48ef675e51590e993a616)

**🎯 变更类型**：功能增强（新增 Qwen3‑ASR 语音识别模型）  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 在 vLLM 中加入对 Qwen3‑ASR 系列模型的完整支持：包括模型配置、权重映射、前向实现、音频特征抽取与多模态处理。  
- 更新模型注册表、文档、示例脚本以及测试用例，使用户能够像使用其它 LLM 一样在离线推理时直接调用 `Qwen3ASRForConditionalGeneration` 完成语音转文字。  

**🎯 影响范围**  
- **模型执行层**：`vllm/model_executor/models/qwen3_asr.py`（新模型实现）。  
- **配置系统**：`transformers_utils/configs/qwen3_asr.py`、`transformers_utils/config.py`、`registry.py`。  
- **多模态处理**：`multimodal` 框架新增 `Qwen3ASRMultiModalProcessor`、`Qwen3ASRDummyInputsBuilder`。  
- **示例/测试**：`examples/offline_inference/audio_language.py`、`tests/models/registry.py`、文档 `supported_models.md`。  

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | - 将 ASR 视作 **音频‑文本** 双模态模型，复用了 Qwen3‑Omni 的音频塔 (`Qwen3OmniMoeAudioEncoder`) 与文本 transformer (`Qwen3ForCausalLM`)。<br>- 新增 `SupportsTranscription` 接口，统一了 VLLM 对语音任务的 API（与 Whisper、GraniteSpeech 等保持一致）。<br>- 通过 `MULTIMODAL_REGISTRY` 完成处理器、dummy 输入的自动发现，保持框架的插件化特性。 |
| **性能影响** | - 推理阶段需要额外执行音频编码塔（卷积+Transformer），相较纯文本模型会增加 **CPU/GPU 计算量** 与 **显存占用**（音频特征长度随采样率和时长线性增长）。<br>- `get_mrope_input_positions` 为音频特征生成 RoPE 位置信息，复杂度与音频帧数成正比，极端长音频（>30 s）可能触发位置 ID 超出 `max_position_embeddings`。<br>- 已在 `examples/offline_inference/audio_language.py` 中设置 `max_num_seqs=5`、`limit_mm_per_prompt={"audio": audio_count}`，对并发数和每个请求的音频数量做了保护，避免 OOM。 |
| **安全考虑** | - 代码未引入外部执行，仅处理 **NumPy / Torch** 张量，安全风险低。<br>- 接收原始音频（`np.ndarray`）时未做大小或格式校验，攻击者可发送超大或恶意构造的数组导致 **内存泄露 / DoS**。<br>- `Qwen3ASRProcessor.replace_multimodal_special_tokens` 中对 `regex` 替换使用了用户可控的 `audio_token`，若 tokenizer 被篡改可能产生意外的字符串替换。 |
| **可维护性** | - 新增文件较多，遵循项目已有的 **Config → Model → Processor → Registry** 流程，易于对齐。<br>- 大量硬编码（如音频 PAD token `<|audio_pad|>`、默认采样率 16000、特征长度公式）散落在多处，后续若模型改动需同步更新。<br>- 权重映射 `hf_to_vllm_mapper` 只重写前缀，若 HF 端增加新前缀可能未被覆盖。 |

**⚠️ 潜在风险**  
1. **显存/算力激增**：音频塔在高采样率或长音频下可能导致显存 OOM，尤其在多序列并发 (`max_num_seqs`) 较大时。  
2. **位置编码溢出**：`_get_feat_extract_output_lengths` 计算的音频输出长度若超过模型 `max_position_embeddings`，会在 `get_mrope_input_positions` 抛异常。  
3. **权重加载不完整**：`AutoWeightsLoader` 跳过 `talker.`、`code2wav.` 前缀，若未来模型在这两个子模块中添加参数，权重将被遗漏。  
4. **输入校验不足**：缺少对 audio ndarray 长度、 dtype、取值范围的检查，可能导致非法张量进入音频塔，引发数值异常或崩溃。  
5. **兼容性冲突**：`registry.py` 中新增条目与已有 `Qwen3OmniMoeThinkerForConditionalGeneration` 共用同一子模块 `qwen3_asr`，若后续出现模块名冲突会导致 import 错误。  

**💡 关注建议**  
- **资源限制**：在 EngineArgs 中加入 `max_audio_len_secs` 或 `max_audio_features` 参数，防止单请求占用过多显存；对 `audio_count` 做上限校验。  
- **位置编码防护**：在 `get_mrope_input_positions` 前提前检查 `audio_len` 是否超出 `self.config.max_position_embeddings`，若超出则截断或返回明确错误。  
- **输入校验**：在 `Qwen3ASRProcessor.__call__` 增加 `np.asarray(audio).dtype == np.float32`、`audio.shape[0] <= max_samples` 检查；对异常音频抛出友好异常。  
- **单元/集成测试**：补充极端音频长度（如 0 s、60 s）以及非法 dtype、非连续内存的测试，确保异常路径不导致内存泄漏。  
- **文档同步**：在模型文档中明确列出 **显存需求**、**采样率要求**、**最大支持时长**，并在 `examples` 中给出 `limit_mm_per_prompt` 的推荐设置。  
- **权重映射审计**：定期比对最新 HF `qwen3-asr` 的 checkpoint 结构，确保 `hf_to_vllm_mapper` 完全覆盖；必要时在 `load_weights` 中加入日志提示未匹配的前缀。  
- **安全审计**：对 `replace_multimodal_special_tokens` 的正则表达式使用 `re.escape` 已够安全，仍建议在 tokenizer 加载阶段校验 `audio_token` 是否唯一，避免误替换。  

总体来看，此次变更为 vLLM 引入了完整的 Qwen3‑ASR 支持，扩展了平台的语音识别能力。只要在上线前做好资源限制、输入校验以及对应的回归测试，即可将风险控制在可接受范围。

---

### [Misc] Cleanup Kimi-K2.5's vision chunk modality entrypoints (#33157)
**SHA**: `3a92c6f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3a92c6f3b5f010453d81f871f642839c15402cda)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 为 Kimi‑K2.5 模型新增 **vision‑chunk**（统一图像/视频块）处理流程：在 `chat_utils` 中拆分、解析、组装视频块的逻辑被迁移至 `hf.py` 渲染层，并提供统一的 `replace_vision_chunk_video_placeholder`、`build_video_prompts_from_mm_data`、`rebuild_mm_uuids_from_mm_data` 辅助函数。  
2. 增加大量针对 vision‑chunk 的单元/异步测试，覆盖 **image、video、mixed、带 UUID** 四种情况。  
3. 在模型注册表与离线推理测试中对 Kimi‑K2.5 加入 **skip** 标记，防止 CI 因尚未完成的离线实现爆炸。  
4. 移除 `IdentityVideoLoader`（仅供 Kimi‑K2.5 暂时使用），恢复默认 `opencv` 加载路径。  
5. 将部分公共函数从 `chat_utils` 移动到渲染层 `hf.py`，并使用 `TYPE_CHECKING` 进行懒加载，提升模块解耦度。  

---

### 🎯 影响范围
| 受影响模块/组件 | 说明 |
|----------------|------|
| `vllm/entrypoints/chat_utils.py` | 视图块解析逻辑被精简，核心功能迁移至渲染层。 |
| `vllm/renderers/hf.py` | 新增统一 vision‑chunk 处理、占位符替换、UUID 重建函数；`HfRenderer` 新增 `use_unified_vision_chunk` 配置读取。 |
| `vllm/multimodal/video.py` | 删除 `IdentityVideoLoader`（Kimi‑K2.5 临时实现），恢复仅 `opencv` 实现。 |
| 测试目录 `tests/entrypoints/…`、`tests/models/multimodal/…` | 大量新增/修改针对 vision‑chunk 的单元测试与 CI 跳过逻辑。 |
| `vllm/models/registry.py` | 解除对 Kimi‑K2.5 “不可在线”的硬限制（`is_available_online=False` 被删除）。 |
| 相关文档/配置 (`hf_config.use_unified_vision_chunk`) | 仅 Kimi‑K2.5 会打开此标记，其他模型默认关闭。 |

---

### 🔍 技术洞察

#### 架构影响
- **职责划分更清晰**：原本在 `chat_utils` 同时负责 **解析→处理→渲染** 的代码被拆分，解析阶段仍在 `chat_utils`，但 **vision‑chunk 后处理**（UUID 重建、视频占位符替换）转移至渲染层 `hf.py`。这降低了 `chat_utils` 的耦合度，使其专注于“输入 → 多模态数据结构化”，而渲染层负责“多模态数据 → Prompt”。  
- **统一 Vision‑Chunk 接口**：通过 `use_unified_vision_chunk` 标记，渲染层能够在不改动模型原始 `hf_config` 的情况下激活统一处理路径，对其他模型保持向后兼容。  
- **扩展性提升**：新增的 `replace_vision_chunk_video_placeholder` 使用 `itertools.chain.from_iterable` 实现占位符与多段 Prompt 的动态拼接，便于以后支持 **多视频、分段拼接** 等更复杂的场景。  

#### 性能影响
- **CPU/IO 负担**：`IdentityVideoLoader` 被移除，统一走 `opencv`，意味着 **视频解码仍在后端**（VideoLoader）完成，避免了在模型内部重复解码的潜在开销。  
- **占位符替换**：采用一次性 `split` + `chain` 操作，时间复杂度 O(N)（N 为占位符出现次数），对常规对话几乎不产生可感知的性能损耗。  
- **额外内存**：`build_video_prompts_from_mm_data` 会临时创建 `defaultdict(list)`，内存占用与视频块数量线性相关，通常在 10‑20 个块以内，影响可忽略。  

#### 安全考虑
- **无新增外部依赖**：代码仅使用标准库与已有 `vllm` 包，未引入网络或文件系统访问。  
- **UUID 处理**：在 `rebuild_mm_uuids_from_mm_data` 中仅提取已在 `mm_data` 中生成的 UUID，未进行任何外部验证，保持与原有实现一致。  
- **模型配置开关**：`use_unified_vision_chunk` 默认 `False`，防止误开启导致未充分测试的路径被生产环境使用。  

#### 可维护性
- **函数抽取**：`_resolve_vision_chunk_items` 将对图片/视频块的细粒度处理抽离，代码可读性提升，并且在 `chat_utils` 中仅保留调用入口。  
- **类型提示**：对 `mm_data` 与 `mm_uuids` 使用 `TYPE_CHECKING` 进行前向声明，避免在运行时循环依赖，提高 IDE 自动补全与静态检查的准确性。  
- **测试覆盖**：新增 30+ 条针对 vision‑chunk 的同步/异步单元测试，覆盖 **空 UUID、带 UUID、混合顺序、异常跳过** 四大场景，大幅提升该功能的回归安全网。  

---

### ⚠️ 潜在风险

| 风险点 | 可能后果 | 缓解措施 |
|--------|----------|----------|
| **Kimi‑K2.5 离线推理仍未实现**（CI 中 `pytest.skip`）| 生产环境如果误开启 `use_unified_vision_chunk`，会触发未实现的代码路径导致运行时错误。 | 确保部署脚本或模型配置文件显式将 `use_unified_vision_chunk` 设为 `False`，或在模型注册阶段加入硬性校验。 |
| **视频块分割 (`split_video_chunks`) 依赖自定义实现**| 若自定义 `VideoLoader` 未实现 `split_video_chunks`，会回退到原始 data，导致占位符未被替换，提示不完整。| 在 `BaseMultiModalProcessor` 或对应的模型实现中加入明确的 `NotImplementedError`，并在渲染层捕获警告，确保用户可见。 |
| **占位符数量与视频块数量不匹配**| `replace_vision_chunk_video_placeholder` 只记录警告，仍返回原始 Prompt，可能导致模型生成错误或上下文缺失。| 在关键业务场景（如对话机器人）增加检测：若警告次数 > 0，抛出异常或返回错误码。 |
| **删除 `IdentityVideoLoader` 可能影响其他依赖**| 若项目的其他分支或插件仍显式指定 `loader="identity"`，会触发 `KeyError`。| 在 `VIDEO_LOADER_REGISTRY` 添加兼容性别名或在文档中明确废弃该 loader。 |
| **`_resolve_vision_chunk_items` 中对 `data.media` 的假设**| 不同 `BaseMultimodalItem` 实现可能不含 `media` 属性，导致属性错误。| 在函数首部加入 `hasattr(data, "media")` 检查，若不存在则使用通用 `data`，并在日志中记录调试信息。 |

---

### 💡 关注建议

1. **CI / 发布流程**  
   - 将 `use_unified_vision_chunk` 的默认为 `False` 设为 **必填**（通过 schema），并在 CI 中加入检测：当模型 ID 为 `moonshotai/Kimi-K2.5` 时确保该字段为 `True` **且**对应测试不被跳过。  
   - 为 `IdentityVideoLoader` 添加 **DeprecationWarning**，防止未来误用。

2. **文档与使用指南**  
   - 在模型配置文档中补充 `use_unified_vision_chunk`、`video_placeholder`、`vision_chunk` 的完整说明，示例展示如何在 **Chat API** 中提供 `image_url`/`video_url` 

---

### [Refactor] Define MM data parser in processing info instead of processor itself (#33260)
**SHA**: `5155017` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/51550179fc04871f58427dc325748434cc363849)

**🎯 变更类型**：重构 / 架构变更  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 将多模态数据解析器（`MultiModalDataParser`）的创建从各个 **Processor** 中抽离，统一交由对应的 **ProcessingInfo** 负责，实现 `info.get_data_parser()`。  
- 在 `BaseProcessingInfo` 中新增 `_get_expected_hidden_size()` 与 `get_data_parser()`，统一提供 `expected_hidden_size` 参数，用于 **mm_embeds** 场景的嵌入维度校验。  
- 为大量模型（AudioFlamingo3、Ernie4.5VL、FunAudioChat、Gemma3n、GlmAsr、GraniteSpeech、HunYuan、Keye、MiniCPM、Molmo2、NanoNemotron、Phi4、Qwen2‑Omni、Qwen2‑Audio、Terratorch、Ultravox、Voxtral、Whisper 等）补充或迁移相应的 `get_data_parser` 实现，删除旧的 `_get_data_parser` 重写。  
- 更新了若干单元测试以配合新的信息获取方式。  

**🎯 影响范围**  
- 核心多模态解析模块：`vllm/multimodal/processing/context.py`、`vllm/multimodal/processing/processor.py`。  
- 所有模型实现目录下的 `*_processing_info.py` 与对应的 `*_processor.py`（约 30+ 个文件）。  
- 受影响的单元测试文件：`tests/model_executor/test_qwen3_omni.py`。  

**🔍 技术洞察**  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | - **职责划分更清晰**：`ProcessingInfo` 负责提供模型相关的配置（HF Config、Processor、FeatureExtractor、DataParser），`Processor` 只关注将已解析好的 multimodal 数据送入 HF Processor。<br>- **去除重复实现**：大量模型中原本重复实现的 `_get_data_parser` 被统一到基类，减少代码维护成本。<br>- **向后兼容层**：在 `BaseMultiModalProcessor.__init__` 中保留对已废弃 `_get_data_parser` 的兼容警告，确保旧实现仍可运行。 |
| **性能影响** | - 解析器实例化从每个 Processor 实例化时迁移到 ProcessingInfo 初始化阶段，理论上 **无额外运行时开销**（实例化一次即被复用）。<br>- 移除重复 `_get_data_parser` 调用，略微降低实例化次数，提升启动/模型加载阶段的效率。 |
| **安全考虑** | - 引入 `_get_expected_hidden_size()` 与 `expected_hidden_size` 参数，在 `MultiModalDataParser` 中对 **mm_embeds** 的维度进行校验，防止恶意或错误的嵌入张量导致的 **内存泄漏 / 崩溃**（原先缺少此校验的漏洞已被修复）。<br>- 统一的解析入口降低了因自定义子类忘记实现隐藏维度校验而产生的安全风险。 |
| **可维护性** | - 新增的 `get_data_parser` 统一接口便于未来在 **ProcessingInfo** 层面加入更多通用参数（如自定义采样率、通道数、视频元数据需求等）。<br>- 代码量明显下降（原有 300+ 行的 `_get_data_parser` 实现被删除），阅读与审查更简洁。 |
| **兼容性** | - 对已经实现自己的 `_get_data_parser` 的模型仍能工作（仅发出一次 `logger.warning_once`），但在 **v0.16** 之后将被强制迁移。<br>- 外部插件若直接继承 `BaseMultiModalProcessor` 并重写 `_get_data_parser`，需要同步改为在自定义 `ProcessingInfo` 中实现 `get_data_parser`。 |

**⚠️ 潜在风险**  
1. **遗漏实现**：如果某个模型的 `ProcessingInfo` 未显式覆盖 `get_data_parser`，将使用基类默认（仅 `expected_hidden_size`），可能导致该模型缺少特定字段（如 `audio_embeds`、自定义图像字段）而在运行时抛出 **KeyError**。  
2. **向后兼容警告噪声**：大量模型仍保留 `_get_data_parser`，在启动时会产生大量 **warning**，如果项目对日志过滤不当，可能掩盖其他重要信息。  
3. **`expected_hidden_size` 为 `None` 时的潜在空指针**：部分自定义 `MultiModalDataParser` 子类内部可能直接使用 `expected_hidden_size` 而未做 `None` 检查，需确保新基类实现的 `None` 语义被正确处理。  
4. **缓存/复用问题**：`data_parser` 现在由 `ProcessingInfo` 提供，若同一 `ProcessingInfo` 实例被多次复用（如在多线程共享场景），需要确认 `MultiModalDataParser` 本身是 **无状态** 或已做好线程安全。  

**💡 关注建议**  
- **全链路测试**：在合并后立即运行完整的模型测试矩阵（`pytest -m multimodal`），确认所有模型的 `get_data_parser` 已被适配。  
- **审查自定义插件**：若项目或第三方库中有自定义 `BaseMultiModalProcessor` 子类，务必迁移到在对应的 `ProcessingInfo` 实现 `get_data_parser`，并去除 `_get_data_parser`。  
- **日志治理**：对 `logger.warning_once` 的输出做统一收敛，确保在正式发布前已清除所有已废弃路径的警告。  
- **文档更新**：在 `vllm` 文档或开发者指南中加入 “在 ProcessingInfo 中实现 `get_data_parser`” 的最佳实践说明，避免新贡献者继续使用旧式方式。  
- **安全回归**：针对 `mm_embeds` 开启的模型，加入单元测试验证 **embedding 维度不匹配** 时能抛出友好错误，防止潜在的 DoS 场景。  
- **监控废弃迁移**：在 v0.16 之前设立 CI 检查，确保所有仓库代码已不再出现 `_get_data_parser` 方法的实现。  

---  

---

#### 🟡 中重要度变更 (20)

### [Multimodal] Simplify MM input definitions (#33331)
**SHA**: `c6e7404` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c6e7404cc5713a926e8b6c187b5f197a5436e9ff)

**变更概览**  
本次 PR 通过 “去除 modality / key 在 `MultiModalFieldElem` 中的显式存储”、简化 `MultiModalKwargsItem/Items` 的构造方式、以及统一 `group_mm_kwargs_by_modality` 的输入签名，实现了对多模态输入描述的统一抽象。核心改动集中在 `vllm/multimodal/inputs.py`、`utils.py`、`cache.py`、以及所有依赖旧接口的 test/worker/serial_utils 等文件。

**受影响模块**  
- `vllm/multimodal/inputs.py`（`MultiModalFieldElem`、`BaseMultiModalField`、`MultiModalKwargsItem`、`MultiModalKwargsItems`）  
- `vllm/multimodal/utils.py`（`group_mm_kwargs_by_modality`）  
- `vllm/multimodal/cache.py`（P0 cache 暂存结构）  
- V1 代码路径：`vllm/v1/worker/gpu/*`、`vllm/v1/serial_utils.py`、`vllm/v1/core/*`  
- 测试套件全局更新。

**关键影响**  
1. **API 向后不兼容**  
   - `MultiModalFieldElem` 不再公开 `modality` 与 `key`，外部若依赖 `elem.modality`、`elem.key` 将抛 `AttributeError`。  
   - `MultiModalKwargsItem.from_elems` 被删除，构造只能使用 `MultiModalKwargsItem({key: elem, ...})`；`dummy` 仅接受 `nbytes` 参数。  
   - `group_mm_kwargs_by_modality` 现在要求 `(modality, item)` 元组列表，调用方需同步改写。  

2. **内部缓存结构调整**  
   - `_p0_cache` 去掉了 `modality` 信息，`address_as_item` 生成的 `MultiModalKwargsItem` 只包含 `address`、`monotonic_id` 两个字段，且不再携带 `modality`。这不会影响现有缓存逻辑，但需确认其他模块（如 LoRA 相关）是否仍假设 `modality` 存在。  

3. **序列化/反序列化**  
   - `v1/serial_utils` 的 `_encode_mm_item/_decode_mm_item` 从列表改为 dict，序列化格式变化（测试中已相应调整）。若用户自行实现 `msgspec` 编码器，需要同步修改。  

4. **测试覆盖**  
   - 所有涉及多模态的单元测试均已迁移到新 API，说明功能在内部已经通过。但仍建议在 CI 中保留一次完整的集成测试（包括 HF‑compatible 处理器、LoRA、Streaming 等）以捕获潜在遗漏。  

**建议**  
- 在 `vllm/multimodal/inputs.py` 中加入 **deprecation warnings**（例如 `__getattr__` 捕获 `modality/key` 并提示），帮助第三方库平滑过渡。  
- 更新项目文档（README、API Reference）中的 `MultiModalFieldElem`、`MultiModalKwargsItem` 示例。  
- 在 `vllm/multimodal/cache.py` 添加注释说明 `_p0_cache` 现在只存 `prompt_updates`，防止 future 维护者误以为仍需 `modality`。  
- 为向后兼容提供一个轻量包装层（如 `LegacyMultiModalKwargsItem`）供旧代码临时使用。  
- 完整运行 `pytest -m "multimodal"` 并在 CI 中加入对旧 API 的兼容性检查，以避免意外破坏。  

总体而言，本次改动使多模态输入的内部表示更加简洁统一，但涉及的 API 更改较广，需要在文档、兼容层和 CI 上做好支撑。

---

### [Intel GPU] refine xpu worker (#32894)
**SHA**: `8bb6271` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8bb6271c77520c8df3bd7d17899c50225bc42e0a)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. `vllm/platforms/__init__.py`：在检测 XPU 平台时，先确认是否启用了 XCCL 后端；若未启用则记录 warning 并保持 `is_xpu` 判定不变。  
2. `vllm/v1/worker/xpu_worker.py`：彻底重构 XPU‑worker 的初始化逻辑。去掉原有的 `determine_available_memory`、手工估算显存的实现，改为使用 `MemorySnapshot`、`request_memory` 统一做显存快照与需求计算；在 `init_device` 中加入 `torch.xpu.empty_cache()`、工作区管理 (`init_workspace_manager`) 以及使用统计上报 (`report_usage_stats`)。

**🎯 影响范围**  
- **平台检测**：`vllm/platforms`（XPU 插件入口）。  
- **XPU 工作线程**：`vllm/v1/worker/xpu_worker.py`、相关的 `xpu_model_runner.py`、`utils/mem_utils.py`、`utils/torch_utils.py`、`v1/worker/workspace.py`、`v1/utils.py`。  
- **分布式后端**：间接影响 `torch.distributed` 初始化（XCCL/CCL 选择）。  

**💡 关注建议**  

1. **显存计算准确性**  
   - 新的显存需求由 `MemorySnapshot`+`request_memory` 完成，确保 `vllm.utils.mem_utils` 在所有受支持的 XPU 环境下能够正确读取 `torch.xpu.mem_get_info()`。  
   - 对比旧版 `determine_available_memory` 的输出，特别是使用非数据中心 GPU（需要加上 128 MiB 的 “non‑torch” 预留），防止 KV‑cache 过度分配导致 OOM。  

2. **后端兼容性**  
   - 现在如果构建的 PyTorch 未编译 XCCL，会在平台插件阶段仅打印 warning，而不自动回退到 CCL。若用户依赖 CCL，需要自行在环境中显式安装 `oneccl_bindings_for_pytorch`。建议在发布说明中明确该行为。  

3. **工作区管理**  
   - `init_workspace_manager` 被加入初始化，默认使用 2 个 ubatch（开启 DBO 时）或 1 个。确认 `vllm/v1/worker/workspace.py` 能在多进程/多卡场景下正确创建和回收缓冲区，避免泄露。  

4. **使用统计**  
   - `report_usage_stats` 只在 rank 0 上调用，确保该函数在用户关闭 telemetry 时不会抛异常。可在 `vllm/v1/utils.py` 添加 guard：`if vllm_config.enable_usage_stats:`。  

5. **回归测试**  
   - 添加针对 Intel Xe‑GPU（Ponte Vecchio 等）和非数据中心 GPU 的显存快照单元测试。  
   - 验证分布式 XPU 环境下 (`torch.distributed` + XCCL) 能够成功执行 `all_reduce`，并在未启用 XCCL 时给出明确错误信息而不是 silent 失败。  

6. **文档更新**  
   - 更新 “平台支持” 与 “显存配置” 章节，说明新显存计算方式、XCCL 必要性以及如何手动安装 `oneccl_bindings_for_pytorch`。  

总体来看，本次提交为 Intel XPU 路径引入了更结构化的显存管理与后台初始化，提升了可维护性和可观测性。但同时也削减了旧的手工显存估算路径，需要在不同 XPU 环境下充分回归，以防出现意外的 OOM 或后端加载错误。  

---

### [Bugfix][CPU] Fix thread num for shared memory communication (#33317)
**SHA**: `8311f08` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8311f083bd53ac761556f2fb8733e0cf22ef7024)

**🎯 变更类型**：Bugfix（CPU 共享内存通信线程数修正）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- `SHMManager` 构造函数原本直接使用 `omp_get_max_threads()` 作为内部线程数，但在多进程/多节点环境下各进程的 OMP 上限可能不一致，导致共享内存通信线程数错配。  
- 新增 `thread_num` 参数，统一由 Python 层通过 `torch.ops._C.init_shm_manager` 传入，并在所有 rank 上通过 `all_reduce(MIN)` 取最小值，保证各进程使用相同且安全的线程数。  
- 相应的 C++ 接口、绑定以及 Python 调用全部进行参数扩展和适配。  

**🎯 影响范围**  
- **csrc/cpu/shm.cpp**：`SHMManager` 构造函数、`create_singleton_instance`、`init_shm_manager` 增加 `thread_num` 参数。  
- **csrc/cpu/torch_bindings.cpp**：对外导出函数签名修改。  
- **vllm/distributed/device_communicators/cpu_communicator.py**：在初始化 SHM 时收集并统一 `thread_num` 并传递给 C++。  

**💡 关注建议**  
1. **兼容性**：旧的二进制/库仍会调用旧签名，确保重新编译后对应的 Python 包全部更新。  
2. **线程数验证**：在不同机器、不同 OMP 环境下运行单元测试，确认 `torch.get_num_threads()` 与 `omp_get_max_threads()` 的取值一致，且 `all_reduce(MIN)` 能得到全局最小值。  
3. **性能回归**：若提升 `thread_num` 会显著加速通信，建议在多数节点上统一提高 OMP 线程上限；否则保持当前最小值即可避免 oversubscription。  
4. **错误排查**：如果出现共享内存创建失败或死锁，检查传入的 `thread_num` 是否为 0 或异常值（如负数），并确认 `torch.distributed.barrier` 已在同一 `device_group` 中完成。  

总体而言，此次改动通过显式传递并统一线程数，消除了因各进程 OMP 上限不一致导致的共享内存通信错误，是一次必要且风险可控的 bug 修复。

---

### [Voxtral] Streaming example (#33042)
**SHA**: `40c3503` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/40c35038d28891488f2a784cfeadd20fc259a215)

**🎯 变更类型**：功能增强（新增 Voxtral 流式示例）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `mistral_common` 依赖升级至 1.9.0。  
- 在 `tests/models/multimodal/generation/test_voxtral_streaming.py` 中加入完整的流式推理示例，使用 `AsyncLLM` 与自定义 `RealTimeAudioInput` 实现实时音频分片。  
- 新增 `async_engine` fixture、`audio_assets`、`tokenizer` 等辅助 fixture，重构原有同步调用逻辑。  

**🎯 影响范围**  
- **依赖管理**：所有 `requirements` 文件、`requirements/test.txt` 均升级至 `mistral-common==1.9.0`，可能触发 CI 环境重新拉取。  
- **测试**：新增两个测试函数（同步与异步），均被 `skip(reason="Voxtral streaming is not yet public")` 标记，暂不执行。  
- **代码路径**：涉及 `vllm.engine.arg_utils.AsyncEngineArgs`、`vllm.v1.engine.async_llm.AsyncLLM`、`vllm.inputs.data.TokensPrompt` 等异步入口。  
- **文档/示例**：提供了音频实时流式处理的参考实现。  

**💡 关注建议**  
1. **依赖兼容性**：确认 `mistral-common 1.9.0` 与已有的 `torch`、`transformers` 等版本无冲突，必要时在 CI 中加入兼容性验证。  
2. **测试稳定性**：虽然当前测试被 `skip`，但日后可能开启。建议为 `RealTimeAudioInput` 添加超时/异常处理，防止因音频文件或队列阻塞导致的死锁。  
3. **异步资源释放**：`AsyncLLM` 在测试结束后应显式 `await async_engine.shutdown()`，避免残留 GPU/进程。  
4. **文档同步**：在项目 README 或示例文档中加入对应的使用说明，标明需要 `mistral_common[image,audio]` 才能运行。  
5. **CI 更新**：若以后解锁该测试，确保 `pytest-asyncio` 与 `torch` 的 GPU 环境已准备好，以免出现 “no CUDA devices” 的错误。  

整体而言，此次改动为用户提供了 Voxtral 实时流式的参考实现，影响主要在依赖升级和新增异步测试框架，建议在正式打开测试前做好兼容性和资源清理的验证。

---

### [Bug Fix] Handle variable-length tensors in MultiModalFlatField batching (#31751)
**SHA**: `615e803` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/615e8033e567171b4af15024038b302f386d2ee4)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `MultiModalFlatField._reduce_data` 中加入对 **可变长度张量**（如不同时长的音频）批处理的专门路径，采用切片‑赋值的方式拼接，而不是原先只能在所有维度形状相同的情况下使用 `torch.concat`。同时新增 `test_variable_length_audio_batching` 用例验证该路径在实际推理时能够正确生成输出。  

**🎯 影响范围**  
- **核心模块**：`vllm/multimodal/inputs.py`（`_shape_before_after` 逻辑）  
- **测试套件**：`tests/models/multimodal/generation/test_ultravox.py`（新增变量长度音频批处理测试）  
- **运行时**：涉及多模态输入（audio、image 等）且 **每个请求的非拼接维度大小不一致** 时的批处理流程。  

**💡 关注建议**  

1. **兼容性检查**  
   - 目前只在 `enforce_eager=True` 环境下触发（因为新路径依赖 `torch.zeros` 与切片赋值），需确认在 `torch.compile` 或 `torch.fx` 路径下仍能正常工作，或在文档中明确限制。  
   - 确认 `pin_memory` 参数在 `torch.zeros` 中的传递与原 `torch.concat` 路径保持一致，防止在 CUDA‑pinned 场景下出现意外内存拷贝。  

2. **边界条件**  
   - 空 batch、单元素 batch、或某维度为 0（如全零音频）时的行为尚未显式测试，建议补充对应单元测试。  
   - 当非拼接维度大小差距极大时（如 1 s vs 30 s 音频），生成的输出张量会很大，可能导致显存峰值爆炸。可考虑在 `max_sizes` 计算前加入阈值或 fallback 到 padding。  

3. **性能评估**  
   - 切片‑赋值相比 padding 更省内存，但在大 batch/高并发场景下仍会产生大量复制。建议在 CI 中加入基准测试，比较 `torch.concat`‑pad 与新实现的时间/显存差异。  
   - `torch.zeros` 初始化可能产生额外的全零写入，若 batch 较小且拼接维度占比不大，可考虑使用 `torch.empty` + `copy_`，但需确保安全性。  

4. **文档与代码注释**  
   - 在 `inputs.py` 中当前注释已指向 Issue #31658，建议在 `MultiModalFlatField` 类的 docstring 中加入“可变长度张量批处理仅在 concat‑dim 之外尺寸不一致时使用”。  
   - 在 `test_variable_length_audio_batching` 中加入对 **输出张量形状** 的断言，确保拼接顺序与输入保持一致，防止后续改动破坏逻辑。  

5. **回归影响**  
   - 该改动仅在 `dim != 0` 且 **非同形** 批次进入路径时触发，理论上不影响已有的 `torch.concat`‑path（如视觉特征、文本 token）。仍建议在完整模型回归（包括 LLM + 视觉、音频）上跑一遍，以捕获潜在的 dtype/device 隐式转换错误。  

总体而言，此次修复填补了多模态批处理的关键盲点，代码实现简洁且易于维护。若按以上建议补齐边界测试并进行显存/速度基准，能够进一步提升稳健性和生产可用性。

---

### [Chore] Remove `use_data_parallel` kwargs from ViT implementation  (#33310)
**SHA**: `5400014` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5400014d55b4cb388fdf7dbaab6c90d2161e2764)

**🎯 变更类型**：重构 / 其他  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：此次提交统一了 Vision Transformer（ViT）系列模型对 Data‑Parallel 开关的获取方式。原先各模型通过构造函数的 `use_data_parallel` 参数控制是否开启数据并行，现改为统一调用 `vision.is_vit_use_data_parallel()`，并在所有相关类的 `__init__` 中去掉该参数的文档/签名。相应地，`run_dp_sharded_vision_model`、`run_dp_sharded_mrope_vision_model` 等入口也改为使用该函数。

**🎯 影响范围**  
- `vllm/model_executor/models/*_vision*`：Idefics2、InternViT、Llama4、Step3、Step、MiniCPM‑V、KimiVL、Lfm2VL、MLLama4 等多个多模态模型。  
- `vllm/model_executor/models/vision.py`（新增 `is_vit_use_data_parallel`）以及相关 sharding 实现。  
- 所有调用这些模型的上层包装（如 `*_for_conditional_generation`）的构造函数签名已被修改。

**💡 关注建议**  
1. **兼容性**：外部代码若仍显式传入 `use_data_parallel`（如自定义子类或旧脚本），会因参数不存在而报错。建议在项目文档或迁移指南中明确该参数已移除，并提供示例改写方式。  
2. **功能验证**：`is_vit_use_data_parallel` 的实现依赖环境变量或配置，需要确保在多卡、TP（tensor‑parallel）以及单卡场景下返回值一致，否则可能导致 TP/DP 切换异常。建议新增单元测试覆盖：① 环境变量开启时返回 True，② 关闭时返回 False，③ 在不支持 DP 的模型（如 head 数不被 tp 整除）仍能正确回退。  
3. **代码清理**：大量 `use_data_parallel` 参数仍残留在注释或 `super().__init__` 调用中，已删除但未完全清理的 `self.use_data_parallel` 成员变量仍在少数类里保留（如 `Llama4VisionModel` 的 `self.use_data_parallel`），可进一步统一。  
4. **性能回归**：DP/TP 切换逻辑从各层分散到统一函数后，可能改变 `tp_size` 的计算时机，建议在多机多卡基准上跑一次完整的推理/批处理基准，确认 sharding 行为与之前保持一致。  
5. **文档更新**：Vision‑Transformer 相关的配置文档、示例脚本（尤其是 `vllm.entrypoints` 的 `--use-data-parallel` 等 CLI 参数）需要同步删除或改为使用新的全局开关。  

总体来看，此次改动提升了代码可维护性，避免了在众多模型构造函数中重复传播同一开关。但需注意向后兼容与功能验证，确保在不同并行配置下模型仍能正常加载与运行。适当补充测试与文档即可平滑过渡。

---

### Make `mypy` opt-out instead of opt-in (#33205)
**SHA**: `fb946a7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fb946a7f893da21808a016c86973bdeb2b3c6c95)

**🎯 变更类型**：重构 / 类型检查增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `tools/pre_commit/mypy.py` 从“显式列出需要检查的文件”改为“默认全库检查”，仅在 `SEPARATE_GROUPS` 中单独分组，去掉了 `FILES` 常量。  
- 为通过全库 `mypy`，在多个核心模块（如 `_aiter_ops.py`、`config/compilation.py`、`forward_context.py`、`logger.py`、`logprobs.py`、`profiler/utils.py` 等）补齐或修正了类型注解、容器声明及 `dataclass` 参数。  
- 新增若干目录到 `SEPARATE_GROUPS`（如 `benchmarks、config、profiler` 等），作为临时“skip”列表，待后续消除 `mypy` 错误后可移除。

**🎯 影响范围**  
- **工具链**：`tools/pre_commit/mypy.py`（预提交钩子）逻辑全局更改。  
- **核心库**：`vllm/_aiter_ops.py`、`vllm/config/compilation.py`、`vllm/config/utils.py`、`vllm/forward_context.py`、`vllm/logger.py`、`vllm/logprobs.py`、`vllm/profiler/utils.py`、`vllm/v1/kv_cache_interface.py` 等文件的类型声明。  
- **运行时**：对行为基本无影响，但个别变量的默认值类型（如 `max_cudagraph_capture_size` 从 `int | None` 改为 `int`）若在运行时仍依赖 `None`，需确认代码路径已做兼容处理。  

**💡 关注建议**  
1. **本地验证**：在本地运行 `pre-commit run mypy -a`，确保所有文件均通过，否则 CI 将报错。  
2. **运行时兼容**：检查 `max_cudagraph_capture_size`、`slot_mapping`、`logging_config` 等新注解的默认值是否在代码中被显式检查为 `None`，必要时添加 `cast` 或 `if … is not None` 防御。  
3. **逐步清理**：`SEPARATE_GROUPS` 中的临时目录列在注释里标记为 “TODO”，后续应逐步修复对应模块的 `mypy` 错误并移除这些条目，以实现真正的全库类型检查。  
4. **文档同步**：更新贡献指南，说明 `mypy` 已默认为 **opt‑out**（即全库检查），如果新加入文件需避免检查，请在 `SEPARATE_GROUPS` 中添加相应路径并在 PR 中说明原因。  
5. **回归测试**：运行完整单元/集成测试，确认上述类型改动未引入运行时异常（尤其是 `dict[str, Any]` 相关的初始化代码）。  

总体来看，此次提交把 `mypy` 从显式列文件的 **opt‑in** 模式切换为默认全库检查，提升了类型安全性，但也带来了大量注解修正，需要开发者在后续迭代中持续清理临时跳过的目录。仅要确保类型改动不破坏现有运行时逻辑，整体风险可控。

---

### support returning tokenids in responses api (#33212)
**SHA**: `3bba2ed` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3bba2edb0ffb9c15daa73c56289c2e12448249d8)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 OpenAI 接口的 `logprobs` 与 `top_logprobs` 增加 `return_tokens_as_token_ids` 开关，使返回的 token 可以直接以 `token_id:<id>` 形式输出，而非解码后的文本。实现方式是统一通过 `OpenAIServing._get_decoded_token` 进行解码（或直接返回 id），并在 `responses.serving` 中相应改写两处生成 logprob 的路径。新增对应单元测试。

**🎯 影响范围**  
- `vllm/entrypoints/openai/engine/serving.py`：`_get_decoded_token` 参数签名保持不变但实现改为 `tokenizer.decode([token_id])`，兼容 `return_as_token_id` 场景。  
- `vllm/entrypoints/openai/responses/serving.py`：`_topk_logprobs` 与 `_create_response_logprobs` 统一使用 `_get_decoded_token`，并传递 `self.return_tokens_as_token_ids`。  
- 测试文件 `tests/entrypoints/openai/test_return_tokens_as_ids.py`：覆盖新行为。  

**💡 关注建议**  
1. **兼容性检查**：确认项目中除 `Responses` 之外的其他调用仍适配 `tokenizer.decode([id])`（某些 tokenizer 可能只接受 `int`），必要时加上类型适配。  
2. **默认行为**：当前 flag 默认 `False`，避免意外返回 `token_id:` 前缀，文档需明确说明打开方式。  
3. **性能评估**：每个 token 现在会调用一次 `tokenizer.decode([id])`，在高频路径上可能略有开销，建议在大量返回时开启 `return_tokens_as_token_ids=True` 以省去解码。  
4. **错误处理**：若 tokenizer 初始化被跳过（`skip_tokenizer_init=True`），仍会抛出异常，保持原有错误语义。  
5. **后续维护**：如新增 tokenizer 支持 `decode` 单个整数的实现，记得同步更新此处调用逻辑。  

总体而言，此改动为用户提供了更灵活的 token 表示方式，对现有功能影响有限，主要在响应生成路径。建议在发布说明中加入使用示例，并在 CI 中确保所有旧接口测试仍通过。

---

### [Doc]: fixing multiple typos in diverse files (#33256)
**SHA**: `31b25f6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/31b25f6516c8459c48d70bd3a57ed8dedf320b60)

**变更类型**：其他（文档/注释/轻量级代码修正）  
**重要程度**：🟡 中  

**变更概述**  
本次提交主要修正了 20 余处拼写、用词及注释错误，涉及 .buildkite 脚本、CPU 自定义算子源文件、torch‑extension 接口声明、以及多篇 Markdown 文档。唯一涉及代码逻辑的改动是 `torch_bindings.cpp` 中的 op schema 字段名从 `sheduler_metadata` 修正为 `scheduler_metadata`。

**影响范围**  
- **构建/发布脚本**：`.buildkite/scripts/generate-nightly-index.py` 中的 HTML 注释文字变更，不影响生成结果。  
- **CPU 算子**：`csrc/cpu/cpu_wna16.cpp`、`csrc/cpu/torch_bindings.cpp` 中仅是注释或 op schema 名称修正；后者会影响 TorchScript 的 `op_schema`，但不改变实现，只是更正字段名，保持与 Python 层调用一致。  
- **文档**：`docs/**/*.md` 中的多处说明、插件系统、模型说明等文字纠正，仅影响阅读体验。  
- **内部工具**：`vllm/utils/system_utils.py`、`vllm/v1/core/sched/scheduler.py`、`vllm/model_executor/...` 中的注释与 docstring 纠正，同样不影响运行时行为。  

**关注建议**  
1. **测试覆盖**：运行完整单元/集成测试，确保 `torch_bindings` 中的字段名改动未导致 Python 调用签名不匹配（如已有自定义插件依赖旧拼写）。  
2. **文档同步**：若有外部文档或脚本引用旧拼写（如 `sheduler_metadata`），请对应更新；搜索仓库中出现的旧字段名并清理。  
3. **CI 检查**：确认 `.buildkite` 脚本的变更未破坏 nightly 索引生成；建议在 CI 中保留一次完整索引构建验证。  
4. **代码风格**：本次修改展示了对注释、docstring 的细致打磨，建议在后续 PR 中继续保持，尤其是涉及公共 API（如 op schema）时同步更新文档。  

总体来看，此次提交为纯粹的文字和注释校正，对功能、性能或兼容性无实质影响，风险极低。只需确保字段名修正已在所有调用方统一后即可合并。

---

### [Bugfix] Fix Qwen3-VL-Reranker load. (#33298)
**SHA**: `abb34ac` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/abb34ac43a5a14dc0a918687e08e51abe73f74a5)

**🎯 变更类型**：Bug 修复（兼容 Qwen3‑VL‑Reranker）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm/model_executor/models/adapters.py` 中，原先仅凭 `is_vlm` 判断是否使用 VLM 的 `score` 层，导致在某些 VLM（如 Qwen3‑VL‑Reranker）没有 `score` 属性时出现加载错误。新增 `using_vlm_head = is_vlm and hasattr(language_model, "score")` 并相应改写权值加载、名称记录逻辑，确保只有真的存在 `score` 层时才走 VLM 分支。  
2. 示例脚本和测试用例改为使用 `vllm.multimodal.utils.encode_image_url / fetch_image` 替代手写的 Base64 编码函数，统一图片处理方式并删除冗余 `encode_base64_content_from_url`。  
3. 对 `tests/entrypoints` 中的实用函数做了清理，移除不再使用的手写 Base64 编码实现。

**🎯 影响范围**  
- **模型加载**：`Qwen3‑VL‑Reranker` 及其他使用 VLM 结构但未显式暴露 `score` 的模型。  
- **示例/文档**：`examples/pooling/score`、`examples/pooling/score/vision_*` 脚本。  
- **测试套件**：新增 vision‑score 测试，用新的多模态工具验证 API。  

**💡 关注建议**  
- 开发者在自定义 VLM 适配器时，确认模型类是否实现 `score`，否则仍会走普通 `lm_head` 路径；若实现请确保属性名保持一致。  
- 已迁移到 `vllm.multimodal.utils` 的图片编码函数，建议在所有自研示例和插件中统一使用，以避免未来重复实现。  
- 在升级后运行新增的 `test_online_score_vision.py`，确保所有权重加载路径均通过。若项目中仍保留旧 `encode_base64_content_from_url` 调用，请及时改为新工具或删除。  

整体来说，此次改动恢复了 Qwen3‑VL‑Reranker 的权重加载能力，同时统一了多模态图片处理方式，对现有 API 向后兼容，风险有限。

---

### [Release] [CI] Optim release pipeline (#33156)
**SHA**: `f9d0359` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f9d03599ef1528b1b1d35f90d858db88b5857ce1)

**变更类型**：功能增强（CI/Release 流程）  
**重要程度**：🟡 中  

**核心变更**  
1. **CI Pipeline**：在 `.buildkite/release-pipeline.yaml` 中新增 ROCm‑release 相关步骤  
   - 生成 ROCm wheels 根索引的阻塞节点与实际生成任务。  
   - 构建并发布 ROCm‑base 与完整 ROCm Docker 镜像（包括登录 ECR、拉取基础镜像、SCCACHE 配置、GPU 架构参数等）。  
   - 相应的 `annotate‑release.sh` 与 `annotate‑rocm‑release.sh` 脚本同步更新，加入对 ROCm‑base 镜像的标签/推送说明，并对根索引的 URL、路径以及可下载安装指令做了细化。  

2. **Dockerfile.rocm**：  
   - 在获取 vLLM 版本时额外安装 `regex` 依赖，避免 `setuptools_scm` 在缺少正则库时报错。  
   - 在 `final` 阶段删除 `sccache` 二进制及相关环境变量，避免在发布镜像中泄露 S3 配置。  

3. **根索引生成脚本** `tools/vllm-rocm/generate-rocm-wheels-root-index.sh`：全新脚本，实现  
   - 自动扫描 S3 `rocm/` 前缀下的语义化版本，选取最新（或手动指定）版本。  
   - 生成 PEP 503 兼容的根 `index.html` 与各 package 的相对链接 `index.html`，并支持 dry‑run 预览。  
   - 将结果上传至 `s3://<bucket>/rocm/`，为 `uv pip install vllm --extra-index-url https://wheels.vllm.ai/rocm/` 提供即插即用的入口。  

**影响范围**  
- Buildkite CI 环境（CPU 队列、ECR、S3 权限）。  
- ROCm wheel 与 Docker 镜像的发布流程。  
- `docker/Dockerfile.rocm` 与发布镜像的体积/安全性。  
- 文档/使用指引（`annotate-rocm-release.sh` 中的 URL 与安装示例）。  

**关注建议**  
- **权限校验**：确保 Buildkite 步骤拥有 `s3:GetObject/PutObject`、`ecr-public:*` 权限；尤其是新生成根索引的上传路径。  
- **变量一致性**：`S3_BUCKET`、`BUCKET`、`ROCM_PATH` 等在脚本与 CI 中保持统一，避免因拼写差异导致 404。  
- **错误容错**：`generate-rocm-wheels-root-index.sh` 在下载 `index.html` 失败时仅给出警告，后续上传仍会产生空文件，建议在关键路径加入 `set -e` 或显式退出。  
- **镜像标签**：新增 `-rocm-base` 与 `-rocm` 双标签策略已在注释脚本中同步，确认 downstream 使用方（如 Helm charts、K8s 部署）已更新对应镜像名。  
- **测试验证**：在非生产分支跑一次完整 ROCm release 流程，检查根索引 URL、包的相对链接以及 Docker 镜像的 `ENTRYPOINT` 是否仍然可用。  

总体而言，此次提交为 ROCm 发行提供了完整的自动化支撑，提升了可维护性和安全性。若上述建议落实，可进一步降低发布故障风险。

---

### [ez] Delete more torch version checks <= 2.8 (#33288)
**SHA**: `07ea184` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/07ea184f00c1dc2dbc163efc7dc1ac91e98d9ed5)

**变更类型**：其他（清理 torch 版本判断）  
**重要程度**：🟡 中  

**核心改动**  
1. **vllm/compilation/compiler_interface.py**  
   - 删除了对 `torch.__version__` 为 2.5 的特殊分支以及 2.6 以上的分支差异，统一使用 `torch._inductor.compile_fx.compile_fx_inner` 并在返回结果后统一提取 `hash_str` 与 `file_path`。  
   - 取消了对 `FxGraphCache.load` 的 patch（原先用于在 2.5 中获取编译图），改为只在 `ExitStack` 中 patch `compiled_fx_graph_hash`。  
   - 统一了缓存查找的实现：不再根据版本选择不同的 `CompiledFxGraphConstantsWithGm` 引入路径，直接在统一路径下调用 `_lookup_graph`。  
   - 精简了闭包遍历逻辑，只保留对 `cell_contents.__code__.co_filename` 的一次检查。  

2. **vllm/model_executor/layers/quantization/kernels/scaled_mm/pytorch.py**  
   - 删除了对 `packaging.version` 的依赖以及 `torch` 版本 >= 2.7 的显式检查，改为仅在 `is_supported` 中判断 CUDA 计算能力。  

**影响范围**  
- 编译路径（`vllm.compilation`）的缓存定位与哈希生成逻辑。  
- 量化 kernel（`scaled_mm`）的可用性检查。  
- 依赖 `torch>=2.5` 的旧环境将不再兼容（尤其是 2.5 系列），因为对应的兼容代码已被删除。  

**关注建议**  
- **开发者**：请确认项目的最低 PyTorch 版本已在文档或 `setup.cfg` 中明确（建议 >=2.6），并补充对应的 CI 测试矩阵。若仍需支持 2.5，需自行保留旧分支或加入回退实现。  
- **代码维护**：及时移除未使用的 `packaging` 依赖，保持 `requirements.txt` 的精简。  
- **用户**：在升级到新 vLLM 版本前，确保本地的 Torch 版本满足 >=2.6，否则可能出现编译缓存失效或运行时 `ImportError`。如遇缓存不匹配的错误，请手动清除 `self.base_cache_dir` 目录后重新运行。  

整体来看，此次提交通过统一实现、删除冗余分支，降低了代码复杂度并降低了未来维护成本，但也将最低 torch 依赖上调，需在发布说明中明确告知。

---

### [ModelRunner V2] Misc code simplification and cleanup (#33266)
**SHA**: `6bf3b46` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6bf3b46d7840364907405c7b02eaa66886a90839)

**🎯 变更类型**：代码重构 / 功能增强（V2 Model Runner 简化）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- `async_utils.py` 删除了不再使用的 `async_barrier`、`contextmanager`，改用更直接的流/事件同步，并使用字典推导式一次性构造 `prompt_logprobs_dict`。  
- `model_runner.py` 去掉了在同步调度下的冗余 CUDA 事件，`get_supported_tasks` 改为 `@staticmethod`，统一了参数写法，精简了 dummy‑run、输入准备、后处理等实现，提升可读性并稍微减少了拷贝次数。  
- `gpu_worker.py` 在启用 V2 Runner 时新增一次性日志输出，帮助用户确认已切换。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/async_utils.py`  
- `vllm/v1/worker/gpu/model_runner.py`  
- `vllm/v1/worker/gpu_worker.py`  

**💡 关注建议**  
1. **功能兼容**：`async_barrier` 被删除后，确认没有外部模块仍在导入该函数；如有，请在文档或兼容层中给出迁移指引。  
2. **V2 Runner 开关**：`VLLM_USE_V2_MODEL_RUNNER` 只在 `gpu_worker` 中打印信息，建议在 `ModelRunner` 初始化时也做一次安全校验（如检查依赖是否满足），防止误开启导致未实现的特性被调用。  
3. **CUDA 事件/流**：虽然简化了流/事件的创建，但保留了 `copy_event.record(copy_stream)` 的行为，需在高并发场景下回归测试，确保不会出现潜在的同步瓶颈。  
4. **单元/集成测试**：重点跑包含 `prompt_logprobs_dict`、`logprobs_tensors`、`num_nans` 的推理路径，验证 `get_output` 仍能正确裁剪 `sampled_token_ids` 并映射 `num_nans`。  
5. **文档更新**：在 vLLM 文档中补充 “V2 Model Runner 已默认开启” 的说明，以及已移除的 `async_barrier` 接口信息。  

整体来看，此次提交主要是代码整洁与轻量化，改动对功能的直接影响有限，但涉及 CUDA 流/事件的细节，建议在实际 GPU 负载下做一次性能回归，以确认没有意外的延迟或资源竞争。

---

### [7/N][Attention][Docs] Add documentation for attention backends (#32477)
**SHA**: `77c4f45` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/77c4f45c6c949c4e56f838c43b6907f6473ea65f)

**变更概览**  
- 为注意力后端新增完整的文档 (`docs/design/attention_backends.md`) 并在 `optimization.md` 中加入概览入口。  
- 新增预提交 hook `attention-backend-docs`，调用 `tools/pre_commit/generate_attention_backend_docs.py` 检查文档是否同步。  
- 实现了一个 1200 行的 **AST‑based 文档生成脚本**，负责解析后端注册、特性标识、CUDA 优先级、FlashAttention/FlashInfer 变体以及 MLA 前置/解码后端，并自动生成 Markdown 表格、使用示例、优先级说明及图例。

**影响范围**  
- **文档层面**：用户在查阅注意力后端特性、选择方式以及自动/手动后端选择规则时会看到更完整的信息。  
- **CI / 开发流程**：`pre‑commit` 现在会在每次提交时运行该脚本，若后端实现或支持特性变动而文档未更新，提交将被阻止并自动重新生成文档。  
- **代码库**：仅新增 `tools/pre_commit/generate_attention_backend_docs.py` 和相关配置，不影响运行时代码，但引入了大量 AST 解析逻辑，依赖标准库（不需要 CUDA/ROCM），在没有相应源文件时会 gracefully 退出。

**关注建议**  

| 关注点 | 建议 |
|--------|------|
| **脚本健壮性** | ① 对缺失文件、语法错误等异常已有捕获，但建议加入更细致的日志（如 `logging`）以便排查；② 解析新版后端类时可能出现新属性或方法，最好在 `analyze_backend` 中使用 `getattr`‑style 防御式读取，防止 `AttributeError` 中断。 |
| **CI 性能** | 虽然脚本会先判断 `is_relevant_file`，但首次全量运行仍会遍历所有后端文件。可考虑缓存解析结果（如 `pickle`）或仅在 `--check` 时读取已有文档进行 diff，降低提交前的耗时。 |
| **文档生成一致性** | 生成的 Markdown 依赖硬编码列顺序和 emoji 符号，后端新增字段（如 `requires_async`）时需同步更新 `generate_markdown_table`。建议把列映射抽象为配置结构，以免忘记同步。 |
| **预提交 Hook 行为** | `--check` 模式会在文档不一致时直接写回文件并返回 `1`，导致提交被阻止。若团队对文档改动不想自动提交，考虑提供 `--no-write` 选项，仅提示差异。 |
| **代码维护** | 生成脚本已超过 1000 行，函数高度耦合。建议拆分为 **解析层**（AST 读取）、**特性抽取层**、**渲染层**（Markdown），配合单元测试，防止后续修改导致隐藏错误。 |
| **兼容性** | 脚本默认写入 `docs/design/attention_backends.md`，在打包或发布时请在 `MANIFEST.in` / `setup.cfg` 中排除或包含该文件，避免出现 “missing file” 警告。 |

**结论**  
本次改动主要是提升文档可维护性和防止文档脱节，对运行时功能无直接影响，但引入的 AST 生成脚本在 CI 中会增加检查时间并可能因解析错误导致提交阻塞。按照上述建议完善异常处理、性能优化与代码结构，可让该功能更加可靠且易于后续扩展。

---

### [UX] Enable nested configs in config yaml files (#33193)
**SHA**: `ca19691` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ca1969186dde5fc0f76d22f2124dc0e6e0c9b792)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- `FlexibleArgumentParser.load_config_file` 现在兼容 YAML 中的嵌套字典：对普通键值仍保持平展输出，对值为 `dict` 的键会先 `json.dumps` 再加入 `--key` 后，交由 argparse 的 `type=json.loads` 解析。  
- 为此新增 3 条单元测试，覆盖平面、嵌套以及 end‑to‑end 场景。  

**🎯 影响范围**  
- `vllm/utils/argparse_utils.py`（核心解析逻辑）  
- 依赖 `load_config_file` 的所有启动脚本或库入口（如 `vllm.entrypoints`、`vllm.serving` 等）。  

**💡 关注建议**  
1. **文档与使用说明**：在 README/CLI 文档中明确指出 “嵌套配置会被转为 JSON 字符串”，并建议在 `add_argument` 时使用 `type=json.loads`（如 `--compilation-config` 示例）。  
2. **向后兼容**：现有平面配置保持原行为，安全；但若用户已有自定义 `type` 处理字典，需确认仍能接受 JSON 字符串。  
3. **异常处理**：当前只捕获文件读取/YAML 解析错误，建议在 `json.dumps` 前检测不可序列化对象，或在出现 `TypeError` 时给出可读提示。  
4. **更复杂结构**：列表中出现字典、深层嵌套仍会被整体 JSON 化，若未来需要细粒度展开，请预留扩展点（如递归 flatten）。  
5. **性能**：`json.dumps` 开销极小，影响可忽略；但大量嵌套配置时可通过一次性 `json.dumps` 替代逐键处理以提升效率。  

总体来说，此次改动提升了配置文件的表达能力，风险主要在于使用方未按需添加 `json.loads` 解析。建议在项目文档和示例中同步更新说明。

---

### [ez] Remove checks for torch version <= 2.8 (#33209)
**SHA**: `4197168` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4197168ea5877de51dc8aede9388c9be9ecc76ac)

**🎯 变更类型**：重构 / 其他  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交统一移除 `is_torch_equal_or_newer` 版本判断，所有原本仅在 Torch ≤ 2.8.0（或 2.7）下启用的分支已删去，改为直接使用新实现或统一路径。涉及自定义 OP 注册、Inductor 编译、分布式进程组创建、量化后端判定等多个模块。  

**🎯 影响范围**  
- `vllm/_aiter_ops.py`、`vllm/compilation/*`（后端、decorators、inductor_pass、compiler_interface）  
- `vllm/model_executor/layers/*`（fused_moe、quantization/mxfp4、torchao）  
- `vllm/distributed/utils.py`  
- `vllm/utils/torch_utils.py`（`supports_xccl`）  

**💡 关注建议**  
1. **兼容性测试**：在旧版 PyTorch（如 2.6、2.7）环境下跑完整单元/集成测试，确保去掉 `Tag.needs_fixed_stride_order` 与 `torch._dynamo` 配置后仍不产生运行时错误。  
2. **编译路径验证**：确认 `InductorStandaloneAdaptor`、`InductorAdaptor` 在没有版本判断的情况下均能正常初始化，尤其是 `fx_graph_remote_cache` 与 `autograd_cache` 的补丁是否仍生效。  
3. **分布式进程组**：`ProcessGroup._set_default_backend` 现在默认调用，验证在不支持该方法的极老 Torch 版本上（若仍有用户）是否会抛异常。  
4. **量化后端**：删除 `is_torch_equal_or_newer("2.8.0")` 判定后，`has_triton_kernels()` 成为唯一开关，需确认在 2.7 环境下不会误启用不兼容的 Triton kernel。  
5. **文档与 CI**：更新 README/CHANGELOG 中对应的 Torch 版本要求说明，并在 CI 中加入跨版本矩阵（2.6‑2.9）确保回归。  

如上检查通过，改动可提升代码可维护性并消除冗余版本分支。

---

### Use aiter triton fused_add_rmsnorm_pad for gpt-oss (#30976)
**SHA**: `59bcc5b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/59bcc5b6f2e68dab87426f4b77a3a72d72e6ec48)

**🟦 变更类型**：功能增强（新增 ROCm/AITER‑支持的 RMSNorm + padding 融合）

**⚡ 重要程度**：🟡 中  
（新特性只在 ROCm + AITER 环境且 hidden‑size = 2880 时生效，对大多数用户是可选的）

**📋 变更摘要**  
1. 在 `vllm/_aiter_ops.py` 中注册了 `rocm_aiter_triton_add_rmsnorm_pad` 自定义算子，并提供了真实实现与伪实现。  
2. 新增编译 Pass `RocmAiterTritonAddRMSNormPadFusionPass`，通过模式匹配把 “RMSNorm + add + pad” 替换为上述自定义算子。  
3. `PassManager`、`PassConfig`、`VllmConfig` 和默认优化配置加入 `fuse_act_padding` 标记，使该融合在配置层可开关。  
4. `vllm/model_executor/utils.py` 中改为优先走 AITER triton gemm，实现路径保持不变。  
5. `gpt_oss` 前向里对专家输出做了显式切片，以适配融合后可能产生的填充维度。  
6. 新增单元测试 `tests/compile/test_fuse_act_padding.py`，验证融合前后数值一致并检查算子替换次数。

**🎯 影响范围**  
- **算子层**：`_aiter_ops`、`rocm_aiter_fusion`、`utils.rocm_unquantized_gemm`  
- **编译器层**：`PassManager`、`PassConfig`、`VllmConfig`、`rocm_aiter_fusion`（新 Pass）  
- **模型层**：`gpt_oss`（切片改动）  
- **测试**：新增和修改的 compile 测例  

**💡 关注建议**  

1. **平台检测**：当前实现对 hidden‑size 硬编码为 2880（gpt‑oss）且仅在 ROCm 环境下开启。建议在文档中明确说明此限制，并在代码里使用 `assert` 或更友好的报错信息提醒用户。  
2. **配置一致性**：`enable_norm_pad_fusion` 与 `PassConfig.fuse_act_padding` 两层开关可能产生不一致（如用户自行在 `VllmConfig` 中打开但 `enable_norm_pad_fusion` 为 `False`）。可以在 `PassManager.configure` 处统一判断并给出 warning。  
3. **伪实现**：`_rocm_aiter_triton_add_rmsnorm_pad_fake` 只返回空张量，未考虑 `x_pad_to_multiple` 为 0 时的维度。确保该路径在 `torch.compile` 的 shape‑propagation 阶段不导致维度错误。  
4. **回退路径**：若 AITER 未安装或不满足 `x_pad_to_multiple` 条件，图中仍会残留 `aten.constant_pad_nd`。建议在 Pass 中加入 “no‑match” 日志，以便调试。  
5. **单元测试覆盖**：现有测试仅覆盖 `dtype=bfloat16`、hidden‑size = 2880、pad = 256。考虑添加不同 `dtype`（fp16、float32）以及不同 `hidden_size`（如 4096） 的负测试，确保在不满足条件时不会误触融合。  
6. **性能基准**：融合后主要受益于 Triton‑实现的 kernel 合并，建议在 CI 中加入微基准（如 latency / TFLOPs）对比，以防止在某些显卡或驱动版本上出现性能退化。  

总体来看，新增的 RMSNorm + padding 融合为 GPU + ROCm 环境下的 GPT‑OSS 提供了明显的算子合并潜力，改动范围集中且已配套单元测试。只要在文档、容错以及测试覆盖上再做少量补齐，即可安全合入主线。

---

### [Feature] Fully support for async scheduling + PP, 30.8% E2E throughput improvement, 31.8% TPOT improvement (#32618)
**SHA**: `3e44078` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3e440786afe763e892e12125ee7529f95f141c54)

**🎯 变更类型**：功能增强（异步调度 + Pipeline Parallel 完整支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `VllmConfig` 中加入对 `async_scheduling` 与 `pipeline_parallel_size>1` 同时开启的校验，确保两者可以共存。  
2. `Scheduler` 取消了 “PP 场景下有 output placeholder 时禁止再次调度” 的限制，使单个请求在飞行中可多步调度。  
3. `GPUModelRunner` 增加了 PP‑async 场景下的 GPU Broadcast：  
   - 最后一层 PP 通过 `_pp_broadcast_prev_sampled_token_ids` 将采样 token 广播到前层。  
   - 前层通过 `_pp_receive_prev_sampled_token_ids_to_input_batch` 接收并在输入 batch 中填充占位符。  
4. 相应单元测试补全，验证配置合法性及多步 in‑flight 调度行为。  

**🎯 影响范围**  
- `vllm/v1/core/sched/scheduler.py`（调度逻辑）  
- `vllm/v1/worker/gpu_model_runner.py`（GPU 端通信、状态更新）  
- 配置层 `VllmConfig` 与 `SchedulerConfig`  
- 新增/修改的测试文件 `tests/test_config.py`、`tests/v1/core/test_scheduler.py`  

**💡 关注建议**  
- **开发者**：在多节点、PP + async 环境下跑 CI，关注 `torch.distributed.broadcast` 的同步语义，防止因 rank 不匹配导致死锁。建议在 `gpu_model_runner` 中添加异常保护，确保 `prev_sampled_token_ids` 的 shape 正确。  
- **用户**：开启 `async_scheduling=True` 并将 `pipeline_parallel_size>1` 时，需要使用支持 NCCL/torch.distributed 的后端（如 `mp`）并确保每个 PP 阶段的 GPU 能相互通信。若使用旧版部署脚本，可能需要更新 `VllmConfig` 参数。  

整体而言，此次改动提升了端到端吞吐约 30%，但对分布式环境的依赖更强，务必在正式推送前完成全链路测试。

---

### [Perf] Optimize `moe_permute` for CUTLASS FP8 (#32892)
**SHA**: `c4e744d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c4e744dbd41f23ee7fd554a86cc1bf516082552d)

**变更类型**：性能优化  
**重要程度**：🟡 中  

**核心改动**  
1. 在 `moe_permute` 中新增对 `getMIndices` 的调用，生成 `align_expert_first_token_offset` 与 `m_indices`，并在有 `align_block_size` 时把对齐后的前缀偏移传递给后续 kernel。  
2. `expandInputRowsKernelLauncher` 的签名由 `int64_t* expert_first_token_offset` 改为 `int64_t const* expert_first_token_offset, int64_t const* aligned_expert_first_token_offset`，并在 launch 时一起传入。  
3. `expandInputRowsKernel` 删除原先的共享内存缓存 (`smem_expert_first_token_offset`) ，改为直接读取对齐后的前缀起点 `aligned_expert_first_token_offset`，并在 `ALIGN_BLOCK_SIZE` 分支中用 `__ldg` 完成行号映射，避免了额外的共享内存占用和同步。  
4. launch 参数中的共享内存大小置为 0，改为纯全局读。

**影响范围**  
- `csrc/moe/moe_permute_unpermute_op.cu`、`moe_permute_unpermute_kernel.h`、`moe_permute_unpermute_kernel.inl`  
- 依赖 `moe_permute` 的上层 Python 接口及 CUTLASS/DeepGemm 两种路径。

**关注建议**  
- **指针有效性**：在 `align_block_size` 为 `nullopt` 时 `aligned_expert_first_token_offset_ptr` 为 nullptr，确保所有 `ALIGN_BLOCK_SIZE` 为 false 的分支不读取该指针（当前通过运行时 `is_align_block_size` 选择函数实现，但模板 `if constexpr` 仍会实例化代码，建议在 `if constexpr` 前加 `static_assert` 或在函数体内做 nullptr 检查）。  
- **同步**：`getMIndices` 与后续 kernel 之间未显式 `cudaStreamSynchronize`，若 `getMIndices` 使用 async launch，需要确认其已完成写入 `aligned_expert_first_token_offset` 再进入 `expandInputRowsKernel`，否则可能出现读旧值的竞争。  
- **回退兼容**：`expert_first_token_offset.copy_(align_expert_first_token_offset);` 仍在 `moe_permute` 结尾执行，保持旧接口行为，但可能导致不必要的拷贝，可在 `align_block_size` 为 `nullopt` 时直接跳过。  
- **测试**：新增对齐块大小不同（1、4、8）以及本地 / 非本地 expert 场景的单元测试，验证 `expanded_dest_row` 计算与原实现完全一致，并测量内存占用及执行时间的提升。  

总体来看，此改动通过提前计算对齐前缀并在 kernel 中直接使用，可显著降低共享内存使用和同步开销，预计在深度 GEMM 场景下提升约 5‑10% 的吞吐。但需关注指针空值安全和 kernel 串行化的同步细节，确保在所有配置下结果保持一致。

---

### [Benchmark] Add startup benchmarking to buildkite run (#33183)
**SHA**: `392c5af` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/392c5af4fe1aa767e669070e8b1a94fa48011714)

**变更类型**：功能增强（在 CI 中新增“startup”基准）  
**重要程度**：🟡 中  

**核心变更**  
1. **统一基准执行逻辑**：原来的 `run_latency_tests`、`run_throughput_tests` 被抽象为 `run_benchmark_tests`，通过传入 `test_type`（latency / throughput / startup）实现复用，代码量大幅下降。  
2. **新增 startup 基准**：在 `main()` 中加入 `run_startup_tests`，并默认读取 `startup-tests${ARCH}.json`。对应的 JSON 文件需遵循 `test_name` 必须以 `startup_` 开头的约定。  
3. **脚本细节调整**：去除多余的空行、统一变量命名 (`bench_*`)、统一记录命令的 JSON 键名为 `${test_type}_command`，并在 `run_latency_tests`、`run_throughput_tests` 中保持向后兼容的轻量包装。  

**影响范围**  
- `.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh`（CI 基准入口）  
- 任何依赖该脚本的 CI 配置、监控和结果解析脚本  
- 可能的文档或 CI 示例中对 JSON 文件名称/结构的说明  

**关注建议**  
- **JSON 格式**：在项目根目录或 CI 环境中准备 `startup-tests*.json`，确保 `test_name` 前缀为 `startup_`，否则脚本会直接退出。  
- **兼容性检查**：旧的 `latency-tests*.json`、`throughput-tests*.json` 仍可使用，但若直接调用 `run_latency_tests`/`run_throughput_tests`（而非 `run_benchmark_tests`）会触发包装函数，行为保持不变。  
- **本地调试**：建议在本地机器上运行 `bash run-performance-benchmarks.sh` 并手动指定 `STARTUP_JSON`，验证 GPU/CPU 参数、环境变量是否被正确传递。  
- **结果解析**：原来的 `*_command` 键名已改为 `${test_type}_command`，若有 downstream 解析脚本需要同步更新。  
- **CI 负载**：startup 基准可能在 CPU 上执行（`--tensor_parallel_size` 为 1），请确认 `ON_CPU=1` 时的 NUMA 检查逻辑仍满足需求。  

总体来说，此次提交通过抽象复用提升了脚本可维护性，并在 CI 中加入了启动时延基准，但需要确保新增 JSON 文件与解析逻辑同步更新。

---

#### 🟢 低重要度变更 (22)

### [Bugfix] Fix broken GLM-OCR initialization (#33350)
**SHA**: `5e73e49` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5e73e4900c803253d84496b2eadad5be3b387aae)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `GlmOcrVisionTransformer` 构造函数的 `vision_config` 参数类型注解改为前向引用字符串，避免导入循环导致初始化错误，修复 GLM‑OCR 初始化问题。

---

### [Backport] [Kimi-K2.5] Replace torch.cuda with current_platform for d… (#33320)
**SHA**: `17b17c0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/17b17c068453e6dc6af79240bb94857ae175cc51)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `kimi_k25.py` 中将 `torch.cuda.current_device()` 替换为平台抽象 `current_platform.current_device()`，统一设备获取方式。

---

### [Quantization][Refactor]  use platform dict to choose kernel (#33130)
**SHA**: `a5aa4d5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a5aa4d5c0f31bba0491a2d9328785dd39dac33c0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将混合精度 kernel 的候选列表改为 `PlatformEnum → kernel 列表` 的字典，并在 `choose_mp_linear_kernel` 中根据当前平台选择对应 kernel，实现平台化选择，简化实现。

---

### [BugFix] Async Eplb fix potential race condition (#32881)
**SHA**: `d09135f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d09135fbd0b02046f3d75a6f0e8de28e70249e46)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `async_worker.py` 中新增对 `buffer_consumed_event` 的等待，以防止主线程未消费完缓冲区就被覆盖；在 `eplb_state.py` 为 `EplbModelState` 添加 `buffer_consumed_event` 并在主线程消费后记录 CUDA 事件，实现生产者‑消费者同步，避免潜在的竞争条件。

---

### [fix] tesdt mcp_tool_calling_streaming with a more complex math question (#32769)
**SHA**: `8688c3d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8688c3d460cecdf245775624707f91a5a275038e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在测试 `test_mcp_tool_calling_streaming_types` 中，将乘法问题从 `13 * 24` 替换为更复杂的 `123 * 456`，以提升对工具调用流式处理的覆盖度。

---

### Bugfix: Pass router logits dtype in nemotron shared experts (#32669)
**SHA**: `e01ff5c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e01ff5c070f47a0562711e61f5f885d4ccf241c9)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `nemotron_h.py` 中新增 `router_logits_dtype` 参数，统一使用 `torch.float32`，并将该 dtype 传递给 `ReplicatedLinear` 与共享专家模块，以修正路由器 logits 的类型错误。

---

### [Misc] Remove missed `pad_for_cudagraph` (#33283)
**SHA**: `a650ad1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a650ad1588059d59fdd432089c9abaf254657fe2)

**变更类型**：代码重构  
**重要程度**：🟢低  
**摘要**：在 `vllm/config/vllm.py` 中删除了未使用的 `pad_for_cudagraph` 方法，移除7行实现代码。

---

### [Doc] Update outdated link to Ray documentation (#32660)
**SHA**: `d697581` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d697581a7c28668d00b2284477e20a2cd774ea6e)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 OpenAI 兼容服务器文档中指向 Ray Serve LLM 的链接从已失效的 `serving-llms.html` 更新为最新的 `index.html`。

---

### Adding optional speculator tests for larger models (#32943)
**SHA**: `5eeba80` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5eeba80c7422c3e01d0dea424e663668950cd334)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 中新增可选的大模型接受长度测试，仅在非 slow 场景下运行；为 `test_acceptance_length.py` 添加模型配置，支持自定义容差 (`rtol`) 与 pytest 标记，实现更灵活的回归检测。

---

### [PluggableLayer][2/N] Apply PluggableLayer to linear layers (#33152)
**SHA**: `08b1195` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/08b1195e6250820c4d44875bf61822e846e9ae28)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将线性层的基类由 `CustomOp` 替换为 `PluggableLayer`，并相应更改装饰器注册，实现线性层的可插拔机制。

---

### [BugFix] Fix EPLB fail for MoeFP4 model with Marlin backend (#33262)
**SHA**: `53fc166` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/53fc16640281a939e01c98be80fa67179ba9a088)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `replace_parameter` 中加入 `new_data=None` 的处理，允许将层参数设为 `None`，解决 MoeFP4 模型使用 Marlin 后端时的 EPLB 崩溃问题。

---

### [CI/Build][BugFix] fix cuda/compat loading order issue in docker build (#33116)
**SHA**: `2515bbd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2515bbd02798a0f736a82bbdd10dbf9d5accfa7c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Dockerfile 中将 CUDA 兼容库的 ld.so 配置文件名从 `00-cuda-compat.conf` 改为 `cuda-compat.conf`，确保加载顺序正确，避免冲突。

---

### [Release] [ROCm] Remove old build step (#33316)
**SHA**: `c487a8e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c487a8eef4d89d858c68f4262da14b1940abf950)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `.buildkite/release-pipeline.yaml` 中删除了旧的 ROCm 构建步骤，简化发布流程；同时对 ROCm 相关步骤的标签和依赖进行重命名和微调。

---

### [Misc][Build] Lazy load cv2 in nemotron_parse.py (#33189)
**SHA**: `9e138cb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9e138cb01d656e571257f61b0289824d452a6098)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `nemotron_parse.py` 中将 `cv2` 的导入从模块顶部延迟到实际使用处，实现惰性加载，减少不必要的依赖加载，保持功能不变。

---

### Fix tool call indexing double-counting (#33141)
**SHA**: `39037d2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/39037d258e68da3926d99681ea63e46212e519f9)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复工具调用索引双计数错误，新增 `history_tool_call_cnt` 累计并在生成 tool‑call ID 时改为使用该计数，从而确保每次调用的索引唯一准确。

---

### [Misc] Add orozery to CODEOWNERS (core, kv_transfer, kv_offload) (#33227)
**SHA**: `a663b21` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a663b218aefa8f807970a6cab52f9cd7f9afa011)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `.github/CODEOWNERS` 中新增 `@orozery` 为 `kv_transfer`、`kv_offload`、`core`、`kv_connector` 等目录及相应测试路径的代码所有者。

---

### [Bugfix] Register fp8 cutlass_group_gemm as supported for only SM90+SM100 (#33285)
**SHA**: `1bd47d6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1bd47d6e5a57000e404af60838eadb86b6f457c0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `cutlass_group_gemm_supported` 中新增对 CUDA 能力的检查，仅在 SM90‑SM109（即 90≤cap<110）上返回 True，防止在不受支持的 SM 版本（如 SM80 或 SM110+）误报。 

---

### [UX] Remove noisy CT UnquantizedLinearMethod warn (#33273)
**SHA**: `141cd43` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/141cd43967d20b2206353975ad0ebcce4d7d7c69)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除了对非量化权重的重复警告日志，改为直接注释说明回退至 `UnquantizedLinearMethod`，保持行为不变，降低日志噪声。

---

### [Bugfix] Add missing encoder only guard for do_kv_cache_update (#33269)
**SHA**: `ab597c8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ab597c869a78530fe6495ccda2bdc03f6f5c712e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 encoder‑only attention 增加判定，直接返回，不再使用 KV 缓存，防止缺失的缓存更新导致错误。

---

### [CI] Change GPU key to device key for B200 test (#33275)
**SHA**: `8bdd397` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8bdd3979d8c2f15feb475485ba1af1917cefbe5a)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.buildkite/test_areas/lm_eval.yaml` 中将临时 B200 测试步骤的键名从 `gpu` 改为 `device`，保持其他配置不变。

---

### [CI] Whisper tests `enforce_eager=False` (#33098)
**SHA**: `8ebf372` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8ebf372e9d612a325f54aadf5c0c3c6588b6afa3)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/entrypoints/openai/test_transcription_validation_whisper.py` 中移除 `--enforce-eager` 参数，改为以空参数启动 Whisper 模型的远程服务器，使测试在非 eager 模式下运行。

---

### [lora/moe] Avoid extra intermediate buffer & Python slicing in expand phase when split_k == 1 (#32774)
**SHA**: `f210f0b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f210f0b7b1e4f179cf194c09c8805898134b70d8)

**变更类型**：代码重构/性能优化  
**重要程度**：🟢低  

**摘要**：在 expand 阶段直接对 output 切片累加，去除 b_intermediate_cache1 中间缓冲并通过 kernel 参数 `ADD_INPUTS` 实现合并写入，避免额外的 Python 切片和中间拷贝，提升运行效率。

---

