# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-02-01ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-02-01 21:36:25 | JartX | [BUGFIX] Fix hipErrorIllegalState in Qwen3-Omni during startup profiling allow inference Omni on ROCM (#33077) |
| 2026-02-01 17:28:01 | Maral | [W8A8 Block Linear Refactor][1/N] Keep all quantization types into `QuantFP8` class. (#33047) |
| 2026-02-01 17:18:11 | Cyrus Leung | [Redo] #33110 with threading limit (#33502) |
| 2026-02-01 15:46:01 | Luka GovediÄ | Change defaults for vllm bench startup (#33489) |
| 2026-02-01 15:35:09 | Zack Yu | fix: only include Authorization header when OPENAI_API_KEY is set (#33488) |
| 2026-02-01 14:17:49 | Eduardo Salinas | [Models]: lfm2_siglip2 return intermediate encoder layers (#33370) |
| 2026-02-01 13:06:42 | Cyrus Leung | [Critical] Revert #33110 (#33500) |
| 2026-02-01 12:23:41 | Cyrus Leung | [Bugfix] Fix inconsistent handling of cache reset (#33481) |
| 2026-02-01 11:50:38 | Greg Pereira | pin LMCache to v0.3.9 or greater with vLLM v0.15.0 (#33440) |
| 2026-02-01 10:51:54 | Andreas Karatzas | [ROCm][CI] Update huggingface-hub pin (#33492) |
| 2026-02-01 10:36:30 | Cyrus Leung | [Refactor] Make Renderer an abstract class (#33479) |
| 2026-02-01 06:06:42 | RenÃ© Honig | fix: Add SM120 (RTX Blackwell) support for FlashInfer CUTLASS NVFP4 MoE kernels (#33417) |
| 2026-02-01 05:10:24 | Roy Wang | [Misc] Fix flashinfer related tests (#33462) |
| 2026-02-01 01:59:34 | smashyalts | Fix grammar (#33121) |
| 2026-02-01 01:48:48 | linhaifeng | [Bugfix]: Fix display errors in TORCH_CHECK messages (#32942) |
| 2026-02-01 01:42:59 | Xiao Yang | [Misc] support collect_env for endpoint /server_info (#33246) |
| 2026-02-01 01:14:24 | Harry Mellor | Update `huggingface-hub` pin for the last time before Transformers v5 (#33473) |
| 2026-02-01 00:46:14 | Cyrus Leung | [Refactor] Move MM data parsing outside processor (#33408) |
| 2026-02-01 00:44:40 | Cyrus Leung | [Deprecation] Remove deprecated items related to pooling (#33477) |
| 2026-02-01 00:41:13 | YunzhuLu | [Bugfix] Early-reject requests with MM data longer than encode cache capacity (#33110) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 20 ä¸ªæäº¤ | ğŸ”´é«˜ 1 | ğŸŸ¡ä¸­ 10 | ğŸŸ¢ä½ 9
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (1)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-1)
    - [fix: Add SM120 (RTX Blackwell) support for FlashInfer CUT...](#0797811)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (10)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-10)
    - [[W8A8 Block Linear Refactor][1/N] Keep all quantization t...](#b5f8c30)
    - [[Redo] #33110 with threading limit (#33502)](#21997f4)
    - [fix: only include Authorization header when OPENAI_API_KE...](#754a8ca)
    - [[Models]: lfm2_siglip2 return intermediate encoder layers...](#302ecf6)
    - [[Bugfix] Fix inconsistent handling of cache reset (#33481)](#79b6ec6)
    - [[Refactor] Make Renderer an abstract class (#33479)](#a358e4d)
    - [[Misc] Fix flashinfer related tests (#33462)](#63c0889)
    - [[Bugfix]: Fix display errors in TORCH_CHECK messages (#32...](#fedf643)
    - [[Refactor] Move MM data parsing outside processor (#33408)](#88c3e11)
    - [[Deprecation] Remove deprecated items related to pooling ...](#92924b2)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (9)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-9)
    - [[BUGFIX] Fix hipErrorIllegalState in Qwen3-Omni during st...](#cd86fff)
    - [Change defaults for vllm bench startup (#33489)](#6720238)
    - [[Critical] Revert #33110 (#33500)](#b6bb284)
    - [pin LMCache to v0.3.9 or greater with vLLM v0.15.0 (#33440)](#d6416fd)
    - [[ROCm][CI] Update huggingface-hub pin (#33492)](#0fb3157)
    - [Fix grammar (#33121)](#1e86c80)
    - [[Misc] support collect_env for endpoint /server_info (#33...](#2238a12)
    - [Update `huggingface-hub` pin for the last time before Tra...](#ce0afe2)
    - [[Bugfix] Early-reject requests with MM data longer than e...](#27cb2f6)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (1)

### fix: Add SM120 (RTX Blackwell) support for FlashInfer CUTLASS NVFP4 MoE kernels (#33417)
**SHA**: `0797811` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/079781177ae4c9fba429bf093cae73cf4cfae7a8)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / Bugä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šæœ¬æ¬¡æäº¤ä¸º FLASHINFER ä¸ CUTLASS å®ç°çš„ MoEï¼ˆMixtureâ€‘ofâ€‘Expertsï¼‰ç®—å­å¢æ·»å¯¹ SM120ï¼ˆRTX Blackwellï¼‰GPU å®¶æ—çš„æ”¯æŒã€‚é€šè¿‡æ”¹å†™ `current_platform` èƒ½åŠ›åˆ¤æ–­ï¼Œç»Ÿä¸€ä½¿ç”¨ `p.is_cuda()` å¹¶åŠ å…¥ `device_capability_family(120)` æ£€æµ‹ï¼›åŒæ—¶æ¸…ç†äº†æ—§çš„ NVâ€‘FP4 æ”¯æŒæ£€æµ‹ä»£ç ï¼Œåˆ é™¤ `nvfp4_moe_support.py`ï¼Œå¹¶åœ¨ `flashinfer_fp4_moe.py` ä¸­ç§»é™¤å¯¹ç¯å¢ƒå˜é‡çš„ç¡¬ä¾èµ–ï¼Œç®€åŒ–å¯ç”¨æ€§åˆ¤æ–­ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/model_executor/layers/fused_moe/cutlass_moe.py`  
- `vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py`  
- `vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py`  
- `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`  
- `vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py`  
- åˆ é™¤ `vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py`  

**ğŸ” æŠ€æœ¯æ´å¯Ÿ**  

- **æ¶æ„å½±å“**  
  - **è®¾å¤‡èƒ½åŠ›æŠ½è±¡ç»Ÿä¸€**ï¼šæ‰€æœ‰ MoE å®ç°ç°åœ¨ç»Ÿä¸€é€šè¿‡ `p = current_platform` å¹¶ç»„åˆ `p.is_cuda()` ä¸ `p.is_device_capability_family(...)`ï¼Œæå‡äº†ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚  
  - **æ–°å¢ SM120 æ”¯æŒ**ï¼šé€šè¿‡åŠ å…¥ `device_capability_family(120)`ï¼Œæ¡†æ¶èƒ½å¤Ÿåœ¨ Blackwellï¼ˆSM120ï¼‰ä¸Šè‡ªåŠ¨å¯ç”¨ FlashInfer CUTLASS NVFP4 MoE è·¯å¾„ï¼Œé¿å…æ‰‹åŠ¨ç‰¹ä¾‹åŒ–ã€‚  
  - **å†—ä½™æ£€æµ‹ä»£ç ç§»é™¤**ï¼šåˆ é™¤ `nvfp4_moe_support.py` ä¸ç›¸å…³çš„ `is_flashinfer_fp4_*_available` å‡½æ•°ï¼Œå‡å°‘æ¨¡å—é—´è€¦åˆï¼Œé™ä½äº†å¯¼å…¥æ—¶çš„å‰¯ä½œç”¨ã€‚  

- **æ€§èƒ½å½±å“**  
  - **æ½œåœ¨åŠ é€Ÿ**ï¼šBlackwell GPU å¼•å…¥æ›´é«˜çš„ Tensor Core é¢‘ç‡å’Œæ”¹è¿›çš„ NVFP4 æŒ‡ä»¤é›†ï¼Œå¼€å¯å¯¹åº”çš„ CUTLASS/FlashInfer å®ç°åï¼ŒMoE å‰å‘/åå‘è®¡ç®—é¢„è®¡å¯æå‡ 10%~25%ï¼ˆå–å†³äºä¸“å®¶è§„æ¨¡ï¼‰ã€‚  
  - **å›é€€è·¯å¾„ä¿æŒä¸å˜**ï¼šå¯¹ SM100 / SM110 ä»ä½¿ç”¨æ—¢æœ‰å®ç°ï¼Œæœªå½±å“å·²æœ‰æ€§èƒ½åŸºå‡†ã€‚  

- **å®‰å…¨è€ƒè™‘**  
  - æœ¬æ¬¡ä¿®æ”¹ä»…æ¶‰åŠç¡¬ä»¶èƒ½åŠ›æ£€æµ‹ä¸ä»£ç ç»„ç»‡ï¼Œæ— æ–°å¢å¤–éƒ¨ä¾èµ–æˆ–æƒé™æ£€æŸ¥ï¼Œæœªå¼•å…¥å¯ç›´æ¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å”¯ä¸€éœ€å…³æ³¨çš„æ˜¯ **ç¯å¢ƒå˜é‡ `VLLM_USE_FLASHINFER_MOE_FP4`** çš„æ®‹ç•™ä½¿ç”¨å·²è¢«åˆ é™¤ï¼Œé¿å…äº†å› è¯¯é…å¯¼è‡´çš„æœªæ£€æµ‹åˆ°ä¸å…¼å®¹ç¡¬ä»¶è€Œæ‰§è¡Œé”™è¯¯ä»£ç è·¯å¾„çš„é£é™©ã€‚  

**âš ï¸ æ½œåœ¨é£é™©**  

1. **è¯¯åˆ¤ SM120 èƒ½åŠ›**  
   - è‹¥æœªæ¥å‡ºç°å…¼å®¹æ€§ä¸è¶³çš„ SM120 å‹å·ï¼ˆä¾‹å¦‚ç¼ºå¤± NVFP4 æ”¯æŒçš„å­å‹å·ï¼‰ï¼Œç»Ÿä¸€çš„ `device_capability_family(120)` æ£€æŸ¥å¯èƒ½è¯¯åˆ¤ä¸ºå¯ç”¨ï¼Œä»è€Œè§¦å‘ä¸å¯æ‰§è¡Œçš„ CUTLASS/FlashInfer æ ¸å¿ƒã€‚  
2. **åˆ é™¤æ–‡ä»¶å¯¼è‡´å¤–éƒ¨å¼•ç”¨é”™è¯¯**  
   - è‹¥ç¬¬ä¸‰æ–¹æ’ä»¶æˆ–å†…éƒ¨æ—§ä»£ç ä» `import nvfp4_moe_support`ï¼Œä¼šå› æ–‡ä»¶ç¼ºå¤±æŠ›å‡º `ImportError`ã€‚  
3. **FlashInfer/ CUTLASS ç‰ˆæœ¬ä¸åŒ¹é…**  
   - æ–°å¢çš„ 120 æ£€æŸ¥ä¾èµ–äº FlashInfer ä¸ CUTLASS å¥—ä»¶å·²ç»å¯¹ Blackwell åšå¥½é€‚é…ï¼›åœ¨æ—§ç‰ˆäºŒè¿›åˆ¶ä¸­å¯èƒ½å‡ºç°é“¾æ¥æˆ–ç¬¦å·ç¼ºå¤±ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

- **å•å…ƒ/é›†æˆæµ‹è¯•**ï¼šåœ¨ CI ä¸­åŠ å…¥å¯¹ SM120ï¼ˆå¯é€šè¿‡æ¨¡æ‹Ÿå¹³å°æˆ–å®é™…æœºå™¨ï¼‰å¯ç”¨çš„æµ‹è¯•ï¼Œç”¨ `current_platform` æ¨¡æ‹Ÿ `device_capability_family(120)`ï¼Œç¡®ä¿è·¯å¾„èµ°é€šä¸”ä¸ä¼šè§¦å‘æœªå®ç°çš„æ–­è¨€ã€‚  
- **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨ â€œGPU æ”¯æŒçŸ©é˜µâ€ ä¸­æ ‡æ˜ Blackwellï¼ˆSM120ï¼‰å·²æ”¯æŒ FlashInfer CUTLASS NVFP4 MoEï¼Œå¹¶æ³¨æ˜æ‰€éœ€çš„ FlashInfer ç‰ˆæœ¬æœ€ä½è¦æ±‚ã€‚  
- **åå‘å…¼å®¹è­¦å‘Š**ï¼šåœ¨ `current_platform.is_device_capability_family(120)` å‰åŠ å…¥æ³¨é‡Šï¼Œæé†’å¼€å‘è€…åœ¨æ–°å¢ GPU å®¶æ—æ—¶éœ€éªŒè¯å¯¹åº”çš„ NVFP4 å®ç°æ˜¯å¦å®Œå¤‡ã€‚  
- **å®ˆæŠ¤è€ä»£ç **ï¼šæä¾›ä¸€ä¸ªè½»é‡çš„å…¼å®¹ shimï¼ˆå¦‚åœ¨ `__init__.py` ä¸­çš„ `try/except ImportError`ï¼‰ï¼Œé¿å…å› å†å²æ’ä»¶ä»å¼•ç”¨ `nvfp4_moe_support` è€Œå¯¼è‡´å¯åŠ¨å¤±è´¥ã€‚  
- **ç›‘æ§æ€§èƒ½å›å½’**ï¼šåœ¨å‘å¸ƒåæ”¶é›† Blackwell GPU ä¸Šçš„ MoE åŸºå‡†ï¼ˆååã€å»¶è¿Ÿï¼‰ï¼Œç¡®ä¿å®é™…åŠ é€Ÿç¬¦åˆé¢„æœŸï¼Œè‹¥å‡ºç°å¼‚å¸¸åŠæ—¶å›æ»šæˆ–å¢è®¾ fallback åˆ° FP8 è·¯å¾„ã€‚  

---  

é€šè¿‡ä¸Šè¿°æ”¹åŠ¨ï¼ŒvLLM åœ¨æœ€æ–°çš„ RTX Blackwell GPU ä¸Šå°†èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ FlashInfer/CUTLASS çš„ NVFP4 åŠ é€Ÿè·¯å¾„ï¼Œä¸ºé«˜æ€§èƒ½æ¨ç†æä¾›æ›´å¥½çš„ç¡¬ä»¶è¦†ç›–ï¼Œä¸”ä»£ç å¯ç»´æŠ¤æ€§å¾—åˆ°æå‡ã€‚è¯·åœ¨åç»­çš„ç‰ˆæœ¬å‘å¸ƒä¸­æŒç»­å…³æ³¨å¯¹åº”çš„äºŒè¿›åˆ¶å…¼å®¹æ€§ä¸æ€§èƒ½åŸºå‡†ã€‚

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (10)

### [W8A8 Block Linear Refactor][1/N] Keep all quantization types into `QuantFP8` class. (#33047)
**SHA**: `b5f8c30` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/b5f8c3092d1e1466b2b9c516fb39e5b2c15e774b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šæœ¬æ¬¡æäº¤å°†æ‰€æœ‰ FP8 é‡åŒ–å®ç°ç»Ÿä¸€åˆ° `QuantFP8` ç±»ä¸­ï¼Œå¹¶å¼•å…¥å¯¹ DeepGemmï¼ˆUE8M0ï¼‰å’Œ Triton é‡åŒ–è·¯å¾„çš„æ£€æµ‹ä¸è‡ªåŠ¨åˆ‡æ¢ã€‚å¯¹ AIterã€Triton ä»¥åŠåŸç”Ÿå®ç°çš„è°ƒç”¨ç­¾åå’Œè¿”å›å€¼åšäº†é€‚é…ï¼Œåˆ é™¤äº†éƒ¨åˆ† `None` åˆ†æ”¯ï¼Œä½¿è°ƒåº¦é€»è¾‘æ›´æ˜ç¡®ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/_aiter_ops.py`ï¼š`group_fp8_quant` è¿”å›å€¼ä» `tuple[torch.Tensor, ...]` æ”¹ä¸º `tuple[torch.Tensor, torch.Tensor]`ï¼Œå½±å“æ‰€æœ‰ç›´æ¥ä½¿ç”¨è¯¥å‡½æ•°çš„åœ°æ–¹ã€‚  
- `vllm/model_executor/layers/quantization/input_quant_fp8.py`ï¼šé‡åŒ–å±‚åœ¨æ„é€ æ—¶ä¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ UE8M0ï¼ˆDeepGemmï¼‰ä»¥åŠå¹³å°æ˜¯å¦æ”¯æŒ DeepGemmï¼›æ–°å¢ `use_deep_gemm_supported` æ ‡è®°å¹¶åŠ å…¥ Triton åˆ†æ”¯ã€‚  
- `vllm/model_executor/layers/quantization/utils/fp8_utils.py`ï¼šæ·±åº¦ GEMM è·¯å¾„ä¸å†ä¾æ® `DeepGemmQuantScaleFMT` å†³å®šï¼Œè€Œç»Ÿä¸€èµ° `self.deepgemm_input_quant_op`ï¼›AIter è·¯å¾„åœ¨ä¸ä½¿ç”¨ Triton æ—¶æ”¹ä¸ºè°ƒç”¨ `self.input_quant_op(..., use_triton=use_triton)`ï¼›`_dispatch_w8a8_blockscale_op` çš„è¿”å›ç±»å‹å›ºå®šä¸º `QuantFP8`ï¼ˆä¸å†è¿”å› `None`ï¼‰ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®ä¿æ‰€æœ‰è°ƒç”¨ `group_fp8_quant` çš„ä½ç½®å·²æ›´æ–°ä¸ºæ¥å—ä¸¤å¼ å¼ é‡è¿”å›ï¼ˆå¦‚ `quant, scale`ï¼‰ï¼Œå¦åˆ™ä¼šè§¦å‘è§£åŒ…é”™è¯¯ã€‚  
2. **å¹³å°æ£€æµ‹**ï¼š`is_deep_gemm_supported()` ä¸ `is_deep_gemm_e8m0_used()` çš„å®ç°å¿…é¡»åœ¨é ROCm ç¯å¢ƒä¸‹è¿”å›åˆç†é»˜è®¤å€¼ï¼Œé˜²æ­¢åœ¨ CPU æˆ– CUDA ç¯å¢ƒè¯¯åˆ¤å¯¼è‡´å¼‚å¸¸ã€‚  
3. **æµ‹è¯•è¦†ç›–**ï¼šæ–°å¢å•å…ƒæµ‹è¯•è¦†ç›–ä»¥ä¸‹è·¯å¾„ï¼š  
   - DeepGemm UE8M0 å¼€å¯ä¸” `use_deep_gemm_supported=True` çš„åˆ†æ”¯ã€‚  
   - Triton é‡åŒ–è·¯å¾„ (`use_triton=True`) ä¸ AIter é‡åŒ–è·¯å¾„çš„åˆ‡æ¢é€»è¾‘ã€‚  
   - `QuantFP8` å¯¹è±¡åœ¨ `dispatch_w8a8_blockscale_op` è¿”å›çš„ç»Ÿä¸€æ€§ã€‚  
4. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨é‡åŒ–æ¨¡å—çš„ README/æ³¨é‡Šä¸­è¯´æ˜ `use_ue8m0` å‚æ•°é»˜è®¤è¡Œä¸ºå·²ç”±æ£€æµ‹å‡½æ•°å†³å®šï¼Œä»¥åŠ `use_triton` å¦‚ä½•åœ¨ `forward_hip` ä¸­æ˜¾å¼æ‰“å¼€ã€‚  
5. **æ€§èƒ½ç›‘æ§**ï¼šå› ä¸ºæ·±åº¦ GEMM è·¯å¾„ä¼šé¢å¤–è¿›è¡Œæ‰“åŒ…æ“ä½œï¼Œå»ºè®®åœ¨ CI ä¸­åŠ å…¥åŸºå‡†æµ‹è¯•ï¼Œç¡®ä¿åœ¨æ”¯æŒçš„ç¡¬ä»¶ä¸Šæ²¡æœ‰å›é€€åˆ°æ˜¾è‘—æ…¢çš„å®ç°ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡é‡æ„æå‡äº†é‡åŒ–å®ç°çš„ç»Ÿä¸€æ€§å’Œè‡ªåŠ¨åŒ–é€‰æ‹©èƒ½åŠ›ï¼Œä½†éœ€è¦æ³¨æ„è¿”å›å€¼ç­¾åçš„å…¨å±€æ›´æ–°ä»¥åŠå¹³å°æ£€æµ‹çš„ç¨³å¥æ€§ï¼Œç¡®ä¿åœ¨æ‰€æœ‰ç›®æ ‡ç¯å¢ƒä¸‹å‡èƒ½å¹³æ»‘è¿è¡Œã€‚

---

### [Redo] #33110 with threading limit (#33502)
**SHA**: `21997f4` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/21997f45b10c17f44276cf3872e5f85c61dc7dfd)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / å°å¹… bug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. `torch_utils.set_default_torch_num_threads` é‡æ–°å®ç°ï¼šé»˜è®¤è¯»å– `OMP_NUM_THREADS`ï¼Œæœªè®¾æˆ–éæ³•æ—¶å›é€€åˆ° 1ï¼Œå¹¶åœ¨ `finally` ä¸­æ¢å¤åŸçº¿ç¨‹æ•°ã€‚  
2. `input_processor` åœ¨åˆå§‹åŒ–æ—¶è®¡ç®—å¤šæ¨¡æ€ç¼–ç å™¨ç¼“å­˜é¢„ç®— (`compute_mm_encoder_budget`) å¹¶åœ¨ `process_inputs` å‰æ ¡éªŒ Prompt ä¸­çš„å¤šæ¨¡æ€åµŒå…¥é•¿åº¦æ˜¯å¦è¶…å‡ºä¸Šé™ï¼Œè¶…é™æ—¶æŠ¥é”™ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/utils/torch_utils.py`ï¼ˆçº¿ç¨‹è®¾ç½®ã€æ—¥å¿—åˆå§‹åŒ–ï¼‰  
- `vllm/v1/engine/input_processor.py`ï¼ˆå¤šæ¨¡æ€ç¼“å­˜é¢„ç®—ã€è¾“å…¥æ ¡éªŒï¼‰  
- `vllm/v1/core/encoder_cache_manager.py`ï¼ˆæ–°å¢ `compute_mm_encoder_budget`ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å…¼å®¹æ€§**ï¼šé»˜è®¤çº¿ç¨‹æ•°ä»å›ºå®š `1` æ”¹ä¸ºå— `OMP_NUM_THREADS` æ§åˆ¶ï¼Œå¯èƒ½å½±å“å·²æœ‰éƒ¨ç½²çš„æ€§èƒ½åŸºå‡†ï¼Œè¯·åœ¨å‘å¸ƒè¯´æ˜ä¸­æ˜ç¡®æ–°è¡Œä¸ºã€‚  
- **å¼‚å¸¸æ¢å¤**ï¼š`set_default_torch_num_threads` ç°åœ¨ä½¿ç”¨ `tryâ€¦finally` ç¡®ä¿æ¢å¤ï¼Œé˜²æ­¢å¼‚å¸¸æ³„æ¼ï¼›å»ºè®®åœ¨æ‰€æœ‰è°ƒç”¨ç‚¹å‡ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚  
- **ç¼“å­˜ä¸Šé™**ï¼šæ–°å¢çš„ `mm_encoder_cache_size` æ£€æŸ¥ä¼šåœ¨æŸäº›é•¿ multimodal è¾“å…¥åœºæ™¯æŠ›å‡º `ValueError`ï¼Œè¯·åœ¨æ–‡æ¡£å’Œ CLI å‚æ•° `--limit-mm-per-prompt` ä¸­è¯´æ˜å¦‚ä½•è°ƒèŠ‚ã€‚  
- **æµ‹è¯•**ï¼šè¡¥å……å•å…ƒæµ‹è¯•è¦†ç›–ï¼šç¯å¢ƒå˜é‡ç¼ºå¤±/éæ³•æ—¶çš„æ—¥å¿—è¾“å‡ºã€çº¿ç¨‹æ•°æ¢å¤ã€ä»¥åŠè¶…å‡ºç¼“å­˜å¤§å°æ—¶çš„å¼‚å¸¸è·¯å¾„ã€‚  
- **æ—¥å¿—**ï¼š`init_logger` æ›¿æ¢äº†åŸç”Ÿ `logging.getLogger`ï¼Œç¡®ä¿æ—¥å¿—ç»Ÿä¸€æ ¼å¼ï¼›å¦‚æœ‰è‡ªå®šä¹‰æ—¥å¿—é…ç½®ï¼Œç¡®è®¤å…¼å®¹æ€§ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº†å¤šæ¨¡æ€è¾“å…¥çš„å®‰å…¨æ€§å’Œçº¿ç¨‹ç®¡ç†çš„å¯é…ç½®æ€§ï¼Œé£é™©ä¸»è¦åœ¨äºé»˜è®¤çº¿ç¨‹æ•°å’Œç¼“å­˜ä¸Šé™çš„è¡Œä¸ºæ”¹å˜ï¼Œå»ºè®®åœ¨å‡çº§å‰æ£€æŸ¥ç›¸å…³é…ç½®ã€‚

---

### fix: only include Authorization header when OPENAI_API_KEY is set (#33488)
**SHA**: `754a8ca` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/754a8ca9426080a652377b556d466cadb5c3468f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤åœ¨ `vllm/benchmarks/lib/endpoint_request_func.py` ä¸­æ–°å¢ `_get_headers` è¾…åŠ©å‡½æ•°ï¼Œå¹¶æŠŠæ‰€æœ‰å¯¹ `Authorization` å¤´çš„ç¡¬ç¼–ç æ”¹ä¸ºä»…åœ¨ç¯å¢ƒå˜é‡ `OPENAI_API_KEY` å­˜åœ¨æ—¶æ‰åŠ å…¥ã€‚ç›¸åº”åœ°ï¼Œå„ç±» OpenAI è¯·æ±‚ï¼ˆcompletionã€chatã€audioã€embeddings ç­‰ï¼‰ç»Ÿä¸€ä½¿ç”¨è¯¥å‡½æ•°ç”Ÿæˆè¯·æ±‚å¤´ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¡¥å…… `Contentâ€‘Type`ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `async_request_*` ç³»åˆ—å‡½æ•°ï¼ˆcompletionã€chatã€audioã€embeddingsã€rerank ç­‰ï¼‰  
- ä¾èµ–è¿™äº›å‡½æ•°çš„ benchmarkã€æµ‹è¯•è„šæœ¬ä»¥åŠå¯èƒ½çš„è‡ªå®šä¹‰è°ƒç”¨ä»£ç   

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **è¡Œä¸ºå˜åŒ–**ï¼šå½“æœªè®¾ç½® `OPENAI_API_KEY` æ—¶ï¼Œä¹‹å‰ä¼šå‘é€ `Authorization: Bearer None`ï¼Œç°åœ¨å°†ä¸å†å‘é€è¯¥å¤´ï¼Œé¿å…å‘ OpenAI ç«¯ç‚¹æäº¤æ— æ•ˆå‡­è¯å¯¼è‡´ 401 é”™è¯¯ã€‚ä½¿ç”¨æ–¹è‹¥ä¾èµ–é»˜è®¤å‘é€ç©º tokenï¼Œéœ€è¦è‡ªè¡Œæ·»åŠ  `Authorization`ã€‚  
2. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤ CI/Test ä¸­å·²æœ‰è¦†ç›–æœªè®¾ç½® API key çš„åœºæ™¯ï¼Œè‹¥ç¼ºå¤±å¯è¡¥å……ç›¸åº”æµ‹è¯•ï¼Œç¡®ä¿åœ¨ç¼ºå¤±å‡­è¯æ—¶è¿”å›é¢„æœŸé”™è¯¯ä¿¡æ¯ã€‚  
3. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨ benchmark ä½¿ç”¨è¯´æ˜æˆ–ç¯å¢ƒå˜é‡è¯´æ˜ä¸­æ˜ç¡® â€œä»…åœ¨è®¾ç½® `OPENAI_API_KEY` æ—¶æ‰ä¼šè‡ªåŠ¨æ·»åŠ  Authorizationâ€ã€‚  
4. **åç»­æ‰©å±•**ï¼šå¦‚æœæœªæ¥æ”¯æŒå…¶ä»–è®¤è¯æ–¹å¼ï¼ˆå¦‚ Azure AD tokenï¼‰ï¼Œå»ºè®®å°† `_get_headers` æŠ½è±¡ä¸ºå¯æ’æ‹”çš„ Auth Providerï¼Œä»¥å…å†æ¬¡å‡ºç°ç¡¬ç¼–ç ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤ä¿®å¤æå‡äº†è¯·æ±‚çš„å¥å£®æ€§å’Œå®‰å…¨æ€§ï¼Œå½±å“èŒƒå›´å±€é™äºå†…éƒ¨ benchmark è°ƒç”¨ï¼Œé£é™©è¾ƒä½ï¼Œåªéœ€å…³æ³¨æœªè®¾ç½®å¯†é’¥æ—¶çš„é”™è¯¯æç¤ºå³å¯ã€‚

---

### [Models]: lfm2_siglip2 return intermediate encoder layers (#33370)
**SHA**: `302ecf6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/302ecf64ff22e6e2f59d108e95f0c66d96c9ffc0)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
- ä¸º `Siglip2VisionTransformer`/`Siglip2Model` å¢åŠ  `select_layers` å‚æ•°ï¼Œæ”¯æŒè¿”å›ä»»æ„ä¸­é—´ encoder å±‚çš„è¾“å‡ºï¼ˆå¯ä½¿ç”¨è´Ÿç´¢å¼•ï¼‰ã€‚  
- æ–°å¢ `num_hidden_layers_override`ï¼ˆå¯è£å‰ª encoder å±‚æ•°ï¼‰å’Œ `require_post_norm`ï¼ˆå¯å…³é—­æœ«å±‚ LayerNormï¼‰é€‰é¡¹ã€‚  
- `Siglip2Encoder` ç°åœ¨å¯åœ¨ `forward` ä¸­è¿”å›å…¨éƒ¨ hiddenâ€‘statesï¼›`resolve_visual_encoder_outputs` è´Ÿè´£ç»Ÿä¸€åå¤„ç†ã€‚  
- `load_weights` ä¸­åŠ å…¥å¯¹å¯é€‰ `post_layernorm`ã€å±‚æ•°è£å‰ªçš„å…¼å®¹é€»è¾‘ï¼Œé¿å…åŠ è½½å¤šä½™æƒé‡ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/model_executor/models/lfm2_siglip2.py`ï¼ˆVision ç¼–ç å™¨ã€æ¨¡å‹åŒ…è£…ã€æƒé‡åŠ è½½ï¼‰  
- å¯èƒ½å½±å“è°ƒç”¨ vision æ¨¡å‹çš„ä¸Šå±‚ä»£ç ï¼ˆå¦‚å¤šæ¨¡æ€èåˆã€ç‰¹å¾æå–æ’ä»¶ï¼‰ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å‘åå…¼å®¹**ï¼šé»˜è®¤è¡Œä¸ºä»è¿”å›æœ€åä¸€å±‚å¹¶æ‰§è¡Œ `post_layernorm`ï¼Œè¯·åœ¨å‘å¸ƒè¯´æ˜ä¸­æ˜ç¡®æ–°å‚æ•°çš„é»˜è®¤å€¼åŠå…¶å¯¹å·²æœ‰è°ƒç”¨çš„å½±å“ã€‚  
2. **æµ‹è¯•è¦†ç›–**ï¼šåŠ å…¥å•å…ƒæµ‹è¯•éªŒè¯ `select_layers` çš„æ­£ã€è´Ÿç´¢å¼•ã€åˆ—è¡¨è¿”å›ä»¥åŠ `num_hidden_layers_override` è£å‰ªæ—¶çš„æƒé‡åŠ è½½æƒ…å†µã€‚  
3. **æ–‡æ¡£æ›´æ–°**ï¼šåœ¨æ¨¡å‹ API æ–‡æ¡£ä¸­è¯´æ˜ `select_layers`ã€`num_hidden_layers_override`ã€`require_post_norm` çš„ç”¨é€”ã€å–å€¼èŒƒå›´åŠæ³¨æ„äº‹é¡¹ã€‚  
4. **æ€§èƒ½**ï¼šè¿”å›å…¨éƒ¨ hiddenâ€‘states ä¼šå¢åŠ æ˜¾å­˜å ç”¨ï¼Œå»ºè®®åœ¨æ–‡æ¡£ä¸­æé†’ç”¨æˆ·ä»…åœ¨éœ€è¦æ—¶æ‰“å¼€ `return_all_hidden_states`ã€‚  
5. **é‡åŒ–å…¼å®¹**ï¼šç¡®è®¤åœ¨é‡åŒ–æ¨¡å¼ä¸‹ï¼ˆ`QuantizationConfig`ï¼‰ä½¿ç”¨è£å‰ªå±‚æ•°æˆ–å…³é—­åå±‚å½’ä¸€åŒ–ä¸ä¼šå¯¼è‡´ shape ä¸åŒ¹é…ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæœ¬æ¬¡æ”¹åŠ¨ä¸ºè§†è§‰ç¼–ç å™¨æä¾›äº†æ›´çµæ´»çš„ç‰¹å¾æŠ½å–èƒ½åŠ›ï¼Œå½±å“èŒƒå›´é›†ä¸­åœ¨ vision å­æ¨¡å—ï¼Œæ³¨æ„å…¼å®¹æ€§å’Œæµ‹è¯•å³å¯å¹³æ»‘ä¸Šçº¿ã€‚

---

### [Bugfix] Fix inconsistent handling of cache reset (#33481)
**SHA**: `79b6ec6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/79b6ec6aab4b3d93e421a15ab41ba6d09faadd8f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
ç»Ÿä¸€äº† benchmark sweep ä¸­çš„ç¼“å­˜é‡ç½®æ–¹å¼ï¼Œæ–°å¢å¯¹ encoder ç¼“å­˜çš„æ¸…ç†ã€‚åŸæœ¬åªè°ƒç”¨ `/reset_prefix_cache`ã€`/reset_mm_cache`ï¼Œå¯¼è‡´åœ¨ä½¿ç”¨åŒ…å«è§†è§‰ç¼–ç å™¨çš„æ¨¡å‹æ—¶æ®‹ç•™æ—§çš„ Vision embeddingã€‚ç°åœ¨ `ServerProcess.VLLM_RESET_CACHE_ENDPOINTS` åŒ…å« `/reset_encoder_cache`ï¼Œ`reset_caches` å¾ªç¯è°ƒç”¨å…¨éƒ¨ `/reset_*_cache` æ¥å£ã€‚ç›¸å…³åç«¯å®ç°ä¹ŸåŒæ­¥æ·»åŠ  `reset_encoder_cache`ï¼Œå¹¶åœ¨ `pause_generation` ä¸­ä¸€å¹¶è°ƒç”¨ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/benchmarks/sweep/server.py`ï¼ˆbenchmark è¿›ç¨‹ï¼‰  
- `vllm/entrypoints/openai/engine/serving.py`ï¼ˆåˆ é™¤å†—ä½™ `reset_mm_cache` æ–¹æ³•ï¼‰  
- `vllm/v1/engine/async_llm.py`ï¼ˆæš‚åœæ—¶åŒæ­¥æ¸…ç† encoder ç¼“å­˜ï¼‰  
- `vllm/v1/worker/gpu/mm/encoder_runner.py`ã€`vllm/v1/worker/gpu/model_runner.py`ã€`vllm/v1/worker/gpu_model_runner.py`ï¼ˆå®ç°/è½¬å‘ `reset_encoder_cache`ï¼‰  
- æ–‡æ¡£ `docs/benchmarking/sweeps.md`ï¼ˆæè¿°æ›´æ–°ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **æœåŠ¡å™¨ç«¯**ï¼šç¡®è®¤å¯¹åº”çš„ `/reset_encoder_cache` è·¯ç”±å·²åœ¨æœåŠ¡å®ç°ä¸­æ³¨å†Œï¼Œå¦åˆ™ benchmark ä¼šå›  404 æŠ¥é”™ã€‚  
2. **å…¼å®¹æ€§**ï¼šæ—§ç‰ˆå®¢æˆ·ç«¯ä»å¯èƒ½åªè°ƒç”¨å‰ä¸¤ä¸ª endpointï¼Œå»ºè®®åœ¨æ–‡æ¡£æˆ–å‘å¸ƒè¯´æ˜ä¸­æé†’ç”¨æˆ·å‡çº§ã€‚  
3. **æµ‹è¯•**ï¼šæ–°å¢å•å…ƒ/é›†æˆæµ‹è¯•ï¼ŒéªŒè¯ `pause_generation` å encoder ç¼“å­˜è¢«æ¸…ç©ºï¼Œä»¥åŠå¤šæ¬¡ benchmark è¿è¡Œä¹‹é—´çŠ¶æ€ä¿æŒä¸€è‡´ã€‚  
4. **æ€§èƒ½**ï¼š`reset_encoder_cache` ç›®å‰ä»… `clear()`ï¼Œè‹¥ future å¼•å…¥æ›´å¤æ‚çš„ encoder å…±äº«æœºåˆ¶ï¼Œéœ€è¯„ä¼°æ¸…ç©ºä»£ä»·ã€‚  

æ€»ä½“è€Œè¨€ï¼Œå˜æ›´æ¶ˆé™¤äº†ç¼“å­˜ä¸ä¸€è‡´å¯¼è‡´çš„æ¨ç†é”™è¯¯ï¼Œä»£ç ç»“æ„æ›´ç»Ÿä¸€ï¼Œåç»­å¯åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥å®Œå–„å¤šæ¨¡æ€ç¼“å­˜ç®¡ç†ã€‚

---

### [Refactor] Make Renderer an abstract class (#33479)
**SHA**: `a358e4d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/a358e4dffe10ebff2c44434958bcc190ad2e5542)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆå°† Renderer ä» Protocol æ”¹ä¸ºæŠ½è±¡åŸºç±»ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. æ–°å¢ `BaseRenderer` æŠ½è±¡åŸºç±»ï¼Œç»Ÿä¸€ `from_config`ã€`__init__`ã€`tokenizer`ã€`render_*` ç­‰æ¥å£ã€‚  
2. æ‰€æœ‰å…·ä½“æ¸²æŸ“å™¨ï¼ˆDeepseekV32ã€Grok2ã€Hfã€Mistralã€Terratorch ç­‰ï¼‰æ”¹ä¸ºç»§æ‰¿ `BaseRenderer`ï¼Œå¹¶åœ¨æ„é€ å‡½æ•°é‡Œæ˜¾å¼è°ƒç”¨ `super().__init__(config)`ã€‚  
3. ç›¸å…³æ¨¡å—ï¼ˆengineã€protocolã€registryã€`__init__` å¯¼å‡ºï¼‰åŒæ­¥æ”¹ä¸ºä½¿ç”¨ `BaseRenderer` ç±»å‹ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/renderers/`ï¼ˆæ‰€æœ‰æ¸²æŸ“å™¨å®ç°ã€åè®®ã€æ³¨å†Œè¡¨ï¼‰  
- `vllm/engine/`ï¼ˆ`EngineClient`ã€`AsyncLLM`ã€`LLMEngine`ã€`InputProcessor`ï¼‰  
- `vllm/renderers/__init__.py`ï¼ˆå¯¼å‡ºåç§°ï¼‰  
- ä¾èµ–æ¸²æŸ“å™¨çš„æ’ä»¶æˆ–ç¬¬ä¸‰æ–¹æ‰©å±•ä»£ç ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼š  
   - ä»»ä½•è‡ªè¡Œå®ç°çš„æ¸²æŸ“å™¨ï¼ˆåœ¨é¡¹ç›®å¤–éƒ¨æˆ–æ’ä»¶ä¸­ï¼‰ç°åœ¨å¿…é¡»ç»§æ‰¿ `BaseRenderer` å¹¶å®ç° `tokenizer`ã€`render_completions`ã€`render_messages` ç­‰æŠ½è±¡æ–¹æ³•ã€‚è‹¥ä»ä½¿ç”¨æ—§çš„ `RendererLike` åè®®ï¼Œç±»å‹æ£€æŸ¥ä¼šæŠ¥é”™ã€‚å»ºè®®åœ¨æ–‡æ¡£ä¸­åŠ å…¥è¿ç§»æŒ‡å—æˆ–ä¿ç•™å‘åå…¼å®¹çš„åˆ«åï¼ˆå¦‚ `RendererLike = BaseRenderer`ï¼‰ä»¥é™ä½å‡çº§é£é™©ã€‚  

2. **åˆå§‹åŒ–é€»è¾‘**ï¼š  
   - `BaseRenderer.__init__` å·²ç»Ÿä¸€ä¿å­˜ `self.config` å¹¶å»¶è¿Ÿåˆ›å»º `_async_tokenizer`ã€‚æ‰€æœ‰æ¸²æŸ“å™¨å·²æ”¹ä¸º `super().__init__(config)`ï¼Œä½†ä»æœ‰å°‘é‡æ®‹ç•™ `self.config = config` æ³¨é‡Šæœªåˆ é™¤ï¼Œç¡®è®¤æ²¡æœ‰é—æ¼ã€‚  
   - `get_async_tokenizer` ä¸­çš„ç©ºå€¼æ£€æŸ¥å·²æ”¹ä¸º `if self._async_tokenizer is None:`ï¼Œç¡®ä¿é¦–æ¬¡è°ƒç”¨æ—¶æ­£å¸¸åˆ›å»ºã€‚  

3. **æ–‡æ¡£ä¸å¯¼å‡º**ï¼š  
   - `__all__` å·²æ”¹ä¸ºå¯¼å‡º `BaseRenderer`ï¼Œéœ€è¦åŒæ­¥æ›´æ–°å®˜æ–¹æ–‡æ¡£ã€ç¤ºä¾‹ä»£ç ä»¥åŠ Sphinx è‡ªåŠ¨ç”Ÿæˆçš„ APIã€‚  
   - æ£€æŸ¥ `setup.cfg`ã€`pyproject.toml` æ˜¯å¦ä»ä½¿ç”¨ `RendererLike` è¿›è¡Œ type hint ç”Ÿæˆï¼Œé˜²æ­¢æ„å»ºæŠ¥è­¦ã€‚  

4. **æµ‹è¯•è¦†ç›–**ï¼š  
   - è¿è¡Œå®Œæ•´çš„å•å…ƒ/é›†æˆæµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨ `EngineClient.renderer` çš„å±æ€§è®¿é—®ã€`AsyncLLM` çš„ tokenizer è°ƒç”¨ä»¥åŠæ¸²æŸ“å™¨æ³¨å†Œ/åŠ è½½è·¯å¾„ã€‚  
   - æ·»åŠ é’ˆå¯¹ `BaseRenderer` çš„æŠ½è±¡æ–¹æ³•æœªå®ç°æ—¶çš„é”™è¯¯æç¤ºæµ‹è¯•ï¼Œé˜²æ­¢æœªæ¥å­ç±»é—æ¼å®ç°ã€‚  

5. **æ€§èƒ½æ³¨æ„**ï¼š  
   - æ–°çš„æŠ½è±¡åŸºç±»å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§ `ABC`ï¼Œå¯¹è¿è¡Œæ—¶å¼€é”€å‡ ä¹æ²¡æœ‰å½±å“ï¼Œä½†ä»å»ºè®®åœ¨é«˜å¹¶å‘è·¯å¾„ï¼ˆå¦‚ `render_completions_async`ï¼‰ç¡®è®¤æ²¡æœ‰é¢å¤–çš„å±æ€§æŸ¥æ‰¾æˆ–åŒ…è£…å±‚å¯¼è‡´å¾®å°å»¶è¿Ÿã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº†æ¸²æŸ“å™¨çš„å¯ç»´æŠ¤æ€§å’Œç±»å‹å®‰å…¨ï¼Œåªè¦åœ¨è‡ªå®šä¹‰æ¸²æŸ“å™¨å’Œæ–‡æ¡£ä¸Šåšå¥½è¿ç§»æŒ‡å¼•ï¼Œé£é™©å¯æ§ã€‚å»ºè®®åœ¨ä¸‹ä¸€ä¸ªå‘å¸ƒå‘¨æœŸå‰å‘å¸ƒè¿ç§»æŒ‡å—å¹¶ç¡®ä¿æ‰€æœ‰ä¾èµ–åº“åŒæ­¥å‡çº§ã€‚

---

### [Misc] Fix flashinfer related tests (#33462)
**SHA**: `63c0889` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/63c0889416f0d4c3979c4046c6bf41c43143080c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / Bug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. å°†å¤šå¤„æµ‹è¯•ä¸­å¯¹éšæœºç§å­çš„è®¾ç½®ç»Ÿä¸€æ”¹ä¸º `set_random_seed`ï¼Œæå‡è·¨å¹³å°ä¸€è‡´æ€§ã€‚  
2. ä¸º FlashInferâ€‘NVFP4 ç›¸å…³æµ‹è¯•æ–°å¢ `cudnn` åç«¯å‚æ•°ï¼Œå¹¶åœ¨ `nvfp4_utils.py` ä¸ `flashinfer.py` ä¸­åŠ å…¥å¯¹ `FLASHINFER_CUDNN` çš„åˆ†æ”¯å¤„ç†ï¼Œä½¿å…¶åœ¨ `cutlass` ä¸ `cudnn` ä¸¤ç§å®ç°é—´åˆ‡æ¢ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `tests/kernels/*`ï¼ˆMOEã€é‡åŒ–ç›¸å…³ï¼‰  
- `vllm/model_executor/layers/quantization/utils/nvfp4_utils.py`  
- `vllm/utils/flashinfer.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- ç¡®è®¤ `set_random_seed` å·²åœ¨æµ‹è¯•ç¯å¢ƒå¯¼å…¥ï¼Œå¦åˆ™ä¼šå¯¼è‡´ ImportErrorã€‚  
- éªŒè¯ `NvFp4LinearBackend.FLASHINFER_CUDNN` åœ¨è¿è¡Œæ—¶å®é™…æ³¨å†Œï¼ˆå¯¹åº”çš„ CUDA/cuDNN å®ç°æ˜¯å¦å·²å®ç°ï¼‰ï¼Œé˜²æ­¢å‡ºç° â€œbackend not supportedâ€ çš„è¿è¡Œæ—¶å¼‚å¸¸ã€‚  
- è¿è¡Œå®Œæ•´æµ‹è¯•çŸ©é˜µï¼Œç‰¹åˆ«æ˜¯ `cuda` + `cudnn` ç»„åˆï¼Œç¡®ä¿æ–°åˆ†æ”¯ä¸ä¼šå½±å“å·²æœ‰çš„ `cutlass`/`trtllm` è¡Œä¸ºã€‚  
- å¦‚åœ¨æ–‡æ¡£æˆ–ç¤ºä¾‹ä¸­æåŠ FlashInfer åç«¯ï¼Œè¯·åŒæ­¥æ›´æ–°ï¼Œæ³¨æ˜æ–°å¢ `cudnn` é€‰é¡¹åŠå…¶é€‚ç”¨åœºæ™¯ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæœ¬æ¬¡æ”¹åŠ¨ä¸»è¦æ˜¯æ‰©å±• FlashInfer çš„åç«¯æ”¯æŒå¹¶ç»Ÿä¸€éšæœºç§å­è®¾ç½®ï¼Œå¯¹æ ¸å¿ƒåŠŸèƒ½å½±å“æœ‰é™ï¼Œä½†éœ€æ³¨æ„æ–°å¢åç«¯çš„å®ç°å®Œæ•´æ€§åŠç›¸åº”æµ‹è¯•è¦†ç›–ã€‚

---

### [Bugfix]: Fix display errors in TORCH_CHECK messages (#32942)
**SHA**: `fedf643` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/fedf64332e9940c8119f584a74b9c08bab6852b7)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBugä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šä¿®æ­£äº†å¤šä¸ª CPU å°çŸ©é˜µä¹˜ï¼ˆtinygemmï¼‰å®ç°ä»¥åŠ MOE ç›¸å…³ kernel ä¸­ `TORCH_CHECK` çš„é”™è¯¯ä¿¡æ¯æ‹¼å†™ï¼Œç¡®ä¿åœ¨å¼‚å¸¸å—å°ºå¯¸æ—¶æ‰“å°çœŸå®çš„ `nb_size`ã€‚å¦å¤–ï¼Œæ”¹æ­£äº† `marlin_moe_wna16` CUDA å®ç°ä¸­å¯¹ bias å¼ é‡ç»´åº¦çš„æ£€æŸ¥ï¼Œå°†é”™è¯¯çš„ `size(0)` ä¿®æ­£ä¸º `size(1)`ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `csrc/cpu/sglâ€‘kernels/*.cpp`ï¼ˆgemmã€gemm_fp8ã€gemm_int8ã€moeã€moe_int8ï¼‰  
- `csrc/moe/marlin_moe_wna16/ops.cu`ï¼ˆbias ç»´åº¦æ£€æŸ¥ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
1. **é”™è¯¯ä¿¡æ¯å¯é æ€§**ï¼šä½¿ç”¨çœŸå®å˜é‡ `nb_size` åï¼Œå¼‚å¸¸æç¤ºæ›´æ˜“å®šä½ï¼Œå»ºè®®åœ¨ CI ä¸­åŠ å…¥è§¦å‘è·¯å¾„çš„å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ `TORCH_CHECK` ç¡®å®æŠ›å‡ºæœŸæœ›ä¿¡æ¯ã€‚  
2. **å›å½’æµ‹è¯•**ï¼šè¿™äº›æ–‡ä»¶æ¶‰åŠ CPUã€GPU åŒç«¯å®ç°ï¼Œç¡®ä¿æ‰€æœ‰å¹³å°çš„ç¼–è¯‘å’Œè¿è¡Œæµ‹è¯•ä»ç„¶é€šè¿‡ï¼Œå°¤å…¶æ˜¯ `tinygemm` çš„ä¸åŒå—å¤§å°ç»„åˆã€‚  
3. **ä»£ç é£æ ¼**ï¼šè‹¥ä»¥åè¿˜æœ‰ç±»ä¼¼æ‹¼æ¥å­—ç¬¦ä¸²çš„æ£€æŸ¥ï¼Œè€ƒè™‘ä½¿ç”¨ `c10::str` æˆ– `fmt::format` ç»Ÿä¸€æ„é€ ä¿¡æ¯ï¼Œé¿å…å†æ¬¡å†™æˆå­—é¢é‡ã€‚  
4. **æ–‡æ¡£/æ³¨é‡Š**ï¼šé€‚å½“åœ¨å…³é”® `default` åˆ†æ”¯æ·»åŠ æ³¨é‡Šï¼Œè¯´æ˜è¯¥åˆ†æ”¯ä»…åœ¨å†…éƒ¨é”™è¯¯è·¯å¾„è§¦å‘ï¼Œå¸®åŠ©åç»­ç»´æŠ¤è€…å¿«é€Ÿå®šä½ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡ä¿®æ”¹ä»…æ¶‰åŠé”™è¯¯æç¤ºå±‚é¢ï¼Œä¸ä¼šå½±å“åŠŸèƒ½æˆ–æ€§èƒ½ï¼Œé£é™©ä½ï¼Œä½†å»ºè®®è¡¥å……ç›¸åº”çš„å¼‚å¸¸è·¯å¾„æµ‹è¯•ä»¥ç¡®ä¿ä¿¡æ¯å®Œæ•´ã€‚

---

### [Refactor] Move MM data parsing outside processor (#33408)
**SHA**: `88c3e11` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/88c3e114d8f564d1e88ff2ef5be56a979792e90b)

**æ ¸å¿ƒå˜æ›´æ¦‚è¿°**  
æœ¬æ¬¡ PR å°†å¤šæ¨¡æ€æ•°æ®è§£æä» `BaseMultiModalProcessor` å†…éƒ¨è¿ç§»åˆ°ç»Ÿä¸€çš„ `ProcessingInfo`ï¼ˆ`info`ï¼‰å±‚ï¼Œæ–°å¢ `info.parse_mm_data` æ–¹æ³•ï¼Œå¹¶æŠŠåŸå…ˆ `apply(..., mm_data=â€¦)` æ¥å£ç»Ÿä¸€æ”¹ä¸º `apply(..., mm_items=â€¦)`ã€‚åŒæ—¶åœ¨ `ProcessorInputs`ã€å†…éƒ¨ç¼“å­˜ã€ä¼ªè¾“å…¥ç”Ÿæˆã€æ¨¡å‹ä¸“ç”¨å¤„ç†å™¨ç­‰å¤§é‡ä½ç½®åˆ‡æ¢åˆ° `mm_items`ï¼Œå¹¶åœ¨è§£æé˜¶æ®µåŠ å…¥äº†å¯é€‰çš„æ ¡éªŒï¼ˆ`validate` å‚æ•°ï¼‰ä»¥åŠå¯¹ `--enable-mmâ€‘embeds` æ ‡å¿—çš„ç»Ÿä¸€æ£€æŸ¥ã€‚

**å½±å“èŒƒå›´**  
- **API å±‚é¢**ï¼š`apply`ã€`_to_mm_items`ã€`_parse_mm_items` ç­‰æ‰€æœ‰å…¥å£å‡æ”¹ä¸ºæ¥å— `MultiModalDataItems`ï¼Œæ—§çš„ `mm_data` å‚æ•°å·²è¢«åˆ é™¤æˆ–æ ‡è®°ä¸ºå†…éƒ¨å®ç°ï¼Œç›´æ¥è°ƒç”¨ä¼šè§¦å‘ `TypeError`ã€‚  
- **å†…éƒ¨å®ç°**ï¼š`BaseMultiModalProcessor`ã€å„æ¨¡å‹å…·ä½“å®ç°ï¼ˆ`clip`, `siglip`, `llava`, `paligemma` ç­‰ï¼‰ä»¥åŠ `inputs/preprocess.py`ã€`engine/input_processor.py` å‡ä½¿ç”¨æ–°è§£æè·¯å¾„ã€‚  
- **æµ‹è¯•/å·¥å…·é“¾**ï¼šæµ‹è¯•æ–‡ä»¶å…¨éƒ¨æ”¹ä¸ºå…ˆè°ƒç”¨ `processor.info.parse_mm_data` å†ä¼ å…¥ `apply`ï¼Œç¡®ä¿å‘åå…¼å®¹ã€‚  
- **æ€§èƒ½ä¸å¯é æ€§**ï¼šè§£æé€»è¾‘é›†ä¸­åˆ° `ProcessingInfo`ï¼Œå¯åœ¨ä¸€æ¬¡è§£æåå¤ç”¨ `mm_items`ï¼Œé¿å…å¤šæ¬¡é‡å¤è§£æï¼›ç»Ÿä¸€çš„æ ¡éªŒæå‡é”™è¯¯æç¤ºä¸€è‡´æ€§ã€‚

**æ½œåœ¨é£é™©**  
1. **å‘åå…¼å®¹æ€§**ï¼šç”¨æˆ·ä»£ç ã€ç¬¬ä¸‰æ–¹æ’ä»¶ä»å¯èƒ½ä½¿ç”¨æ—§çš„ `processor.apply(prompt, mm_data, â€¦)`ï¼Œä¼šç›´æ¥æŠ¥é”™ã€‚  
2. **ç±»å‹å£°æ˜**ï¼š`MultiModalDataItems` ä¸æ—§ `MultiModalDataDict` çš„åŒºåˆ«åœ¨æ–‡æ¡£å’Œ IDE æç¤ºä¸Šéœ€è¦åŒæ­¥ï¼Œé˜²æ­¢è¯¯ç”¨ã€‚  
3. **ç¼“å­˜é€»è¾‘**ï¼š`_get_cache_missing_items` ç°åœ¨å…ˆç”Ÿæˆç¼ºå¤±çš„åŸå§‹ dict å†è°ƒç”¨ `info.parse_mm_data`ï¼Œè‹¥ `validate=False` è¢«è¯¯åˆ ï¼Œå¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„æ ¡éªŒå¼€é”€ã€‚  
4. **å¹¶å‘/è£…é¥°å™¨**ï¼š`cached_property` å¢åŠ äº† `data_parser` çš„æƒ°æ€§åˆ›å»ºï¼Œéœ€ç¡®è®¤åœ¨å¤šè¿›ç¨‹/å¤šçº¿ç¨‹å¯åŠ¨æ—¶ä¸ä¼šå‡ºç°é‡å¤å®ä¾‹åŒ–æˆ–çŠ¶æ€æ³„éœ²ã€‚

**å»ºè®®æªæ–½**  
- **æ–‡æ¡£å‡çº§**ï¼šåœ¨ READMEã€API å‚è€ƒå’Œè¿ç§»æŒ‡å—ä¸­æ˜ç¡®æ ‡æ³¨ `mm_data` å·²åºŸå¼ƒï¼Œæ¨èä½¿ç”¨ `mm_items`ï¼Œå¹¶æä¾›ç¤ºä¾‹ `processor.info.parse_mm_data(mm_data)`ã€‚  
- **å…¼å®¹å±‚**ï¼šå¯åœ¨ `BaseMultiModalProcessor.apply` ä¸­åŠ å…¥ä¸´æ—¶å…¼å®¹ç­¾åï¼ˆæ¥å— `mm_data` å¹¶å†…éƒ¨è½¬å‘ï¼‰ï¼Œå¹¶åœ¨ 1â€‘2 ç‰ˆæœ¬åç§»é™¤ï¼Œç»™ç”¨æˆ·ç•™å‡ºè¿ç§»çª—å£ã€‚  
- **ç±»å‹æç¤º**ï¼šåœ¨ `vllm/multimodal/processing/context.py`ã€`processor.py` ç­‰æ–‡ä»¶çš„å‡½æ•°ç­¾åä¸ŠåŠ å…¥ `# type: ignore[override]` æˆ– `typing.overload`ï¼Œè®©é™æ€æ£€æŸ¥å·¥å…·èƒ½æ„ŸçŸ¥åŒç­¾åã€‚  
- **æ€§èƒ½å›å½’æµ‹è¯•**ï¼šæ–°å¢åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯”å•æ¬¡è§£æ vs æ—§æ¨¡å‹å¤šæ¬¡è§£æçš„æ—¶é—´å·®ï¼Œç¡®ä¿ä¸ä¼šå› é¢å¤–åŒ…è£…å¯¼è‡´æ˜¾è‘—å¼€é”€ã€‚  
- **å¼‚å¸¸ä¿¡æ¯**ï¼šåœ¨ `info.parse_mm_data` çš„æ ¡éªŒåˆ†æ”¯åŠ å…¥æ˜ç¡®çš„é”™è¯¯ç æˆ–ä¿¡æ¯ï¼ˆå¦‚ `enable_mm_embeds` æœªå¼€å¯ï¼‰ï¼Œä¾¿äºè°ƒè¯•ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡é‡æ„æå‡äº†ä»£ç ç»„ç»‡å’Œè§£æå¤ç”¨æ€§ï¼Œä½†éœ€è¦åšå¥½å…¼å®¹å’Œæ–‡æ¡£çš„åŒæ­¥ï¼Œä»¥å…å½±å“å·²æœ‰ä½¿ç”¨è€…ã€‚

---

### [Deprecation] Remove deprecated items related to pooling (#33477)
**SHA**: `92924b2` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/92924b2dddebc6dc80b3fff6c84ff095530b8e58)

**ğŸ”§ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / ä»£ç æ¸…ç†ï¼ˆDeprecationï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤å›´ç»• **Pooling** ç›¸å…³çš„æ—§å‚æ•°ä¸é€‰é¡¹è¿›è¡Œå…¨é¢å»é™¤å’Œç»Ÿä¸€ï¼š  
1. åˆ é™¤ `ConvertType` ä¸­çš„ `reward` ä¸ `mm_encoder_only`ï¼Œå¹¶æ”¹ä¸ºåœ¨æ¨¡å‹å±‚é¢é€šè¿‡ `--mm-encoder-only` æ§åˆ¶ã€‚  
2. ç§»é™¤ `PoolingParams`ã€`PoolerConfig` ä¸ API ä¸­çš„ `softmax`ã€`activation`ã€`normalize` ç­‰å†—ä½™å­—æ®µï¼Œç»Ÿä¸€ä½¿ç”¨ `use_activation` è¡¨ç¤ºæ˜¯å¦åœ¨æ± åŒ–ååŠ å…¥æ¿€æ´»ã€‚  
3. ç›¸åº”åœ°åœ¨æ‰€æœ‰å…¥å£åè®®ï¼ˆembeddingã€classifyã€scoreã€completionï¼‰ä¸­å»æ‰æ—§å­—æ®µçš„è§£æé€»è¾‘ï¼Œæ”¹ä¸ºç›´æ¥è¯»å– `use_activation`ï¼Œå¹¶åœ¨ä»ä¿ç•™ `normalize` æ—¶ç»™å‡ºä¸€æ¬¡æ€§è­¦å‘Šã€‚  
4. æ–‡æ¡£åŒæ­¥æ›´æ–°ï¼Œæé†’ç”¨æˆ·æ”¹ç”¨ `--convert embed` ä¸ `use_activation`ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **æ ¸å¿ƒé…ç½®**ï¼š`vllm/config/model.py`ã€`vllm/config/pooler.py`ã€`vllm/pooling_params.py`ã€‚  
- **å…¥å£åè®®**ï¼š`vllm/entrypoints/pooling/*/protocol.py`ï¼ˆåŒ…æ‹¬ embedã€classifyã€scoreã€completionï¼‰ã€‚  
- **æ¨¡å‹æ‰§è¡Œ**ï¼š`vllm/model_executor/models/config.py` å¯¹å¥–åŠ±æ¨¡å‹çš„é»˜è®¤è¡Œä¸ºåšäº†é€‚é…ã€‚  
- **æ–‡æ¡£**ï¼š`docs/models/pooling_models.md`ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å‡çº§å…¼å®¹**ï¼šè‹¥ç”¨æˆ·ä»£ç æˆ–è„šæœ¬ä¸­ä»ä½¿ç”¨ `softmax`ã€`activation`ã€`normalize`ã€`--convert reward` ç­‰æ—§å­—æ®µ/é€‰é¡¹ï¼Œéœ€è¦å°½å¿«è¿ç§»è‡³ `use_activation` ä¸ `--convert embed`ã€‚é¦–æ¬¡è¿è¡Œæ—¶ä¼šè§¦å‘ä¸€æ¬¡æ€§ `warning_once`ï¼Œä½†åœ¨ v0.15+ å°†ç›´æ¥æŠ¥é”™ã€‚  
2. **æµ‹è¯•è¦†ç›–**ï¼šç¡®è®¤æ‰€æœ‰è‡ªå®šä¹‰æ± åŒ–æ¨¡å‹åœ¨ä¸æ˜¾å¼æŒ‡å®š `use_activation` æ—¶ä»éµå¾ªé»˜è®¤ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹ä¸º `True`ï¼‰ï¼Œé¿å…åŠŸèƒ½å›å½’ã€‚  
3. **ä¾èµ–æ£€æŸ¥**ï¼šç¬¬ä¸‰æ–¹æ’ä»¶æˆ–æ—§ç‰ˆæœ¬çš„ OpenAIâ€‘compatible å®¢æˆ·ç«¯å¯èƒ½ä»å‘é€è¿‡æ—¶å­—æ®µï¼Œå»ºè®®åœ¨å…¥å£å±‚é¢åŠ å…¥å…¼å®¹å±‚æˆ–æ˜ç¡®æ–‡æ¡£æç¤ºã€‚  
4. **æ–‡æ¡£ä¸ç¤ºä¾‹æ›´æ–°**ï¼šç¡®ä¿ç¤ºä¾‹ä»£ç ã€CI é…ç½®å’Œ README ä¸­çš„å‚æ•°è¯´æ˜å…¨éƒ¨åŒæ­¥ä¸º `use_activation`ï¼Œä»¥å…ç”¨æˆ·è¯¯ç”¨ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ¸…ç†ä¸ä¼šæ”¹å˜æ¨¡å‹çš„ç®—å­å®ç°ï¼Œä»…æ˜¯å‚æ•°åçš„ç»Ÿä¸€ä¸åºŸé™¤ï¼Œé£é™©æœ‰é™ã€‚ä½†åœ¨å‡çº§é“¾è·¯ä¸Šéœ€æ³¨æ„å‚æ•°è¿ç§»ï¼Œé¿å…å› æ—§å­—æ®µè¢«ç›´æ¥åˆ é™¤è€Œå¯¼è‡´å¯åŠ¨é”™è¯¯ã€‚  

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (9)

### [BUGFIX] Fix hipErrorIllegalState in Qwen3-Omni during startup profiling allow inference Omni on ROCM (#33077)
**SHA**: `cd86fff` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/cd86fff38feed579a22768ac9f9464360a0819fe)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ Qwen3â€‘Omni çš„å‰å‘è®¡ç®—ä¸­ï¼Œä¸º ROCm (gfx11) ä¸Š `torch.repeat_interleave` ä¼šå¯¼è‡´ kernel å´©æºƒçš„é—®é¢˜æ·»åŠ äº†å¼‚å¸¸æ•è·ï¼Œè‹¥å¤±è´¥åˆ™æ”¹ç”¨å‘é‡åŒ–çš„ `cumsum + searchsorted` å®ç°ï¼Œå¹¶è®°å½•è­¦å‘Šï¼Œä»è€Œä¿®å¤å¯åŠ¨ profiling æ—¶çš„ `hipErrorIllegalState`ã€‚

---

### Change defaults for vllm bench startup (#33489)
**SHA**: `6720238` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/672023877bfd17a8720ae05ed198456d9b454363)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `benchmarks/startup.py` ä¸­å¯åŠ¨åŸºå‡†çš„é»˜è®¤è¿­ä»£æ¬¡æ•°ä¸‹è°ƒï¼šå†·å¯åŠ¨é»˜è®¤ 5â†’3ï¼Œçƒ­èº«é»˜è®¤ 3â†’1ï¼Œçƒ­å¯åŠ¨é»˜è®¤ 5â†’3ã€‚

---

### [Critical] Revert #33110 (#33500)
**SHA**: `b6bb284` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/b6bb2842cf30251951ccf6d22f808a9a2884d96e)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ’¤é”€æ­¤å‰å¼•å…¥çš„å¤šæ¨¡æ€ç¼–ç å™¨ç¼“å­˜é¢„ç®—è®¡ç®—åŠå…¶åœ¨ `InputProcessor` ä¸­çš„æ ¡éªŒé€»è¾‘ï¼Œåˆ é™¤ç›¸å…³å¯¼å…¥ã€å­—æ®µ `mm_encoder_cache_size` ä»¥åŠè¶…å‡ºç¼“å­˜å¤§å°çš„æ£€æŸ¥ä»£ç ã€‚è¿™æ ·ç®€åŒ–äº†è¾“å…¥å¤„ç†æµç¨‹ï¼Œé¿å…ä¸å¿…è¦çš„ç¼“å­˜é™åˆ¶ã€‚

---

### pin LMCache to v0.3.9 or greater with vLLM v0.15.0 (#33440)
**SHA**: `d6416fd` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d6416fdde93476afc51a51e391a9b44d2c25df2f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `requirements/kv_connectors.txt` ä¸­å°† `lmcache` çš„ä¾èµ–é™å®šä¸º `>= 0.3.9`ï¼Œç¡®ä¿ä¸ vLLMâ€¯0.15.0 å…¼å®¹ã€‚

---

### [ROCm][CI] Update huggingface-hub pin (#33492)
**SHA**: `0fb3157` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/0fb3157267127a69667f5c9b92d030c0de10614a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `requirements/rocm-test.txt` ä¸­æ–°å¢ `huggingface-hub==0.36.0` çš„ç‰ˆæœ¬é”å®šï¼Œç¡®ä¿ ROCm CI ç¯å¢ƒä½¿ç”¨æŒ‡å®šçš„ HF Hub ç‰ˆæœ¬ã€‚

---

### Fix grammar (#33121)
**SHA**: `1e86c80` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/1e86c802d473b4ca4e889a1b2585a05aad7220ba)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `cpu_worker.py` ä¸­å°†é”™è¯¯æç¤ºæ–‡å­— â€œNo enough allowed NUMA nodes â€¦â€ ä¿®æ­£ä¸ºè¯­æ³•æ­£ç¡®çš„ â€œNot enough allowed NUMA nodes â€¦â€ã€‚

---

### [Misc] support collect_env for endpoint /server_info (#33246)
**SHA**: `2238a12` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/2238a12c13b9a1e27f132473a0fff1b4b233b1dc)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `server_info` æ¥å£ä¸­æ–°å¢å¯¹ç³»ç»Ÿç¯å¢ƒä¿¡æ¯çš„æ”¶é›†ï¼Œä½¿ç”¨ `vllm.collect_env.get_env_info` å¹¶é€šè¿‡ `functools.lru_cache` ç¼“å­˜ï¼Œé‡‡ç”¨ `asyncio.to_thread` å¼‚æ­¥æ‰§è¡Œï¼Œä»¥æä¾›æ›´å®Œæ•´çš„æœåŠ¡å™¨ä¿¡æ¯ã€‚

---

### Update `huggingface-hub` pin for the last time before Transformers v5 (#33473)
**SHA**: `ce0afe2` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/ce0afe2451b0f8a04547514f9efcac98e39beab0)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `requirements/test.txt` ä¸­çš„ `huggingface-hub` ç‰ˆæœ¬å›ºå®šä» **0.34.3** æ›´æ–°è‡³ **0.36.0**ï¼Œä¸ºå³å°†å‘å¸ƒçš„ Transformers v5 åšå‡†å¤‡ã€‚

---

### [Bugfix] Early-reject requests with MM data longer than encode cache capacity (#33110)
**SHA**: `27cb2f6` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/27cb2f678f7567b14a13a2f7e085137dca1a4129)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `input_processor.py` ä¸­æ–°å¢å¯¹å¤šæ¨¡æ€è¾“å…¥çš„ encoder ç¼“å­˜å®¹é‡æ£€æŸ¥ï¼Œä½¿ç”¨ `compute_mm_encoder_budget` è®¡ç®—å¹¶åœ¨è¯·æ±‚æ—©æœŸè¿›è¡Œé•¿åº¦æ ¡éªŒï¼Œè¶…å‡ºå®¹é‡æ—¶æŠ›å‡ºæ˜ç¡®é”™è¯¯ã€‚

---

