# 每日更新报告（2026-02-17）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-17 22:46:23 | almayne | Fixed whisper CPU test that does not spawn properly. (#34324) |
| 2026-02-17 21:35:40 | Nicolò Lucchesi | [CI][Nixl] Add CrossLayer KV layout tests (#34615) |
| 2026-02-17 21:29:01 | Cyrus Leung | [Renderer] Move InputPreprocessor into Renderer (2/2) (#34560) |
| 2026-02-17 20:22:56 | junuxyz | [CI][BugFix] ShellCheck cleanup to remove baseline and preserve runtime behavior (#34514) |
| 2026-02-17 18:31:40 | Harry Mellor | Fix docs build warning (#34686) |
| 2026-02-17 18:06:53 | ChenqianCao | [Bugfix] Fix 'remove_instance_endpoint' method logic in disagg_proxy_demo (#32922) |
| 2026-02-17 17:49:14 | Tim Dettmers | Remove dead bitsandbytes CxB code from 8-bit inference path (#34633) |
| 2026-02-17 17:29:27 | Jiangyun Zhu | Revert "[Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj" (#34683) |
| 2026-02-17 17:08:42 | kourosh hakhamaneshi | [Ray] Propagate third-party env vars to Ray workers via prefix matching (#34383) |
| 2026-02-17 14:39:44 | Amr Mahdi | [CI] Fix bake config artifact path for AMI rebuild pipeline (#34656) |
| 2026-02-17 13:43:00 | Woosuk Kwon | [Model Runner V2] Minor refactoring for penalties (#34662) |
| 2026-02-17 13:27:24 | Woosuk Kwon | [Model Runner V2] Minor simplification for BadWordsState (#34669) |
| 2026-02-17 11:15:31 | Woosuk Kwon | [Model Runner V2] Minor cleanup for PP (#34666) |
| 2026-02-17 11:00:29 | Woosuk Kwon | [Model Runner V2] Fix unintended CPU-GPU sync in make_dummy (#34667) |
| 2026-02-17 09:58:49 | haosdent | [Bugfix] Fix fused MoE int32 overflow in stride*offset without perf regression (#34507) |
| 2026-02-17 09:58:15 | Aneesh Puttur | [CI] Enable mypy import following for vllm/v1/kv_offload (#34639) |
| 2026-02-17 09:48:16 | zhanqiuhu | [Core] Pipeline Parallel support for Model Runner V2 (#33960) |
| 2026-02-17 08:36:06 | zhrrr | [Model Runner V2] support bad_words sampling param (#33433) |
| 2026-02-17 02:15:32 | roikoren755 | [NemotronH] Do not force router to run in fp32 (#34582) |
| 2026-02-17 01:02:27 | Alexei-V-Ivanov-AMD | Targeting the MI355 agent pool with all existing tests (#34629) |
| 2026-02-17 00:11:07 | Nicolò Lucchesi | [Bugfix][CI] Fix flaky `entrypoints/openai/test_response_api_with_harmony.py::test_function_calling[openai/gpt-oss-20b]` (#34624) |

### 📊 统计摘要
> 本日共 21 个提交 | 🔴高 2 | 🟡中 8 | 🟢低 11
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[Renderer] Move InputPreprocessor into Renderer (2/2) (#3...](#574fe75)
    - [[CI][BugFix] ShellCheck cleanup to remove baseline and pr...](#c61a98f)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [Revert "[Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj...](#1d65283)
    - [[Ray] Propagate third-party env vars to Ray workers via p...](#c464b57)
    - [[Model Runner V2] Minor refactoring for penalties (#34662)](#d00df62)
    - [[Model Runner V2] Minor cleanup for PP (#34666)](#04925b2)
    - [[Bugfix] Fix fused MoE int32 overflow in stride*offset wi...](#b68fd89)
    - [[Core] Pipeline Parallel support for Model Runner V2 (#33...](#9a8853f)
    - [[Model Runner V2] support bad_words sampling param (#33433)](#387a189)
    - [Targeting the MI355 agent pool with all existing tests (#...](#824f9e8)
  - [🟢 低重要度变更 (11)](#-🟢-低重要度变更-11)
    - [Fixed whisper CPU test that does not spawn properly. (#34...](#6bd6d0c)
    - [[CI][Nixl] Add CrossLayer KV layout tests (#34615)](#8e962fe)
    - [Fix docs build warning (#34686)](#28bffe9)
    - [[Bugfix] Fix 'remove_instance_endpoint' method logic in d...](#ad65177)
    - [Remove dead bitsandbytes CxB code from 8-bit inference pa...](#d44a5b6)
    - [[CI] Fix bake config artifact path for AMI rebuild pipeli...](#c5c38e1)
    - [[Model Runner V2] Minor simplification for BadWordsState ...](#9752da9)
    - [[Model Runner V2] Fix unintended CPU-GPU sync in make_dum...](#d74278f)
    - [[CI] Enable mypy import following for vllm/v1/kv_offload ...](#0b5f9b7)
    - [[NemotronH] Do not force router to run in fp32 (#34582)](#3b30e61)
    - [[Bugfix][CI] Fix flaky `entrypoints/openai/test_response_...](#6cc403e)
#### 🔴 高重要度变更 (2)

### [Renderer] Move InputPreprocessor into Renderer (2/2) (#34560)
**SHA**: `574fe75` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/574fe75245fdadfa61b9c00b24dc84177540e3a5)

**🎯 变更类型**：重构 / 架构变更  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 将原本位于 `vllm/v1/engine/input_processor.py` 的 **InputPreprocessor** 完全迁入渲染器 (`BaseRenderer` 及子类)。  
- 引入统一的内部数据结构 **ProcessorInputs**（取代之前的 `DictPrompt`/`TokPrompt`），并在渲染器层完成 **tokenization、detokenization、multi‑modal 预处理**。  
- 大幅度重写 `vllm/engine`, `vllm/entrypoints/*`, `vllm/beam_search.py` 以及相关测试，使其直接使用 `ProcessorInputs`。  
- 新增 `vllm/utils/tqdm_utils.py` 用于在渲染阶段可选的进度条。  
- 为多模态 UUID 处理、缓存统计、时间戳记录等功能加入统一实现。  

**🎯 影响范围**  
- **核心渲染层**：`vllm/renderers/base.py`、`vllm/renderers/hf.py`、`vllm/renderers/inputs/*`。  
- **引擎层**：`vllm/v1/engine/async_llm.py`、`vllm/v1/engine/input_processor.py`、`vllm/v1/engine/llm_engine.py`、`vllm/beam_search.py`。  
- **API 入口**：OpenAI‑compatible serving (`completion`, `chat_completion`, `speech_to_text` 等），`vllm/entrypoints/serve/*`。  
- **多模态子系统**：`vllm/multimodal/*` 相关的 UUID、缓存统计逻辑。  
- **测试套件**：新增/重写大量测试，删除旧的 v1 `test_process_multi_modal_uuids.py`。  

**🔍 技术洞察**  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • **渲染器成为唯一入口**：所有输入的 token 化、detokenization、以及 multimodal 处理均在渲染器内部完成，取消了 `InputPreprocessor` 在引擎层的独立存在。<br>• **统一类型 `ProcessorInputs`**：对外统一接受 `ProcessorInputs`（DecoderOnly / EncoderDecoder），降低了不同模块之间的类型转换成本。<br>• **请求 ID 与 MM UUID 分离**：渲染器内部生成的 `_mm_req_counter` 用于生成内部的 multi‑modal request ID，避免用户必须手动构造 UUID。 |
| **性能影响** | • **一次性渲染**：原先在 `LLMEngine` 前需要 `InputPreprocessor → Renderer → EngineCore` 的两轮处理，现在合并为 **Renderer → EngineCore**，理论上减少一次深拷贝和一次 `set_default_torch_num_threads` 的上下文切换。<br>• **并行渲染**：`maybe_tqdm` 允许在渲染阶段使用生成器实时渲染，提前启动引擎执行，提升首批请求的 **latency**。<br>• **缓存统计**：`BaseRenderer` 现在在每次 multi‑modal 处理后调用 `update_mm_cache_stats()`，对缓存命中率的监控更及时，但也会在极端高并发场景下产生轻微的锁竞争（`AtomicCounter`、`MultiModalCacheStats`）。 |
| **安全考虑** | • 变更仅在内部 pipeline 中重组，没有新增外部网络交互或文件 I/O，安全风险极低。<br>• 新增 `arrival_time` 字段（可选）在序列化时存入 `ProcessorInputs`，如果用户自行序列化并跨进程传递，需要确保 **时间戳不泄露敏感业务信息**（非强制要求）。 |
| **可维护性** | • **接口统一**：所有上层 API（`LLM.generate`, `LLM.encode`, OpenAI serving 等）只需构造 `ProcessorInputs`，不必关心内部的 `DictPrompt`/`TokPrompt`。<br>• **代码体量下降**：删除了 `InputPreprocessor` 相关的 150+ 行代码，删除了重复的 UUID 验证函数。<br>• **迁移成本**：对外仍保留 `PromptType`（原始用户输入）可直接传入，内部会在渲染阶段转为 `ProcessorInputs`，兼容性保持。<br>• **测试覆盖**：新增 `tests/renderers/test_process_multi_modal_uuids.py` 替代旧的 v1 测试，实现同等逻辑的单元测试，保证功能不回退。 |
| **兼容性** | • `vllm/entrypoints/*` 中仍接受旧的 `PromptType`、`TokPrompt` 等对象，但在内部统一调用 `Renderer.render_*`，返回 `ProcessorInputs`。<br>• 对于 **直接依赖 `InputPreprocessor`** 的第三方插件或自定义 `IOProcessor`，需要更新导入路径（已在 `vllm/platforms/interface.py` 中改为 `ProcessorInputs`），否则会出现 `ImportError`。 |
| **错误处理** | • 将原本在 `InputPreprocessor` 的异常（如多模态数据长度不匹配）搬到渲染器的 `_process_mm_uuids`，保持原有 `ValueError` 抛出语义。<br>• 在 `AsyncLLM.add_request` 中新增 `reasoning_ended` 参数的向下传递，保持对 `ReasoningParser` 的支持。 |

**⚠️ 潜在风险**  

1. **向后兼容破坏**  
   - 第三方代码直接导入 `vllm.renderers.inputs.DictPrompt` / `TokPrompt` 或 `vllm.v1.engine.input_processor.InputPreprocessor` 将在运行时失效。  
2. **多模态 UUID 逻辑回归**  
   - 新的内部 UUID 生成在 **禁用前缀缓存+MM cache=0** 时覆盖用户提供的 UUID，若用户依赖自定义 UUID 做后处理（如外部缓存），可能导致行为不一致。  
3. **并发计数器冲突**  
   - `AtomicCounter` 在极端高并发（>10⁶ 请求/秒）下可能出现内部 Python 整数溢出或锁争用，导致渲染延迟。  
4. **`arrival_time` 字段未被清理**  
   - 当 `ProcessorInputs` 被序列化发送到远端（如在分布式部署）时，未使用的 `arrival_time` 仍会随对象传输，增加网络负载。  
5. **BeamSearch 迁移**  
   - 对 `BeamSearchSequence` 的 `orig_prompt` 引入后，旧的 `multi_modal_data`/`mm_processor_kwargs` 字段被删除，若外部代码仍依赖这些字段（如自定义 beams），会抛 `AttributeError`。  
6. **性能回退风险**  
   - 在 **多模态大批量** 场景下，渲染器一次性完成所有 multimodal 处理，内部 `set_default_torch_num_threads` 的调用频率提升，可能导致 **线程竞争**，导致吞吐量下降。  

**💡 关注建议**  

| 对象 | 建议 |
|------|------|
| **库维护者** | • 在发布 **次要版本**（例如 `v0.6.x → v0.7.0`）时，提供 **迁移指南**，列出已删除的导入路径与对应新 API。<br>• 将 `InputPreprocessor` 设为 **deprecated stub**（仅抛出明确的迁移错误），避免用户因导入失败而不知所措。 |
| **测试/CI** | • 在 CI 中

---

### [CI][BugFix] ShellCheck cleanup to remove baseline and preserve runtime behavior (#34514)
**SHA**: `c61a98f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c61a98f52993550e42d1bded121341fb9afb2ddf)

**🎯 变更类型**：重构 / Bug修复（ShellCheck 警告清理、变量安全引用、脚本行为保持不变）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交对 `.buildkite` 以及多个示例、基准、测试、工具脚本进行全面的 ShellCheck 清理：统一使用双引号包裹变量、改写 `$(…)` 为安全的 `$(…)`、使用数组构造参数、删除不必要的 `basename`、优化 `grep -c`、消除 `eval`／`sc2086` 等潜在的单词拆分与未转义风险。整体目标是提升 CI/CD、示例运行脚本的可维护性、可靠性与安全性，同时确保运行时行为保持兼容。

---

### 🎯 影响范围
- **CI/CD**：`.buildkite/*` 系列脚本（镜像构建、推送、基准、硬件 CI、调度任务、发布等）  
- **示例/演示**：`examples/online_serving/*`、`examples/others/*`、`examples/pooling/*`  
- **基准/自动调参**：`benchmarks/*`  
- **测试脚本**：`tests/*`（包括 EC、KV、Nixl、TPU、CPU、HPU 等）  
- **工具链脚本**：`tools/*`（EP‑kernel 安装、FlashInfer、DeepGEMM、pre‑commit）  
- **未涉及核心 Python 包**：`vllm` 库本身的业务逻辑、API、模型推理代码不受影响。

---

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | 仅是 CI/脚本层面的轻量重构，无任何库内部结构、模块依赖或运行时接口变更。对 vllm‑core、模型并行、调度等核心架构保持不变。 |
| **性能影响** | 脚本层面的改动不涉及模型推理路径或算子实现；唯一可能的性能波动在 CI 步骤中：<br>• 使用 `grep -c` 替代 `wc -l` 可略微提升统计速度。<br>• 引入 `mapfile`、数组构造在大循环中略有开销，但对整体构建/测试时间影响可忽略 (< 1 %). |
| **安全考虑** | - **防止未转义导致的命令注入或路径遍历**：所有 `$VAR` 现在均在双引号内，避免空格、通配符、换行等特殊字符造成意外拆分。<br>- **消除 `eval`/隐式 GLOB**：移除 `eval` 用法，改用数组安全传参，降低脚本被恶意变量污染的风险。<br>- **改进 trap 与临时文件使用**：使用 `mktemp` 并在 `EXIT` trap 中安全删除，防止残留敏感信息。 |
| **可维护性** | - 统一编码风格（统一使用 `${VAR}`、`"$VAR"`、数组），便于后续审计与静态分析。<br>- 删除了冗余的 baseline 文件 (`shellcheck.baseline`) 并简化 `pre_commit/shellcheck.sh`，降低维护成本。<br>- 使用 `read -r -a`、`mapfile` 读取配置，提升脚本可读性。 |
| **兼容性** | 所有改动均保持**运行时行为不变**（已在提交信息中声明），仅在极少数变量可能包含空格/特殊字符的极端场景下行为会更安全。若之前脚本依赖未转义的拆分（不太可能），可能会导致在特定 CI 环境中报错。 |

---

### ⚠️ 潜在风险  

| 风险点 | 可能后果 | 防范措施 |
|--------|----------|----------|
| **变量包含空格/换行** – 旧脚本未加引号时会产生多参数，改为加引号后会把整个值视为单一参数。若 CI 环境（如自定义标签、镜像 tag）意外包含空格，可能导致 Docker/Buildkite 命令失败。 | CI 构建或发布中断。 | - 确认所有涉及的变量（如 `$REGISTRY`、`$REPO`、`$BUILDKITE_COMMIT`、`$TAG_NAME`）在实际使用中不含空格。<br>- 在本地或容器中跑一次完整的 CI 流程做回归测试。 |
| **数组构造与 `mapfile`** 在极老的 Bash 版本（< 4.0）上不兼容。大多数 CI 镜像使用 Bash 5.x，风险极低。 | 脚本在旧平台报 “syntax error”。 | 明确在 CI 镜像中使用 Bash 5+（已是默认）。如需向后兼容，可保留旧 `while read` 方式的回退分支。 |
| **删除 baseline**：`tools/pre_commit/shellcheck.baseline` 被删除，后续如果有人依赖该文件做自定义检查会失效。 | CI 中的 `pre_commit` 检查脚本可能报错。 | 该文件仅用于本项目的内部基线对比，删除后脚本已改为直接运行 `shellcheck`，无需基线文件；如有自定义需求，请自行维护自己的基线。 |
| **新脚本的 `set -euo pipefail`**：若已有未声明的变量被引用，将导致脚本立即退出。 | 可能在不显式设置的环境变量（如 `CUDA_HOME`）不存在时中断。 | CI 环境已保证这些变量；若在本地调试，提前 `export` 必要变量或改为 `set +u`。 |

---

### 💡 关注建议  

1. **完整 CI 回归**：在 Pull Request 或本地通过 `buildkite-agent pipeline upload .buildkite/pipeline.yml` 完整跑一遍全部 Buildkite 步骤，确保没有因引号导致的参数错位。  
2. **变量校验**：在关键脚本（如 `push-nightly-builds.sh`、`image_build_*`）加入简易校验，例如 `[[ -z $REGISTRY ]] && { echo "REGISTRY is empty"; exit 1; }`，防止因缺失导致的意外空参数。  
3. **文档更新**：在项目的 CI/脚本章节（README、CONTRIBUTING）加入“所有 Bash 脚本已使用安全引用，若自行修改请保持双引号”。  
4. **安全审计**：虽然已消除大多数 `SC2086`/`SC2317` 等警告，建议在未来的 PR 中继续运行 `pre-commit run shellcheck`，保持零警告的目标。  
5. **兼容性标注**：若计划在更老的 Linux 镜像或容器中运行这些脚本，请在脚本头部添加 `BASH_MIN_VERSION=4` 并在入口处检查 `BASH_VERSINFO`.  
6. **监控 CI 失败率**：在 Buildkite 仪表板中关注因脚本错误导致的失败率，上涨时快速定位是新改动还是外部因素。  

---

> **结论**：此提交通过系统化的 ShellCheck 整改提升了 CI 脚本的安全性、可维护性与可读性，对核心业务代码无任何功能或

---

#### 🟡 中重要度变更 (8)

### Revert "[Models] Fuse Qwen3.5 GDN's qkvz_proj and ba_proj" (#34683)
**SHA**: `1d65283` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1d65283e95f4d978c984df8585ca3f477166e651)

**🎯 变更类型**：重构 / 功能回退  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
本次提交撤销了 Qwen 3.5 系列模型中 “Fuse Qwen3.5 GDN’s qkvz_proj and ba_proj” 的实现。核心改动包括：  
1. `Linear.weight_loader` 与 `weight_loader_v2` 参数签名统一为 `loaded_shard_id: int | None`，删除了对多维 tuple‑type shard‑id 的兼容逻辑；相应的 `_load_fused_module_from_checkpoint` 也简化为仅接受 `loaded_weight`。  
2. Qwen 3.5 的 `GatedDeltaNet` 重新实现：拆分原先的 fused `in_proj_qkvz` / `in_proj_ba` 为 `ColumnParallelLinear` / `MergedColumnParallelLinear`/`RowParallelLinear` 等独立层；加入卷积‑1D 投影、单独的 `dt_bias`、`A_log` 参数以及 `RMSNormGated`。  
3. `Qwen3Next` 中相应的 `in_proj_qkvz`、`in_proj_ba` 也改为普通的 `ColumnParallelLinear`，去掉了 `create_qkvz_proj` 辅助函数。  
4. `load_fused_expert_weights` 的映射表同步删去对 `in_proj_qkvz` / `in_proj_ba` 的 fused‑shard 处理。  

**🎯 影响范围**  
- `vllm/model_executor/layers/linear.py`（权重加载路径）  
- `vllm/model_executor/models/qwen3_5.py`（模型结构、权重挂载）  
- `vllm/model_executor/models/qwen3_next.py`（同上）  
- 可能间接受到 `vllm/model_executor/utils/set_weight_attrs`、`mamba` 权重加载器等模块的影响。

**💡 关注建议**  
1. **权重兼容性**：旧的检查点在使用 tuple‑type shard‑id 时将触发 `NotImplementedError`。若仍需支持，请在迁移脚本中将多维 shard‑id 转换为单一 int，或在调用方显式使用 `weight_loader_v2`。  
2. **模型正确性**：拆分投影后，`forward` 中的切分逻辑已改为分别调用 `in_proj_qkv`、`in_proj_z`、`in_proj_b`、`in_proj_a`，务必跑通常规推理与 Speculative‑Decoding 场景的单元测试，确保输出一致。  
3. **编译与并行**：新建的 `conv1d`、`A_log`、`dt_bias` 通过 `set_weight_attrs` 注入 `sharded_weight_loader`，请确认 `vllm` 的分布式加载路径已覆盖这些属性。  
4. **文档与示例**：更新模型配置文档，尤其是关于 `CacheConfig/ModelConfig/SpeculativeConfig` 的导入路径，防止用户因 import 失效而报错。  
5. **回退风险**：若其他模型（如 Qwen 2、Gemma 等）仍依赖旧的 fused 加载逻辑，确保它们的 `output_sizes` 未受此改动影响；必要时在 `Linear` 中保留兼容层实现或添加警告。  

总体而言，此次回退使 Qwen 3.5/3 Next 的投影实现更为显式且易于调试，但也带来了权重加载路径的向后不兼容，需要在迁移文档中明确说明并在 CI 中加入对应的迁移测试。

---

### [Ray] Propagate third-party env vars to Ray workers via prefix matching (#34383)
**SHA**: `c464b57` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c464b573749d956a1e7a5721c44853677b5c9659)

**🎯 变更类型**：功能增强（Ray 环境变量传播可配置化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `vllm.envs` 中新增 `VLLM_RAY_EXTRA_ENV_VAR_PREFIXES_TO_COPY`、`VLLM_RAY_EXTRA_ENV_VARS_TO_COPY` 两个可通过环境变量配置的列表。  
2. `vllm.ray.ray_env.get_env_vars_to_copy` 重构：默认前缀与额外变量统一在函数内部维护，并在运行时合并用户自定义的 CSV 列表，实现 **additive**（累计）行为；同时加入 `additional_vars` 参数供平台自行注入。  
3. `RayDistributedExecutor` 移除硬编码的 `ADDITIONAL_ENV_VARS`，改为仅使用平台层的 `additional_env_vars`。  
4. 新增 194 行测试 `tests/test_ray_env.py`，覆盖默认前缀、默认额外变量、用户扩展、排除逻辑、平台变量以及边界情况。  

**🎯 影响范围**  
- `vllm/envs.py`（环境变量定义）  
- `vllm/ray/ray_env.py`（核心传播实现）  
- `vllm/v1/executor/ray_executor.py`（使用点）  
- CI 测试配置及新增测试文件  

**💡 关注建议**  
1. **向后兼容**：确保外部调用仍能获取旧行为——已实现前缀和变量的累计合并，建议在发布说明中明确“不会删除已有默认前缀”。  
2. **文档更新**：在用户手册和 Ray 相关文档中补充 `VLLM_RAY_EXTRA_ENV_VAR_PREFIXES_TO_COPY`、`VLLM_RAY_EXTRA_ENV_VARS_TO_COPY` 的使用示例与 CSV 语法（空格会被 trim），并说明 `RAY_NON_CARRY_OVER_ENV_VARS` 的优先级。  
3. **配置文件**：`ray_non_carry_over_env_vars.json` 仍是排除来源，建议检查该文件在部署环境中的可达性，避免误删默认前缀导致敏感变量泄露。  
4. **性能**：`get_env_vars_to_copy` 现在会遍历一次 `os.environ` 并对每个键做 `startswith` 检查，复杂度 O(N·P)。在极端环境变量数量很大时可留意潜在的微小开销。  
5. **测试可靠性**：新增测试覆盖了空格处理、前缀严格匹配等细节，建议在 CI 中保持 `--run-slow`（若有）打开，以防未来更改导致回归。  

总体来说，此次改动提升了 Ray 环境变量的可配置性，降低了硬编码风险，只要同步文档并留意排除列表的优先级即可安全上线。

---

### [Model Runner V2] Minor refactoring for penalties (#34662)
**SHA**: `d00df62` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d00df624f313a6a5a7a6245b71448b068b080cd7)

**🎯 变更类型**：重构 / 性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 `ModelRunner`、`Sampler`、`BadWordsState`、`PenaltiesState` 等核心组件的参数从直接传入 *all_token_ids / prompt_len / total_len* 改为统一的 `RequestState` 对象，实现状态统一管理。  
2. `PenaltiesState` 重新实现 `apply_staged_writes`：先通过 `async_tensor_h2d` 把新增请求的索引拷贝到 GPU，再在 Triton kernel 中使用映射索引完成 bin‑count，去掉了对整个张量的遍历。  
3. Triton kernel 参数与调用方式同步更新，新增 stride 与 idx‑mapping 传递，确保多请求批处理时只处理本轮新增的请求。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`  
- `vllm/v1/worker/gpu/sample/*.py`（sampler、bad_words、penalties）  
- 可能波及调用这些类的上层调度/推理入口（如 `Engine`, `Scheduler`）  

**💡 关注建议**  
1. **接口兼容**：所有创建 `Sampler`、`PenaltiesState`、`BadWordsState` 的地方必须改为传入 `RequestState`，否则会出现参数缺失错误。确保 `ModelRunner` 中 `req_states` 已经在 `self.req_states` 中完整初始化。  
2. **正确清零**：`bincount` 现在仅对 `idx_mapping` 对应的行做 `zero_()`，若旧请求未在本轮 `idx_mapping` 中，需要在后续调用前手动保持其状态不被意外覆盖。  
3. **异步拷贝**：`async_tensor_h2d` 需要在目标设备上支持 pinned‑memory，检查运行环境（CPU↔GPU）是否满足，否则可能回退到同步拷贝导致性能下降。  
4. **Kernel 启动维度**：`_bincount_kernel` 现在使用二维 grid `(num_reqs, num_blocks)`，确认 `triton` 版本兼容此写法，并在不同 batch 大小下验证 kernel 触发次数与预期一致。  
5. **回归测试**：重点跑包含 repetition/frequency penalty、bad‑words、logit‑bias 的混合请求，用 `--disable-logprobs` 与 `--logprobs` 两种模式对比旧版输出，确保概率、采样结果一致且无额外 CUDA 错误。  
6. **性能基准**：在多请求并发（如 64/128）场景下测量 `apply_staged_writes` 的耗时，确认新实现相较旧的 `for req_idx in self._penalties_reqs:` 有明显加速或至少不退化。  

总体来看，此次重构提升了状态管理的一致性，同时利用异步拷贝和批量 Triton kernel 减少了 CPU‑GPU 同步开销。但需对所有受影响的调用点同步调整并完成充分的功能/性能回归。

---

### [Model Runner V2] Minor cleanup for PP (#34666)
**SHA**: `04925b2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/04925b2202efccb51e47fa944660dc97a8e86444)

**🎯 变更类型**：重构 + 轻量功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `PPHandler` 的创建从工厂 `get_pp_handler` 改为直接在 `ModelRunner` 中实例化，并在构造时注入运行设备。  
- 引入 `is_first_pp_rank / is_last_pp_rank` 标记，统一对 “是否为第一个/最后一个 PP 进程” 的判断，原先散落的 `self.use_pp and get_pp_group().is_*` 判定被简化。  
- 相应地在 `_dummy_run、profile_run、execute_model、sample_tokens` 等关键路径中改用上述标记，去掉了多余的 `get_pp_group()` 调用。  
- `PPHandler` 现在保存 `device`，内部广播/接收函数省去显式 `device` 参数，移除了已废弃的 `get_pp_handler` 工厂函数。

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`（模型运行、前向传播、采样、profiling）  
- `vllm/v1/worker/gpu/pp_handler.py`（PP 进程间通信）  

**💡 关注建议**  
1. **实例化一致性**：确认项目其他位置（如单元测试、示例脚本）仍然使用 `PPHandler`，而不是旧的 `get_pp_handler`。若仍有调用，需要统一改为 `PPHandler(device)`。  
2. **`get_pp_group` 可达性**：`ModelRunner.__init__` 现在直接调用 `get_pp_group()`；请检查该函数是否已在当前作用域导入，防止运行时 `NameError`。  
3. **广播路径容错**：`sample_tokens` 中对 `maybe_receive_sampled_tokens` 的返回值直接 `assert received is not None`。在异常或超时情况下，这可能导致进程卡死，建议保留原来的 `if received is not None` 保护或在异常路径记录错误信息。  
4. **设备一致性**：`PPHandler` 持有的 `device` 与 `ModelRunner.device` 必须保持一致，建议在 `ModelRunner.__init__` 中加入断言 `self.pp_handler.device == self.device` 以防意外。  
5. **回归测试**：重点跑 `pipeline_parallel`（`pp_size>1`）和单卡（`pp_size=1`）两套测试，覆盖多模态输入、Speculative Decode、KV‑Connector 等路径，确保新标记不会误判导致 `intermediate_tensors` 生成错误。  

总体来看，此次改动提升了代码可读性和分支判断的统一性，风险主要在于残留对已删除工厂函数的引用以及对广播返回值的强制断言，建议对应检查并补充相应测试。

---

### [Bugfix] Fix fused MoE int32 overflow in stride*offset without perf regression (#34507)
**SHA**: `b68fd89` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b68fd899d1317259d61d8bbaf79dcb2749d17634)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `fused_moe` 的 Triton 内核中，`stride_cm * offs_token` 的乘积在 token 数量极大时会超过 32‑bit 整数上限，引发溢出并导致结果错误。此次将 `offs_token` 强制转为 `int64`，防止溢出，并新增针对该场景的回归测试。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/fused_moe/fused_moe.py`（核心 Triton kernel）  
- `tests/kernels/moe/test_moe.py`（新增高内存、极大 M 的测试）  

**💡 关注建议**：  
1. **兼容性**：改动仅在内核内部提升了整数宽度，对外部 API 没有影响，安全发布。  
2. **性能**：`int64` 的乘法在 Triton 中略有开销，但作者已验证未出现回归，仍建议在大模型/长序列场景下监测 GPU 利用率。  
3. **测试**：新测试需要约 12 GB GPU 空间，CI 环境若内存不足会被跳过；请确保 CI GPU 容量足够或在本地跑完整套测试。  
4. **配置**：通过环境变量 `VLLM_FUSED_MOE_CHUNK_SIZE` 可以关闭分块以触发该路径，保持默认配置不受影响。  

总体而言，此次修复解决了极端规模下的数值错误，风险低，建议尽快合并并在下一个发布中开启。

---

### [Core] Pipeline Parallel support for Model Runner V2 (#33960)
**SHA**: `9a8853f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9a8853f781d0d5c0964eba5ea61ac25d9eca0185)

**🎯 变更类型**：功能增强（为 Model Runner V2 引入 Pipeline Parallel (PP) 支持）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `model_runner.py` 中加入 PP 相关的标识、初始化 `PPHandler`，并在前向、采样及 dummy‑run 流程中根据 `pp_group` 的角色（首/尾 rank）选择不同的执行路径。  
2. 新增 `pp_handler.py`，实现最后一个 PP rank 对采样结果的 `torch.distributed.broadcast`，以及非最后 rank 的接收逻辑。  
3. 对多模态、KV‑Connector、CUDA‑graph 等路径做了兼容性处理，使 PP 只能在 eager 模式下使用。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/model_runner.py`（核心推理路径）  
- 新增 `vllm/v1/worker/gpu/pp_handler.py`（PP 同步模块）  
- 相关的分布式状态 `vllm/distributed/parallel_state.py`（`get_pp_group`、`prepare_communication_buffer_for_model`）  
- 可能影响 `vllm/v1/worker/gpu/sample/*`、`vllm/v1/worker/gpu/mm/*` 的调用方式  

**💡 关注建议**  

1. **正确性**  
   - 确认 `IntermediateTensors` 在非首 rank 前向时能够被模型接受并在后续 `kv_connector` 中使用；若模型实现未覆盖该路径，可能出现 `TypeError`。  
   - `dummy_run` 现在在非首 rank 会生成空的 `intermediate_tensors`，但 `self.execute_model_state` 仍返回 `(None, ...)`；确保上层调用（如 profile、warm‑up）不会误判 `None` 为错误。  

2. **性能**  
   - PP 只在 eager 模式下生效，`capture_model` 直接跳过 CUDA‑graph。若用户开启了 CUDA‑graph 与 PP，需在文档或日志中提前提示。  
   - `maybe_broadcast_sampled_tokens` 采用阻塞的 `torch.distributed.broadcast`，在高并发或大 batch 时可能成为瓶颈；建议后续考虑使用 `all_gather` 或分层广播优化。  

3. **容错与兼容**  
   - `get_pp_handler` 只在 `pipeline_parallel_size>1` 时被调用，但 `self.pp_handler` 在构造函数里仍可能为 `None`；在所有使用前请保持 `self.use_pp` 判定一致，避免 `AttributeError`。  
   - 对多模态输入的处理已加上 “首 rank” 限制，确认 `supports_mm_inputs` 在非首 rank 为 `True` 时不会触发不必要的 `get_mm_embeddings` 调用。  

4. **测试**  
   - 增加 PP 场景的单元/集成测试，覆盖：  
     a) 单卡（PP=1）保持原有行为；  
     b) 2‑4 卡的 PP 前向、采样、spec‑decode 流程；  
     c) dummy‑run 与 CUDA‑graph 跳过的日志验证。  
   - 验证在异常退出（如 NCCL 超时）时，各 rank 能正确清理 `intermediate_tensors` 与 `execute_model_state`，防止内存泄漏。  

5. **文档**  
   - 在使用手册中注明：PP 目前仅支持 eager 推理、与 KV‑Connector、spec‑decode 兼容；若需要 CUDA‑graph，请关闭 PP。  

整体来看，改动实现了 PP 的最小可用功能，代码结构清晰，但需注意上述细节以防在多卡部署时出现隐蔽错误。

---

### [Model Runner V2] support bad_words sampling param (#33433)
**SHA**: `387a189` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/387a1898d9593f001734527946af9aafa4e24ae6)

**变更概览**  
本次 PR 在 Model‑Runner V2 中引入了 **bad_words** 采样参数。为实现该功能，核心改动包括：

1. **统一 token 存储**  
   - 将原来的 `prefill_token_ids` 重命名为 `all_token_ids`，并把它从 `StagedWriteTensor` 替换为统一的 `all_token_ids`。  
   - 新增 `prompt_len`（UVA）与 `total_len`（StagedWriteTensor）两段状态，分别记录用户 Prompt 长度和已生成的总长度，以区分 “prompt” 与 “output”。  
   - 所有涉及 token 读取的 Triton kernel（prefill、post_update、penalties、prompt_logprob 等）均改用 `all_token_ids`，并在 `post_update` 中同步更新 `total_len`。

2. **Bad‑Words 过滤实现**  
   - 新增 `BadWordsState`，负责在采样前把每个请求的禁用词列表展平、构建偏移表并写入 GPU。  
   - `apply_bad_words` 在采样阶段基于 `expanded_idx_mapping`、`expanded_local_pos`、`all_token_ids`、`prompt_len/total_len` 等信息，启动二维 Triton kernel `_bad_words_kernel`，若检测到完整的禁用词匹配则把对应 logits 设为 `-inf`。  
   - `Sampler` 在 `__call__` 中加入对 `bad_words_state.apply_bad_words` 的调用。

3. **ModelRunner 与 State 结构适配**  
   - `ModelRunner.__init__` 新增 `all_token_ids、prompt_len、total_len` 参数传递。  
   - `add_requests`、`prepare_inputs`、`postprocess`、`sample_tokens` 等调用点相应改为使用 `all_token_ids`。  
   - `ReqStates` 中引入 `total_len` 并在 `add_request`、`apply_staged_writes`、`remove_request` 中维护。

**影响范围**  
- **GPU 内核**: 多个 Triton kernel 被重新编译，可能产生额外的寄存器/共享内存压力。  
- **内存占用**: 新增 `total_len`（int32）与 `prompt_len`（UVA）以及 `bad_word_*`（最多 128 × 1024 int32）会略增显存/主机内存。  
- **调度/采样**: 采样路径多了一次 `bad_words` 检查，少数情况下会轻微增加延迟。  
- **API**: `SamplingParams` 现需支持 `bad_words_token_ids`，用户若未提供保持兼容。

**建议**  

1. **同步与错误检查**  
   - `BadWordsState.use_bad_words` 使用 Numpy bool，确保在多线程/异步环境下对该数组的写入在 `apply_staged_writes` 前已完成。可考虑改为 GPU Tensor 或在 `apply_staged_writes` 前同步复制。  
   - `total_len` 在 `_post_update_kernel` 中先读取再写入，若同一请求并发多次 `post_update`（例如 speculative 并行），需确认不会产生竞争。

2. **性能验证**  
   - 在高并发（数千请求）和长序列（> 4k）场景下跑基准，关注 `all_token_ids` 的写入带宽以及 `_bad_words_kernel` 的 launch 配置（`max_num_bad_words` 维度），防止因占用过多 block 导致 SM 资源被抢占。  
   - 对 `bad_words` 长度上限（1024 tokens、128 词）进行压力测试，确保 kernel 边界检查不会溢出。

3. **测试覆盖**  
   - 增加单元测试：  
     - 禁用词在 prompt、已生成输出、speculative buffer 中均能被正确屏蔽。  
     - 当 `bad_words` 超出限制时抛出明确异常。  
   - 回归测试：确保原有 `prefill_token_ids` 逻辑（prompt‑logprob、频率惩罚等）在改名后仍返回相同结果。

4. **文档与示例**  
   - 在 README / SamplingParams 文档中补充 `bad_words_token_ids` 的使用说明、格式（list[list[int]]）以及上限说明。  
   - 提供一个小示例演示如何在 `vLLM` 客户端传入禁用词并观察 logits 被置为 `-inf`。

总体来看，变更实现了预期的功能并保持了对现有代码的兼容，但涉及多个内核和状态结构的改动，需要在高负载下进行充分的性能与并发测试，避免因新引入的共享状态导致的竞争或显存泄漏。

---

### Targeting the MI355 agent pool with all existing tests (#34629)
**SHA**: `824f9e8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/824f9e8f3c7f0a688b6093d0c85ed6b39ba314e1)

**变更类型**：其他（CI 配置扩展）  
**重要程度**：🟡 中  

**变更摘要**  
此提交在 `.buildkite/test-amd.yaml` 中新增约 60 条针对 MI355（AMD GPU）agent pool 的测试步骤，涵盖依赖检查、单 GPU 与多 GPU 的异步引擎、入口点、分布式、模型初始化、语言/多模态模型、量化、黑盒硬件（Blackwell）以及 benchmark 等完整测试链路。所有步骤均使用 `mirror_hardwares: [amdexperimental, amdproduction, amdtentative]`，并在需要时指定 `agent_pool`（如 `mi355_1/2/4/8`）和 `num_gpus`。

**影响范围**  
- **CI 基础设施**：新增的 `mi355_*` agent pool 必须在 Buildkite 中预先配置；若缺失会导致整个 CI 失效。  
- **测试资源**：大量 GPU 需求（1‑8 卡）和特定硬件（H100、B200）会显著延长每日流水线时长，可能导致排队或超时。  
- **依赖环境**：多处使用 `torch_nightly`、`torch_nccL_blocking_wait`、`VLLM_WORKER_MULTIPROC_METHOD=spawn`、以及自定义 pip/uv 安装，若 AMD 镜像中缺少相应包或版本不兼容，将产生报错。  
- **代码路径**：虽然未修改业务代码，但新增的测试覆盖了现有的大部分模块（engine、distributed、model_executor、kernels、quantization、multimodal 等），相当于一次全量回归。  

**关注建议**  
1. **确认 Agent Pool**：在 Buildkite admin 页面检查 `mi355_1/2/4/8` 是否已创建，并确保对应硬件标签（`amdexperimental`、`amdproduction`、`amdtentative`）准确映射。  
2. **资源配额**：评估每日可用 GPU 时长，必要时对非关键或可选 (`optional: true`) 的步骤做分支或 nightly-only 运行，以免阻塞主线 PR。  
3. **依赖兼容性**：确保 AMD 镜像中已预装符合 `torch_nightly`、`torchao==0.14.1`、`conch-triton-kernels` 等版本的库；若有冲突，建议在 CI 前置步骤统一 `uv pip install --system`。  
4. **环境变量统一**：多个步骤依赖 `VLLM_WORKER_MULTIPROC_METHOD=spawn`、`TORCH_NCCL_BLOCKING_WAIT=1` 等，建议在全局 `environment` 块中统一声明，减少重复。  
5. **测试分片**：对大规模模型/语言/多模态测试已经使用 `--num-shards`/`--shard-id`，请再次核实分片总数与 `parallelism` 设置匹配，避免出现“未跑完所有 shard”导致误报。  
6. **回归监控**：首次在 MI355 上运行全部步骤后，关注 Buildkite 报表中的失败率；若出现平台特有错误（如 HIP bug），可在对应步骤中保留 `soft_fail` 或 `optional` 标记，防止阻塞主线。  

总体而言，此次改动显著提升了 AMD GPU 环境的回归覆盖，若上述基础设施与依赖配置妥当，将有助于在 MI355 硬件上提前捕获兼容性回归。建议在合并前在本地 CI（或小规模 MI355）跑一次全链路，以验证 agent pool 与依赖的完整性。

---

#### 🟢 低重要度变更 (11)

### Fixed whisper CPU test that does not spawn properly. (#34324)
**SHA**: `6bd6d0c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6bd6d0c3c1195e007925b8c3c5ee214745f721d9)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/models/multimodal/generation/test_whisper.py` 中移除 `@create_new_process_for_each_test("spawn")` 装饰，以修复 Whisper CPU 测试无法正确启动的问题。

---

### [CI][Nixl] Add CrossLayer KV layout tests (#34615)
**SHA**: `8e962fe` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8e962fef5fec5197d8c027ed2e00ecace5b47aac)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.buildkite/test_areas/distributed.yaml` 中新增一个针对 4 GPU 环境的 CrossLayer KV 布局分布式 NixlConnector 精度测试步骤。

---

### Fix docs build warning (#34686)
**SHA**: `28bffe9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/28bffe9466aeea56b7301e1d27c2cb7e18dcbc15)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `maybe_receive_sampled_tokens` 函数参数 `device` 的文档说明，消除文档构建警告。

---

### [Bugfix] Fix 'remove_instance_endpoint' method logic in disagg_proxy_demo (#32922)
**SHA**: `ad65177` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ad65177a1977af535ac0f82312f898a03c425632)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 `remove_instance_endpoint` 中对 `prefill` 实例的判断与循环更新，避免误删 `decode_instances` 并正确重建 `prefill_cycler`。

---

### Remove dead bitsandbytes CxB code from 8-bit inference path (#34633)
**SHA**: `d44a5b6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d44a5b6c474ae02f1daf61f6db623a4311294ef8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：删除了 8 位推理路径中已失效的 bitsandbytes `CxB` 相关代码，简化了权重量化逻辑并去除不必要的状态维护。

---

### [CI] Fix bake config artifact path for AMI rebuild pipeline (#34656)
**SHA**: `c5c38e1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c5c38e152ae83d3116f5956060951a81209a3407)

**🎯 变更类型**：其他（CI 脚本修正）  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.buildkite/image_build/image_build.sh` 中，将 Buildkite artifact 的上传方式改为先切换到文件所在目录再上传，仅修正了 AMI 重建流水线中 bake 配置文件路径错误的问题。

---

### [Model Runner V2] Minor simplification for BadWordsState (#34669)
**SHA**: `9752da9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9752da9d9c17f85bddee2f869130e72961f4d54d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：简化 `BadWordsState` 实现，去除 `use_bad_words` 标记，直接依据 `num_bad_words` 判断是否需要执行过滤，合并 `add_request` 参数写法，删除冗余变量。

---

### [Model Runner V2] Fix unintended CPU-GPU sync in make_dummy (#34667)
**SHA**: `d74278f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d74278fb676cbafc835fab9e970f6bcc9fd5413d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `make_dummy` 中将 `input_buffers.query_start_loc[0] = 0` 改为切片写法 `input_buffers.query_start_loc[:1] = 0`，避免不必要的 CPU‑GPU 同步。

---

### [CI] Enable mypy import following for vllm/v1/kv_offload (#34639)
**SHA**: `0b5f9b7` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0b5f9b720451dab9d2fcba2a697fa59e0c0add01)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tools/pre_commit/mypy.py` 中删除对 `vllm/v1/kv_offload` 的排除，使 CI 的 mypy 检查包含该模块的导入。

---

### [NemotronH] Do not force router to run in fp32 (#34582)
**SHA**: `3b30e61` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3b30e6150777de549b11f67dde3ecc0d3b1f3f50)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：取消对路由器 logits 必须为 FP32 的强制转换，针对 DeepSeekV3 路由在需要时显式转为 float32，并在 NemotronH 实现中去除 `router_logits_dtype` 参数，直接使用输入 dtype。

---

### [Bugfix][CI] Fix flaky `entrypoints/openai/test_response_api_with_harmony.py::test_function_calling[openai/gpt-oss-20b]` (#34624)
**SHA**: `6cc403e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6cc403e67d9ca8b4fc8c93a84096ed98161c938b)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_harmony.py` 中移除对外部天气 API 的真实请求，改为返回固定温度 15.0，防止 CI 环境出现 SSL/网络导致的间歇性失败。

---

