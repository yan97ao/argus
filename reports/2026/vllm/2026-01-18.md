# 每日更新报告（2026-01-18）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-18 22:07:01 | tjp_zju | "refactor: refactor_repeated_interfaces" (#32486) |
| 2026-01-18 19:17:59 | Isotr0py | [Bugfix] Fix GLM-ASR audio encoder RoPE dim (#32540) |
| 2026-01-18 18:20:46 | Li Xie | [Model] Support Step1 Model (#32511) |
| 2026-01-18 16:07:28 | Canlin Guo | [Model] Remove the unnecessary dtype conversion in MiniCPM (#32523) |
| 2026-01-18 13:55:17 | Woosuk Kwon | [Model Runner V2] Minor optimization for eagle input processing (#32535) |
| 2026-01-18 12:19:59 | Isotr0py | [Performance] Improve Triton prefill attention kernel's performance  (#32403) |
| 2026-01-18 12:16:59 | Robert Shaw | [MoE Refactor] Move Test Impl into Test Dirs (#32129) |
| 2026-01-18 12:09:48 | Woosuk Kwon | [Model Runner V2] Move mrope_positions buffer to MRopeState (#32532) |
| 2026-01-18 11:02:01 | Karan Bansal | [Feature] Add FIPS 140-3 compliant hash algorithm option for multimodal hashing (#32386) |
| 2026-01-18 10:36:11 | Shengqi Chen | [build] fix cu130 related release pipeline steps and publish as nightly image (#32522) |

### 📊 统计摘要
> 本日共 10 个提交 | 🔴高 0 | 🟡中 7 | 🟢低 3
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🟡 中重要度变更 (7)](#-🟡-中重要度变更-7)
    - ["refactor: refactor_repeated_interfaces" (#32486)](#2f03035)
    - [[Bugfix] Fix GLM-ASR audio encoder RoPE dim (#32540)](#38bf2ff)
    - [[Model] Support Step1 Model (#32511)](#c826c72)
    - [[Performance] Improve Triton prefill attention kernel's p...](#8cc26ac)
    - [[MoE Refactor] Move Test Impl into Test Dirs (#32129)](#4a6af88)
    - [[Model Runner V2] Move mrope_positions buffer to MRopeSta...](#4147910)
    - [[build] fix cu130 related release pipeline steps and publ...](#965765a)
  - [🟢 低重要度变更 (3)](#-🟢-低重要度变更-3)
    - [[Model] Remove the unnecessary dtype conversion in MiniCP...](#fe36bf5)
    - [[Model Runner V2] Minor optimization for eagle input proc...](#963dc0b)
    - [[Feature] Add FIPS 140-3 compliant hash algorithm option ...](#3055232)
#### 🟡 中重要度变更 (7)

### "refactor: refactor_repeated_interfaces" (#32486)
**SHA**: `2f03035` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2f03035a618f03c412b8a540606c7b1816839390)

**🎯 变更类型**：重构（refactor）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将项目中散落的 `ceil_div` 实现统一抽取到 `vllm.utils.math_utils.cdiv`，并在所有原先自行实现的地方统一使用；新增 `get_layer_index` 统一处理视觉层负索引的逻辑；相应地删除了多余的本地实现，并修正了相关 import。  

**🎯 影响范围**  
- `vllm/_custom_ops.py`、`vllm/model_executor/layers/quantization/*`、`vllm/model_executor/layers/fused_moe/*`、`vllm/model_executor/warmup/deep_gemm_warmup.py` 等算子与量化实现。  
- 视觉模型加载路径（`aya_vision.py、llava.py、mistral3.py、tarsier.py`）以及通用工具 `model_executor/models/utils.py`。  
- `bitsandbytes_loader.py` 中对 `is_moe_model` 的 import 调整。  

**💡 关注建议**  
1. **功能等价**：`cdiv` 直接复用了原 `ceil_div` 的实现，确保在除数为 0 时仍会抛异常，保持原有行为。  
2. **导入路径**：新增的 `cdiv` 与 `get_layer_index` 已统一放在公共模块，确认所有旧的相对导入均已更新，防止运行时找不到函数。  
3. **兼容性**：若外部用户在自定义插件中仍引用项目内部的 `ceil_div`，可能出现 ImportError，建议在发布说明中标明已废弃并提供迁移指南。  
4. **测试覆盖**：新增/修改的函数涉及负索引及块大小计算，建议补充单元测试，覆盖 `feature_layer_index` 为负数、0、超出范围等边界情况。  
5. **文档同步**：在相应文档或代码注释中说明 `cdiv` 与 `get_layer_index` 的语义，避免新加入的开发者误用旧实现。  

总体而言，此次重构提升了代码复用性与可维护性，影响面主要在算子、量化和视觉模型初始化路径，风险较低。后续关注兼容性提示与测试完整性即可。

---

### [Bugfix] Fix GLM-ASR audio encoder RoPE dim (#32540)
**SHA**: `38bf2ff` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/38bf2ffb21d54eb67a68d4ba82125922f4a4c7ad)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 在 `examples/offline_inference/audio_language.py` 中把 GLM‑ASR 示例代码前移，保持示例顺序不变。  
- 在 `vllm/model_executor/models/glmasr.py` 中对 RoPE（Rotary Position Embedding）维度的获取与使用做了两项修正：  
  1. 新增对 `config.rope_parameters`（若存在）或 `config.partial_rotary_factor` 的兼容读取，避免因字段缺失导致 `AttributeError`。  
  2. 根据 `partial_rotary_factor` 计算 `rotary_dim`，并在前向传播时仅对前 `rotary_dim` 的切片应用 `ApplyRotaryEmb`，防止对全 `head_dim` 进行错误的旋转。

**🎯 影响范围**  
- `vllm/model_executor/models/glmasr.py`（GLM‑ASR 模型的注意力实现）  
- 示例脚本 `examples/offline_inference/audio_language.py`（使用 GLM‑ASR 时的入口）  
- 可能涉及到其它使用 `rope_parameters` 配置的模型（如 GLM‑4‑Chat）在读取 RoPE 参数时的兼容性。

**💡 关注建议**  
1. **回归测试**：在本地或 CI 中跑一次“GLM‑ASR‑Nano‑2512”模型的推理，验证多音频输入（`audio_count>1`）的长度、生成质量以及运行时是否仍会触发 `ApplyRotaryEmb` 报错。  
2. **配置兼容**：检查已有的模型配置文件，确认是否仍使用旧版 `partial_rotary_factor` 字段；若已迁移至 `rope_parameters`，上述兼容读取保持向后兼容。  
3. **文档更新**：在 `docs` 或示例章节加入“GLM‑ASR 使用说明”，说明 `<|pad|>` 为音频占位符以及 `max_model_len/limit_mm_per_prompt` 的推荐取值。  
4. **性能评估**：因为只对 `rotary_dim` 部分做旋转操作，理论上会略微提升注意力计算效率，建议在大 batch 下对比前后吞吐量，以确认无负面影响。  

总体来看，此次改动修复了 GLM‑ASR 在 vLLM 中因 RoPE 维度不匹配导致的崩溃问题，同时提升了配置的容错性，影响局限在 GLM‑ASR 相关路径，兼容性风险较低。及时加入相应的单元/集成测试即可确保后续合并安全。

---

### [Model] Support Step1 Model (#32511)
**SHA**: `c826c72` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c826c72a9633454679871fcb81fbc31fe03fb150)

**🟢 变更概览**  
- 新增 Step1（Step‑Audio）模型实现，完成权重加载、前向、日志its 以及 PP 支持。  
- 为 Alibi 位置编码加入 *sqrt* 变体：在 `Attention`、`AttentionBackend` 接口中新增 `use_alibi_sqrt` 参数并在 Triton‑Attn 实现里默认支持。  
- 文档、模型注册表、测试用例同步更新：`supported_models.md`、`registry.py`、`test_initialization.py`。  
- 相关底层改动：`vllm/attention/layer.py` 检查后端是否支持 `alibi_sqrt`，并在 `triton_attn`、统一 Triton kernel 中加入对应编译常量。

**🎯 影响范围**  
- **模型层**：`step1.py`、`registry.py`、`step1` 相关类。  
- **注意力实现**：`vllm/attention/layer.py`、`v1/attention/backend.py`、`triton_attn.py`、`triton_unified_attention.py`。  
- **测试/文档**：`tests/models/*`、`docs/models/supported_models.md`。

**🔧 开发者建议**  
1. **后端兼容性**：如果自行实现或替换 AttentionBackend，请实现 `supports_alibi_sqrt()` 并在构造函数接受 `use_alibi_sqrt` 参数，防止运行时 `ValueError`。  
2. **配置统一**：在模型配置（HF config）中添加 `use_alibi_sqrt=True` 只对 Step1/支持的模型生效；其它模型仍可保持默认 `False`。  
3. **性能回归**：`use_alibi_sqrt` 会在 Triton kernel 中增加 `sqrt` 计算，建议在 CI 中对比吞吐量，确认对大 batch / 长序列的影响在可接受范围。  
4. **权重加载**：Step1 采用「packed module」方式（`qkv_proj`、`gate_up_proj`），确保自定义 checkpoint 的键名保持兼容，否则可能导致未加载的参数被默认为零。  
5. **文档同步**：若后续新增其他支持 `alibi_sqrt` 的模型，请及时在 `supported_models.md` 与 registry 中登记，并补充相应的测试。

**👥 用户建议**  
- 使用 Step1 时请在 `vllm.Config` 中将 `attention_backend="TRITON_ATTN"`（或其它支持的后端）并保持 `use_alibi_sqrt` 为默认 `True`；否则会报错。  
- 对不支持 `alibi_sqrt` 的旧显卡（如不具备相应 CUDA Compute Capability）仍可回退到 `TRITON_ATTN`，该后端已实现兼容路径。  

整体来说，功能扩展合理，新增的后端特性向下兼容，唯一需要关注的是自定义后端实现和潜在的性能变化。祝测试顺利 🚀.

---

### [Performance] Improve Triton prefill attention kernel's performance  (#32403)
**SHA**: `8cc26ac` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/8cc26acd8b77c8af12d2fbf1f5bd38d09861928c)

**🎯 变更类型**：性能优化  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 `tests/models/language/pooling/test_token_classification.py` 中放宽了 token‑classification 的数值容差（`atol` 从 1.2e‑2 提升至 3.2e‑2），防止因新 kernel 的轻微数值差异导致测试失效。  
2. 在 `vllm/utils/math_utils.py` 新增 `RCP_LN2`（≈ 1/ln 2），用于后续的对数/指数基底转换。  
3. 重写了 Triton 前置（prefill）注意力算子 `vllm/v1/attention/ops/triton_prefill_attention.py`，主要改动包括：  
   - 统一使用 `exp2` 代替 `exp`，并在软缩放 `sm_scale` 上乘以 `RCP_LN2`，消除一次 `log2` 转换的开销。  
   - 合并 mask 生成与加载，去除多余的 `multiple_of`、冗余的 `float("-inf")` 处理。  
   - 采用增量式 `m_i`、`l_i` 更新公式，简化 `alpha`、`beta` 计算，提升寄存器利用率。  
   - 最终结果通过一次除法 `acc / l_i[:, None]` 完成归一化。  

**🎯 影响范围**：  
- **核心算子**：`vllm/v1/attention/ops/triton_prefill_attention.py`（所有使用 Triton 前置注意力的模型）  
- **数学工具**：`vllm/utils/math_utils.py`（新增常量可能被其他模块复用）  
- **单元测试**：`tests/models/language/pooling/test_token_classification.py`（容差放宽）  

**💡 关注建议**：  
1. **数值稳健性**：`exp2` 与 `exp` 的误差不同，尤其在极端 `sm_scale` 下可能出现上溢/下溢。建议在 CI 中加入对比旧 kernel 与新 kernel 的最大相对误差统计，确保在常用的 `head_dim` 与序列长度范围内误差仍在可接受阈值。  
2. **性能基准**：提供一套包含不同 batch、seq_len、head_num 组合的微基准，展示相对提升幅度，防止在特殊输入（如极短序列）出现退化。  
3. **向后兼容**：`RCP_LN2` 的引入改变了软缩放因子，若用户自行传入 `softmax_scale`，需确认仍满足预期。文档中应说明该常数仅在 Triton 实现内部使用。  
4. **代码可维护性**：当前 kernel 逻辑已大幅压缩，建议在代码注释中注明每一步的数学推导（尤其 `m_i`、`l_i` 的增量更新），便于后续调优或迁移到其他后端。  

总体而言，此次改动通过更高效的指数计算与简化的 mask 处理，显著降低了 Triton 前置注意力的算子开销，对大模型推理的吞吐提升有帮助。只要在数值误差与兼容性方面做好验证，即可安全合入主分支。

---

### [MoE Refactor] Move Test Impl into Test Dirs (#32129)
**SHA**: `4a6af88` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4a6af8813fb53ebde21ffadc2bfce94c965e98b1)

**🎯 变更类型**：重构（MoE 基线实现迁移）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原本位于 `vllm/model_executor/layers/fused_moe/moe_torch_iterative.py` 的 `iterative_moe` 基线实现移入测试目录 `tests/kernels/moe/test_moe.py`，并在源码中删除该文件。  
- 对文档表格中 “iterative” 条目作删除处理。  

**🎯 影响范围**  
- **模型执行层**：`vllm.model_executor.layers.fused_moe`（失去 `iterative_moe` 导入路径）。  
- **测试套件**：新增本地实现，使 `test_moe.py` 成为唯一使用该基线的地方。  
- **用户代码**：任何直接 `from ...moe_torch_iterative import iterative_moe` 的外部项目将报 ImportError。  

**💡 关注建议**  
1. **保持向后兼容**：如该实现仍可能被外部调研或基准使用，建议在原路径保留一个小的包装模块，抛出已弃用警告或转发到测试实现。  
2. **更新 CI/文档**：确保所有 CI 步骤已改为使用新的本地实现，文档中删除的表格项同步到公开 API 列表，避免用户误解。  
3. **代码体积**：把完整实现放在测试文件会让测试加载时间稍增，若测试运行成本成为瓶颈，可考虑将实现抽离为内部私有模块，仅在测试中引用。  
4. **回归验证**：确认生产代码路径不再依赖 `iterative_moe`，否则可能导致运行时错误。  

总体来看，此次重构主要是“清理”不再在主库提供的基线实现，风险在于潜在的 API 破坏。若确认该实现仅作参考基准，以上措施即可保证平滑迁移。

---

### [Model Runner V2] Move mrope_positions buffer to MRopeState (#32532)
**SHA**: `4147910` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4147910f1e893ba69aa86a210c73e02ae8a0dfde)

**变更概述**  
本次 PR 将原本在 `InputBuffers` 中的 **mrope_positions** 迁移至专门的 `MRopeState`，并相应修改了 CUDA‑graph 捕获、dummy‑run、输入准备以及 `InputBatch` 的字段定义。核心思路是让 M‑RoPE 相关的状态与其它 KV‑Cache、Prefill 状态解耦，避免在非 M‑RoPE 场景下仍分配/维护多余的张量。

**受影响的模块**  
- `vllm/v1/worker/gpu/cudagraph_utils.py` – 捕获函数签名新增 `mrope_positions` 参数，使用 `self.uses_mrope` 时取 `MRopeState.mrope_positions`。  
- `vllm/v1/worker/gpu/input_batch.py` – `InputBatch.mrope_positions` 类型改为可空，`InputBuffers` 删除原始张量的创建与零化。  
- `vllm/v1/worker/gpu/mm/mrope_utils.py` – `MRopeState` 新增 `max_num_tokens`、`mrope_positions` 缓冲区，并在 `prepare_mrope_positions` 中直接引用该缓冲区。  
- `vllm/v1/worker/gpu/model_runner.py` – 多处加入对 `self.uses_mrope` 的判断，统一从 `MRopeState` 读取/写入位置张量；dummy‑run、capture、execute、prepare_inputs 均作相应适配。

**关键影响**  
1. **内存管理**：`mrope_positions` 只在开启 M‑RoPE 时分配，降低了无关请求的显存占用。  
2. **代码路径统一**：所有使用位置的地方均通过 `self.uses_mrope` 与 `MRopeState` 访问，减少了分支差异，提升可维护性。  
3. **torch‑compile 兼容**：保留了原先通过在维度 0 前添加 dummy 位实现的非连续布局，仍能在 Torch‑compile 场景下工作。  
4. **向后兼容**：`InputBatch.mrope_positions` 变为 `Optional`，需要确保所有调用方在非 M‑RoPE 情况下不再访问该字段，否则会触发 `None` 错误。

**建议**  
- **单元/集成测试**：在开启/关闭 M‑RoPE 两种配置下跑完整的推理路径（prefill、decode、CUDA‑graph、piecewise）并检查 `torch.cuda.memory_allocated`，确保没有未释放的缓冲区。  
- **类型安全**：在所有使用 `input_batch.mrope_positions` 的代码块前添加 `assert input_batch.mrope_positions is not None`（如已有），防止运行时空指针。  
- **文档更新**：说明 `MRopeState` 现在持有 `mrope_positions`，并在 `ModelRunner` 初始化参数中加入 `max_num_tokens` 的意义。  
- **性能监控**：对比迁移前后的显存占用与图捕获时间，确保迁移带来的显存节省大于可能的额外一次拷贝或索引开销。  

总体而言，此次迁移使 M‑RoPE 的状态管理更清晰，显存利用更高，但需对空值检查和测试覆盖加以强化，以防在非 M‑RoPE 场景下出现意外的 `None` 访问。

---

### [build] fix cu130 related release pipeline steps and publish as nightly image (#32522)
**SHA**: `965765a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/965765aef94e4497c67ff30e0c8325e01e16d284)

**🎯 变更类型**：功能增强 / CI 流水线修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 CUDA 13.0 的基础镜像版本从 13.0.2 调整为 13.0.1，并在 ARM CPU 阶段补充对 `12.0`、`12.1` 计算能力的编译参数。  
2. 将原来的手工 Nightly 推送逻辑抽象为脚本 `push‑nightly‑builds.sh`，并在流水线上新增 “CUDA 13.0 Nightly” 任务。  
3. `cleanup-nightly-builds.sh` 现在接受标签前缀参数，实现对 `nightly-` 与 `cu130-nightly-` 两套标签的独立清理。

**🎯 影响范围**  
- Buildkite CI 配置（`.buildkite/release-pipeline.yaml`）  
- Docker 镜像发布流程（ECR → DockerHub）  
- 自动清理脚本，影响 DockerHub 上的 Nightly 标签保留策略  

**💡 关注建议**  
- 在提交后检查 CUDA 13.0 镜像是否成功在 x86_64 与 aarch64 上构建、推送并生成多架构 manifest。  
- 确认 `push‑nightly‑builds.sh` 在无参数和 `cu130` 参数两种情况下均能正确拼接原始 ECR tag（如 `-cu130`）并生成对应的 Nightly tag。  
- 运行一次 `cleanup-nightly-builds.sh`，验证它只删除指定前缀的旧标签，防止误删其他 Nightly 镜像。  
- 更新文档或 README，说明新增的 `cu130-nightly` 标签及其保留周期（14 天）。  
- 如使用 DockerHub Token，确保 CI 环境变量 `DOCKERHUB_TOKEN` 正确设置，避免登录失败。  

整体来看，此次改动提升了 CUDA 13.0 的发布兼容性，并把 Nightly 推送与清理抽象为可复用脚本，降低后续维护成本。建议在正式合并前跑一次完整的 CI 流程以捕获潜在的标签拼写或权限问题。

---

#### 🟢 低重要度变更 (3)

### [Model] Remove the unnecessary dtype conversion in MiniCPM (#32523)
**SHA**: `fe36bf5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fe36bf5e808d96ab18b8db393380dab3efb6c3f6)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `MiniCPM` 的 `forward` 中去除冗余的 dtype 转换，直接在原始数据类型上进行旋转嵌入和注意力计算，简化代码并略微提升性能。

---

### [Model Runner V2] Minor optimization for eagle input processing (#32535)
**SHA**: `963dc0b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/963dc0b865a3b6011fde7e0d938f86245dccbfac)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `model_runner` 中直接使用 `self.req_states`，去除不必要的索引映射；在 Eagle 输入准备的 Triton kernel 中加入 `idx_mapping` 参数，使其在处理请求状态时使用映射索引，提升输入处理效率。

---

### [Feature] Add FIPS 140-3 compliant hash algorithm option for multimodal hashing (#32386)
**SHA**: `3055232` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3055232ba0b95a68b3609cd95c029f94809e8bf1)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增环境变量 `VLLM_MM_HASHER_ALGORITHM`，支持 `blake3`、`sha256`、`sha512` 三种哈希算法；在 `MultiModalHasher` 中根据配置动态创建哈希对象，提供 FIPS 140‑3 合规选项。

---

