# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-02-15ï¼‰

## vllm-project/vllm

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-02-15 22:33:57 | Luka GovediÄ | [torch.compile] Disable ar-rms fusion for ds3-fp4 & DP, fix CI test (#34392) |
| 2026-02-15 22:33:08 | Maryam Tahhan | [CPU][ARM] Add ARM BF16 cross-compilation support and improve documenâ€¦ (#33079) |
| 2026-02-15 22:32:47 | Isotr0py | [MM Encoder] Add Triton ViT attention backend (#32183) |
| 2026-02-15 20:18:57 | Isotr0py | [Doc] Update Encoder-Decoder models support doc with Florence-2 (#34581) |
| 2026-02-15 15:26:10 | Seiji Eicher | [KV Connector] Add temporary, off-by-default `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION` workaround (#34415) |
| 2026-02-15 15:25:46 | haosdent | [Bugfix] Handle num_expert_group=None in flashinfer block-scale FP8 MoE (#34494) |
| 2026-02-15 15:25:17 | Vadim Gimpelson | [BUGFIX] Fix accuracy regression for NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4 with TP>1 (#34476) |
| 2026-02-15 15:24:25 | Stanislav Kirillov | [bugfix] Fix critical bug when reporting for all paths where handler.create_error_response is used (#34516) |
| 2026-02-15 15:08:38 | Andreas Karatzas | [CI][Entrypoints] Validate detokenize token IDs to prevent int64 overflow causing 500 (#34468) |
| 2026-02-15 12:29:23 | Andreas Karatzas | [Kernels] Fix Helion GPU utils to use platform-agnostic device name API (#34537) |
| 2026-02-15 10:29:03 | Woosuk Kwon | [Model Runner V2] Minor cleanup for Sampler (#34563) |
| 2026-02-15 05:15:56 | Thomas Parnell | [Hybrid] Enable mamba prefix cache "align" mode with async scheduling  (#33997) |
| 2026-02-15 02:14:21 | Cyrus Leung | [Renderer] Move InputPreprocessor into Renderer (1/2) (#34510) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 13 ä¸ªæäº¤ | ğŸ”´é«˜ 1 | ğŸŸ¡ä¸­ 7 | ğŸŸ¢ä½ 5
## ğŸ“‹ ç›®å½•

- [vllm-project/vllm](#vllm-project-vllm)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (1)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-1)
    - [[Renderer] Move InputPreprocessor into Renderer (1/2) (#3...](#73391a1)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (7)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-7)
    - [[torch.compile] Disable ar-rms fusion for ds3-fp4 & DP, f...](#23d825a)
    - [[MM Encoder] Add Triton ViT attention backend (#32183)](#71cd892)
    - [[Bugfix] Handle num_expert_group=None in flashinfer block...](#79f3fab)
    - [[BUGFIX] Fix accuracy regression for NVIDIA-Nemotron-3-Na...](#604b9ea)
    - [[bugfix] Fix critical bug when reporting for all paths wh...](#50dbd6c)
    - [[Model Runner V2] Minor cleanup for Sampler (#34563)](#9ca768c)
    - [[Hybrid] Enable mamba prefix cache "align" mode with asyn...](#d5fe3f7)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (5)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-5)
    - [[CPU][ARM] Add ARM BF16 cross-compilation support and imp...](#f07a128)
    - [[Doc] Update Encoder-Decoder models support doc with Flor...](#19fab44)
    - [[KV Connector] Add temporary, off-by-default `VLLM_DISABL...](#79c7e09)
    - [[CI][Entrypoints] Validate detokenize token IDs to preven...](#98bcc6c)
    - [[Kernels] Fix Helion GPU utils to use platform-agnostic d...](#f13e86d)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (1)

### [Renderer] Move InputPreprocessor into Renderer (1/2) (#34510)
**SHA**: `73391a1` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/73391a1baa459e78be1ade466517c7206ab7dd7c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ / æ¶æ„å˜æ›´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´ é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤å°† **InputPreprocessor** çš„èŒè´£å…¨éƒ¨è¿ç§»åˆ° **Renderer**ï¼Œå®ç°â€œæ¸²æŸ“å³é¢„å¤„ç†â€ã€‚  
- `Renderer` ç°åœ¨è´Ÿè´£å¤šæ¨¡æ€å¤„ç†ã€ç¼“å­˜ç®¡ç†ã€é»˜è®¤ Tokenize å‚æ•°ç­‰ï¼›  
- `InputProcessor` åªä¿ç•™è°ƒåº¦å±‚é¢çš„åŠŸèƒ½ï¼ŒåŸæœ‰çš„ `mm_processor`ã€`mm_cache_stats` ç­‰å­—æ®µå…¨éƒ¨ç§»è‡³ `Renderer`ï¼›  
- `StreamingInput` ç±»å‹ä» `vllm.inputs` ç§»åŠ¨åˆ° `vllm.engine.protocol`ï¼Œç»Ÿä¸€åè®®å±‚å®šä¹‰ï¼›  
- æ‰€æœ‰å·²æœ‰çš„æ¸²æŸ“å™¨ï¼ˆHfã€Mistralã€DeepseekV32ã€Grok2 ç­‰ï¼‰æ”¹ä¸º **æ„é€ å‡½æ•°æ¥å— `tokenizer` å®ä¾‹**ï¼Œå¹¶å®ç° `BaseRenderer[Tokenizer]` æ³›å‹ï¼›  
- æ–°å¢ `default_cmpl_tok_params`ã€`default_chat_tok_params`ã€`stat_mm_cache`ã€`update_mm_cache_stats`ã€`clear_mm_cache`ã€`shutdown` ç­‰ç»Ÿä¸€æ¥å£ï¼›  
- ç›¸å…³æµ‹è¯•ã€benchmarkã€engineã€åè®®å±‚ä»£ç åŒæ­¥æ›´æ–°ï¼Œå»é™¤æ—§çš„ `InputPreprocessor` ç›¸å…³å®ç°ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **æ ¸å¿ƒæ¸²æŸ“å±‚**ï¼š`vllm/renderers/*`ï¼ˆHfRendererã€MistralRendererã€DeepseekV32Rendererã€Grok2Rendererã€TerratorchRendererï¼‰  
- **è¾“å…¥/åè®®å±‚**ï¼š`vllm/engine/protocol.py`ã€`vllm/inputs/__init__.py`ã€`vllm/inputs/data.py`  
- **å¤šæ¨¡æ€å¤„ç†**ï¼š`vllm/multimodal/processing/context.py`ã€`vllm/inputs/preprocess.py`ã€`vllm/v1/engine/input_processor.py`  
- **å¼•æ“å…¥å£**ï¼š`vllm/v1/engine/async_llm.py`ã€`vllm/v1/engine/llm_engine.py`ã€`vllm/entrypoints/llm.py`ã€`vllm/entrypoints/openai/...`  
- **åº¦é‡ç»Ÿè®¡**ï¼š`vllm/v1/metrics/stats.py`ã€`vllm/benchmarks/mm_processor.py`  
- **æµ‹è¯•å¥—ä»¶**ï¼šæ‰€æœ‰ `tests/renderers/*`ã€`tests/entrypoints/openai/*`ã€`tests/v1/*`  

---

### ğŸ” æŠ€æœ¯æ´å¯Ÿ  

| ç»´åº¦ | å½±å“åˆ†æ |
|------|----------|
| **æ¶æ„å½±å“** | - **èŒè´£åˆå¹¶**ï¼šRenderer ç°åœ¨ç»Ÿä¸€è´Ÿè´£ **Prompt æ¸²æŸ“ã€Tokenizationã€Multimodal é¢„å¤„ç†ã€ç¼“å­˜ç»Ÿè®¡**ï¼Œé¿å…äº† `InputProcessor` ä¸ `Renderer` ä¹‹é—´çš„ç›¸äº’è°ƒç”¨å¯¼è‡´çš„å¾ªç¯ä¾èµ–ã€‚<br>- **æ¨¡å—è¾¹ç•Œæ¸…æ™°**ï¼š`InputProcessor` åªå‰©ä¸‹è°ƒåº¦ç›¸å…³çš„ **è¯·æ±‚æ‹†åˆ†ã€æ‰¹å¤„ç†ã€è¾“å‡ºæ”¶é›†**ï¼Œç®€åŒ–äº†å…¶æ¥å£ã€‚<br>- **æ³›å‹åŒ–æ¸²æŸ“å™¨**ï¼šé€šè¿‡ `BaseRenderer[Tokenizer]` æ³›å‹ï¼Œæ¸²æŸ“å™¨å¯ä»¥åœ¨ç±»å‹å±‚é¢æ˜ç¡® tokenizer ç±»å‹ï¼Œæå‡ IDE/ç±»å‹æ£€æŸ¥çš„å‡†ç¡®æ€§ã€‚<br>- **ç»Ÿä¸€ StreamingInput**ï¼šå°†æµå¼è¾“å…¥æŠ½è±¡ç§»åˆ°åè®®å±‚ï¼Œç»Ÿä¸€äº† Engine ä¸ Client çš„äº¤äº’å¥‘çº¦ã€‚ |
| **æ€§èƒ½å½±å“** | - **å‡å°‘ä¸€æ¬¡å¯¹è±¡åˆ›å»º**ï¼šä»¥å‰ `InputProcessor` ä¼šåœ¨æ¯æ¬¡è¯·æ±‚æ—¶è°ƒç”¨ `renderer._get_mm_processor()`ï¼Œç°åœ¨ç›´æ¥åœ¨ `Renderer` åˆå§‹åŒ–é˜¶æ®µåˆ›å»ºä¸€æ¬¡ `mm_processor`ï¼Œé™ä½äº†æ¯æ¬¡è¯·æ±‚çš„åˆå§‹åŒ–å¼€é”€ã€‚<br>- **ç¼“å­˜ç»Ÿè®¡åˆå¹¶**ï¼š`Renderer.update_mm_cache_stats()` åœ¨æ¯æ¬¡è¯·æ±‚ç»“æŸæ—¶ä¸€æ¬¡æ€§ç»Ÿè®¡ï¼Œé¿å…äº†åœ¨ `InputProcessor` ä¸­å¤šæ¬¡è°ƒç”¨ `stat_mm_cache()` å¸¦æ¥çš„é¢å¤–é”ç«äº‰ã€‚<br>- **Tokenizer å¤ç”¨**ï¼šRenderer æŒæœ‰ `self.tokenizer`ï¼Œé¿å…äº† `BaseRenderer.get_tokenizer()` çš„é‡å¤å±æ€§æ£€æŸ¥ã€‚<br>- **é»˜è®¤ Tokenize å‚æ•°**ï¼š`default_cmpl_tok_params` / `default_chat_tok_params` é€šè¿‡ `cached_property` æ‡’åŠ è½½ï¼Œå‡å°‘äº†æ¯æ¬¡è°ƒç”¨æ„é€ å‚æ•°çš„å¼€é”€ã€‚<br>**æ€»ä½“**ï¼šåœ¨å…¸å‹æµæ°´çº¿ï¼ˆæ–‡æœ¬+å¤šæ¨¡æ€ï¼‰ä¸‹é¢„è®¡å¯æå‡ **1â€‘3%** çš„ååé‡ï¼Œå°¤å…¶åœ¨å¤šæ¨¡æ€ç¼“å­˜å¼€å¯æ—¶æ›´æ˜¾è‘—ã€‚ |
| **å®‰å…¨è€ƒè™‘** | - **ç¼“å­˜æ¸…ç†/å…³é—­**ï¼šæ–°å¢ `Renderer.shutdown()` ä¸ `clear_mm_cache()`ï¼Œç¡®ä¿åœ¨è¿›ç¨‹é€€å‡ºæˆ– `reset_mm_cache` æ—¶å¤šæ¨¡æ€ç¼“å­˜è¢«å®‰å…¨é‡Šæ”¾ï¼Œé˜²æ­¢èµ„æºæ³„æ¼ã€‚<br>- **ç±»å‹å®‰å…¨**ï¼š`BaseRenderer` ç°åœ¨æ˜¾å¼å£°æ˜ `tokenizer` ä¸º `TokenizerLike | None`ï¼Œåœ¨ `skip_tokenizer_init=True` åœºæ™¯ä»ä¼šæŠ›å‡ºæ˜ç¡®å¼‚å¸¸ï¼Œé¿å…è¯¯ç”¨æœªåˆå§‹åŒ–çš„ tokenizer å¯¼è‡´æ½œåœ¨çš„ **ç©ºæŒ‡é’ˆ** é”™è¯¯ã€‚<br>- **è¾“å…¥æ ¡éªŒ**ï¼š`vllm/inputs/preprocess.py` ä¸­å¯¹ `mm_hashes` å†…å®¹çš„æ ¡éªŒï¼ˆä»…æ¥å—å­—ç¬¦ä¸²ï¼‰ä»ä¿ç•™ï¼Œä½†è¿ç§»åˆ° `Renderer`ï¼Œé˜²æ­¢è‡ªå®šä¹‰ `MultiModalProcessor` å®ç°è¿”å›éæ³•ç»“æ„å¯¼è‡´åç»­åºåˆ—åŒ–/æ—¥å¿—æ³„éœ²ã€‚ |
| **å¯ç»´æŠ¤æ€§** | - **ç»Ÿä¸€æ¥å£**ï¼š`renderer.stat_mm_cache()`ã€`renderer.clear_mm_cache()`ã€`renderer.shutdown()` ç­‰ç»Ÿä¸€å…¥å£ï¼Œä½¿å¾—æœªæ¥å¯¹å¤šæ¨¡æ€ç¼“å­˜çš„ç‰¹æ€§ï¼ˆå¦‚ç»Ÿè®¡ã€æŒä¹…åŒ–ï¼‰åªéœ€åœ¨æ¸²æŸ“å±‚å®ç°ä¸€æ¬¡ã€‚<br>- **ä»£ç é‡å¤åº¦é™ä½**ï¼šå¤šä¸ªæ¸²æŸ“å™¨çš„ `from_config` å®ç°ç»Ÿä¸€ä¸º `return cls(config, tokenizer)`ï¼Œæ¶ˆé™¤å¤§é‡å†—ä½™ `_tokenizer` å±æ€§ä¸ `get_tokenizer` æ–¹æ³•ã€‚<br>- **æµ‹è¯•è¦†ç›–**ï¼šå¤§é‡æµ‹è¯•å·²åŒæ­¥æ”¹ä¸ºä½¿ç”¨ `Renderer.from_config`ï¼Œç¡®ä¿æ–°è·¯å¾„çš„æ­£ç¡®æ€§ã€‚ |
| **å…¼å®¹æ€§** | - **å‘åå…¼å®¹**ï¼š`Renderer.from_config` ä»æ¥å— `tokenizer_kwargs`ï¼Œä½†å†…éƒ¨è‡ªè¡Œå†³å®šæ˜¯å¦å®ä¾‹åŒ– tokenizerï¼›æ—§çš„ `skip_tokenizer_init` è¡Œä¸ºä¿æŒä¸å˜ã€‚<br>- **API å˜æ›´**ï¼šå¤–éƒ¨è°ƒç”¨ `input_processor.clear_mm_cache()`ã€`input_processor.stat_mm_cache()` å·²å…¨éƒ¨è¿ç§»åˆ° `renderer`ï¼Œæ—§ä»£ç å¦‚æœç›´æ¥å¼•ç”¨ `InputProcessor` å°†å‡ºç°å±æ€§é”™è¯¯ï¼Œéœ€è¦ç›¸åº”è¿ç§»ã€‚<br>- **StreamingInput**ï¼šä» `vllm.inputs` ç§»åŠ¨åˆ° `vllm.engine.protocol`ï¼Œä½†ä¿æŒåŒåå¯¼å‡ºï¼Œä»å¯é€šè¿‡æ—§è·¯å¾„å¯¼å…¥ï¼ˆå·²åœ¨ `__init__` ä¸­åˆ é™¤æ—§å¯¼å‡ºï¼‰ï¼Œå› æ­¤éœ€è¦æ›´æ–°æ‰€æœ‰ `import StreamingInput` ä½ç½®ã€‚ |

---

### âš ï¸ æ½œåœ¨é£é™©  

| é£é™©ç‚¹ | è¯´æ˜ | ç¼“è§£æªæ–½ |
|--------|------|----------|
| **ç ´åæ—§æ’ä»¶/æ‰©å±•** | ç¬¬ä¸‰æ–¹æ’ä»¶å¯èƒ½ç›´æ¥å¼•ç”¨ `input_processor.input_preprocessor` æˆ– `InputPreprocessor` çš„å†…éƒ¨æ–¹æ³•ï¼›è¿ç§»åè¿™äº›å±æ€§æ¶ˆå¤±ã€‚ | - åœ¨æ–‡æ¡£ä¸­æ˜ç¡®è¿ç§»æŒ‡å—ã€‚<br>- æä¾›å…¼å®¹å±‚ï¼ˆå¯åœ¨ `vllm/input_processor.py` ä¸­ä¿ç•™å°‘é‡åŒ…è£…å±æ€§ï¼Œæ ‡è®°ä¸ºå·²åºŸå¼ƒï¼‰ã€‚ |
| **å¤šæ¨¡æ€ç¼“å­˜èµ„æºæ³„æ¼** | `Renderer.shutdown()` åªåœ¨ `AsyncLLM.shutdown` ä¸­è°ƒç”¨ï¼›å¦‚æœç”¨æˆ·è‡ªè¡Œåˆ›å»º `Renderer` å®ä¾‹è€Œä¸èµ° engine è·¯å¾„ï¼Œå¯èƒ½å¿˜è®°è°ƒç”¨ `shutdown`ã€‚ | - åœ¨ `BaseRenderer.__del__` ä¸­æ·»åŠ è­¦å‘Šæ—¥å¿—æé†’ï¼›<br>- åœ¨ `renderer_from_config` è¿”å›çš„å®ä¾‹ä¸Šä½¿ç”¨ `contextmanager`ï¼ˆå¯åœ¨åç»­ PR å¼•å…¥ï¼‰ã€‚ |
| **å¹¶å‘ç»Ÿè®¡ç«äº‰** | `update_mm_cache_stats` åœ¨æ¯ä¸ªè¯·æ±‚ç»“æŸåè°ƒç”¨ï¼Œè‹¥å¤šçº¿ç¨‹/å¤šè¿›ç¨‹å…±äº«åŒä¸€ `Renderer`ï¼ˆå¦‚åœ¨ forked å­è¿›ç¨‹ä¸­ï¼‰ï¼Œç»Ÿè®¡å¯¹è±¡å¯èƒ½æœªä½¿ç”¨è¿›ç¨‹å®‰å…¨çš„åŒæ­¥ã€‚ | - `MultiModalCacheStats` å·²æ˜¯çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨ï¼›ç¡®è®¤åœ¨å¤šè¿›ç¨‹ fork åé‡æ–°å®ä¾‹åŒ– `Renderer`ï¼ˆå·²åœ¨ `AsyncLLM` ä¸­é€šè¿‡ `set_default_torch_num_threads` éš”ç¦»ï¼‰ã€‚ |
| **Tokenizer åˆå§‹åŒ–å»¶è¿Ÿ** | å¯¹äº `skip_tokenizer_init=True` åœºæ™¯ï¼Œæ¸²æŸ“å™¨æŒæœ‰ `tokenizer=None`ã€‚éƒ¨åˆ†æ—§ä»£ç å¯èƒ½ç›´æ¥è®¿é—® `renderer.tokenizer` è€Œæœªæ£€æŸ¥ `None`ã€‚ | - å·²åœ¨ `BaseRenderer.get_tokenizer` æŠ›å‡ºæ˜ç¡®å¼‚å¸¸ï¼›å»ºè®®ä½¿ç”¨ `renderer.get_tokenizer()` å‰å…ˆæ£€æŸ¥ `renderer.tokenizer`ã€‚ |
| **é»˜è®¤ Tokenize å‚æ•°å˜æ›´**

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (7)

### [torch.compile] Disable ar-rms fusion for ds3-fp4 & DP, fix CI test (#34392)
**SHA**: `23d825a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/23d825aba11afcc6713e9b11acb54c473a734501)

**å˜æ›´æ¦‚è§ˆ**  
1. åœ¨ `ModelConfig.is_nvfp4_quantized` ä¸­åŠ å…¥å¯¹ ModelOptâ€¯FP4 ä¸ Compressedâ€‘Tensorsï¼ˆ`nvfp4-pack-quantized`ï¼‰çš„åˆ¤å®šã€‚  
2. `enable_allreduce_rms_fusion` é€»è¾‘æ”¾å®½åˆ° Hopper ä¸ Blackwellï¼ˆdeviceâ€¯capâ€¯90â€¯æˆ–â€¯100ï¼‰ï¼Œå¹¶å¼ºåˆ¶åœ¨ Tensorâ€‘Parallelâ€¯>â€¯1 ä¸” Dataâ€‘Parallelâ€¯=â€¯1 æ—¶æ‰å¼€å¯ï¼Œè§„é¿å·²çŸ¥ TPâ€‘DP ç»„åˆç ´åã€‚  
3. ä¸º DeepSeekâ€¯V3 æ–°å¢é…ç½®ç±» `DeepseekV3ForCausalLM`ï¼Œé»˜è®¤å…³é—­ ARâ€‘RMSâ€‘FP4 èåˆï¼Œå¹¶åœ¨ç”¨æˆ·æ˜¾å¼å¼€å¯æ—¶ç»™å‡ºè­¦å‘Šï¼›`DeepseekV32ForCausalLM` ç»§æ‰¿å¹¶å¤ç”¨è¯¥é€»è¾‘ã€‚  
4. å°†æ–°ç±»åŠ å…¥æ¨¡å‹æ˜ å°„è¡¨ã€‚

**å½±å“èŒƒå›´**  
- `vllm/config/model.py`ã€`vllm/config/vllm.py`ã€`vllm/model_executor/models/config.py` ä»¥åŠæ¨¡å‹æ˜ å°„è¡¨ã€‚  
- ä¾èµ– `torch.compile`â€‘fusion ä¸ FlashInfer çš„è¿è¡Œæ—¶è·¯å¾„ï¼›å°¤å…¶æ˜¯ä½¿ç”¨ DeepSeekâ€¯V3â€¯+â€¯NVFP4 é‡åŒ–ã€ä»¥åŠå¤šå¡ï¼ˆTP>1ï¼‰åœºæ™¯ã€‚

**å…³æ³¨ç‚¹ & å»ºè®®**  
- **å…¼å®¹æ€§**ï¼š`is_nvfp4_quantized` åªæ£€æŸ¥ `self.quantization` ä¸º `"modelopt_fp4"` æˆ– `"compressed-tensors"`ï¼Œè‹¥æœªæ¥å¼•å…¥å…¶ä»–é‡åŒ–æ ‡è¯†éœ€åŒæ­¥æ›´æ–°ã€‚  
- **é…ç½®é»˜è®¤**ï¼š`fuse_allreduce_rms` ä¸º `None` æ—¶ä¼šè¢«è‡ªåŠ¨ç½® `False`ï¼Œç¡®ä¿æ–‡æ¡£è¯´æ˜é»˜è®¤è¡Œä¸ºå·²æ”¹ä¸ºå…³é—­ã€‚  
- **æµ‹è¯•**ï¼šå¢åŠ é’ˆå¯¹ `cfg.parallel_config.data_parallel_size>1` çš„è´Ÿå‘æµ‹è¯•ï¼ŒéªŒè¯ fusion è¢«æ­£ç¡®å±è”½ï¼›ä»¥åŠé’ˆå¯¹ä¸åŒ device capabilityï¼ˆ90ã€100ï¼‰ çš„æ­£å‘æµ‹è¯•ã€‚  
- **æ—¥å¿—**ï¼šå½“å‰åªåœ¨æ˜¾å¼å¼€å¯æ—¶ warningï¼Œå»ºè®®åœ¨ CI ä¸­åŠ å…¥å¯¹åº”æ—¥å¿—æˆªå–ï¼Œé˜²æ­¢ç”¨æˆ·å¿½è§†ã€‚  
- **æ–‡æ¡£**ï¼šæ›´æ–° README/Config éƒ¨åˆ†ï¼Œæ˜ç¡® â€œNVFP4 + DeepSeekV3 ä¸æ”¯æŒ ARâ€‘RMS èåˆâ€ ä»¥åŠ â€œFlashInfer ä»…åœ¨ Hopper/Blackwellã€TP>1ã€DP=1 æ—¶å¯ç”¨â€ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨å®šä½æ˜ç¡®ã€é£é™©å¯æ§ï¼Œåªéœ€è¡¥é½æµ‹è¯•ä¸æ–‡æ¡£å³å¯ç¡®ä¿å¹³æ»‘ä¸Šçº¿ã€‚

---

### [MM Encoder] Add Triton ViT attention backend (#32183)
**SHA**: `71cd892` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/71cd89264f6c88ee4cca7c3cc556885e8844fc92)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º Visionâ€‘Transformer åŠ å…¥ Triton æ³¨æ„åŠ›å®ç°ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `mm_encoder_attention.py` ä¸­å®ç°äº† `_forward_triton`ï¼Œè°ƒç”¨ `vit_triton_attn_wrapper` å®Œæˆ ViT å‰å‘æ³¨æ„åŠ›ï¼›åœ¨ `forward_cuda` ä¸­åŠ å…¥å¯¹ `TRITON_ATTN` çš„åˆ†æ”¯ã€‚  
2. `vllm/v1/attention/ops/vit_attn_wrappers.py` æ–°å¢ Triton å‰å‘å®ç°åŠå¯¹åº”çš„ â€œfakeâ€ å­å‡½æ•°ï¼Œå¹¶é€šè¿‡ `direct_register_custom_op` æ³¨å†Œä¸ºè‡ªå®šä¹‰ opã€‚  
3. `platforms.cuda / rocm` æ‰©å±• `get_supported_vit_attn_backends`ï¼Œå°† Triton åŠ å…¥å¯é€‰åˆ—è¡¨ï¼›åœ¨ `get_vit_attn_backend` ä¸­éå†åç«¯å¹¶åŠ å…¥ computeâ€‘capability æ£€æŸ¥ã€‚  
4. å¤šä¸ªæ¨¡å‹ï¼ˆvision ç³»åˆ—ï¼‰å¯¹ `TRITON_ATTN` æ”¾å®½äº†åç«¯æ£€æŸ¥ï¼Œç»Ÿä¸€ä½¿ç”¨ `cu_seqlens` è®¡ç®— `max_seqlen`ã€‚  
5. æµ‹ä¾‹ `test_mha_attn.py` å¢åŠ äº† headâ€‘size 72ï¼ˆä¸æ»¡è¶³ FlashAttention 32 æ•´é™¤ï¼‰æ—¶åº”è½åˆ° Triton åç«¯çš„æ–­è¨€ï¼Œå¹¶å¯¹ Triton ç»“æœæ”¾å®½å®¹å·®ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- æ ¸å¿ƒï¼š`mm_encoder_attention.py`ã€`vit_attn_wrappers.py`ã€å¹³å°åç«¯æ£€æµ‹ (`cuda.py`, `rocm.py`)  
- ç›¸å…³æ¨¡å‹ï¼š`dots_ocr`, `ernie45_vl`, `glm4_1v`, `paddleocr_vl`, `qwen2_vl`, `qwen2_5_vl`, `qwen3_vl`, `qwen3_omni_moe_thinker` ç­‰æ‰€æœ‰ ViTâ€‘type æ¨¡å‹  
- æµ‹è¯•ï¼š`tests/kernels/attention/test_mha_attn.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **å…¼å®¹æ€§**ï¼šTriton ä¾èµ–åœ¨æœªç¼–è¯‘/æœªå®‰è£…æ—¶ä¼šæŠ› `ImportError`ã€‚ç›®å‰ `triton_attn_wrapper` åœ¨å¯¼å…¥æ—¶æ‰åŠ è½½ `context_attention_fwd`ï¼Œä½† `direct_register_custom_op` ä¼šåœ¨æ¨¡å—å¯¼å…¥æ—¶æ‰§è¡Œã€‚å»ºè®®åœ¨ `triton_attn_wrapper_fake` ä¸­åŠ å…¥ `try/except`ï¼Œæˆ–åœ¨æ³¨å†Œå‰æ£€æµ‹ `torch.ops.vllm` æ˜¯å¦å·²ç”Ÿæˆï¼Œä»¥å…åœ¨ CPU/ä¸æ”¯æŒå¹³å°å¯åŠ¨æ—¶å´©æºƒã€‚  

2. **æ•°å€¼è¯¯å·®**ï¼šæµ‹è¯•å¯¹ Triton ç»“æœä½¿ç”¨ `rtol=1e-3, atol=1e-3`ï¼Œè¯´æ˜ç²¾åº¦ç•¥é€Šäº FlashAttentionã€‚äº§å“çº¿è‹¥å¯¹æ•°å€¼ä¸¥æ ¼ï¼ˆå¦‚ OCRï¼‰éœ€åœ¨æ–‡æ¡£ä¸­æ³¨æ˜å¯èƒ½çš„å¾®å°åå·®ï¼Œå¹¶åœ¨å…³é”®è·¯å¾„ï¼ˆå¦‚æ£€ç´¢ï¼‰åšå®¹å·®æ ¡éªŒã€‚  

3. **è®¡ç®—èƒ½åŠ›æ£€æŸ¥**ï¼š`get_vit_attn_backend` ç°åœ¨å…ˆéå†æ‰€æœ‰åç«¯å¹¶åœ¨æ¯æ¬¡å¾ªç¯ä¸­æ£€æŸ¥ `supports_compute_capability`ã€‚å¯¹ Torchâ€‘SDPA ä»ä¿ç•™åœ¨åˆ—è¡¨æœ€åï¼Œç¡®ä¿åœ¨ä¸æ»¡è¶³ Flash/Triton æ¡ä»¶æ—¶ä»èƒ½å›é€€ã€‚ä½†è‹¥æœªæ¥æ–°å¢åç«¯ï¼Œéœ€ç¡®ä¿ `supports_compute_capability` æ–¹æ³•å­˜åœ¨å¹¶è¿”å› `True`ã€‚  

4. **cu_seqlens å¿…é¡»åŒæ­¥**ï¼š`_forward_triton` å¼ºåˆ¶ `cu_seqlens` ä¸ `max_seqlen` åŒæ—¶æä¾›æˆ–å‡ä¸º `None`ã€‚å½“å‰ `forward_cuda` å§‹ç»ˆæŠŠ selector è®¡ç®—çš„ `cu_seqlens`/`max_seqlen` ä¼ å…¥ï¼Œå»ºè®®åœ¨æ–‡æ¡£æˆ–æ³¨é‡Šä¸­æ˜ç¡®è¿™ä¸€å‰æï¼Œé˜²æ­¢å¤–éƒ¨ç›´æ¥è°ƒç”¨æ—¶æ¼ä¼ ã€‚  

5. **å…¨å±€ dtype æ”¹åŠ¨**ï¼šæµ‹è¯•ä¸­ä½¿ç”¨ `set_default_torch_dtype(torch.float32)`ï¼Œè¯¥å‡½æ•°ä¼šå½±å“åç»­æ‰€æœ‰å¼ é‡åˆ›å»ºã€‚è‹¥å…¶ä»–å¹¶è¡Œæµ‹è¯•ä¾èµ–é»˜è®¤ dtypeï¼Œå¯èƒ½äº§ç”Ÿäº¤å‰å½±å“ã€‚å»ºè®®åœ¨æ¯ä¸ªæµ‹è¯•ç»“æŸåæ¢å¤åŸ dtypeï¼Œæˆ–æ”¹ä¸º `with torch.autocast(...):` çš„å±€éƒ¨åˆ‡æ¢ã€‚  

6. **æ¨¡å‹çº§åç«¯é™åˆ¶**ï¼š`paddleocr_vl`ã€`qwen*` ç³»åˆ—åŸæœ¬åœ¨æ„é€ æ—¶æ˜¾å¼é™åˆ¶åç«¯ï¼Œç°åœ¨ä»…åœ¨ `compute_attn_mask_seqlen` ä¸­åŠ å…¥ Tritonï¼Œä»ä¿ç•™æ—§çš„ `if self.attn_backend not in {...}` æ£€æŸ¥è¢«åˆ é™¤ã€‚ç¡®è®¤è¿™äº›æ¨¡å‹åœ¨è¿è¡Œæ—¶ä¸ä¼šå›  Triton ä¸æ”¯æŒçš„ headâ€‘size/dtype ç›´æ¥æŠ¥é”™ï¼›è‹¥æœ‰ï¼Œæœ€å¥½åœ¨æ„é€ é˜¶æ®µæå‰æŠ›å‡ºå‹å¥½é”™è¯¯ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ–°å¢ Triton ViT æ³¨æ„åŠ›å®ç°æå‡äº†åœ¨é 32 æ•´é™¤ headâ€‘size åœºæ™¯ä¸‹çš„ CUDA æ”¯æŒï¼Œæ”¹åŠ¨èŒƒå›´é›†ä¸­ä¸”å·²è¦†ç›–ä¸»è¦æ¨¡å‹ã€‚é‡ç‚¹æ£€æŸ¥ Triton ä¾èµ–çš„å¯ç”¨æ€§åŠæ•°å€¼è¯¯å·®ä¼ æ’­ï¼Œå³å¯åœ¨ç”Ÿäº§ç¯å¢ƒå®‰å…¨ä¸Šçº¿ã€‚

---

### [Bugfix] Handle num_expert_group=None in flashinfer block-scale FP8 MoE (#34494)
**SHA**: `79f3fab` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/79f3fab05a2d88a5db73591cd3b8afdf956ee723)

**å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**å˜æ›´æ‘˜è¦**  
- ä¸º `flashinfer_fused_moe_blockscale_fp8` æ·»åŠ å¯¹ `num_expert_group=None` çš„å®¹é”™å¤„ç†ï¼Œå°†å…¶åœ¨è°ƒç”¨åº•å±‚ kernel å‰ç»Ÿä¸€è½¬ä¸º `0`ï¼Œé˜²æ­¢ DeepSeekV3 è·¯ç”±åœ¨ç¼ºå°‘åˆ†ç»„ä¿¡æ¯æ—¶å´©æºƒã€‚  
- æ–°å¢å›å½’æµ‹è¯• `test_flashinfer_blockscale_fp8_none_expert_group`ï¼Œæ¨¡æ‹Ÿ MiniMaxâ€‘M2.1 åœºæ™¯éªŒè¯ `num_expert_group=None` ä¸ä¼šå¯¼è‡´å¼‚å¸¸ï¼Œå¹¶æ£€æŸ¥è¾“å‡ºå½¢çŠ¶ã€‚

**å½±å“èŒƒå›´**  
- `vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py`ï¼ˆæ ¸å¿ƒ MoE è°ƒåº¦å…¥å£ï¼‰ã€‚  
- `tests/kernels/moe/test_flashinfer.py`ï¼ˆæ–°å¢å•å…ƒæµ‹è¯•ï¼‰ã€‚  
- ä¾èµ– `flashinfer_trtllm_fp8_block_scale_moe` çš„æ‰€æœ‰æ¨¡å‹æ¨ç†è·¯å¾„ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ DeepSeekV3 è·¯ç”±çš„ FP8 blockâ€‘scale MoEã€‚

**å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®è®¤åº•å±‚ kernel åœ¨ `num_expert_group==0` æ—¶çš„è¡Œä¸ºç­‰ä»·äº â€œæ— åˆ†ç»„â€ã€‚è‹¥åç»­å®ç°å¯¹ `num_expert_group` è¿›è¡Œæ˜¾å¼åˆ¤æ–­ï¼Œéœ€åŒæ­¥æ›´æ–°ã€‚  
2. **æ–‡æ¡£åŒæ­¥**ï¼šåœ¨ API æ–‡æ¡£å’Œ `vllm.utils.flashinfer` çš„ä½¿ç”¨è¯´æ˜ä¸­æ³¨æ˜ `num_expert_group` ä¸ºå¯é€‰ï¼Œ`None` / `0` è¡¨ç¤ºä¸åˆ†ç»„ã€‚  
3. **ä»£ç å¯è¯»æ€§**ï¼šå»ºè®®åœ¨å‡½æ•°ä½“å‰åŠ å…¥æ³¨é‡Šï¼Œè§£é‡Š `num_expert_group` ä¸º `None` æ—¶è½¬ä¸º `0` çš„æ„å›¾ï¼Œé¿å…è¯¯è§£ã€‚  
4. **æµ‹è¯•è¦†ç›–**ï¼šå½“å‰åªè¦†ç›– DeepSeekV3 åœºæ™¯ï¼Œå»ºè®®è¡¥å……å…¶ä»–è·¯ç”±ï¼ˆå¦‚ Topâ€‘Kã€Randomï¼‰åœ¨ `num_expert_group=None` ä¸‹çš„è¡Œä¸ºï¼Œç¡®ä¿ç»Ÿä¸€å®¹é”™ã€‚  
5. **æ€§èƒ½è¯„ä¼°**ï¼šè¯¥æ”¹åŠ¨ä»…æ˜¯å‚æ•°åŒ…è£…ï¼Œå¯¹æ€§èƒ½æ— ç›´æ¥å½±å“ï¼Œä½†è¯·åœ¨ CI ä¸­è·‘ä¸€æ¬¡å®Œæ•´çš„æ€§èƒ½åŸºå‡†ï¼Œç¡®ä¿ `0` å‚æ•°ä¸ä¼šè§¦å‘ä¸å¿…è¦çš„åˆ†æ”¯æˆ–é¢å¤–è®¡ç®—ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¿®å¤äº†çœŸå®éƒ¨ç½²ä¸­å› ç¼ºå°‘åˆ†ç»„ä¿¡æ¯å¯¼è‡´çš„ Crashï¼Œé£é™©å¯æ§ï¼Œå»ºè®®åˆå¹¶ã€‚

---

### [BUGFIX] Fix accuracy regression for NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4 with TP>1 (#34476)
**SHA**: `604b9ea` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/604b9eaec53a11ae5193348f6a623dd7cdef48bf)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBugâ€¯Fixï¼ˆä¿®æ­£ NVIDIAâ€‘Nemotronâ€‘3â€‘Nanoâ€‘30Bâ€‘A3Bâ€‘NVFP4 åœ¨ TP>1 æ—¶çš„ç²¾åº¦å›é€€ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
åœ¨ `vllm/model_executor/layers/mamba/mamba_mixer2.py` ä¸­ï¼Œé’ˆå¯¹ **n_groups èƒ½è¢« TP å¤§å°æ•´é™¤** çš„æƒ…å†µæ”¹ç”¨ `MergedColumnParallelLinear` è¿›è¡Œä¸€æ¬¡æ€§æ‹†åˆ†ï¼Œä»¥é¿å…åŸå…ˆé€šè¿‡ `ColumnParallelLinear` + è‡ªå®šä¹‰ weight_loader çš„é‡å¤åŠ è½½å¯¼è‡´çš„æ•°å€¼è¯¯å·®ï¼›åŒæ—¶ä¿ç•™ n_groupsâ€¯=â€¯1 æ—¶çš„â€œå¤åˆ¶ç»„â€è·¯å¾„ã€‚æ–°å¢äº†å¯¹ `conv1d`ã€`in_proj` çš„ weight_loader åˆ é™¤ä¸é‡æ–°è®¾ç½®é€»è¾‘ï¼Œå¹¶åœ¨é‡åŒ–å±‚ï¼ˆ`BasevLLMParameter`ï¼‰ä¸Šç›´æ¥èµ‹å€¼ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- Mamba ç›¸å…³å±‚å®ç° (`mamba_mixer2.py`)  
- è‡ªå®šä¹‰ç®—å­åŠ è½½ (`custom_op`, `mamba_v2_sharded_weight_loader`)  
- å¹¶è¡Œçº¿æ€§å±‚ (`ColumnParallelLinear`, `MergedColumnParallelLinear`)  
- é‡åŒ–è·¯å¾„çš„æƒé‡åŠ è½½ï¼ˆFP8 ç­‰ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®ä¿ `MergedColumnParallelLinear` åœ¨æ—§ç‰ˆæ¨¡å‹æˆ– n_groups%tp!=0 åœºæ™¯ä»ä¿æŒè¡Œä¸ºä¸€è‡´ã€‚  
2. **é‡åŒ–è·¯å¾„æµ‹è¯•**ï¼šç‰¹åˆ«æ˜¯ `BasevLLMParameter` åˆ†æ”¯ï¼ŒéªŒè¯ FP8/FP4 ç­‰é‡åŒ–æ¨¡å‹åœ¨ TP>1 æ—¶åŠ è½½æ­£ç¡®ã€‚  
3. **æ€§èƒ½éªŒè¯**ï¼šå¯¹æ¯”æ”¹åŠ¨å‰åçš„æ˜¾å­˜å ç”¨ä¸æ¨ç†ååï¼Œç¡®è®¤åˆå¹¶æ‹†åˆ†ä¸ä¼šå¼•å…¥é¢å¤–å¤åˆ¶å¼€é”€ã€‚  
4. **å›å½’å¥—ä»¶**ï¼šæ·»åŠ é’ˆå¯¹ Nemotronâ€‘3â€‘Nanoâ€‘30Bâ€‘A3Bâ€‘NVFP4 çš„å¤š TPï¼ˆâ‰¥2ï¼‰ç²¾åº¦å¯¹æ¯”æµ‹è¯•ï¼Œé˜²æ­¢ç±»ä¼¼å›å½’ã€‚  
5. **æ–‡æ¡£/æ³¨é‡Š**ï¼šæ˜ç¡® `MergedColumnParallelLinear` ä½¿ç”¨æ¡ä»¶åŠ `group_shard_settings` å«ä¹‰ï¼Œä¾¿äºåç»­ç»´æŠ¤ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡ä¿®å¤èšç„¦åœ¨æƒé‡åˆ‡åˆ†é€»è¾‘ï¼Œèƒ½å¤Ÿæ¢å¤ç‰¹å®šæ¨¡å‹åœ¨å¤šå¡æƒ…å†µä¸‹çš„æ•°å€¼å‡†ç¡®æ€§ï¼Œé£é™©ä¸»è¦åœ¨æ–°å±‚çš„å®ç°å’Œé‡åŒ–åŠ è½½çš„å…¼å®¹æ€§ï¼Œéœ€è¦é€šè¿‡å®Œæ•´çš„å¤š TP å›å½’æµ‹è¯•æ¥éªŒè¯ã€‚

---

### [bugfix] Fix critical bug when reporting for all paths where handler.create_error_response is used (#34516)
**SHA**: `50dbd6c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/50dbd6c9e6637589b404a499dbc34df85ad9b1ad)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨æ‰€æœ‰ OpenAI/Pooling/Serve è·¯ç”±ä¸­ï¼Œå¼‚å¸¸åˆ†æ”¯åŸæœ¬ç›´æ¥ `return handler.create_error_response(e)`ï¼Œå¯¼è‡´åç»­ç»Ÿä¸€çš„ `isinstance(..., ErrorResponse)` åˆ¤å®šå¤±æ•ˆï¼ˆè¿”å›çš„å¯¹è±¡æœªèµ‹ç»™ `generator/ result/ response`ï¼‰ï¼Œäº§ç”Ÿ `UnboundLocalError` æˆ–é”™è¯¯çš„å“åº”ç»“æ„ã€‚æ­¤æäº¤æ”¹ä¸ºå…ˆæŠŠé”™è¯¯å¯¹è±¡èµ‹ç»™å¯¹åº”å˜é‡ï¼Œå†äº¤ç”±ç»Ÿä¸€çš„ `ErrorResponse` åˆ¤æ–­ä¸ `JSONResponse` åŒ…è£…ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `vllm/entrypoints/openai/*/api_router.py`ï¼ˆchatã€completionã€responsesï¼‰  
- `vllm/entrypoints/pooling/*/api_router.py`ï¼ˆclassifyã€embedã€poolingã€scoreï¼‰  
- `vllm/entrypoints/serve/*/api_router.py`ï¼ˆdisaggã€tokenizeï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
1. **é”™è¯¯å¯¹è±¡ç±»å‹**ï¼šç¡®è®¤ `handler.create_error_response` å§‹ç»ˆè¿”å› `ErrorResponse`ï¼ˆåŒæ­¥å¯¹è±¡ï¼‰ï¼Œè€Œä¸æ˜¯åç¨‹æˆ– `Response`ï¼Œå¦åˆ™ä»ä¼šåœ¨åç»­ `isinstance` æ£€æŸ¥å¤±æ•ˆã€‚  
2. **ä¸€è‡´æ€§**ï¼šæ‰€æœ‰è·¯ç”±ç°åœ¨é‡‡ç”¨ â€œèµ‹å€¼â€‘åˆ¤å®šâ€‘è¿”å›â€ æ¨¡å¼ï¼Œå»ºè®®åœ¨é¡¹ç›®æ–‡æ¡£æˆ–ä»£ç æ³¨é‡Šé‡Œç»Ÿä¸€è¯´æ˜æ­¤çº¦å®šï¼Œé˜²æ­¢æœªæ¥è¯¯ç”¨ `return`ã€‚  
3. **å•å…ƒæµ‹è¯•**ï¼šè¡¥å……æˆ–å¼ºåŒ–å¼‚å¸¸è·¯å¾„çš„æµ‹è¯•ï¼ŒéªŒè¯åœ¨å¼‚å¸¸æŠ›å‡ºæ—¶è¿”å›çš„ HTTP çŠ¶æ€ç ã€é”™è¯¯ç ã€JSON ç»“æ„å‡ç¬¦åˆé¢„æœŸã€‚  
4. **æ€§èƒ½å½±å“**ï¼šæ”¹åŠ¨ä»…æ˜¯æ§åˆ¶æµï¼Œæ— é¢å¤– I/Oï¼Œå‡ ä¹ä¸å½±å“ååæˆ–å»¶è¿Ÿã€‚  
5. **ä»£ç å®¡æŸ¥**ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ `return` å†™æ³•ï¼ˆå¦‚åœ¨ `retrieve_responses` ä¸­å˜é‡åæ”¹ä¸º `response`ï¼‰ï¼Œä¿æŒå˜é‡å‘½åä¸åç»­åˆ¤å®šä¸€è‡´ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡ä¿®å¤æ¶ˆé™¤äº†åœ¨é”™è¯¯å¤„ç†ä¸­å› æå‰è¿”å›å¯¼è‡´çš„ â€œæœªå®šä¹‰å˜é‡â€/â€œé”™è¯¯å“åº”æ ¼å¼â€ é—®é¢˜ï¼Œæå‡äº† API çš„å¥å£®æ€§ã€‚åç»­è¯·å…³æ³¨å¼‚å¸¸å¤„ç†ç»Ÿä¸€æ€§ä»¥åŠç›¸åº”çš„æµ‹è¯•è¦†ç›–ç‡ã€‚

---

### [Model Runner V2] Minor cleanup for Sampler (#34563)
**SHA**: `9ca768c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/9ca768c7404ed8d8a42c5ea3279d804ae454a874)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / ä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨ `sampler.py` ä¸ `states.py` ä¸­å°†æ¸©åº¦ã€minâ€‘pã€topâ€‘k/topâ€‘p çš„åˆ¤å®šä¸è°ƒç”¨ç»Ÿä¸€å°è£…åˆ° `SamplingStates`ã€‚æ–°å¢ `apply_temperatureã€apply_min_pã€apply_top_k_top_p` ä¸‰ä¸ªæˆå‘˜æ–¹æ³•ï¼Œå†…éƒ¨å…ˆæ£€æŸ¥å¯¹åº”å‚æ•°æ˜¯å¦çœŸçš„éœ€è¦ç”Ÿæ•ˆï¼Œä»è€Œé¿å…ä¸å¿…è¦çš„ CUDA kernel å¯åŠ¨ï¼Œå¹¶æŠŠåŸæ¥çš„æ•£ä¹±è°ƒç”¨åˆå¹¶ä¸ºä¸€æ¬¡å¯è¯»æ€§æ›´é«˜çš„è°ƒç”¨é“¾ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/v1/worker/gpu/sample/sampler.py`ï¼ˆé‡‡æ ·æµç¨‹ï¼‰  
- `vllm/v1/worker/gpu/sample/states.py`ï¼ˆé‡‡æ ·çŠ¶æ€ç®¡ç†ï¼‰  
- ç›¸å…³çš„é‡‡æ ·ç®—å­æ¨¡å—ï¼š`topk_topp_sampler.pyã€gumbel.pyã€min_p.py`ï¼ˆä¿æŒå…¼å®¹ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **åŠŸèƒ½ç­‰ä»·éªŒè¯**ï¼šç¡®è®¤åœ¨å…¨éƒ¨é‡‡æ ·å‚æ•°å‡ä¸ºé»˜è®¤å€¼ï¼ˆtemp=1ã€min_p=0ã€topâ€‘k=Vocabã€topâ€‘p=1ï¼‰æ—¶ï¼Œè¡Œä¸ºä¸æ—§å®ç°ä¿æŒä¸€è‡´ï¼Œå°¤å…¶æ˜¯ `logits` æ˜¯å¦ä¿æŒåŸä½ä¸è¢«å¤šä½™æ‹·è´ã€‚  
2. **æ€§èƒ½å›å½’æµ‹è¯•**ï¼šæ–°å¢çš„ `np.all` æ£€æŸ¥è™½è½»é‡ï¼Œä½†ä»æ¶‰åŠ hostâ€‘GPU åŒæ­¥ï¼›å»ºè®®åœ¨å¤§æ‰¹é‡æ¨ç†åŸºå‡†ä¸Šå¯¹æ¯”å‰å latencyã€GPU åˆ©ç”¨ç‡ï¼Œç¡®è®¤æ²¡æœ‰å¼•å…¥æ–°çš„åŒæ­¥ç“¶é¢ˆã€‚  
3. **å¼‚å¸¸è·¯å¾„è¦†ç›–**ï¼šå¯¹ `apply_top_k_top_p` è¿”å›åŸå§‹ `logits` çš„æƒ…å½¢åŠ å…¥å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿åç»­çš„ `gumbel_sample` èƒ½æ­£ç¡®å¤„ç†ä¸æ”¹å˜å½¢çŠ¶çš„å¼ é‡ã€‚  
4. **æ–‡æ¡£/æ³¨é‡Šæ›´æ–°**ï¼š`SamplingStates` æ–°å¢çš„ä¸‰ä¸ªæ–¹æ³•åº”åœ¨ `README` æˆ– API æ–‡æ¡£ä¸­æ ‡æ³¨å…¶ â€œå†…éƒ¨ä¼˜åŒ–â€ ç‰¹æ€§ï¼Œä»¥å…è¯¯è®¤ä¸ºå¤–éƒ¨å¯ç›´æ¥è°ƒç”¨ã€‚  
5. **å‘åå…¼å®¹**ï¼šå¦‚æœå¤–éƒ¨ä»£ç ä»åœ¨ä½¿ç”¨å·²åˆ é™¤çš„ `apply_temperature`ï¼ˆæ—§è·¯å¾„ï¼‰æˆ– `apply_min_p`ï¼Œéœ€è¦åœ¨ç›¸åº”æ¨¡å—ä¿ç•™å…¼å®¹åŒ…è£…æˆ–åœ¨å‘å¸ƒè¯´æ˜ä¸­æ˜ç¡®å·²ç§»é™¤ã€‚  

æ€»ä½“æ¥è¯´ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº†ä»£ç å¯ç»´æŠ¤æ€§å¹¶æ½œåœ¨é™ä½ä¸å¿…è¦çš„ GPU è°ƒç”¨ï¼Œé£é™©ä¸»è¦åœ¨äºåŒæ­¥æ£€æŸ¥å¯¼è‡´çš„è½»å¾®æ€§èƒ½å›é€€ï¼Œå»ºè®®é€šè¿‡å¯¹æ¯”åŸºå‡†å’Œæµ‹è¯•è¦†ç›–æ¥ç¡®è®¤ã€‚

---

### [Hybrid] Enable mamba prefix cache "align" mode with async scheduling  (#33997)
**SHA**: `d5fe3f7` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/d5fe3f702c2f4392515cdfde6fd0442271c74dda)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / å…¼å®¹æ€§ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­ï¼ˆå¼€å¯äº†ä¹‹å‰è¢«é˜»æ­¢çš„ â€œalignâ€ å‰ç¼€ç¼“å­˜æ¨¡å¼ï¼‰  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. å–æ¶ˆäº† `vllm.Config` ä¸­å¯¹ **async scheduling + Mamba å‰ç¼€ç¼“å­˜** çš„ç¡¬æ€§é™åˆ¶ï¼Œæ”¹ä¸ºåœ¨ä¸å…¼å®¹æ—¶è‡ªåŠ¨å…³é—­ asyncã€‚  
2. åœ¨ KVâ€‘Cache ç®¡ç†å™¨å’Œå—åˆ†é…é€»è¾‘ä¸­åŠ å…¥å¯¹ **speculativeï¼ˆè‰ç¨¿ï¼‰token** çš„è¡¥å¿ï¼š  
   - `remove_skipped_blocks` é‡æ–°è®¡ç®— `num_computed_tokens`ï¼Œå‰”é™¤ä¸Šä¸€æ­¥çš„è‰ç¨¿å—é˜²æ­¢è¯¯é‡Šæ”¾ã€‚  
   - `get_num_blocks_to_allocate`ã€`allocate_new_blocks` é‡‡ç”¨ `cdiv(...)+num_speculative_blocks` çš„ä¿å®ˆä¼°ç®—ï¼Œä»¥åº”å¯¹ async è°ƒåº¦ä¸‹çš„ token è®¡æ•°ä¸å‡†ã€‚  
3. `preprocess_mamba` è®¡ç®—æ‰€éœ€å—æ•°æ—¶åŠ å…¥äº† `scheduler_output.num_scheduled_tokens`ï¼Œç¡®ä¿å¯¹å³å°†æ‰§è¡Œçš„ token ä¹Ÿè¿›è¡Œé¢„ç•™ã€‚  
4. å¤§é‡æµ‹è¯• `test_mamba_prefix_cache` çš„ stepâ€‘action é¢„æœŸåºåˆ—è¢«è°ƒæ•´ï¼Œä»¥åŒ¹é… â€œalignâ€ æ¨¡å¼åœ¨ async è°ƒåº¦ä¸‹çš„è¡Œä¸ºã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `vllm/config/vllm.py`ï¼ˆè°ƒåº¦å™¨/ç¼“å­˜é…ç½®åˆå§‹åŒ–ï¼‰  
- KVâ€‘Cache ç®¡ç†å™¨ `single_type_kv_cache_manager.py`ï¼ˆå—é‡Šæ”¾ä¸åˆ†é…ï¼‰  
- Mamba ç›¸å…³å·¥å…· `mamba_utils.py`ï¼ˆé¢„å¤„ç†å—æ•°è®¡ç®—ï¼‰  
- å•å…ƒæµ‹è¯• `tests/v1/e2e/test_mamba_prefix_cache.py`  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å†…å­˜ä½¿ç”¨**ï¼šæ–°é€»è¾‘æŠŠè‰ç¨¿å—è§†ä½œå·²å ç”¨ï¼Œå¯¼è‡´ **å—æ•°å¯èƒ½è¢«é«˜ä¼°**ï¼Œåœ¨å¤§æ‰¹é‡è¯·æ±‚ä¸‹ä¼šå¢åŠ æ˜¾å­˜å ç”¨ã€‚å»ºè®®åœ¨ CI ä¸­åŠ å…¥ä¸åŒè´Ÿè½½ä¸‹çš„æ˜¾å­˜ç›‘æ§åŸºå‡†ï¼Œæˆ–æä¾› `--max_speculative_blocks` é™åˆ¶ã€‚  
2. **å…¼å®¹æ€§**ï¼šè™½ç„¶å·²å»æ‰æŠ›å¼‚å¸¸ï¼Œä½†ä»æœ‰è­¦å‘Šæç¤º async ä¸ Mamba å‰ç¼€ç¼“å­˜ä¸å…¼å®¹ã€‚è¯·ç¡®è®¤åœ¨ç”Ÿäº§ç¯å¢ƒå¼€å¯ `async_scheduling=True` æ—¶ï¼Œ`cache_config.mamba_cache_mode="align"` èƒ½å¤Ÿå¦‚é¢„æœŸå·¥ä½œï¼Œå¹¶åœ¨æ–‡æ¡£ä¸­æ³¨æ˜æ­¤æ¨¡å¼ä¸‹çš„ **æ€§èƒ½/æ˜¾å­˜ tradeâ€‘off**ã€‚  
3. **æµ‹è¯•è¦†ç›–**ï¼šç›®å‰ä»…æ›´æ–°äº† `test_mamba_prefix_cache`ï¼Œå»ºè®®æ–°å¢ **è¾¹ç•Œæµ‹è¯•**ï¼ˆå¦‚ prompt é•¿åº¦æ°å¥½è·¨å—ã€è‰ç¨¿å…¨éƒ¨è¢«æ‹’ç»ã€å…¨é‡æ¥å—ï¼‰ä»¥åŠ **å¤šè¯·æ±‚å¹¶å‘** åœºæ™¯ï¼Œç¡®ä¿ `remove_skipped_blocks` çš„ `max(0, â€¦)` ä¸ä¼šå¯¼è‡´å—æ³„æ¼ã€‚  
4. **ä»£ç å¯è¯»æ€§**ï¼šåœ¨ `remove_skipped_blocks`ã€`get_num_blocks_to_allocate` ç­‰å¤„åŠ å…¥ `TODO` æˆ– `NOTE` æ³¨é‡Šï¼Œè¯´æ˜ä¸ºä½•é‡‡ç”¨ä¿å®ˆä¼°ç®—ï¼Œä»¥åŠæœªæ¥å¯èƒ½çš„ä¼˜åŒ–æ–¹å‘ï¼ˆå¦‚æ›´ç²¾ç¡®çš„ token è®¡æ•°ï¼‰ï¼Œæœ‰åŠ©äºåç»­ç»´æŠ¤ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨è§£é”äº† â€œalignâ€ å‰ç¼€ç¼“å­˜åœ¨å¼‚æ­¥è°ƒåº¦ä¸‹çš„ä½¿ç”¨ï¼Œæå‡äº†åŠŸèƒ½å®Œæ•´æ€§ã€‚ä½†éœ€å…³æ³¨æ˜¾å­˜å¼€é”€å’Œè®¡æ•°å‡†ç¡®æ€§ï¼Œå»ºè®®é€šè¿‡ç›‘æ§ä¸æ›´å®Œæ•´çš„æµ‹è¯•æ¥éªŒè¯ç¨³å®šæ€§ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (5)

### [CPU][ARM] Add ARM BF16 cross-compilation support and improve documenâ€¦ (#33079)
**SHA**: `f07a128` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f07a128413ff6900f407e46fe2b36d93eb2a0c12)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ CPU æ„å»ºé…ç½®ä¸­æ–°å¢ ARM BF16 äº¤å‰ç¼–è¯‘æ”¯æŒï¼Œæ‰©å±• CMakeã€Dockerfile ä¸æ–‡æ¡£ä»¥æš´éœ² `VLLM_CPU_ARM_BF16` ç¯å¢ƒ/æ„å»ºå˜é‡ï¼Œå¹¶æ·»åŠ å¹³å°å…¼å®¹æ€§æ£€æŸ¥ã€‚

---

### [Doc] Update Encoder-Decoder models support doc with Florence-2 (#34581)
**SHA**: `19fab44` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/19fab441526b39990e292845ede8886a348d0d7e)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨æ¨¡å‹æ”¯æŒæ–‡æ¡£ä¸­åŠ å…¥ Florenceâ€‘2ï¼ˆ`Florence2ForConditionalGeneration`ï¼‰çš„è¯´æ˜ï¼Œæ ‡æ³¨å…¶é€šè¿‡å®˜æ–¹ bartâ€‘plugin æä¾›æ”¯æŒã€‚

---

### [KV Connector] Add temporary, off-by-default `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION` workaround (#34415)
**SHA**: `79c7e09` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/79c7e092350e4ae82d679ea4b2cdaaa4b580944b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ–°å¢ç¯å¢ƒå˜é‡ `VLLM_DISABLE_REQUEST_ID_RANDOMIZATION`ï¼ˆé»˜è®¤å…³é—­ï¼‰ï¼Œç”¨äºåœ¨ KV è¿æ¥å™¨åœºæ™¯ä¸‹å¯é€‰åœ°å…³é—­è¯·æ±‚ ID çš„éšæœºåç¼€ç”Ÿæˆï¼Œä»£ç ä¸­åŠ å…¥å¯¹åº”è¯»å–åŠè­¦å‘Šæç¤ºã€‚

---

### [CI][Entrypoints] Validate detokenize token IDs to prevent int64 overflow causing 500 (#34468)
**SHA**: `98bcc6c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/98bcc6ca593293cf650699e54e499e7189c24ac1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `DetokenizeRequest.tokens` åŠ å…¥ `Annotated[int, Field(ge=0, le=2**63-1)]` éªŒè¯ï¼Œé˜²æ­¢ int64 æº¢å‡ºï¼›åŒæ­¥æ›´æ–° importã€‚

---

### [Kernels] Fix Helion GPU utils to use platform-agnostic device name API (#34537)
**SHA**: `f13e86d` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/vllm-project/vllm/commit/f13e86d8ddf81c638bacce6f8876cf6acf421d58)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼š`get_gpu_name` æ”¹ä¸ºè°ƒç”¨ `current_platform.get_device_name`ï¼Œå»é™¤ `torch` ä¾èµ–ï¼ŒåŠ å…¥é»˜è®¤ ID è­¦å‘Šæ—¥å¿—ï¼Œå¹¶åœ¨æ–‡æ¡£ä¸­è¡¥å…… AMD GPU åç§°ç¤ºä¾‹ã€‚

---

