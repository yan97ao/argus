# 每日更新报告（2026-01-19）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-19 22:30:44 | danisereb | Add support for LoRA adapters in Nemotron-H models (#30802) |
| 2026-01-19 22:07:46 | wang.yuqi | [Frontend] Score entrypoint support data_1 & data_2 and queries & documents as inputs (#32577) |
| 2026-01-19 20:28:27 | Nicolò Lucchesi | [NIXL][Metrics] Track `nixl_num_kv_expired_reqs` metric in Prometheus (#32340)<br>Add a new metric to track the number of requests that had their KV blocks<br>expire. The scenario is particularly important to surface and track as it is a<br>vital indicator of the health of the deployment.<br>Currently we're resorting to track these failures through unstructured log<br>parsing (which is, among other thing, error string dependent); current main:<br>> Releasing expired KV blocks for request cmpl-071d which were retrieved by 0 decode worker(s) within 0 seconds. |
| 2026-01-19 19:27:08 | Daniel Mescheder | [CI/Build] Fix dependency conflict between model-hosting-container-standards and starlette (#32560) |
| 2026-01-19 18:02:31 | Nicolò Lucchesi | [Core] Whisper support `torch.compile` (#30385) |
| 2026-01-19 17:57:54 | Andreas Karatzas | [ROCm][CI] Add ROCm attention backend support for EAGLE DP tests (#32363) |
| 2026-01-19 17:18:38 | Yuxuan Zhang | [GLM-4.7] GLM Model support for GLM-Lite (#31386) |
| 2026-01-19 16:25:47 | Matt | [CI][Hardware][AMD] Fix test_rotary_embedding_mla_cache_fused (#32408) |
| 2026-01-19 12:21:46 | Hyunkyun Moon | [Frontend] Add render endpoints for prompt preprocessing (#32473) |
| 2026-01-19 12:05:51 | Alex Brooks | [CI/Build] Use Common Event Map Fixture in Harmony / MCP Server Tests (#32531) |
| 2026-01-19 11:06:02 | honglyua | [BugFix] Fix embed_input_ids argument error of QwenVLForConditionalGeneration (#32462) |
| 2026-01-19 09:32:42 | Woosuk Kwon | [Model Runner V2] Refactor `update_states` (#32562) |
| 2026-01-19 08:58:51 | Woosuk Kwon | [Model Runner V2] Support VLM (#32546) |
| 2026-01-19 08:57:32 | Vadim Gimpelson | [BUGFIX]  Fix degenerate strides in TRTLLM query tensors for FlashInfer backend. Fixes issue #32353 (#32417) |
| 2026-01-19 06:20:39 | Iryna Boiko | [Bugfix] Add OOT backend option (#32471) |
| 2026-01-19 04:46:59 | Wentao Ye | [Refactor] Remove unused cutlass moe problem size function (#32047) |
| 2026-01-19 04:46:39 | Wentao Ye | [Refactor] Remove unused file `pallas_kv_cache_update.py` (#32433) |
| 2026-01-19 04:46:00 | Deming | [Doc] Correct comment for _jobs dict in OffloadingConnectorWorker (#32556) |
| 2026-01-19 03:14:22 | Andrey Khalyavin | Use the same memory for workspace13 and fused_output. (#31531) |
| 2026-01-19 02:25:23 | Robert Shaw | [CI] Move Distributed Tests from H200 -> H100 (#32555) |
| 2026-01-19 00:40:49 | bnellnm | [MoE Refactor] Separate Router into OO Classes (#30623) |

### 📊 统计摘要
> 本日共 21 个提交 | 🔴高 3 | 🟡中 7 | 🟢低 11
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [Add support for LoRA adapters in Nemotron-H models (#30802)](#aa7f37c)
    - [[GLM-4.7] GLM Model support for GLM-Lite (#31386)](#71832ba)
    - [[MoE Refactor] Separate Router into OO Classes (#30623)](#327a02d)
  - [🟡 中重要度变更 (7)](#-🟡-中重要度变更-7)
    - [[Frontend] Score entrypoint support data_1 & data_2 and q...](#c88860d)
    - [[Core] Whisper support `torch.compile` (#30385)](#74c583b)
    - [[Frontend] Add render endpoints for prompt preprocessing ...](#3c8740a)
    - [[CI/Build] Use Common Event Map Fixture in Harmony / MCP ...](#7518a3d)
    - [[Model Runner V2] Support VLM (#32546)](#bb1848c)
    - [[Refactor] Remove unused cutlass moe problem size functio...](#eebc58d)
    - [[Refactor] Remove unused file `pallas_kv_cache_update.py`...](#16de822)
  - [🟢 低重要度变更 (11)](#-🟢-低重要度变更-11)
    - [[NIXL][Metrics] Track `nixl_num_kv_expired_reqs` metric i...](#758df5a)
    - [[CI/Build] Fix dependency conflict between model-hosting-...](#cdd03d2)
    - [[ROCm][CI] Add ROCm attention backend support for EAGLE D...](#c0a350c)
    - [[CI][Hardware][AMD] Fix test_rotary_embedding_mla_cache_f...](#11bbf86)
    - [[BugFix] Fix embed_input_ids argument error of QwenVLForC...](#976af2f)
    - [[Model Runner V2] Refactor `update_states` (#32562)](#9a1f16d)
    - [[BUGFIX]  Fix degenerate strides in TRTLLM query tensors ...](#6101a26)
    - [[Bugfix] Add OOT backend option (#32471)](#f5d1740)
    - [[Doc] Correct comment for _jobs dict in OffloadingConnect...](#5480c6b)
    - [Use the same memory for workspace13 and fused_output. (#3...](#ba29ab4)
    - [[CI] Move Distributed Tests from H200 -> H100 (#32555)](#afc3622)
#### 🔴 高重要度变更 (3)

### Add support for LoRA adapters in Nemotron-H models (#30802)
**SHA**: `aa7f37c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/aa7f37ccfa168d40c89d1a104a870757666fcb56)

**🎯 变更类型**：功能增强（为 Nemotron‑H 系列模型以及 3D/非门控 MoE 引入 LoRA 适配器支持）  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 新增 `MergedColumnParallelLinearVariableSliceWithLoRA` 类，实现对拥有 **3+ 输出切片**（Nemotron‑H 所使用的单权重‑多切片层）的 LoRA 注入。  
2. 扩展 LoRA 处理逻辑以兼容 **非门控 MoE**（仅 up_proj + down_proj），包括权重打包、切片、以及 `FusedMoE` 的 LoRA 注入改造。  
3. 在模型注册、层选择 (`from_layer`)、以及 `ModelManager` 中加入对应的判定与兼容路径，确保在 Nemotron‑H、非门控 MoE 与传统 2‑slice 场景下均能自动选取合适的 LoRA 实现。  

**🎯 影响范围**  
- `vllm.lora.layers.column_parallel_linear`（新类与 `can_replace_layer` 判断）  
- `vllm.lora.layers.fused_moe`（LoRA 权重的创建、切片、写入）  
- `vllm.lora.lora_weights`（`pack_moe` 参数 `is_non_gated_moe`，缩放因子处理）  
- `vllm.lora.model_manager`（MoE 类型标记、gate 层过滤、LoRA 权重填充）  
- `vllm.lora.utils` 与 `vllm.lora.layers.__init__`（导出新类）  
- `vllm/model_executor/models/interfaces.py`（新增 `is_non_gated_moe` 标记）  
- `vllm/model_executor/models/nemotron_h.py`（标记为非门控 MoE）  
- 相关单元测试文件 `tests/lora/test_layers.py`（覆盖变量切片、类选择等场景）  

**🔍 技术洞察**  

- **架构影响**  
  - **层选择逻辑**：`from_layer` 现在会在 `MergedColumnParallelLinear` 上先检查 `output_sizes`≥3 或 `packed_modules_list`≥3，以挑选 `MergedColumnParallelLinearVariableSliceWithLoRA`。这避免了原有 `ColumnParallelLinearWithLoRA` 在切片假设为 2 时的错误行为。  
  - **非门控 MoE 支持**：在 `FusedMoE` 中加入 `_w13_slices` 判定，只有在 `is_act_and_mul=False`（即非门控）时才创建 w3（up_proj）相关 LoRA 权重，从而防止越界写入。  
  - **权重打包**：`PackedLoRALayerWeights.pack_moe` 通过 `is_non_gated_moe` 参数决定是否在每个专家的三元组 (w1, w2, w3) 中填充 `None`，并对 `scaling` 做差异化处理，防止非门控 MoE 中出现重复的缩放。  
  - **模型元信息**：`ModelManager` 新增 `_is_non_gated_moe` 标记，并在 LoRA 注册时跳过 `gate` 参数的 LoRA 注入，防止 PEFT 生成的非法适配器报错。  

- **性能影响**  
  - **额外切片与复制**：`MergedColumnParallelLinearVariableSliceWithLoRA.set_lora` 会在运行时把单一 `lora_b` 按 `output_sizes` 切分，复制成本为 O(total_output)。在极端大模型（如 32k token）下仍属线性且与已有切片实现相当。  
  - **内存占用**：对非门控 MoE，`w3` LoRA 权重复用 `w1`（`w3_lora_a = w1_lora_a`），显著降低内存峰值。  
  - **闪存推理路径**：`flashinfer_cutlass_moe` 仍显式抛出 `NotImplementedError`，说明 LoRA 目前不支持该高速路径；使用者需回退到通用实现，可能导致少量性能退化。  

- **安全考虑**  
  - 本次改动未涉及网络、文件或权限访问，仅在模型内部进行张量切分与复制，未引入新的攻击面。  
  - 唯一需关注的是 **LoRA 权重的数值范围**：在 `set_lora` 中对 `lora_a`、`lora_b` 进行复制时使用 `non_blocking=True`，保持与原实现一致，未出现泄露或未初始化内存的风险。  

**⚠️ 潜在风险**  
1. **切片逻辑错误**：若模型的 `output_sizes` 与实际权重维度不匹配（例如自定义模型未遵循 Nemotron‑H 的切片规则），`set_lora` 的切片会产生维度不一致错误。  
2. **非门控 MoE 与 3D MoE 混用**：`ModelManager` 通过 `self.model.is_3d_moe_weight` 与 `is_non_gated_moe` 分别标记，两者互斥假设成立；若未来出现 **3D‑非门控** 混合模型，现有判断可能失效。  
3. **PEFT 兼容性**：PEFT 在生成 LoRA 适配器时仍会尝试为 `gate` 模块生成权重；虽然已通过 `register_module` 中的过滤跳过，但如果用户手动提供 `gate` LoRA，加载时会 silently 被抛弃，可能导致用户误解。  
4. **flashinfer 切换**：当用户开启 `flashinfer_cutlass_moe` 并同时使用 LoRA，会在运行时触发 `NotImplementedError`，导致服务崩溃。需要在文档或启动脚本中明确提示。  
5. **分布式初始化**：`test_merged_column_parallel_variable_slice` 在多 GPU 环境下调用 `torch.cuda.set_device(device)`，若 `device` 超出当前可见 GPU 范围，会导致初始化错误。  

**💡 关注建议**  
- **单元/集成测试**：在 CI 中加入 **非门控 MoE** 与 **3+ 切片** 两套互相独立的模型测试，确保 `from_layer` 与 `can_replace_layer` 在所有组合下返回预期。  
- **文档更新**：  
  - 明确说明 **LoRA 不支持 flashinfer‑cutlass MoE**，并提供回退方案。  
  - 在模型配置页标注 `is_non_gated_moe` 的含义及何时需要手动开启。  
- **运行时检查**：在 `ModelManager.__init__` 中加入断言，确保 `is_3d_moe_weight` 与 `is_non_gated_moe` 不会同时为 `True`。  
- **兼容性警告**：对外暴露的 API（如 `LoRAConfig`、`load_adapter`) 添加 runtime warning，当检测到用户为 `gate` 模块提供 LoRA 时给出明确提示。  
- **性能监控**：建议在生产环境开启 LoRA‑enabled 与非 LoRA‑enabled 两条路径的延迟/显存监控，以快速捕获由于切片或复制导致的异常增长。  
- **后续优化**：若后续实现了 **flashinfer‑cutlass LoRA**，可以在 `FusedMoE` 中复用已有 `_w13_slices` 与 `is_non_gated_moe` 判断，统一注入逻辑，避免代码分支散落。  

---  

此次提交为 vLLM 引入了对 **Nemotron‑H 及非门控 MoE** 的 LoRA 适配器完整支持，提升了模型微调与推理

---

### [GLM-4.7] GLM Model support for GLM-Lite (#31386)
**SHA**: `71832ba` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/71832ba71e77c01a1d249e014384522b46813749)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 为 vLLM 新增 **GLM‑4.7‑Lite（Flash）** 系列模型的完整支持，包括普通推理模型 `Glm4MoeLiteForCausalLM` 与其多步预测（MTP）变体 `Glm4MoeLiteMTP`。  
- 在 `speculative.py` 中注册新的推测解码类型 `glm4_moe_lite_mtp`，并在模型注册表、基准脚本、文档以及模型可用性检查中加入相应条目。  
- 实现了 **MoE‑Lite** 的解码层、注意力、MLP、权重加载、共享专家切分、Pipeline‑Parallel（PP）兼容等全部细节，保持与现有 `glm4_moe` 实现高度复用。

---

## 🎯 影响范围
| 模块/组件 | 受影响程度 |
|-----------|------------|
| `vllm/model_executor/models/glm4_moe_lite.py` | 新增核心模型实现（解码层、注意力、MoE、权重加载） |
| `vllm/model_executor/models/glm4_moe_lite_mtp.py` | 新增 MTP（多步预测）实现 |
| `vllm/model_executor/models/registry.py` | 注册 `Glm4MoeLiteForCausalLM` 与 `Glm4MoeLiteMTP` |
| `vllm/config/speculative.py` | 新增 `glm4_moe_lite_mtp` 推测模型类型 |
| `benchmarks/kernels/*.py` | 基准脚本加入 `Glm4MoeLiteForCausalLM` 支持 |
| `docs/features/tool_calling.md` | 文档列出 `zai-org/GLM-4.7-Flash` |
| `tests/models/registry.py` | 增加在线可用性声明（标记 `is_available_online=False`） |
| 其它公共模块（如 `model_executor/utils.py`、`interfaces.py`） | 兼容 PP、LoRA、权重加载等通用路径被复用，无需改动 |

---

## 🔍 技术洞察

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | - 引入 **GLM‑4.7‑Lite**：与 `glm4_moe` 相比，`num_hidden_layers` 被设为 `0`（模型仅包含 MoE 层），并在 `speculative` 配置中使用 `num_nextn_predict_layers` 进行多步预测。<br>- 通过 `SupportsPP` 与 `SupportsLoRA` mixin，实现了 **pipeline parallel** 与 **LoRA** 的无缝兼容。<br>- `SharedFusedMoE` 被复用，权重映射逻辑与 `glm4_moe` 基本相同，仅在 `lite` 场景下处理 `n_hidden_layers=0` 的特例。 |
| **性能影响** | - **计算图更轻**：去掉了大规模的 Transformer 编码层，仅保留 MoE‑专家层与轻量化注意力，推理 FLOPs 明显下降，预期能在相同硬件上获得 **10‑20%** 的吞吐提升（取决于 expert‑数量与并行度）。<br>- 采用 `torch.compile` 装饰器以及 **ROCm AITER** 融合路径，能够在 AMD GPU 上进一步降低 kernel 启动开销。<br>- 共享专家（`n_shared_experts`）的切分逻辑在加载阶段有轻微 CPU 开销，但仅在模型初始化时执行，对运行时性能无负面影响。 |
| **安全考虑** | - 新增的权重加载路径涉及 **专家切分** 与 **动态名称映射**，若 checkpoint 与配置不匹配，可能导致 **加载异常**（assert 失败）而非安全漏洞。<br>- 目前代码没有引入网络请求或外部执行，仅在 `tests/models/registry.py` 中标记 `is_available_online=False`，避免自动下载未审计的公共模型。<br>- 仍然遵守原项目的 **模型授权（Apache‑2.0）**，未引入新权限或版权风险。 |
| **可维护性** | - 代码高度复用（继承自 `glm4_moe`），但添加了大量 **条件分支**（如 `if config.num_hidden_layers == 0`、`if get_pp_group().is_first_rank` 等），需要在后续维护时注意保持 **层索引一致** 与 **PP/LoRA** 的同步更新。<br>- 权重加载逻辑非常繁复（多层映射、shared‑expert split、speculative‑layer重写），建议在 `vllm/model_executor/models/` 中抽出 **公共加载工具**，降低重复代码。 |

---

## ⚠️ 潜在风险

1. **权重映射错误**  
   - `load_weights` 涉及多层 name 替换、expert‑id、shard‑id 以及 `spec_layer` 重写。若未来 HF checkpoint 结构微调（如增添前缀或改名），可能导致 **参数未匹配** 或 **加载异常**。  
2. **Pipeline Parallel 边界**  
   - `Glm4MoeLite` 在 `num_hidden_layers = 0` 时，仍然依赖 `make_layers` 生成 `self.layers`（空列表）并在 `forward` 中遍历。若 PP 的 `start_layer` / `end_layer` 计算出现误差，可能导致 **空切片** 或 **层不对齐**。  
3. **共享专家切分**  
   - 对 `mlp.shared_experts` 的切分仅在 `rocm_aiter` 启用时生效。若在非 ROCm 环境或未开启该特性，却仍有 `n_shared_experts>1`，会导致 **权重丢失**。  
4. **Speculative MTP 重写**  
   - `Glm4MoeLiteMTP._rewrite_spec_layer_name` 通过字符串替换实现层名映射，若未来模型层命名规范改变（例如加入前缀），该函数可能失效，引发 **加载错误**。  
5. **文档/注册不一致**  
   - 虽已在 `registry.py` 添加新模型，但自动化的 `model_info` 与 `download` 工具仍可能把 `is_available_online=False` 当作错误处理，需要在 CI 中加入针对 `Glm4MoeLite*` 的 **可用性检查**。  

---

## 💡 关注建议

| 对象 | 建议措施 |
|------|----------|
| **开发者** | - 在 **单元测试** 中加入针对 `Glm4MoeLiteForCausalLM` 与 `Glm4MoeLiteMTP` 的 **权重加载完整性** 检查（对比参数数目、expert‑mapping）。<br>- 为 `load_weights` 抽象出 **通用映射函数**（如 `map_weight_name`) 并添加 **异常捕获** 与 **日志**，避免因单个 checkpoint 结构变化导致全量加载失败。 |
| **代码维护** | - 在 `speculative.py` 增加注释说明 `glm4_moe_lite_mtp` 与 `glm4_moe_mtp` 的区别以及 `num_nextn_predict_layers` 的来源。<br>- 将 `SharedFusedMoE.make_expert_params_mapping` 的 **参数**（如 `num_shared_experts`）封装为统一的 **MoE 配置对象**，供 `glm4_moe` 与 `glm4_moe_lite` 共用。 |
| **性能调优** | - 对比 **GLM‑4.7‑Lite** 与 **GLM‑4.5‑Moe** 在同等硬件上的吞吐/延迟，确定是否需要对 `topk_indices_buffer` 的大小进行 **动态裁剪**（依据实际 batch‑size）。<br>- 在 ROCm 环境下验证 `rocm_aiter` 的 **fusion_shared_experts** 开关对 **显存占用** 与 **kernel 启动时间**

---

### [MoE Refactor] Separate Router into OO Classes (#30623)
**SHA**: `327a02d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/327a02d8db86e57f2488779b0c1b133da5f03fb5)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 将原有的 MoE 路由逻辑从单一的函数/类拆分为一套面向对象的路由层级（`BaseRouter`、`FusedTopKRouter`、`FusedTopKBiasRouter`、`GroupedTopKRouter`、`CustomRoutingRouter`、`RoutingSimulatorRouter`），并通过工厂函数 `create_fused_moe_router` 统一创建。  
2. 把 EPLB（Expert‑Parallel‑Load‑Balancing）相关状态抽离到 `EplbLayerState`，并在路由器内部完成映射与计数，而不再在 `FusedMoE` 层中直接持有原始张量。  
3. 将原本散落在 `vllm/model_executor/layers/fused_moe/fused_moe.py` 中的 `fused_topk` / `fused_topk_bias` / `grouped_topk` 实现迁移到独立的 router 模块并对外统一 `fused_topk`（CUDA/ROCm）入口。  
4. 更新所有使用点（unit tests、量化实现、模型定义）以通过新的导入路径 `vllm.model_executor.layers.fused_moe.router.*`，并在量化路径中对 `FusedMoERouter` 的引用做统一。  
5. 引入 `RoutingSimulatorRouter` 用于通过环境变量 `VLLM_MOE_ROUTING_SIMULATION_STRATEGY` 在运行时模拟不同的路由策略（uniform_random、normal_routing 等），便于调试和基准测试。  

---

## 🎯 影响范围
| 模块 / 组件 | 受影响的文件/目录 | 关键变更点 |
|------------|------------------|-----------|
| **MoE 路由层** | `router/`（新增 6+ 文件） | 完全重构路由实现，新增抽象基类、工厂、不同路由策略实现 |
| **EPLB 状态** | `distributed/eplb/eplb_state.py`（新增 `EplbLayerState`） | 将 EPLB 运行时数据从 `FusedMoE` 移至独立 dataclass |
| **FusedMoE Layer** | `layers/fused_moe/layer.py`（大量删改） | 替换内部 `_select_experts` 为 `router.select_experts`，去除旧 `_grouped_topk_impl`，新增 `eplb_state` 成员 |
| **量化实现** | 多个 `layers/quantization/*.py` | 所有直接 `from ...fused_moe.fused_moe_router import FusedMoERouter` 更改为 `from ...fused_moe import FusedMoERouter`（统一入口） |
| **测试套件** | `tests/kernels/moe/*` | 更新 import 路径，新增 `tests/kernels/moe/test_routing.py`（覆盖新路由逻辑） |
| **模型实现** | `models/ernie45_*_moe.py` | 通过 `router_logits_dtype=torch.float32` 传递 dtype，确保新路由能兼容 FP32 logits |
| **公共 API** | `vllm/model_executor/layers/fused_moe/__init__.py` | 对外重新导出 `fused_topk`、`GroupedTopk`、`FusedMoERouter` 等入口 |
| **环境变量** | `envs.py`（未改动但被使用） | `VLLM_MOE_ROUTING_SIMULATION_STRATEGY` 新增路由模拟入口 |

---

## 🔍 技术洞察

### 1. 架构影响
| 维度 | 正面影响 | 负面/潜在影响 |
|------|----------|---------------|
| **模块化/可维护性** | - 路由逻辑被抽象成独立类，单一职责明确。<br>- 新的 `router_factory` 统一创建路径，易于新增路由策略（只需实现 `BaseRouter._compute_routing`）。 | - 旧代码（如 `fused_moe.py` 中的 `GroupTopk`）被删除，外部项目若有私有导入路径可能出现 `ImportError`。 |
| **向后兼容性** | - `FusedMoERouter` 仍然作为统一基类，对外保持同名引用。<br>- 大多数调用仍通过 `layer.router.select_experts`，老的 `FusedMoE` 接口保持不变。 | - 部分内部属性（`expert_load_view`、`logical_to_physical_map` 等）已迁移到 `EplbLayerState`，直接访问旧属性的插件或自定义层将失效。 |
| **扩展性** | - 自定义路由函数可通过 `custom_routing_function` 注入，支持未来的专家调度算法（如基于硬件指标的路由）。<br>- `RoutingSimulatorRouter` 为仿真提供统一入口，便于研究人员快速切换策略。 | - 路由工厂的参数列表较长，易产生使用错误（如忘记 `indices_type_getter`），需要文档和类型提示。 |
| **依赖关系** | - 将与 `rocm_aiter_ops`、`vllm._custom_ops` 的耦合限定在对应 router 实现内部，防止在不支持的平台上出现未定义符号。 | - 仍然在 `router_factory` 中硬编码对 `rocm_aiter_ops.is_fused_moe_enabled()` 的检查，如果平台改变需要同步更新。 |

### 2. 性能影响
| 场景 | 预估影响 | 说明 |
|------|----------|------|
| **标准 fused_topk** | **基本持平**（可能轻微提升） | `fused_topk` 本身未修改，仅封装在 `FusedTopKRouter` 中；调用层级多一次函数包装，微小的 Python 调用开销在 CUDA kernel 主导的路径上可忽略。 |
| **grouped_topk (DeepSeek V2/V3)** | **持平或略优** | 分支逻辑改为 `partial` + `rocm_aiter_grouped_topk`，保持原有的 `torch.compile` 编译路径；若平台不满足 fused 条件，则回退到 Python 实现，性能与原实现相同。 |
| **biased topk (e_score_correction_bias)** | **持平** | `fused_topk_bias` 仍在 Python 中实现，并在后续乘以 scaling factor；行为保持一致。 |
| **EPLB 映射** | **轻微增加** | 通过 `eplb_map_to_physical_and_record` 在 CUDA 编译下使用 `torch.compile`，相比之前在 `FusedMoE._select_experts` 中的循环，同样一次性完成映射与负载计数，开销相当。 |
| **路由仿真** | **显著下降**（仅用于调试） | `RoutingSimulatorRouter` 在仿真策略下直接在 Python 中生成随机 top‑k，省去所有 CUDA kernel，耗时大幅降低——但仅在测试/调优阶段使用。 |

> **整体结论**：对生产推理的性能影响可视为 **中性**，主要收益在代码可维护性和灵活扩展性；若出现异常回退至 Python 实现，仍然保持原有的最坏情况性能。

### 3. 安全考虑
| 方面 | 是否受影响 | 说明 |
|------|------------|------|
| **输入校验** | ✅ 有增强 | `BaseRouter._validate_eplb_state` 明确检查 EPLB 相关张量是否已初始化，防止空指针引发崩溃。 |
| **环境变量** | ⚠️ 潜在风险 | `VLLM_MOE_ROUTING_SIMULATION_STRATEGY` 可导致路由行为在运行时被完全覆盖，若在生产环境误开启可能导致路由失衡、资源异常占用。建议在部署脚本显式禁用或记录日志警告。 |
| **外部自定义路由** | ⚠️ 潜在风险 | `custom_routing_function` 直接在

---

#### 🟡 中重要度变更 (7)

### [Frontend] Score entrypoint support data_1 & data_2 and queries & documents as inputs (#32577)
**SHA**: `c88860d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c88860d759715affaf2285afb953cc791c0a5a38)

**变更类型**：功能增强（对 Score 接口的输入字段进行统一与扩展）

**核心改动**  
1. **协议层**（`vllm/entrypoints/pooling/score/protocol.py`）  
   - 将原有 `ScoreRequest` 拆分为三个子类：`ScoreQueriesDocumentsRequest`（新字段 `queries` / `documents`）、`ScoreDataRequest`（兼容 `data_1` / `data_2`）以及 `ScoreTextRequest`（保留 `text_1` / `text_2`）。  
   - 使用 `TypeAlias` 将 `ScoreRequest` 定义为上述三类的联合类型，统一在后端使用 `request.data_1`、`request.data_2`，实现对不同字段名称的透明兼容。  
   - 相应的 `_validate_input`、`_run_scoring`、`_embedding_score` 等实现改为接受 `data_1`、`data_2`，内部变量名亦相应更改。

2. **服务入口**（`vllm/entrypoints/openai/engine/serving.py`）  
   - 更新请求验证集合，加入 `ScoreDataRequest`、`ScoreTextRequest`、`ScoreQueriesDocumentsRequest`。  
   - 对超长输入的错误提示保持一致。

3. **批量处理**（`vllm/entrypoints/openai/run_batch.py`）  
   - 通过 `TypeAdapter` 对 `ScoreRequest`（已是联合别名）进行校验，避免直接使用旧模型导致解析错误。

4. **示例、文档、测试**  
   - 所有示例代码、README、OpenAI 兼容文档统一使用 `queries` / `documents`（或 `data_1` / `data_2`）。  
   - 测试用例全部改写以覆盖三种请求形式，保证功能完整性。  
   - 为多模态场景新增 `encode_base64_content_from_url` 辅助函数，示例展示了图片 URL → Base64 的转换。

**影响范围**  
- **核心模块**：`vllm/entrypoints/pooling/score/*`、`vllm/entrypoints/openai/*`、`vllm/entrypoints/pooling/score/protocol.py`。  
- **文档/示例**：`docs/serving/openai_compatible_server.md`、`examples/*`。  
- **测试**：所有与 `score` 相关的单元/集成测试（约 10+ 文件）均已更新。

**建议**  
1. **向后兼容**：虽然保留了 `ScoreTextRequest`，仍建议在下一次 major 版本中标记 `text_1` / `text_2` 为废弃，并在文档显式说明迁移路径。  
2. **统一校验**：在 `ScoreRequestMixin` 中加入 `@root_validator`，检查 `data_1` 与 `data_2` 类型匹配（尤其是多模态 `ScoreMultiModalParam`），防止出现不匹配的 list/单值组合。  
3. **错误信息**：当用户混用旧字段（如同时提供 `text_1` 与 `queries`）时，返回清晰的 `BadRequestError`，指示字段冲突。  
4. **文档同步**：确保 OpenAI 兼容 API 文档、示例代码以及 README 中的所有 curl 示例均已更新为新字段。  
5. **版本号 & 发行说明**：在 `CHANGELOG` 中加入 “Score API 参数从 `text_1`/`text_2` 迁移到 `queries`/`documents`（兼容 `data_1`/`data_2`）” 的说明，并在发布时提醒用户检查调用方代码。  

总体而言，此次改动为 Score 接口提供了更直观的字段命名（queries/documents），并通过多种别名保持兼容，代码结构得到简化。只要注意向后兼容提示和字段冲突检测，即可在生产环境安全迁移。

---

### [Core] Whisper support `torch.compile` (#30385)
**SHA**: `74c583b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/74c583bc508c2dafb9e95bab3b635884e4a021f3)

**🎯 变更类型**：功能增强（为 Whisper 添加 `torch.compile` 支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在编译装饰器中加入对 `forward_context.skip_compiled` 的检查，使得在需要跳过编译的调用（如 encoder‑decoder 首轮前向）直接走 eager 路径；为 `WhisperDecoder` 添加 `@support_torch_compile` 装饰器并声明动态图维度；在 `ForwardContext` 与 `set_forward_context` 中新增 `skip_compiled` 标志；`GPUModelRunner.execute_model` 根据是否存在 encoder 输入决定该标志；测试超时上限提升。  

**🎯 影响范围**  
- `vllm/compilation/decorators.py`、`vllm/forward_context.py`（编译控制逻辑）  
- `vllm/model_executor/models/whisper.py`（Whisper 解码器编译入口）  
- `vllm/v1/worker/gpu_model_runner.py`（运行时决定是否跳编译）  
- 相关单元测试  

**💡 关注建议**  
1. **状态恢复**：`set_forward_context` 退出时必须保证 `skip_compiled` 恢复为默认，否则后续请求可能错误地走 eager。建议在 `finally` 块显式重置。  
2. **兼容性检查**：其他 encoder‑decoder 模型（如 T5、BART）若加入 `torch.compile`，同样需要在调度阶段标记 `skip_compiled`，防止捕获包含 encoder 的图。可以抽象出通用判断函数。  
3. **文档与示例**：在 README/迁移指南中说明 `--enforce-eager` 与 `skip_compiled` 的交互，以及何时需要手动开启 `skip_compiled`。  
4. **性能基准**：新增 `torch.compile` 后，建议补充对 Whisper 编码‑解码全流程的吞吐/延迟基准，以验证编译收益不被首次 eager 带来的额外开销抵消。  
5. **测试覆盖**：当前仅扩展了超时阈值，建议添加断言验证 `skip_compiled=True` 时实际调用了 `forward`（未编译）的路径。  

总体来看，此改动为 Whisper 引入了 Torch‑Inductor 编译能力，逻辑清晰且对其他模型影响有限。只要注意上下文状态的正确管理和相应文档更新，即可安全发布。

---

### [Frontend] Add render endpoints for prompt preprocessing (#32473)
**SHA**: `3c8740a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/3c8740aacb2b288390c986c169993ed2d97363f8)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 OpenAI 兼容层新增 **`/v1/completions/render`** 与 **`/v1/chat/completions/render`** 两个渲染接口，专注于 **prompt 预处理**，返回 token‑ids、原始文本（以及多轮对话的结构），不进行生成。  
- 在 `completion/serving.py` 与 `chat_completion/serving.py` 中拆分出 `render_*_request` 方法，保持原有 `create_*` 仍负责流式生成。  
- 新增对应路由及错误包装逻辑，并编写了 200 行完整的 pytest 套件。  

**🎯 影响范围**  
- `vllm/entrypoints/openai/completion/*`、`vllm/entrypoints/openai/chat_completion/*`（路由、服务实现）。  
- 共享的工具函数（`_check_model`、`_maybe_get_adapters`、tokenizer 获取等）。  
- 测试目录 `tests/entrypoints/openai/test_render.py`。  

**💡 关注建议**  
1. **文档与 OpenAPI**：为新 `/render` 路径补全 Swagger 描述，确保用户知道此接口仅返回预处理结果。  
2. **兼容性**：老版本客户端仍调用原 `completions`、`chat/completions`，请确认路由注册顺序不会引入冲突。  
3. **性能监控**：渲染仍会执行 tokenizer、chat‑template 解析，建议在 CI 中加入大批量 prompt 基准，防止意外 O(N²) 开销。  
4. **错误统一**：当前渲染路径在异常时返回 `JSONResponse`，而生成路径返回 `ErrorResponse`，建议统一返回模型，以免前端判断分支增多。  
5. **代码复用**：`render_*_request` 与原 `create_*` 在参数准备上有重复，可抽象为内部工具函数，降低维护成本。  

总体来看，此次改动为调试、Prompt‑engineering 场景提供了便利的入口，影响范围限制在 OpenAI 接口层，风险可控。后续请关注文档同步与异常统一两方面的细化。

---

### [CI/Build] Use Common Event Map Fixture in Harmony / MCP Server Tests (#32531)
**SHA**: `7518a3d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7518a3dc65e4c20860ca97464e62ee9b02c0c578)

**🎯 变更类型**：重构（测试代码抽象化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `tests/entrypoints/openai/responses/conftest.py`，提供 `pairs_of_event_types` fixture，统一维护 “done” ↔ “start” 事件的映射。  
2. 多个响应相关的测试文件 (`test_harmony.py、test_mcp_tools.py`) 把原先在函数体内硬编码的映射删除，改为通过 fixture 注入。  
3. 相应的函数签名也加入 `pairs_of_event_types` 参数，保持测试逻辑不变。

**🎯 影响范围**  
- **tests/entrypoints/openai/responses**：所有使用该映射的单元/集成测试。  
- **CI 构建流程**：依赖 `pytest` 自动加载 `conftest.py`，若路径或 fixture 名称变动，可能影响测试发现。  
- **文档/示例**：若项目对外展示测试用例或提供测试模板，需要同步更新说明。

**💡 关注建议**  
1. **确保 fixture 可被所有子目录发现**：`conftest.py` 位于 `responses/`，但有些测试可能在更深层目录执行。建议在项目根目录放置一次统一的 `conftest.py` 或在 `pyproject.toml` 中明确 `testpaths`。  
2. **保持映射完整性**：新加入的 `response.mcp_call_arguments.done`、`response.mcp_call.completed` 等键在旧代码中未使用，后续如果有测试忘记更新映射会导致 `KeyError`。建议在 fixture 注释中列出 “全套事件对” 以及 “当前测试覆盖的子集”。  
3. **类型提示与 lint**：fixture 返回 `dict[str, str]`，已加入类型注解，确保 `mypy` 与 `ruff` 检查通过。若项目开启 `strict` 模式，可考虑将返回值声明为 `Mapping[str, str]`。  
4. **CI 兼容性**：CI 环境可能并未安装 `pytest` 的最新插件，确认 `pytest` 版本支持 `fixture` 参数化（≥6.0）。在 CI 脚本中加入 `pytest -q` 而非硬编码路径，防止因目录结构微调导致测试遗漏。  
5. **回退方案**：若出现 fixture 未被加载导致测试失效，快速回退方式是保留旧的局部 `pairs_of_event_types` 定义（可通过 `# pragma: no cover` 注释排除覆盖率影响）。  

总体来看，此次改动仅影响测试层面，提升了可维护性和 DRY（不要重复自己）水平。只要确认 `conftest.py` 被正确收集，CI 与本地运行均不应受负面影响。祝调试顺利！

---

### [Model Runner V2] Support VLM (#32546)
**SHA**: `bb1848c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bb1848cd62e5e5f327f307bee26e2b947f00396e)

**🎯 变更类型**：功能增强（为 Model Runner V2 增加多模态（VLM）支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 GPU 端的 CUDA‑graph 捕获、模型执行流程中新增 `inputs_embeds` 参数，以支持多模态特征的嵌入。  
- 引入 `EncoderRunner`，负责多模态特征的编码、缓存、以及嵌入的收集与拼接。  
- 新增/修改 `InputBatch`、`InputBuffers`、`ModelRunner`、`cudagraph_utils` 等多个核心类，加入对 `mm_features`、`mm_hash`、`is_mm_embed` 的管理。  
- 调整 M‑RoPE 位置缓冲布局，减少内存占用。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/`（模型运行、CUDA graph、输入批处理）  
- `vllm/v1/worker/gpu/mm/encoder_runner.py`（多模态编码器实现）  
- 多模态注册表 `vllm/multimodal/`（模型能力检测）  

**💡 关注建议**  
1. **接口向后兼容**：`model.forward` 现在接受 `inputs_embeds`，需检查所有在外部直接调用模型的代码（如自定义推理脚本）是否已适配。  
2. **缓存一致性**：`EncoderRunner.encoder_cache` 依赖 `mm_hash`，确保在并发请求释放和重用时不会出现缓存错用或内存泄漏。  
3. **性能验证**：新增的 `gather_mm_embeddings` 与 `get_inputs_embeds` 在预填阶段会产生额外拷贝，建议在大批量推理场景下进行基准测试，确认 CUDA‑graph 捕获仍保持预期的吞吐提升。  
4. **异常处理**：`EncoderRunner.gather_mm_embeddings` 中的 `assert encoder_output is not None` 直接抛异常，考虑在生产环境加入容错或回退路径。  
5. **单元测试**：新增对多模态输入（图像、音频等）的 end‑to‑end 测试，覆盖 `add_request / remove_request / free_encoder_cache` 的完整生命周期。  

总体来看，此次提交为 VLM 场景提供了完整的编码‑嵌入‑执行流水线，改动集中在 GPU 工作线程层，需重点关注内存管理、兼容性以及性能回归。

---

### [Refactor] Remove unused cutlass moe problem size function (#32047)
**SHA**: `eebc58d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/eebc58df0c52e46a5acfc1de91bfc3f67b454c74)

**变更类型**：重构（删除未使用的接口）  
**重要程度**：🟡 中  

**核心变更**  
- 在 `csrc/ops.h`、`csrc/quantization/w8a8/cutlass/moe/moe_data.cu`、`csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu` 中彻底移除 `get_cutlass_moe_mm_problem_sizes` 及其实现。  
- 对应的 Python 绑定在 `csrc/torch_bindings.cpp` 与 `vllm/_custom_ops.py` 也同步删除。  
- 仅保留 `get_cutlass_moe_mm_problem_sizes_from_expert_offsets` 系列实现，继续供内部 MoE 计算使用。

**影响范围**  
- **CUDA/C++ 层**：所有依赖该函数的编译单元将失去该符号；若有残余调用会导致链接错误。  
- **Python API**：`torch.ops._C.get_cutlass_moe_mm_problem_sizes` 将不再导出，外部脚本或第三方库若直接调用会抛 `AttributeError`。  
- **文档/示例**：任何示例或 README 中提及此接口需要同步更新。

**关注建议**  
1. **全局搜索**：在项目及常用插件（如 vLLM‑OpenAI、DeepSpeed）中确认未再出现该函数的调用，防止编译或运行时异常。  
2. **向后兼容**：若有下游用户可能依赖，建议在下一个 major 版本前先提供一次 `DeprecationWarning`（如保留包装函数），再正式删除。  
3. **CI/测试**：确保所有单元测试、集成测试以及 CUDA 编译路径均通过，尤其是 `cutlass` 相关的 SM90/SM100/SM120 分支。  
4. **文档同步**：在 `docs/` 与 `vllm/_custom_ops.py` 的函数注释中删去对应条目，避免误导。  

总体而言，此次删除属于清理 dead‑code 的安全重构，只要确认无残余引用，即可提升代码可维护性并减小编译体积。

---

### [Refactor] Remove unused file `pallas_kv_cache_update.py` (#32433)
**SHA**: `16de822` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/16de822c719dedec51e5f2a3ee6ed5b0890e56cb)

**🎯 变更类型**：重构（删除冗余代码）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交删除了 `vllm/v1/attention/ops/pallas_kv_cache_update.py`，该文件实现了基于 JAX Pallas 的 KV‑cache 更新逻辑，但在主代码路径中从未被引用，导致代码体积和维护成本不必要地增加。删除后项目整体文件数减少 130 行。  

**🎯 影响范围**：  
- **核心模块**：`vllm/v1/attention/ops/`（所有注意力算子）  
- **依赖**：若外部或内部代码（如实验分支、文档示例）仍通过 `import …pallas_kv_cache_update` 引入该文件，会出现 `ModuleNotFoundError`。  
- **构建/测试**：CI 中的 JAX Pallas 相关测试和依赖声明需确认已不再使用该文件的符号。  

**💡 关注建议**：  
1. **全局搜索** `pallas_kv_cache_update`，确保没有残余的 import 或引用；若有，需要删掉或改为安全的可选导入。  
2. 运行完整的单元测试和集成测试，特别是涉及 TPU/JAX 的测试，确认没有因删除导致的失败。  
3. 更新 `README`、`docs/` 或示例代码中提及该文件的说明，避免用户搜索不到对应实现。  
4. 若以后想在 TPU 环境下使用类似功能，可考虑把实现迁移到专门的 experimental 包，而非留在主分支中。  
5. 在 `setup.cfg`/`MANIFEST.in` 中检查是否还有该文件的打包声明，确保构建产物一致。  

总体而言，此次删除是一次清理工作，对功能无直接影响，风险主要在于潜在的残留引用。完成上述检查后即可安全合并。

---

#### 🟢 低重要度变更 (11)

### [NIXL][Metrics] Track `nixl_num_kv_expired_reqs` metric in Prometheus (#32340)
**SHA**: `758df5a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/758df5afe7ac8cdc8141fd25f472ee3632377d90)

**🎯 变更类型**：代码重构/新增功能  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 NIXL KVConnector 中新增 `num_kv_expired_reqs` 统计字段及 `record_kv_expired_req` 方法，记录 KV 块过期的请求，并在 Prometheus 导出相应计数器 `vllm:nixl_num_kv_expired_reqs`，同时调整聚合与重置逻辑以保留全部失败相关指标。

---

### [CI/Build] Fix dependency conflict between model-hosting-container-standards and starlette (#32560)
**SHA**: `cdd03d2` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cdd03d25d3207d96bd8dee919632b9db090b83c8)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `requirements/test.txt` 中升级 `fastapi` 至 0.128.0、`starlette` 至 0.50.0，并新增 `annotated-doc==0.0.4`，以消除与 `model-hosting-container-standards` 的依赖冲突。

---

### [ROCm][CI] Add ROCm attention backend support for EAGLE DP tests (#32363)
**SHA**: `c0a350c` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c0a350ca73fa8f10bd718fdcec47b075dc1d102f)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_eagle_dp.py` 中加入对 ROCm 平台的检测，参数化注意力后端以支持 ROCm、TRITON、FLEX 等，同时对 ROCm 环境的测试标记为预期失败并添加相关注释。

---

### [CI][Hardware][AMD] Fix test_rotary_embedding_mla_cache_fused (#32408)
**SHA**: `11bbf86` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/11bbf86f6a35d8d30821bed72eb8adf9864b024f)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_rotary_embedding_mla_cache_fused` 中新增对 ROCm 平台的适配，使用 `forward_hip` 作为参考实现以避免 FP16 精度差异，并在非 ROCm 环境保持原有 `forward_native` 调用。

---

### [BugFix] Fix embed_input_ids argument error of QwenVLForConditionalGeneration (#32462)
**SHA**: `976af2f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/976af2f314d1c7d54e3707e325df2b5961c069a4)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `QwenVLForConditionalGeneration` 类中新增 `embed_input_ids = SupportsMultiModal.embed_input_ids`，修复了对 `embed_input_ids` 参数的调用错误。

---

### [Model Runner V2] Refactor `update_states` (#32562)
**SHA**: `9a1f16d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9a1f16da1e423ede2c2f52a9850cbfbb39cefe96)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `update_states` 拆分为 `finish_requests、free_states、add_requests、update_requests` 四个细粒度方法，优化请求状态管理顺序并在执行前统一调用，删除了冗余的写入操作。

---

### [BUGFIX]  Fix degenerate strides in TRTLLM query tensors for FlashInfer backend. Fixes issue #32353 (#32417)
**SHA**: `6101a26` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6101a26dc95631d9e0cfe217089b1bb7f73d533e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 FlashInfer 后端的 TRTLLM 前缀与解码查询张量上先 `contiguous()` 再 `reshape`，修复因维度为 1 导致的退化 stride 问题。

---

### [Bugfix] Add OOT backend option (#32471)
**SHA**: `f5d1740` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f5d17400303149bbb480f6abfb6f7bb646c1d895)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `UnquantizedMoeBackend` 枚举中新增 `OOT`（Out‑of‑Tree）选项，并在平台检测逻辑中加入对应判断，使后端支持 OOT 环境。

---

### [Doc] Correct comment for _jobs dict in OffloadingConnectorWorker (#32556)
**SHA**: `5480c6b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/5480c6b1fa36271171903476f4dcb057c45c49ea)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `OffloadingConnectorWorker` 中 `_jobs` 字典的注释从 “req_id -> (job_id, store)” 修正为 “job_id -> (req_id, store)”，以准确反映键值含义，代码逻辑未变。

---

### Use the same memory for workspace13 and fused_output. (#31531)
**SHA**: `ba29ab4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ba29ab441e9bae7d84b9c191ec78256201cda699)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `modular_kernel.py` 中统一使用同一块内存作为 `workspace13` 与 `fused_output`，在单块情况下通过取最大尺寸来分配共享缓冲，简化了缓冲区管理逻辑。

---

### [CI] Move Distributed Tests from H200 -> H100 (#32555)
**SHA**: `afc3622` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/afc362260257ab16c2ffc6105678e1403c0475cb)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：CI 流水线将分布式测试的 GPU 从 H200 改为 H100，更新标签、gpu 字段并调整测试命令，同时保留 H200 测试区块的占位。

---

