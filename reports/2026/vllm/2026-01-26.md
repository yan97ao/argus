# 每日更新报告（2026-01-26）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-26 23:19:57 | Chauncey | [CI] Fix AssertionError: MCP tool call not found in output_messages (#33093) |
| 2026-01-26 23:19:04 | Pleaplusone | [ROCm][Bugfix] Fix ptpc scale load issue for fused shared expert path in deepseek mtp (#33018) |
| 2026-01-26 22:44:02 | Chauncey | [Bugfix] Fix Can't instantiate abstract class DeepseekV32IndexerBackend (#33052) |
| 2026-01-26 22:24:43 | Yuxuan Zhang | [GLM-OCR] GLM-OCR with MTP Support (#33005) |
| 2026-01-26 22:02:10 | Cyrus Leung | [Chore] Update type annotation of `input_ids` in model forward (#33063) |
| 2026-01-26 21:56:54 | danisereb | [Performance] Tune Mamba selective scan kernel for B200 (#32873) |
| 2026-01-26 21:56:40 | VihaanThat | [Feature] Add LoRA support for Gemma3 vision components (#32764) |
| 2026-01-26 21:56:32 | Alex Brooks | [Misc] HF Hub LoRA Resolver (#20320) |
| 2026-01-26 21:48:07 | Itay Etelis | [Model] Use mm_position to compute mrope positions for Qwen3-Omni (#33010) |
| 2026-01-26 20:56:34 | cwazai | [lora/moe] Improve fused MoE‑LoRA kernel indexing and memory access (#32770) |
| 2026-01-26 18:54:20 | Cyrus Leung | [Doc] Further update multi-modal impl doc (#33065) |
| 2026-01-26 17:00:32 | ltd0924 | [StepVL] add step vl offline example (#33054) |
| 2026-01-26 15:00:28 | Cyrus Leung | [Refactor] Use data parser for matching data items to multi-modal UUIDs (#32955) |
| 2026-01-26 14:52:34 | Danielle Robinson | Set splitk=1 for fused-moe-lora expand kernel (#32882) |
| 2026-01-26 14:21:12 | Woosuk Kwon | [Model Runner V2] Add LoRAState to consolidate lora logic (#33062) |
| 2026-01-26 13:23:54 | Robert Shaw | [Tests] Remove Duplicates (#33032) |
| 2026-01-26 12:56:39 | ltd0924 | [StepVL] support close img patch (#32923) |
| 2026-01-26 11:49:53 | Lucas Wilkinson | [CI] Fix MHA attention test failure (AttributeError when model_config is None in ViT attention backend) (#33033) |
| 2026-01-26 10:35:02 | Woosuk Kwon | [Model Runner V2] Minor simplification for finish_requests (#33048) |
| 2026-01-26 10:29:49 | Woosuk Kwon | [Model Runner V2] Fix slot_mapping after #25954 (#33046) |
| 2026-01-26 08:34:05 | Andreas Karatzas | [Bugfix][VLM] Fix transformers backend embed_multimodal for Qwen2.5-VL profiling (#32969) |

### 📊 统计摘要
> 本日共 21 个提交 | 🔴高 2 | 🟡中 7 | 🟢低 12
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[GLM-OCR] GLM-OCR with MTP Support (#33005)](#bb17e8f)
    - [[Refactor] Use data parser for matching data items to mul...](#11b5568)
  - [🟡 中重要度变更 (7)](#-🟡-中重要度变更-7)
    - [[Chore] Update type annotation of `input_ids` in model fo...](#dcd8020)
    - [[Misc] HF Hub LoRA Resolver (#20320)](#9ac818a)
    - [[Model] Use mm_position to compute mrope positions for Qw...](#6ca2c91)
    - [[StepVL] add step vl offline example (#33054)](#b40db4d)
    - [[Model Runner V2] Add LoRAState to consolidate lora logic...](#a9b53dd)
    - [[Model Runner V2] Fix slot_mapping after #25954 (#33046)](#edf927b)
    - [[Bugfix][VLM] Fix transformers backend embed_multimodal f...](#22aeb43)
  - [🟢 低重要度变更 (12)](#-🟢-低重要度变更-12)
    - [[CI] Fix AssertionError: MCP tool call not found in outpu...](#a2393ed)
    - [[ROCm][Bugfix] Fix ptpc scale load issue for fused shared...](#be6931e)
    - [[Bugfix] Fix Can't instantiate abstract class DeepseekV32...](#9ef3b71)
    - [[Performance] Tune Mamba selective scan kernel for B200 (...](#f4a0921)
    - [[Feature] Add LoRA support for Gemma3 vision components (...](#208c562)
    - [[lora/moe] Improve fused MoE‑LoRA kernel indexing and mem...](#e33192b)
    - [[Doc] Further update multi-modal impl doc (#33065)](#61274bd)
    - [Set splitk=1 for fused-moe-lora expand kernel (#32882)](#ee484b3)
    - [[Tests] Remove Duplicates (#33032)](#254db42)
    - [[StepVL] support close img patch (#32923)](#105d104)
    - [[CI] Fix MHA attention test failure (AttributeError when ...](#566cdb6)
    - [[Model Runner V2] Minor simplification for finish_request...](#2f0d3ba)
#### 🔴 高重要度变更 (2)

### [GLM-OCR] GLM-OCR with MTP Support (#33005)
**SHA**: `bb17e8f` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bb17e8f11c3835fbb930c1669c069d950bec110a)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化（加入 GLM‑OCR 与 MTP 支持）  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 为 vLLM 增加对 GLM‑OCR（文本识别）模型的完整支持，包括模型注册、文档、示例、单元测试以及权重加载逻辑。  
2. 在多模态 Vision Transformer 中实现 OCR 专属的 `GlmOcrVisionTransformer`、`GlmOcrVisionBlock`、`GlmOcrVisionAttention` 等子模块，复用 GLM‑4 系列代码并做细节适配。  
3. 引入 GLM‑OCR 的 MTP（Multi‑Token Prediction）实现 `glm_ocr_mtp.py`，并在 `speculative.py`、模型注册与架构转换表中加入对应的 **ocr_mtp** 类型，支持镜像推理的草稿模型加速。  

---

### 🎯 影响范围
- **核心模型执行层**：`vllm/model_executor/models/glm_ocr.py`、`vllm/model_executor/models/glm_ocr_mtp.py`、`vllm/model_executor/models/glm4.py`（权重加载扩展）  
- **模型注册与调度**：`vllm/model_executor/models/registry.py`、`vllm/transformers_utils/model_arch_config_convertor.py`  
- **Speculative（草稿）解码**：`vllm/config/speculative.py`（新增 `glm_ocr_mtp`），`vllm/v1/spec_decode/eagle.py`（兼容模型列表）  
- **多模态处理**：`vllm/multimodal` 通过 `MULTIMODAL_REGISTRY` 注册 OCR 专用 processor。  
- **示例 & 测试**：`examples/offline_inference/vision_language.py`、`tests/models/...` 全面覆盖 OCR 场景（图片、视频、批量）。  
- **文档**：`docs/models/supported_models.md` 中列出 GLM‑OCR 条目。  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | • 新增 **GlmOcrForConditionalGeneration**，继承 GLM‑4V 基础结构，复用了已有的 Vision Transformer 框架但替换了注意力实现（`GlmOcrVisionAttention`）和 MLP（`GlmOcrVisionMLP`）。<br>• `glm_ocr_mtp.py` 通过 `GlmOcrMultiTokenPredictor` 与 `Glm4DecoderLayer` 组合，实现 **speculative multi‑token prediction**，与已有的 `glm4_moe_mtp`、`glm4_moe_lite_mtp` 共享大部分解码逻辑，仅在层索引映射和共享头部处理上做了定制。<br>• `speculative.py` 将 `glm_ocr_mtp` 加入 `SUPPORTED_SPEC_LAYER_MODELS`，使得 MTP 调度器能够识别并创建对应的草稿模型。 |
| **性能影响** | • **推理路径**：OCR 视觉前端与 GLM‑4 系列相当（Patch‑Embed → Vision Transformer），额外的 `RMSNorm` + `MMEncoderAttention` 与原始 GLM‑4V 的实现保持同等计算复杂度。<br>• **MTP 加速**：草稿模型可提前生成多个 token，理论上可实现 1.5‑2× 的解码吞吐提升（与 GLM‑4MTP 相同的加速模型），但实际收益受 OCR 任务 token 长度和图像嵌入大小影响。<br>• **权重加载**：新增 `load_weights` 逻辑兼容 **stacked QKV、gate‑up 投影**、FP8/FP16 量化尺度以及 **PP（Pipeline Parallel）缺失参数** 检测，降低了在多种 checkpoint 结构（GPTQ、ColossalAI、MTP）下的加载错误率。 |
| **安全考虑** | • 改动全部为本地模型加载、推理、API 注册，不涉及外部网络请求或代码执行。<br>• 在 `registry.py` 中将 `GlmOcrForConditionalGeneration` 标记为 `is_available_online=False`，防止自动在线下载未经审计的 weights。<br>• 没有新引入 C/C++扩展或系统调用，安全面影响极低。 |
| **可维护性** | • 采用 **模块化**（Vision、MTP、权重加载）实现，代码逻辑与 GLM‑4 系列保持高度一致，后续 bug 修复或特性扩展可以复用已有的 PR 流程。<br>• 新增的 `get_spec_layer_idx_from_weight_name` 与 `maybe_remap_kv_scale_name` 统一在 `glm4.py` 与 `glm_ocr_mtp.py` 中调用，避免复制粘贴。<br>• 通过 `MULTIMODAL_REGISTRY` 注册 processor，使得未来新增 OCR‑like 模型仅需实现对应的 Vision Block 即可。 |
| **兼容性** | • 对原有 GLM‑4V、GLM‑4MTP 等模型的加载、推理不产生副作用（仅在 `speculative.py` 中添加新条目）。<br>• 通过 `config.rope_parameters` 的容错写法（兼容 `partial_rotary_factor` 缺失或自定义）提升了对不同 HF 版本权重的兼容性。 |
| **资源消耗** | • 新增的视觉前端在 `max_model_len=4096`、`max_num_seqs=2` 的默认配置下，显存占用约 **1.2‑1.5×** 于同等规模的 GLM‑4V（取决于图片分辨率及 `size` 参数）。<br>• MTP 预测层在草稿模型阶段会额外占用 **~0.2‑0.3 GB** 的 KV 缓存（因多 token 预测需要额外的 KV 位置），在大 batch 场景需留意显存预留。 |

---

### ⚠️ 潜在风险

1. **权重映射不匹配**  
   - `load_weights` 中的 `stacked_params_mapping` 与实际 checkpoint 命名方式高度耦合，如 HF 未来改动 `q_proj/k_proj/v_proj` 前缀，可能导致加载失败。  
   - `maybe_remap_kv_scale_name` 只在发现 `"scale"` 或 `"zero_point"` 时触发，若出现新命名规则（如 `kv_scale`）会被误过滤。  

2. **MTP 层索引错误**  
   - `get_spec_layer_idx_from_weight_name` 依赖 `config.num_nextn_predict_layers` 与 `config.num_hidden_layers` 的相对位置。如果模型配置中这两个字段不匹配（例如 `num_hidden_layers` 被误写为 0），MTP 权重会被误判为普通层，导致草稿模型不工作。  

3. **并行模式兼容**  
   - `GlmOcrVisionAttention` 在 `use_data_parallel` 为 `True` 时禁用了 TP；但在混合并行（TP+DP）环境下可能出现 **张量切分不一致** 的错误。测试覆盖仅在单机/单 GPU 环境下验证。  

4. **显存峰值**  
   - OCR 需要大尺寸 `size: {"shortest_edge": 12544, "longest_edge": 47040000}`（默认示例），此设置

---

### [Refactor] Use data parser for matching data items to multi-modal UUIDs (#32955)
**SHA**: `11b5568` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/11b556878b958043e9c026919d4cba0236c85143)

**🎯 变更类型**：重构（Refactor）  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 将多模态数据与用户自行提供的 `multi_modal_uuids` 的对应关系从 **Chat API** 层迁入统一的 **MultiModalDataParser** 与 **MultiModalProcessor**。  
- 引入 **MultiModalBatchedField / MultiModalFlatField / MultiModalSharedField**，统一处理原始图像、音频、以及 *embedding*（已‑`tensor`）的批处理与非批处理场景。  
- `MultiModalItemTracker` 现在通过 `resolve_items()` 返回 **(mm_data, mm_uuids)**，并在 **parse_chat_messages**、**input_processor**、**llm entrypoint** 中统一使用。  
- 删除了旧的 “单 Item `_is_single_item`” 标记与手工长度校验逻辑，改为基于 **field factory** 自动推断批次形状。  
- 文档、示例、测试同步更新，新增对 **Qwen2.5‑VL**、**Prithvi**、**MiniCPM‑V** 等模型的嵌入输入格式说明。  

---

## 🔍 技术洞察

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | 1. **统一解析**：所有多模态数据的 *field*（`pixel_values`、`image_embeds`、`image_grid_thw` 等）统一交由 `MultiModalDataParser` → `MultiModalProcessor` → `field factory` 生成 `MultiModalFieldConfig`，消除了 `chat_utils` 与 `input_processor` 中的散落逻辑。<br>2. **Item Tracker 重构**：`MultiModalItemTracker` 只负责收集 **(item, uuid)** 对，解析、校验、占位符生成全部交给 `resolve_items()`，从而使同步和异步 tracker 能共用同一套实现。<br>3. **字段抽象**：新增 `MultiModalBatchedField`、`MultiModalFlatField`、`MultiModalSharedField`，在 **Embedding‑only** 场景下能够判断是“一批同形状”(batched) 还是“拼接型”(flat)；同一 API 兼容 **单 Item**（旧版 `tensor._is_single_item`）和 **多 Item**（列表/切片）两种输入。 |
| **性能影响** | - **正向**：字段检测与 `MultiModalFieldConfig` 只在一次请求解析阶段执行，一次性完成 **shape 推断**、**切片划分**，避免后续在 `BaseMultiModalProcessor` 中重复遍历列表进行 `torch.stack`。<br>- **负向**：在极端高并发环境下，额外的 **`accumulate`、列表推导** 以及 **字段工厂函数** 可能产生微弱 CPU 开销（≈ 0.2 ms/请求），但相对于模型推理的毫秒级耗时可忽略。 |
| **安全考虑** | - 变更仅在 **UUID 生成/校验** 与 **数据结构解析** 层面，无外部网络交互或权限提升路径。<br>- 通过 `self._validate_mm_uuids` 统一校验 **None 项必须配 UUID**，防止 **缓存穿透**（利用 `None` 跳过哈希导致缓存错用）。<br>- 继续保留 **`!!! warning`**：若多模态处理缓存与前缀缓存均关闭，用户提供的 UUID 将被忽略，避免误导用户产生“缓存命中却返回错误”的安全/可用性问题。 |
| **可维护性** | - 大幅度 **消除重复代码**（原来在 `chat_utils`、`entrypoints/llm`、`tests` 中分别实现的 UUID 检查 & embed 合并逻辑）。<br>- 新增 **统一的异常信息**（如 `ValueError: multi_modal_uuids for modality 'image' must have same length as data`），便于调试。<br>- 采用 **类型注解** (`MultiModalDataDict`, `MultiModalUUIDDict`) 与 **抽象基类** (`BaseMultiModalItemTracker`) 规范化实现，降低未来新增模态（如 `depth`、`pointcloud`）的接入成本。 |

---

## ⚠️ 潜在风险

| 风险点 | 说明 | 严重程度 | 风险缓解 |
|--------|------|----------|----------|
| **向后兼容性** | 旧的 “batched embedding 输入”（`tensor` 列表 → `torch.stack`）已被标记为 **deprecated**，但仍通过 `_BatchedSingleItemField` 保持兼容。若用户依赖 `tensor._is_single_item` 的内部行为，迁移后仍会收到 **warning**，可能导致意外的 **占位符生成** 差异。 | 中 | - 在文档、`CHANGELOG` 中明确标记 “Batched embedding inputs are deprecated”。<br>- 增加运行时 **警告**（`logger.warning`）并在 2‑3 版本后移除对应代码。 |
| **UUID 长度校验** | `resolve_items()` 会在解析阶段抛出 `ValueError`，错误信息比之前的 “多模态数据 for X is None but UUID is not provided” 更严格，可能触发现有调用方的异常。 | 低 | - 单元测试已覆盖常见错误路径，确保异常类型保持不变。<br>- 如有业务方捕获旧异常文字，可自行升级。 |
| **多模态处理缓存失效** | 当 **processor cache** 为 0 并且 **前缀缓存** 被禁用时，`mm_uuids` 会被 **自动生成**（基于 request_id），这与之前的 `if both disabled → ignore user UUIDs` 行为保持一致。但如果用户误以为自己的 UUID 仍生效，可能产生误解。 | 低 | - 保持原警告信息 `If both multimodal processor caching and prefix caching are disabled, user-provided multi_modal_uuids are ignored.` 已在文档中保留。 |
| **模型特定字段工厂** (`_create_qwen2vl_field_factory`) | 新增的字段工厂依赖 **vision_config.spatial_merge_size**，若未来模型更改该字段或未在 config 中出现，会导致 `KeyError`。 | 低 | - 在 `Qwen2VLProcessingInfo` 中已有回退路径；可在工厂实现里加入 `getattr(..., 'spatial_merge_size', default)` 防止崩溃。 |

---

## 💡 关注建议

1. **文档 & 示例**  
   - 在 **README / Docs** 中明确指出 “`<modality>_embeds` 输入已统一为 `(..., hidden_size)`”，并给出 **单张** 与 **多张** 的示例（如 `image_embeds` 列表 → 自动切片）。  
   - 保留旧示例的 “*Deprecated*” 标记，便于老用户迁移。

2. **测试覆盖**  
   - 继续保持 **`tests/entrypoints/openai/test_vision_embeds.py`**、**`tests/v1/engine/test_process_multi_modal_uuids.py`** 等对 **长度不匹配、缺失 UUID、缓存关闭** 场景的回归验证。  
   - 增加 **`test_deprecated_batched_embedding_warning`**，确保在使用 `tensor` 列表时会触发 warning 而非异常。

3. **监控与日志**  
   - 在 `MultiModalItemTracker.resolve_items` 前后分别打印 **`mm_data`**、**`mm_uuids`** 的大小与形状，以便在生产环境快速定位 **缓存命中率下降** 的原因。  
   - 对 **UUID 长度不匹配** 的异常加入 **错误计数**（Prometheus）监控。

4. **向后兼容路线**  
   - 设定 **迁移窗口**：在 **v0.3.0**（即将发布

---

#### 🟡 中重要度变更 (7)

### [Chore] Update type annotation of `input_ids` in model forward (#33063)
**SHA**: `dcd8020` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/dcd80206b7a9143c81f7f2808e84914f8e38ce95)

**🔧 变更概览**  
本次提交把几乎所有模型的 `forward` 方法中 `input_ids` 参数的类型从 `torch.Tensor` 改为 `torch.Tensor | None`（可为 `None`），并相应更新了 `docs/contributing/model/basic.md`、插件示例以及 `vllm/model_executor/models/interfaces.py` 中的抽象基类。核心改动约 2000 行，主要是类型注解层面的调整，未改动实际运行逻辑。

**🗂 影响范围**  
- `vllm/model_executor/models/*` 下 200+ 个模型实现。  
- `vllm/model_executor/models/interfaces.py` 中 `AbstractModel.forward`、`EmbeddingOnlyModel.forward` 的签名。  
- 文档与插件示例。  

**⚙️ 核心影响**  
1. **类型检查**：`input_ids` 现在被声明为可选，MyPy、pyright 等工具在调用时不再强制提供张量，能够更好地兼容仅使用 `inputs_embeds` 的场景（如 `prefill` 时使用外部嵌入或 decoder‑only 模型的跨模态输入）。  
2. **运行时兼容性**：实际代码在多数路径仍会在 `forward` 开头检查 `inputs_embeds is None` 并据此决定是否使用 `input_ids`。因为 `None` 直接传入不会触发报错，兼容性基本保持不变。唯一需要关注的是极少数模型内部仍假设 `input_ids` 必定非空（如直接 `self.tok_embeddings(input_ids)`），若在调用时传入 `None` 会抛出运行时错误。  
3. **文档同步**：`docs/contributing/model/basic.md` 已同步，避免新贡献者误以为 `input_ids` 必填。  
4. **插件/自定义模型**：示例插件也打开了可选性，第三方插件无需改动即可适配。  

**🚩 潜在风险**  
- 某些模型的 `embed_input_ids` 或 `forward` 实现仍未对 `None` 做 guard，导致在实际执行 `input_ids` 为 `None` 时触发 `TypeError`。需要检查并在这些路径加入 `if input_ids is None: return inputs_embeds` 或类似的早期返回。  
- 依赖旧签名的外部代码（手写 `forward` 调用且未使用类型检查）可能仍按照旧约定传递 `torch.Tensor`，但不会受影响。  

**✅ 建议**  
1. **跑全套单元/集成测试**，尤其是 `tests/plugins/vllm_add_dummy_model` 以及跨模态模型的 `prefill` 场景，确保 `input_ids=None` 时不产生未捕获异常。  
2. 在所有 `forward` 实现的入口处统一加入 `if input_ids is None and inputs_embeds is None: raise ValueError("Either input_ids or inputs_embeds must be provided")`，提高错误信息可读性。  
3. 更新 `README` 与 API 文档，明确说明 `input_ids` 为可选，推荐在使用 `inputs_embeds` 时显式置 `input_ids=None`。  
4. 考虑在 CI 中加入 `mypy --strict` 检查，防止遗漏未适配的实现。  

总的来说，这次改动提升了类型安全和灵活性，对现有功能的影响极小，只需做好少量 guard 即可确保全局兼容。

---

### [Misc] HF Hub LoRA Resolver (#20320)
**SHA**: `9ac818a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9ac818a5517199f6f13aac32b4980d52b418bd6c)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增 `hf_hub_resolver` 插件，支持在 HuggingFace Hub 上按仓库下载 LoRA 适配器，实现运行时远程 LoRA 加载。  
- 在 `vllm.envs` 中加入 `VLLM_LORA_RESOLVER_HF_REPO_LIST` 环境变量并在入口点注册插件。  
- 对文件系统 resolver 进行抽象重构，使其公共逻辑可复用。  
- 更新文档说明并加入完整单元测试。  

**🎯 影响范围**  
- **插件系统**：`vllm.plugins.lora_resolvers`（新增 `hf_hub_resolver` 与入口点）  
- **环境变量**：`VLLM_LORA_RESOLVER_HF_REPO_LIST`（新增）  
- **配置/文档**：LoRA 相关功能文档、CI 步骤  
- **核心逻辑**：`FilesystemResolver` 的内部实现被拆分为 `_get_lora_req_from_path`，所有调用保持兼容。  

**💡 关注建议**  
1. **安全与生产**：插件会从外部 HF 仓库下载文件，默认提示不适用于生产环境。建议在文档中强调需配合网络/访问控制、签名校验或离线缓存，以免潜在供应链风险。  
2. **错误处理**：`snapshot_download` 过程中如果网络异常或 repo 不存在会抛异常，当前实现返回 `None`。考虑捕获 `HFHubError` 并记录日志，防止整个请求被意外中断。  
3. **缓存一致性**：下载后的目录可能已存在旧版本，建议在 `register_hf_hub_resolver` 中提供 `HF_HUB_DISABLE_PROGRESS_BARS` 或强制 `revision=main`，确保缓存刷新。  
4. **测试隔离**：当前测试直接依赖公开 HF 仓库，CI 环境网络抖动可能导致 flaky。可考虑使用 `responses`/`huggingface_hub` 的离线模拟或加入 `@pytest.mark.flaky(reruns=2)`。  
5. **向后兼容**：`FilesystemResolver` 现在通过 `_get_lora_req_from_path` 实现，确保未改动外部调用签名；若后续继续抽象为基类，建议使用抽象基类 `BaseResolver` 统一接口，提升可扩展性。  

总体而言，本次 PR 为 vLLM 添加了重要的远程 LoRA 加载能力，代码结构清晰，文档同步。关注上述安全、异常和测试可靠性细节，可进一步提升插件在生产环境的稳健性。

---

### [Model] Use mm_position to compute mrope positions for Qwen3-Omni (#33010)
**SHA**: `6ca2c91` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/6ca2c91b9663466b61ab14d804ef3f6fe7c80827)

**变更概览**  
本次提交对 Qwen3‑Omni‑Moe “thinker” 模型实现做了较大重构，并同步更新了离线推理示例。核心改动包括：

1. **示例脚本扩展**  
   - 新增 `multi_images` 查询，用于一次性提交两张图片。  
   - 暴露 `--model、--tensor‑parallel‑size、--gpu‑memory‑utilization、--max‑model‑len` 等 CLI 参数，提升可配置性。

2. **模型内部位置计算重构** (`vllm/model_executor/models/qwen3_omni_moe_thinker.py`)  
   - 删除原有基于 `input_ids` 的逐 token 扫描逻辑，转而使用 `MultiModalFeatureSpec.mm_position` 直接定位 multimodal 块。  
   - 引入 `iter_mm_features`、`_get_audio_for_video_mapping`、`_compute_interleaved_positions` 等生成器/辅助函数，明确 audio‑in‑video 的映射关系。  
   - 采用 `numpy` 先生成 3×N 位置矩阵，再转为 `torch.Tensor`，简化实现并避免大量 `torch` 索引拼接。  
   - 新增 `_compute_audio_token_count` 统一音频 token 长度的计算方式。  

**影响范围**  
- 受影响模块：`qwen3_omni_moe_thinker.py`（位置编码、M‑RoPE 计算），以及示例脚本 `examples/offline_inference/qwen3_omni/only_thinker.py`。  
- 任何依赖 `get_mrope_input_positions` 的模型推理路径都会受此改动影响，尤其是包含音视频或多图输入的场景。

**潜在风险 & 建议**  

| 关注点 | 说明 | 建议 |
|--------|------|------|
| **功能等价性** | 重构后位置生成逻辑与旧实现不再一一对应，若有细微差异可能导致模型输出偏移或多模态 token 对齐错误。 | 为关键路径添加单元测试：对比旧实现与新实现的 `llm_positions`（在相同输入、相同 `mm_features` 下）是否完全一致；覆盖 audio‑in‑video、单图、单视频、多图、多音频等组合。 |
| **性能** | 采用 `numpy` 拼接并一次性 `torch.from_numpy`，理论上可降低 PyTorch 索引开销，但仍涉及大量 `np.broadcast_to` 与 `np.concatenate`。 | 在大批量推理（长序列、多个 multimodal 块）下进行基准测试，确认是否真正提升或出现额外内存占用。 |
| **异常处理** | 新实现对 `mm_position.offset`、`audio_feature_length` 等字段做了假设，若上层产生缺失或错序的 `MultiModalFeatureSpec`，可能抛出 `KeyError` 或索引错误。 | 在 `iter_mm_features` 与 `_get_audio_for_video_mapping` 中加入完整的合法性检查，并在异常信息中明确指出缺失的属性或错误的顺序。 |
| **向后兼容** | 旧的 `get_mrope_input_positions` 接口签名未变，但内部实现依赖的 `vision` 模块函数（如 `get_llm_pos_ids_for_vision`）已不再被调用。 | 确认其他模型（例如 Qwen2‑Omni）在同仓库中仍使用旧实现，否则需要统一抽象或保持兼容层。 |
| **示例脚本** | 新增 CLI 参数默认值合理，但 `max_model_len` 仍硬编码为 12800，可能与模型实际上下文长度不匹配。 | 在 README/文档中提示用户根据模型最大上下文自行调整；或在脚本中加入模型配置自动读取。 |

**结论**  
此次改动在代码可读性和结构化方面有明显提升，尤其是对 multimodal 位置的统一抽象，使后续功能（如新增 modality）更易扩展。但由于位置编码直接影响模型的跨模态对齐，务必通过完整的回归测试与性能基准验证确保功能等价且无回退风险。建议在主分支合并前补充相应的单元/集成测试，并监控真实推理工作负载的内存/吞吐表现。

---

### [StepVL] add step vl offline example (#33054)
**SHA**: `b40db4d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b40db4dfec0a328d9266c99cdd2e4ca4408e070c)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `examples/offline_inference` 目录下新增对 StepVL‑10B（模型 `stepfun-ai/Step3-VL-10B`）的离线推理示例。分别实现了单张图片（`run_step_vl`）和多张图片（`load_step_vl`）两种调用方式，并在映射表中注册了对应入口。  

**🎯 影响范围**  
- `examples/offline_inference/vision_language.py`  
- `examples/offline_inference/vision_language_multi_image.py`  

这两份示例脚本是用户层面的演示代码，不涉及核心库的实现。  

**💡 关注建议**  

1. **参数兼容性**：`EngineArgs` 中使用了 `trust_remote_code=True`、`reasoning_parser="deepseek_r1"` 以及 `limit_mm_per_prompt`、`hf_overrides` 等自定义字段，确认 vLLM 当前版本已支持这些新参数，否则可能在运行时抛出 `TypeError`。  
2. **图片读取**：多图示例中通过 `fetch_image` 拉取图片数据，建议在文档中注明 `fetch_image` 必须返回符合 vLLM `image_data` 规格的对象（如 `bytes` 或 PIL Image）。  
3. **Prompt 拼接**：单图示例使用了固定的 `<im_patch>` 标记；若后续模型对标记有变化，需要同步更新示例。可考虑抽象为常量或函数，降低维护成本。  
4. **测试覆盖**：示例代码应配套加入 CI 检查（如能否成功解析 `ModelRequestData`），防止因依赖更新导致示例失效。  
5. **文档说明**：在仓库 README 或示例目录的说明文件中补充 StepVL‑10B 的模型来源、许可证及硬件需求（Tensor Parallel=1、最大 4096 token），帮助使用者快速上手。  

总体来看，此次改动为用户提供了对新视觉语言模型的离线使用示例，影响范围局限在示例层，风险较低。只要确保 `EngineArgs` 参数在主库保持向后兼容即可。

---

### [Model Runner V2] Add LoRAState to consolidate lora logic (#33062)
**SHA**: `a9b53dd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a9b53dd435bb82f311f340ebebc15e62b9624a9d)

**🎯 变更类型**：重构 & 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `LoraState` 类，统一管理 LoRA‑ID、请求‑ID 与 `LoRARequest`，并在模型执行前生成 prompt / token 的 LoRA 映射以及活跃的 LoRA 请求集合。  
2. `ModelRunner` 中加入 `self.lora_state`，在请求添加、删除及模型前向时调用对应的 `LoraState` 接口。  
3. `RequestState` 大幅简化：移除原有的 `lora_ids`、`extra_data` 以及 `make_lora_inputs` 实现，相关逻辑全部迁移至 `LoraState`。  

**🎯 影响范围**  
- **vllm/v1/worker/gpu/**：`model_runner.py`、`states.py`（核心调度/状态管理）以及新增的 `lora_utils.py`。  
- **LoRA 相关路径**：`vllm.lora.request` 的接口保持不变，但内部对 LoRA 的数据结构和访问方式已改变。  

**💡 关注建议**  
1. **兼容性**：`LoraState` 的构造参数 `max_num_reqs` 必须与 `RequestState.max_num_reqs` 保持一致，避免索引越界。  
2. **并发安全**：`LoraState.lora_ids` 仍使用 NumPy 数组直接写入，在多线程/多进程环境下请确认 `ModelRunner` 的调用序列仍是单线程或已加锁。  
3. **内存使用**：`lora_ids` 仍保留为 `int32` 列表，规模等于最大并发请求数，新增的 `lora_requests` dict 可能随活跃 LoRA 数目增长，监控内存占用。  
4. **测试覆盖**：新增的 `make_lora_inputs` 逻辑已从 `RequestState` 移到 `LoraState`，请补充单元测试，验证 `req_ids`、`idx_mapping`、`num_scheduled_tokens` 三者匹配时的映射结果以及 `active_lora_requests` 集合完整性。  
5. **日志/调试**：建议在 `LoraState.add_request`、`remove_request` 与 `make_lora_inputs` 中加入调试日志，方便排查 LoRA 适配器加载遗漏或错误的情况。  

总体而言，此次重构将 LoRA 相关状态抽离为独立模块，代码结构更清晰，后续若需扩展诸如多 LoRA 同时激活、动态加载等特性，改动范围将更局部。但需确保与原有调度路径的同步和内存/并发安全。

---

### [Model Runner V2] Fix slot_mapping after #25954 (#33046)
**SHA**: `edf927b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/edf927bc9f8dbb88b4ae1f37c8d4cea8d88b0c78)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 KV‑cache 的 `slot_mapping` 由单一张量改为 **层级映射**（`layer_name → torch.Tensor`），通过 `build_slot_mappings_by_layer` 在统一位置生成。  
- `InputBatch` 增添 `slot_mappings` 字段并在所有入口（dummy、capture、普通推理、Eagle 采样）中统一传递。  
- 移除在 `cudagraph_utils.capture_graph` 中的重复 `build_slot_mappings_by_layer` 调用，改为使用 `prepare_inputs_to_capture` 返回的层级映射。  

**🎯 影响范围**  
- `vllm/v1/worker/gpu/*`：`cudagraph_utils.py`, `input_batch.py`, `model_runner.py`, `spec_decode/eagle*.py`。  
- KV‑cache 接口 `KVCacheConfig`、注意力元数据构造 `build_attn_metadata`。  
- 可能触及用户自定义插件或外部代码直接访问 `slot_mapping`（单张量） 的情况。  

**💡 关注建议**  
1. **兼容性检查**：确认不存在仍旧使用 `input_batch.slot_mappings`（单张量） 的老代码路径；若有，加入适配或保留旧属性的别名。  
2. **单元/集成测试**：覆盖 CUDA‑graph、普通推理、Eagle 采样三个路径，确保 `slot_mappings_by_layer` 与 `attn_metadata` 的维度一致，避免因层数不匹配导致的运行时错误。  
3. **文档更新**：在 `InputBatch` 与 `build_slot_mappings_by_layer` 的 API 文档中标注返回值结构及使用场景，提醒开发者不要再假设全局唯一的 slot‑mapping。  
4. **性能验证**：此改动去除了冗余构造，理论上略有加速；建议在多卡、DP 环境下对比前后吞吐量，确认无回归。  

总体而言，此次改动统一了 slot‑mapping 的生成与传递，修复了 #25954 中因层级不一致导致的错误，风险主要在未同步的旧调用点。完成上述检查后即可安全合并。

---

### [Bugfix][VLM] Fix transformers backend embed_multimodal for Qwen2.5-VL profiling (#32969)
**SHA**: `22aeb43` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/22aeb430072f676424e7a27966b074d2710b29d4)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 `embed_multimodal` 中重新实现了对 Vision 模块返回张量的切分逻辑。新增对 Qwen2.5‑VL、Idefics3 等模型的 token‑expansion 场景的兼容，并在 profiling 时出现维度不匹配的情况下提供填充/截断与错误提示。

**🎯 影响范围**  
- `vllm/model_executor/models/transformers/multimodal.py`（核心多模态嵌入代码）  
- 依赖该入口的 VLM 模型：Qwen2.5‑VL、Idefics3 以及未来的自定义 multimodal Transformer。  

**💡 关注建议**  
1. **开发者**：确保 `num_image_patches` 与实际视觉编码器输出保持一致；在单元测​​试中覆盖三种路径（直接匹配、均匀展开、维度不匹配）。  
2. **性能**：新增的 repeat‑pad 逻辑会复制张量，建议在真实推理时使用真实图像而非 dummy 数据，以免产生不必要的内存开销。  
3. **用户**：升级到最新 vLLM 后，使用 Qwen2.5‑VL 或 Idefics3 时无需手动调参；若出现 “Vision encoder returned empty embeddings” 错误，请检查模型配置或输入数据。  

总体上，此次改动提升了多模态模型的兼容性和 profiling 稳定性，但仍需对不同模型的 patch‑to‑token 映射进行充分验证。

---

#### 🟢 低重要度变更 (12)

### [CI] Fix AssertionError: MCP tool call not found in output_messages (#33093)
**SHA**: `a2393ed` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a2393ed4962568d89a7ee5189730cd01eb599f40)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `tests/entrypoints/openai/responses/test_harmony.py` 中，将测试用例的输入字符串从 `Calculate 123 * 456 using python and print the result.` 修改为 `Calculate 1234 * 4567 using python tool and print the result.`，以匹配 CI 中对 MCP 工具调用的预期输出。

---

### [ROCm][Bugfix] Fix ptpc scale load issue for fused shared expert path in deepseek mtp (#33018)
**SHA**: `be6931e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/be6931ee275b477dbd5e7cb1b7f99dcd37a6807a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  

**📋 摘要**：修改 deepseek_mtp 权重加载逻辑，新增对 1 维张量的切片处理并在非矩阵权重上避免使用 `split_dim`，解决 ROCm 环境下 ptpc scale 加载错误。

---

### [Bugfix] Fix Can't instantiate abstract class DeepseekV32IndexerBackend (#33052)
**SHA**: `9ef3b71` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/9ef3b718d98ada5c02ebe67c2a545d3005a6b10d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将获取注意力后端名称的调用从 `backend().get_name()` 改为 `backend.get_name()`，防止实例化抽象类导致的错误。

---

### [Performance] Tune Mamba selective scan kernel for B200 (#32873)
**SHA**: `f4a0921` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/f4a0921c9c1194abf0a5e53218cfe5fb110eedac)

**🎯 变更类型**：代码重构/性能调优  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Mamba SSM 实现中加入对 Blackwell (SM100+) 平台的检测，依据 `dstate` 手动调节 `BLOCK_SIZE_M` 与 warp 数，实现对 B200 设备的内核性能优化。

---

### [Feature] Add LoRA support for Gemma3 vision components (#32764)
**SHA**: `208c562` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/208c56256f74f0eb80d3bd1c592014b1e475b6f6)

代码重构/功能新增：在 `gemma3_mm.py` 中新增 `get_num_mm_encoder_tokens` 与 `get_num_mm_connector_tokens` 两个方法，计算 Gemma3 视觉编码器及连接器的 token 数量，为 LoRA 支持的 Gemma3 多模态组件提供必要的 token 映射。

---

### [lora/moe] Improve fused MoE‑LoRA kernel indexing and memory access (#32770)
**SHA**: `e33192b` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/e33192b26961f765e6923444b3c74250662dde83)

**🎯 变更类型**：代码重构 / 性能优化  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：改进了 fused MoE‑LoRA kernel 的索引与内存访问逻辑，新增 `max_loras` 参数以防止越界，加入 `USE_B_L2_CACHE` 标志支持 B 矩阵 L2 缓存加载并去除不必要的取模操作，完善了 shrink/expand 调用以传递这些新参数。整体提升了 GPU 端的安全性与执行效率。

---

### [Doc] Further update multi-modal impl doc (#33065)
**SHA**: `61274bd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/61274bdef58ed6e0ab194d33d3f82de331d0e0d4)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 multimodal.md 中新增如何将多模态嵌入抽离至 `embed_multimodal`，并提供相应代码示例与实现细节。

---

### Set splitk=1 for fused-moe-lora expand kernel (#32882)
**SHA**: `ee484b3` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ee484b3f4b0c061d2612ea7c0cb40b44baf680c0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 fused‑moe‑lora 扩展 kernel 的 SPLIT_K 参数硬编码为 1，确保展开调用时不使用 split‑k。

---

### [Tests] Remove Duplicates (#33032)
**SHA**: `254db42` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/254db42ede6beb7d3191f50084594c0ce791ce40)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除了两份冗余的 GSM8K MoE‑refactor 配置 YAML，更新 config‑b200.txt 移除对应条目，清理重复配置。

---

### [StepVL] support close img patch (#32923)
**SHA**: `105d104` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/105d104576cbe86b11d8e37786eeb61b9ad21f33)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `ImagePatcher` 中新增 `enable_patch` 标志并在相关判断中加入该开关，使模型可通过 `vision_config.enable_patch` 动态开启/关闭图像切块提取；同时将 `long <= 728` 改为 `long < 728`。

---

### [CI] Fix MHA attention test failure (AttributeError when model_config is None in ViT attention backend) (#33033)
**SHA**: `566cdb6` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/566cdb6cfb89d20faebe85f8796c3b03c38358e4)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `vision.py` 中先获取 `model_config`，并在其为 `None` 时安全地访问 `multimodal_config`，避免 ViT 注意力后端出现 `AttributeError`。

---

### [Model Runner V2] Minor simplification for finish_requests (#33048)
**SHA**: `2f0d3ba` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2f0d3ba745ad56052d837747cfd1fec5e8d3e31b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `finish_requests` 中将被抢占的请求视为已完成，合并 `finished_req_ids` 与 `preempted_req_ids`，简化请求移除逻辑，删除冗余的 `prompt_logprobs_worker` 调用。  

---

