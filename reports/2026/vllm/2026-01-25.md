# 每日更新报告（2026-01-25）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-25 20:15:53 | Itay Etelis | [Model] Use mm_position to compute mrope positions for Qwen2.5-Omni (#32772) |
| 2026-01-25 17:20:46 | zhanqiuhu | [Doc] Add Qwen2.5 models to batch invariance tested models (#33016) |
| 2026-01-25 16:32:30 | Jee Jee Li | [BugFix]  Add env variable to control PDL in LoRA (#32836) |
| 2026-01-25 13:17:31 | JJJYmmm | [Bugfix] fix encoder cache hang in Qwen3VL (#32684) |
| 2026-01-25 09:51:49 | 7. Sun | [Docs] Fix Apple silicon include path in CPU installation docs (#32977) |
| 2026-01-25 09:45:27 | Roberto L. Castro | [Perf][Kernel] Optimize FP4 quantization kernels (SM100F) (#32520) |
| 2026-01-25 09:13:21 | TJian | [DOC] [ROCm] Update doc for v0.14.1 (#32998) |
| 2026-01-25 04:06:28 | Joshua Deng | [Feature] add session based streaming input support to v1 (#28973) |
| 2026-01-25 01:39:30 | yugong333 | Using max_loras + 1 to construct grid in fused_moe_lora (#32277) |
| 2026-01-25 01:08:24 | Maryam Tahhan | [CPU] Improve CPU Docker build  (#30953) |
| 2026-01-25 01:02:22 | Fadi Arafeh | [CPU Backend][BugFix] Fix failing Darwin pipelines (#33002) |
| 2026-01-25 00:39:07 | 7. Sun | [Tests] Replace flaky sleep with polling in test_background_cancel (#32986) |
| 2026-01-25 00:03:02 | Lucas Wilkinson | [MLA] Fuse cat and qaunt for fp8 kv-cache (#32950) |
| 2026-01-25 00:02:44 | Louie Tsai | Update CPU doc according to feedback (#32963) |

### 📊 统计摘要
> 本日共 14 个提交 | 🔴高 2 | 🟡中 4 | 🟢低 8
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [[Perf][Kernel] Optimize FP4 quantization kernels (SM100F)...](#fcb9df9)
    - [[Feature] add session based streaming input support to v1...](#91601ff)
  - [🟡 中重要度变更 (4)](#-🟡-中重要度变更-4)
    - [[Model] Use mm_position to compute mrope positions for Qw...](#a698e8e)
    - [[Bugfix] fix encoder cache hang in Qwen3VL (#32684)](#7e67df5)
    - [[CPU] Improve CPU Docker build  (#30953)](#203d0bc)
    - [[MLA] Fuse cat and qaunt for fp8 kv-cache (#32950)](#da5e7b1)
  - [🟢 低重要度变更 (8)](#-🟢-低重要度变更-8)
    - [[Doc] Add Qwen2.5 models to batch invariance tested model...](#151e545)
    - [[BugFix]  Add env variable to control PDL in LoRA (#32836)](#73b2434)
    - [[Docs] Fix Apple silicon include path in CPU installation...](#ff6c1da)
    - [[DOC] [ROCm] Update doc for v0.14.1 (#32998)](#1ebdff4)
    - [Using max_loras + 1 to construct grid in fused_moe_lora (...](#d4dbb7a)
    - [[CPU Backend][BugFix] Fix failing Darwin pipelines (#33002)](#17ab54d)
    - [[Tests] Replace flaky sleep with polling in test_backgrou...](#cd775bd)
    - [Update CPU doc according to feedback (#32963)](#719ac59)
#### 🔴 高重要度变更 (2)

### [Perf][Kernel] Optimize FP4 quantization kernels (SM100F) (#32520)
**SHA**: `fcb9df9` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fcb9df99bd7d0e532bcf2891db2a85bd927605fe)

**🎯 变更类型**：功能增强 / 性能优化 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 为 FP4 量化内核引入 `is_sf_swizzled_layout` 参数，支持 **SWIZZLED**（Tensor‑Core 推荐的 128×4）和 **非 SWIZZLED** 两种尺度布局。  
- 在 CUDA 端实现 **ELTS_PER_THREAD=16**（即 16 floats / thread）路径，仅在 SM100F（CUDA ≥12.0.9）上启用，显著提升每个线程的吞吐量。  
- 完善 kernel 启动配置、内存填充、scale 写入逻辑，统一在 Python、C++、Triton、FlashInfer、模型执行层的调用链路中传递该标识。  
- 添加 benchmark、单元测试以及对应的 API 文档/签名更新，确保功能向后兼容并可在不同 layout 下验证正确性。

---

### 🎯 影响范围
| 受影响模块 / 组件 | 关键文件 | 影响说明 |
|-------------------|----------|----------|
| **核心量化算子** | `csrc/ops.h`, `csrc/quantization/fp4/*.cu`, `csrc/quantization/fp4/nvfp4_utils.cuh` | 新增 `is_sf_swizzled_layout` 参数，内部实现分支切换，追加 `ELTS_PER_THREAD=16` 路径。 |
| **Python 绑定层** | `vllm/_custom_ops.py`, `csrc/torch_bindings.cpp` | 对外 API 增加布尔参数，默认开启 SWIZZLED，以保持兼容；扩展 `scaled_fp4_quant` 调用。 |
| **Triton 基准** | `benchmarks/kernels/bench_nvfp4_quant.py` | 基准用例加入 layout 参数，测试两种布局下的性能。 |
| **FlashInfer / ModelExecutor** | 多处 (`vllm/model_executor/...`, `vllm/compilation/*.py`) | 所有使用 `scaled_fp4_quant` 的位置均添加 flag，保证 MoE、Fusion、TRT‑LLM 等路径使用统一布局。 |
| **单元测试** | `tests/kernels/quantization/*.py` | 新增非 swizzled 布局的正确性测试以及更大尺寸的覆盖。 |
| **构建系统** | `CMakeLists.txt`（隐含） | 通过宏 `NVFP4_ENABLE_ELTS16` 控制编译路径，需确保 SM100F 与 CUDA ≥12.0.9 环境。 |

---

### 🔍 技术洞察  

#### 架构影响
- **布局抽象化**：将尺度（scale‑factor）布局从固定的 “swizzled” 拆分为可选，使得 `scaled_fp4_quant` 能够在 **Tensor Core 推荐布局** 与 **传统线性布局** 之间自由切换，提升了库在不同硬件/后端（FlashInfer、vLLM 自研）之间的兼容性。  
- **内核重构**：原来的 `cvt_fp16_to_fp4` 只处理 **行主序**（无填充），现在分为两套实现：  
  1. `cvt_fp16_to_fp4`（带填充，适配 swizzled）  
  2. `cvt_fp16_to_fp4_sf_major`（不填充，适配非 swizzled）。  
- **向量化提升**：通过宏 `NVFP4_ENABLE_ELTS16`，在支持的 SM100F GPU 上将每个线程从处理 8 元素提升至 16 元素，配合 32‑byte `PackedVec`（`alignas(32)`）实现更高的寄存器利用率与内存吞吐。  
- **统一 Scale 写入**：Scale 写入逻辑统一放在 `cvt_warp_fp16_to_fp4` 中，使用 `sf_out_rowmajor_u8` 计算写址，避免不同代码路径出现布局不匹配的 bug。

#### 性能影响
| 场景 | 旧实现（SM80） | 新实现（SM100F, ELTS=16） | 估计提升 |
|------|----------------|--------------------------|-----------|
| 单矩阵量化（FP16→FP4） | 每线程 8 元素 → 512 threads/block | 每线程 16 元素 → 512 threads/block | **约 1.8‑2.0×**（受限于内存带宽） |
| 大 Batch（≥8192） | 受限于 block 规模（1024） | 采用 `dim3(grid_y, grid_x)` 灵活拆分 | **30‑50%** 运行时下降（更好的 occupancy） |
| Scale Layout（swizzled） | 需要手动 pad & pack | Kernel 自动 pad & 写入，避免额外 `torch.nn.functional.pad` | **降低内存占用 5‑10%**，并减少 **CPU⇆GPU 同步** |
| FlashInfer / TRT‑LLM 路径 | 固定 swizzled → 必须 unswizzle | 直接生成目标布局 → 省去一次 `unswizzle` | **~10%** 总体 GEMM 前置开销下降 |

> **注意**：实际提升会根据模型宽度、Batch 大小以及 GPU 型号略有差异，benchmark 已在 `bench_nvfp4_quant.py` 中展示了在 SM100F (`A100 80GB`) 上对 4096‑8192 列宽的 **~1.7×** 提速。

#### 安全考虑
- **输入验证**：新布尔参数未做显式检查，若用户手动传入错误的布局与实际硬件不匹配（例如在 SM80 上开启 `is_sf_swizzled_layout=True`），仍会走 **swizzled** 路径并触发 **填充/写址错误**。建议在 `scaled_fp4_quant` 再加入 `torch.cuda.get_device_capability` 检测，如果设备不支持 SM100F，则强制降级为 `False` 或抛异常。  
- **内存越界**：新增的 `ld128_or_zero_cg_u32` / `ld256_or_zero_cg_u32` 已经加入 `pred` 条件防止非法加载，但仍依赖 `valid` 计算的正确性，确保 `elem_idx`、`colIdx` 与 `num_padded_cols` 的关系始终保持一致。  
- **ABI 兼容**：C++/Python 接口签名变更（新增 bool 参数）对旧版二进制兼容性产生破坏。库已在 `torch.ops._C.scaled_fp4_quant` 中保持 **默认值 `True`**，但如果外部自行 `torch.ops.load_library` 旧版 `.so`，会出现 “missing argument” 错误，需要在升级文档中明确说明。  

---

### ⚠️ 潜在风险
1. **旧硬件回退失效**  
   - 仅在 `CUDART_VERSION >= 12090` 且 `ENABLE_NVFP4_SM100` 时开启 `ELTS=16`。若构建时宏未定义或驱动/CUDA 版本不匹配，可能导致 **kernel launch failure**（非法 blockDim）。  
2. **Padding

---

### [Feature] add session based streaming input support to v1 (#28973)
**SHA**: `91601ff` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/91601ff47810de38fa117e09e34c18504a919a68)

**🎯 变更类型**：功能增强（Session‑based Streaming Input 支持）  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 为 vLLM v1 立刻引入 **StreamingInput** API，允许用户通过 **async generator** 持续向同一个 *request_id* 发送多段 prompt（即“会话式”流式输入）。  
- 完整实现从 **AsyncLLM.generate**、**Scheduler**、**Request**、**OutputProcessor**、**GPUModelRunner** 到 **engine utils** 的端到端流水线改造：  
  - 新增 `StreamingUpdate`、`StreamingInput`、`InputStreamError`、`STREAM_FINISHED` 等数据结构。  
  - 通过 `Request.streaming_queue`（deque）和 `Request.resumable` 标记支持可恢复（多轮）请求。  
  - 在调度阶段对 `WAITING_FOR_STREAMING_REQ` 状态进行专门处理，支持 **仍在等待下一段输入** 的请求不参与 KV‑cache、prefill、decode。  
  - Scheduler、GPUModelRunner、OutputProcessor 实现了 **流式会话的状态同步、输出合并、输出完成标记**。  
  - 加入大量端到端和单元测试，确保多轮、并发、错误传播、取消、异常等场景正确工作。  

**🎯 影响范围**  
- **vllm/v1/engine**：`async_llm.py`, `input_processor.py`, `output_processor.py`, `utils.py`  
- **vllm/v1/request.py**, **vllm/v1/core/sched/scheduler.py**、**vllm/v1/worker/gpu_model_runner.py**  
- **vllm/outputs.py**（新增 `STREAM_FINISHED`）  
- 测试套件：`tests/v1/...`（scheduler、streaming_input、e2e、gpu_model_runner）  

---

### 🔍 技术洞察  

| 维度 | 影响说明 |
|------|----------|
| **架构** | 1. **新增会话状态** `RequestStatus.WAITING_FOR_STREAMING_REQ`，将“等待下一段输入”从普通 `WAITING` 中分离，避免调度器误把该请求当作已准备好进行 prefilling/decoding。<br>2. **StreamingUpdate** 作为轻量增量数据结构，避免在 Scheduler 中持有完整 `Request` 对象，实现 **请求状态解耦**（调度层仅依赖增量信息）。<br>3. **InputProcessor** 现在支持 `resumable=True` 参数，生成内部 `EngineCoreRequest`，并在 `AsyncLLM.add_request` 中为流式生成专门的 “最终请求” 用于结束信号。<br>4. **OutputProcessor** 持有 `RequestOutputCollector._input_stream_task`，在生成结束或取消时安全关闭底层任务，防止泄漏。<br>5. **GPUModelRunner** 对于流式请求在 `scheduled_new_reqs` 中重新更新缓存状态（去重、拷贝 `prompt_token_ids`），并 **清空 `output_token_ids`**，防止旧输出再次被计入 KV‑cache。<br>6. **Scheduler** 通过 `self.num_waiting_for_streaming_input` 统计未完成的流式会话，`get_num_unfinished_requests` 现在排除 `WAITING_FOR_STREAMING_REQ`，确保 `engine.has_unfinished_requests()` 的返回意义不被流式等待的请求所干扰。 |
| **性能** | - **调度开销**：对待 `WAITING_FOR_STREAMING_REQ` 的请求不参与 KV‑cache 计算、prefilling、decode，避免无效的 GPU 计算，整体吞吐不受流式等待请求的负面影响。<br>- **内存占用**：在每次流式更新时 `prompt_token_ids` 包含之前生成的 token（已转为 prompt），一次完整会话的 **KV‑cache 长度** 与普通一次性请求保持一致，不会出现因内部 `output_token_ids` 未清空导致的 **缓存泄漏 / KV 过度占用**。<br>- **额外复制成本**：`_create_new_request_data` 仍显式使用 `.copy()`（已在代码中保留），因此在流式场景下仍会产生一次 `prompt_token_ids` 的拷贝，额外的 O(N) 复制成本与一次性请求相当。<br>- **并发会话**：`StreamingInput` 采用 **async generator + task**，每个会话在 `AsyncLLM` 中拥有单独的 `RequestOutputCollector` 与后台 task，多个会话可并发执行，调度层对每个内部 `request_id`（内部唯一）仍保持独立。 |
| **安全** | - 新增 `InputStreamError` 用于包装用户侧输入流异常，**不再把用户异常包装为 EngineGenerateError**，从而避免泄露内部实现细节。<br>- `AsyncLLM._add_streaming_input_request` 中对 **SamplingParams** 做严格校验（`n==1`、`output_kind != FINAL_ONLY`、`stop` 为空），防止非法参数导致模型行为不可预测或潜在的资源耗尽。<br>- 对 `prompt_embeds` 在流式输入场景直接抛错，防止意外使用未实现的特性导致未知行为。<br>- 在 `RequestOutputCollector.__del__` 中确保未完成的 `_input_stream_task` 被取消，避免残留协程导致进程难以安全退出。 |
| **可维护性** | - **明确的数据模型**：`StreamingUpdate`、`StreamingInput` 分离了输入、调度、输出三个层面的职责，代码路径更易追踪。<br>- **单元/端到端测试**覆盖了 **普通请求、流式请求、并发、取消、错误传播、session 重用** 等关键路径，降低回归风险。<br>- 对已有 API（非流式）保持 **向后兼容**：`add_request` 仍接受 `EngineCoreRequest | PromptType`，额外的 `AsyncGenerator` 参数为可选，老代码不受影响。<br>- **代码注释和异常信息** 更加明确，帮助后续贡献者快速定位流式会话的状态转移。 |

---

### ⚠️ 潜在风险  

| 风险类型 | 描述 | 可能后果 |
|----------|------|----------|
| **状态竞争** | 流式请求在 `add_request` 中若快速连续提交多段输入，可能出现 `existing.streaming_queue` 与 `Scheduler._update_request_as_session` 同时操作的竞争（尤其在多线程环境使用 `AsyncLLM` 时）。 | 可能导致更新丢失、重复 token、或请求状态错误。 |
| **内存泄漏** | 若用户 **未关闭** `generate` 生成器（例如在异常路径中忘记 `await gen.aclose()`），`RequestOutputCollector._input_stream_task` 仍可能挂起。虽然 `__del__` 中已尝试取消，但在异常终止的情况下可能不被调用。 | 产生僵尸协程，导致事件循环无法退出，进程卡死。 |
| **错误传播不一致** | `InputStreamError` 直接抛出原异常，调用方需捕获多种异常类型（`EngineGenerateError`、`InputStreamError`）。如果上层代码仍只捕获 `EngineGenerateError`，异常会泄漏到用户层。 | 崩溃或未能够得到统一错误处理。 |
| **兼容性** | 新增的 `resumable` 标记默认 `False`，但 `Request.__init__` 在旧代码中未显式传入该参数，仍保持 `False`。若未来在老的 `EngineCoreRequest` 创建路径忘记传 `resumable=True`，流式会话会错误地走普通路径，导致 **“请求重复 ID”** 错误。 | 生成错误或请求被错误终止。 |
| **性能回退** | 在极端高并发（上千并发流式会话）下，每个会话都有独立的 `asyncio.Task` 来消费输入 generator，任务调度与上下文切换可能成为瓶颈。 | 泄漏 CPU 时间、延迟增大。 |
| **多模态与 Prompt Embeds** | 目前显式禁止 `prompt_embeds` 用于流式输入。如果后续想要支持多模态或嵌入流式输入，需要改动大量验证和 GPUModelRunner 逻辑。 | 若不慎

---

#### 🟡 中重要度变更 (4)

### [Model] Use mm_position to compute mrope positions for Qwen2.5-Omni (#32772)
**SHA**: `a698e8e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a698e8e7ad4bd06c0197bd79c9c200bef71be189)

**🎯 变更类型**：功能增强（为 Qwen2.5‑Omni 添加 *use_audio_in_video* 与基于 `mm_position` 的 M‑RoPE 计算）  
**⚡ 重要程度**：🟡 中  

**📋 变更概要**  
1. **模型层面**：在 `qwen2_5_omni_thinker.py` 中加入 `use_audio_in_video` 支持，新增  
   * `_derive_audio_from_video_placeholders`、`_get_audio_for_video_mapping`、`iter_mm_features`、`_compute_interleaved_positions` 等辅助函数。  
   * 通过 `mm_position` 直接遍历 multimodal 特征并在 `get_mrope_input_positions` 中生成交错的视音频位置序列，实现视音频同步嵌入。  
2. **Prompt‑Update 逻辑**：在 `_maybe_apply_prompt_updates` 中检测 `use_audio_in_video`，过滤掉独立的 audio 更新并使用 video‑placeholder 派生的 audio placeholder。  
3. **运行时**：`gpu_model_runner.py` 中对 `is_mm_embed` 的掩码改为 **按位或**，防止在 audio‑in‑video 场景下被覆盖。  
4. **示例**：`examples/offline_inference/qwen2_5_omni/only_thinker.py` 新增 `multi_images` 查询示例。  

**🎯 影响范围**  
- `vllm/model_executor/models/qwen2_5_omni_thinker.py`（核心模型实现、位置计算、占位符处理）  
- `vllm/v1/worker/gpu_model_runner.py`（embed 合并掩码）  
- 示例目录 `examples/offline_inference/qwen2_5_omni/`（演示新特性）  

**💡 关注建议**  
1. **单元/集成测试**：新增对 `use_audio_in_video=True` 的正负样例，尤其校验 audio、video token 数量、位置 ID 连续性以及 `is_mm_embed` 掩码的 OR 行为。  
2. **边界情况**：检查 video 与 audio 数量不匹配、audio 长度为 0、以及多段 video‑audio 交错时的 `mm_position.offset` 排序是否始终保持一致。  
3. **性能**：`iter_mm_features` 采用 `sorted` 与 Numpy 计算，建议在大批量多模态请求时测量额外的 CPU 开销，并在必要时加入缓存。  
4. **向后兼容**：默认 `use_audio_in_video` 为 False，现有模型行为保持不变；但请确保旧版 `mm_prompt_updates` 中若仍包含 `audio` 键时不会产生冲突（已通过过滤实现）。  
5. **文档/示例**：在 README 或模型卡上补充 “audio‑in‑video” 参数说明，并在示例中展示对应 `multi_audios` 与 `multi_images` 的使用方式。  

整体来看，此次改动为 Qwen2.5‑Omni 引入了重要的多模态同步特性，代码结构清晰且对旧行为保持兼容。重点关注测试覆盖及性能基准即可平稳上线。

---

### [Bugfix] fix encoder cache hang in Qwen3VL (#32684)
**SHA**: `7e67df5` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/7e67df5570e2afc6a92833096898689c91244fb6)

**🎯 变更类型**：Bugfix（修复 Qwen3‑VL 编码器缓存卡死）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为 Qwen3‑VL 的视频占位符与尺寸计算加入安全的 **round_up** 与最小阈值（≥ 2），避免出现仅 1 帧导致的缓存键冲突。  
2. 为 `get_image_size_with_most_features` 增加可选 `max_pixels` 参数，使调用者可自行限定像素上限，同时保持向后兼容。  
3. 删除旧的 `MM_OPTIONS_OVERRIDES` 逻辑、统一使用模型自身的 processor 参数；相应测试亦作相应的尺寸、帧数最小化修改。  

**🎯 影响范围**：  
- `vllm/model_executor/models/qwen3_vl.py`（核心模型执行路径）  
- `vllm/model_executor/models/qwen2_vl.py`（共享图像尺寸计算）  
- `vllm/utils/math_utils.py`（新增 `round_up`）  
- 单元测试 `tests/models/multimodal/processing/test_tensor_schema.py`  

**💡 关注建议**：  
- **调用方**：若在自定义脚本中直接使用 `get_image_size_with_most_features`，请检查是否需要显式传入 `max_pixels`（旧代码仍可运行）。  
- **模型配置**：视频占位帧数现在强制为 2，若业务需要更多 dummy 帧，请通过 `VideoDummyOptions.num_frames` 覆盖。  
- **兼容性**：删除 `MM_OPTIONS_OVERRIDES` 可能影响依赖该变量的外部工具，建议在迁移文档中注明改为使用 `processor.dummy_inputs` 的 `mm_options` 参数。  
- **性能**：`round_up` 防止因奇数帧数导致的额外 padding，略微降低内存占用并彻底消除之前的卡死现象。  

总体来看，此次修改针对 Qwen3‑VL 的视频处理路径做了稳健性提升，风险主要在外部直接引用已删除的常量或旧的 dummy 参数，需做好相应迁移。

---

### [CPU] Improve CPU Docker build  (#30953)
**SHA**: `203d0bc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/203d0bc0c29bfb02e682505fd3813c9b3cd3da55)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `cmake/cpu_extension.cmake` 中新增 `VLLM_CPU_AVX2`、`VLLM_CPU_AVX512` 环境变量，使 CI/CD 或交叉编译时可强制开启相应指令集。  
2. `docker/Dockerfile.cpu` 增加对应的 `ARG/ENV`、复制需求文件、简化缓存挂载并加入 OCI labels，提升 Docker 镜像可配置性与可追溯性。  
3. 文档 `cpu.x86.inc.md` 更新，补充交叉编译与 ISA 自动检测说明，提供典型构建示例。  

**🎯 影响范围**  
- **构建系统**：`cmake/cpu_extension.cmake`、`docker/Dockerfile.cpu`。  
- **文档**：`docs/getting_started/installation/cpu.x86.inc.md`。  
- **CI/CD 与用户自定义镜像**：新增构建参数 `VLLM_CPU_AVX2/AVX512`，对已有 CI 脚本可能产生兼容性影响（若未显式传参，行为保持不变）。  

**💡 关注建议**  
- **开发者**：在 CI 中根据目标机器的 ISA 明确传入对应 `VLLM_CPU_*` 参数，避免因默认自动检测导致的非法指令错误。  
- **用户**：构建自定义 CPU 镜像时务必阅读新版文档，使用 `--build-arg VLLM_CPU_AVX2=1` 或 `VLLM_CPU_AVX512=1` 实现跨平台编译；若不需要交叉编译，可保持默认，让 CMake 自动检测本机特性。  
- **测试**：新增的 Docker 参数和标签未影响现有单元测试，但建议在 CI 中添加针对不同 ISA（AVX2、AVX512）组合的镜像构建验证，以防止参数拼写错误或环境变量未生效。  

---

### [MLA] Fuse cat and qaunt for fp8 kv-cache (#32950)
**SHA**: `da5e7b1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/da5e7b12beb6f4496b01265025faf5a123628ae5)

**🎯 变更类型**：性能优化 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增 `CustomOp` **mla_decode_concat_quant_fp8**，在 FP8 KV‑cache 路径上将 `cat → reshape → fp8_quant` 三步融合为单个自定义算子。  
- 在 `MLAAttention` 初始化时创建该算子实例，并在 `forward` 中用它取代手写的 `torch.empty + copy` 实现。  

**🎯 影响范围**  
- `vllm/model_executor/layers/attention/mla_attention.py`（MLA 注意力层）  
- 相关量化工具：`vllm/model_executor/layers/quantization/input_quant_fp8.py`、`custom_op.py`  
- 依赖 FP8 注意力开关 (`fp8_attention=True`) 的所有模型路径。  

**💡 关注建议**  

1. **兼容性检查**  
   - 确认 `CustomOp.register` 能在 CUDA、ROCm、CPU（编译为 `torch.compile`）三种后端正常注册，尤其是 `compile_native=True` 时的 JIT 编译路径。  
   - `GroupShape.PER_TENSOR` 与原来 `PER_CHANNEL`（若有）保持一致；若后续改为张量级别的标度，需要同步更新。  

2. **功能验证**  
   - 对比旧实现（empty+copy）和新算子在相同随机输入下的数值一致性，尤其是 `scale`、`scale_ub` 的传播。  
   - 在 FP8 关闭时仍应走原始路径，确保不会因 `self._decode_concat_quant_fp8_op` 的实例化导致额外开销或意外触发。  

3. **性能基准**  
   - 建议在多卡（DCP）和单卡两种配置下跑完整的 `benchmark_attn.py`，记录显存占用、前向耗时以及 `torch.compile` 的编译时间。  
   - 注意 `torch.cat` 的开销在大 batch / 长序列时仍可能占主导，融合算子应显著下降整体 latency。  

4. **错误处理**  
   - 当前 `forward_native/cuda/hip` 直接返回 `QuantFP8` 的实现，如果量化内部出现异常（例如 scale 为 NaN），算子会抛出，建议在调用前加入断言或日志提示，防止难以定位的崩溃。  

5. **文档与示例**  
   - 在 `README` 或 `doc/source/quantization.md` 中补充 “FP8 KV‑cache 开启 → 使用 fused cat‑quant op” 的说明，帮助用户了解新特性及其依赖条件。  

**总体评价**  
本次改动通过自定义算子将拼接‑量化过程融合，代码更简洁且有望显著提升 FP8 KV‑cache 的前向吞吐。重点在于确保自定义算子在所有平台的注册/编译路径稳定，并提供充分的回归测试与性能基准，以防止潜在的数值偏差或隐藏的编译开销。若上述检查通过，可视为一次成功的性能增强。

---

#### 🟢 低重要度变更 (8)

### [Doc] Add Qwen2.5 models to batch invariance tested models (#33016)
**SHA**: `151e545` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/151e5451c2eb3025ce451ef30556b7087e54c4b1)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `docs/features/batch_invariance.md` 中新增 Qwen2.5 系列模型列表，扩充已验证的批量一致性模型集合。

---

### [BugFix]  Add env variable to control PDL in LoRA (#32836)
**SHA**: `73b2434` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/73b243463b159433ff49c65b54c2e3d66a2e434e)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增环境变量 `VLLM_LORA_DISABLE_PDL`，用于在 LoRA 场景下关闭 PDL，以规避在 SM100 GPU 上 Triton 编译失败的问题；相应在 `supports_pdl` 判定中加入该变量的判断。

---

### [Docs] Fix Apple silicon include path in CPU installation docs (#32977)
**SHA**: `ff6c1da` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ff6c1da4e6ab8d020b41c23166c7b482c047c81a)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 CPU 安装文档中 Apple silicon 的包含路径从 `cpu.arm.inc.md` 更正为 `cpu.apple.inc.md`，确保构建指令正确。

---

### [DOC] [ROCm] Update doc for v0.14.1 (#32998)
**SHA**: `1ebdff4` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/1ebdff412aa0ac5ac4633d13e3e8831e888f8530)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 ROCm 预编译 wheel 的安装示例从 `0.14.0` 更新为 `0.14.1`，保持文档与最新发布版本一致。

---

### Using max_loras + 1 to construct grid in fused_moe_lora (#32277)
**SHA**: `d4dbb7a` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d4dbb7af632e16e611f496bbbafbc0c276e48ac8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fused_moe_lora_op.py` 中将网格轴 2 的大小改为 `max_loras + 1`，并相应在内核调用和尺寸计算中减去或加一，以正确处理 `lora_id == -1` 的无 LoRA 情况。

---

### [CPU Backend][BugFix] Fix failing Darwin pipelines (#33002)
**SHA**: `17ab54d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/17ab54de81521fa3a3f2485c3897750b8ace6dc9)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 macOS CI 中使用 `uv pip install -r requirements/cpu-build.txt` 添加 CPU 构建依赖，并在安装本地包时加 `--no-build-isolation`，修复 Darwin 流水线失败。

---

### [Tests] Replace flaky sleep with polling in test_background_cancel (#32986)
**SHA**: `cd775bd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/cd775bdbe02f5e636c831ef467281c5b11624bc7)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `test_stateful.py` 中将固定的 `asyncio.sleep` 替换为轮询检测状态的逻辑，以消除测试的偶发性 flaky。

---

### Update CPU doc according to feedback (#32963)
**SHA**: `719ac59` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/719ac592edffd5748ca43ebf66d9311b0e132b24)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 CPU 基准文档中的提交哈希与环境变量，统一标题为 “Recommended Models”，提升文档准确性。

---

