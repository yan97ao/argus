# 每日更新报告（2026-02-14）

## vllm-project/vllm

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-14 23:32:09 | Andreas Karatzas | [ROCm][CI] Guard sparse MLA backend imports for ROCm compatibility in tests (#34538) |
| 2026-02-14 19:56:11 | Roger Wang | [Bugfix] Fix Qwen3.5 config loading (#34554) |
| 2026-02-14 15:03:51 | Christian Pinto | [Misc] Update tests and examples for Prithvi/Terratorch models (#34416) |
| 2026-02-14 12:15:19 | Kata Coder | [new model] add COLQwen3 code & Inference (#34398) |
| 2026-02-14 12:04:29 | Andreas Karatzas | [CI] Heavy refactoring of Voxtral multimodal audio model tests (#34294) |
| 2026-02-14 12:04:01 | Julien Denize | Add explicit validation error for tool calls. (#34438) |
| 2026-02-14 12:03:37 | Christian S. Perone | fix: use `__annotations__` instead of `get_type_hints()` for dynamic `kwargs` detection (#34527) |
| 2026-02-14 12:02:59 | Shiyan Deng | [bug] Make sure get_modality_with_max_tokens is deterministic (#34533) |
| 2026-02-14 12:02:24 | Wei Zhao | [Feature][Perf] Support Selective CPU Weight Offloading (#34535) |
| 2026-02-14 12:01:42 | Andreas Karatzas | [Bugfix] Fix ROCm UVA CPU weight offloading broken by #32993 (#34543) |
| 2026-02-14 05:02:28 | Harry Huang | [Hybrid] Enable spec decoding in mamba cache align mode (#33705) |
| 2026-02-14 03:12:48 | Ben Browning | [Bugfix]: Fix structured output in multi-turn gpt-oss (#34454) |
| 2026-02-14 02:35:29 | Michael Goin | Revert "[Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides" (#34530) |
| 2026-02-14 01:52:20 | Richard Zou | [Misc] vLLM's --enforce-eager should turn off compile and cudagraphs only (#34523) |
| 2026-02-14 00:30:23 | Pushpinder Singh | [Bugfix] Replace c10::optional with std::optional in topk kernel (#34467) |
| 2026-02-14 00:11:26 | Wei Zhao | [Feature] Support CPU Offloading without Pytorch Pinned Memory that leads to doubled allocation (#32993) |
| 2026-02-14 00:05:34 | LoganJane | [Bugfix] Add quant_config in ViT of Kimi-K2.5 (#34501) |

### 📊 统计摘要
> 本日共 17 个提交 | 🔴高 1 | 🟡中 7 | 🟢低 9
## 📋 目录

- [vllm-project/vllm](#vllm-project-vllm)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (1)](#-🔴-高重要度变更-1)
    - [[new model] add COLQwen3 code & Inference (#34398)](#d1ea65d)
  - [🟡 中重要度变更 (7)](#-🟡-中重要度变更-7)
    - [[Misc] Update tests and examples for Prithvi/Terratorch m...](#342a7cd)
    - [[CI] Heavy refactoring of Voxtral multimodal audio model ...](#de42abb)
    - [fix: use `__annotations__` instead of `get_type_hints()` ...](#0ef5b91)
    - [[Feature][Perf] Support Selective CPU Weight Offloading (...](#b37b679)
    - [[Bugfix]: Fix structured output in multi-turn gpt-oss (#3...](#fd267bc)
    - [Revert "[Bugfix] Fix fused MoE IMA (sans chunking) by usi...](#bfaa559)
    - [[Feature] Support CPU Offloading without Pytorch Pinned M...](#59d5306)
  - [🟢 低重要度变更 (9)](#-🟢-低重要度变更-9)
    - [[ROCm][CI] Guard sparse MLA backend imports for ROCm comp...](#b3c1422)
    - [[Bugfix] Fix Qwen3.5 config loading (#34554)](#2f18663)
    - [Add explicit validation error for tool calls. (#34438)](#60ca798)
    - [[bug] Make sure get_modality_with_max_tokens is determini...](#ed24265)
    - [[Bugfix] Fix ROCm UVA CPU weight offloading broken by #32...](#a0638d0)
    - [[Hybrid] Enable spec decoding in mamba cache align mode (...](#c027541)
    - [[Misc] vLLM's --enforce-eager should turn off compile and...](#87789c8)
    - [[Bugfix] Replace c10::optional with std::optional in topk...](#bcd65c1)
    - [[Bugfix] Add quant_config in ViT of Kimi-K2.5 (#34501)](#4a9952e)
#### 🔴 高重要度变更 (1)

### [new model] add COLQwen3 code & Inference (#34398)
**SHA**: `d1ea65d` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/d1ea65d0a1c606ae041b73fd45ccd33980ca08e7)

**🎯 变更类型**：功能增强（新增 ColQwen3 多模态 Late Interaction 模型）  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 在 vLLM 中加入对 ColQwen3 及其衍生模型（OpsColQwen3）的完整支持，包括模型注册、配置映射、权重加载、以及多模态 Token‑wise Pooler。  
- 同时补充文档、示例脚本、单元测试，示例展示文本、图片以及跨模态的 `token_embed`、`score`、`rerank` 三种推理路径。  
- 通过新增 `ColQwen3ProcessingInfo` 与自定义 `WeightsMapper` 解决不同厂商（TomoroAI、OpenSearch‑AI）权重命名差异，实现无 `trust_remote_code` 直接加载。

**🎯 影响范围**：  
- `vllm/model_executor/models/colqwen3.py`（新模型实现）  
- `vllm/model_executor/models/registry.py`（模型注册）  
- `vllm/transformers_utils/config.py`、`configs/colqwen3.py`（新增配置类）  
- 文档 `docs/models/pooling_models.md`（使用指南）  
- 示例 `examples/pooling/*/colqwen3_*.py`（客户端调用示例）  
- 测试 `tests/models/multimodal/pooling/test_colqwen3.py`（功能验证）  
- 其他注册/注册表更新（`tests/models/registry.py`、`transformers_utils/configs/__init__.py`）  

**🔍 技术洞察**：

- **架构影响**  
  - **模型层次**：`ColQwen3Model` 继承自 `Qwen3VLForConditionalGeneration`，通过 `ColQwen3ProcessingInfo` 将 HF 的 `Qwen3VLProcessor` 强制映射为统一的多模态处理器，避免因自定义 processor（如 `ColQwen3Processor`）接口不兼容导致的运行时错误。  
  - **权重映射**：实现 `WeightsMapper`，以支持三类权重前缀（`vlm.model.visual.`、`model.visual.`、`visual.`）的统一映射，同时在 OpenSearch‑AI 模型缺前缀时自动加上 `"model."` 前缀，确保加载顺畅。  
  - **投影层**：在 `__init__` 中动态构建 `custom_text_proj`（线性投影），`embed_dim` 可从配置或权重形状自动推断，保持对不同变体的兼容。  
  - **池化路径**：通过 `is_pooling_model = True` 与 `default_pooling_type(seq_pooling_type="CLS", tok_pooling_type="ALL")`，让 vLLM 直接走 Token‑wise Pooler，实现 `token_embed` 任务返回 `[num_tokens, embed_dim]` 的 L2‑归一化向量。  

- **性能影响**  
  - **计算开销**：增加一次线性投影（`hidden_size -> embed_dim`）和 L2‑归一化，时间复杂度为 `O(N * embed_dim)`，对常规推理影响可忽略（相对 Transformer 主干 <5%）。  
  - **内存占用**：`embed_dim` 在 320‑2560 之间（取决于模型），导致每个 token 的向量大小提升 4‑32×，在大批量检索时需注意 GPU 显存和网络带宽。  
  - **吞吐量**：`token_embed` 返回全部 token 向量，若上层业务只需 Top‑K 相似度，应在客户端侧使用 `maxsim` 或在服务端实现 Top‑K 过滤，以防显存泄漏。  

- **安全考虑**  
  - **trust_remote_code**：`ColQwen3ProcessingInfo.get_hf_processor` 强制使用标准 `Qwen3VLProcessor`，避免执行第三方自定义 processor 中潜在的恶意代码，降低远程代码执行风险。  
  - **输入校验**：示例中对 `messages`（图片 + 文本）使用标准 ChatML 结构，保持与 vLLM 已有的多模态输入校验逻辑一致，未引入新的安全漏洞。  

**⚠️ 潜在风险**：

1. **权重映射不完整**  
   - 由于不同厂商的权重命名差异极大，`WeightsMapper` 的规则若未覆盖全部前缀（如未来发布的子模型）会导致权重加载失败或误映射。  
2. **Embedding 维度不匹配**  
   - `embed_dim` 依赖配置字段或首个投影权重的形状；若模型在 HF 上新增其它投影层或改动命名，可能导致 `custom_text_proj` 未正确初始化，引发运行时错误。  
3. **显存冲击**  
   - 大模型（如 8B）配合高 `embed_dim`（2560）以及长序列（>4k）时，返回的 token‑wise 向量可能导致显存超限，导致 OOM。  
4. **多模态输入兼容性**  
   - `ColQwen3ProcessingInfo.get_supported_mm_limits` 只返回 `image`（及可选 `video`），但若模型实际支持其他 modality（audio），会被误判为不支持。  

**💡 关注建议**：

- **CI/测试**：在 CI 中加入对所有已注册的 ColQwen3 变体（TomoroAI、OpsColQwen3、Nemotron‑Embed）进行权重加载、`token_embed`、`score`、`rerank` 的端到端验证，防止未来模型结构微调导致回归。  
- **显存预警**：在服务端实现 `max_batch_size` 与 `max_input_len` 的自动调节逻辑，或在文档中明确推荐的 `--max-model-len` 与 `batch_size` 参数范围。  
- **权重映射可扩展**：将 `hf_to_vllm_mapper` 的规则抽象为配置文件或插件式注册，方便社区为新变体追加前缀映射，而无需修改代码。  
- **安全审计**：定期审计 `ColQwen3ProcessingInfo.get_hf_processor` 的实现，确保在新增自定义 processor 时仍采用安全的标准 processor。  
- **用户指南**：在文档中强调 **多模态** 输入必须使用 `messages` 结构，并给出 `base64` 编码的实现示例，以避免用户因错误的 JSON 结构导致请求失败。  

---  

*此分析覆盖了架构、性能与安全层面的主要影响，并给出风险点与可操作的后续建议，供开发者在合并与后续维护时参考。*

---

#### 🟡 中重要度变更 (7)

### [Misc] Update tests and examples for Prithvi/Terratorch models (#34416)
**SHA**: `342a7cd` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/342a7cda2d212205c4874f82a59559400bbec311)

**🎯 变更类型**：功能增强 / 测试改进  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 示例脚本统一改为新版模型 `ibm-nasa-geospatial/Prithvi-EO-2.0-300M‑TL‑Sen1Floods11`，移除 `trust_remote_code` 并显式启用 `enforce_eager`。  
2. `requirements/test.*` 大幅升级：`terratorch>=1.2.2`、`datasets 3.3.0‑3.6.0`、新增 `imagehash、segmentation‑models‑pytorch` 等，去除旧的 `mlflow‑skinny` 等冲突依赖。  
3. Tests 与插件代码同步：  
   - 使用 `imagehash.phash` 对插件输出的 TIFF 进行哈希校验，提升验证可靠性。  
   - 参数化多模型（原模型、BurnScars），并在在线、离线两种执行路径上统一校验。  
   - 简化 Albumentations 配置，去除冗余 `always_apply` 参数。  

**🎯 影响范围**  
- `examples/pooling/plugin/*`（模型路径 & 参数）  
- `requirements/test.in / test.txt`（依赖版本、冲突解决）  
- `tests/models/multimodal/pooling/test_prithvi_mae.py`、`tests/models/test_terratorch.py`（模型列表）  
- `tests/plugins/prithvi_io_processor_plugin/prithvi_processor.py`（预处理 pipeline）  
- `tests/plugins_tests/test_io_processor_plugins.py`（插件加载、哈希校验、fixture 参数化）  

**💡 关注建议**  
1. **向后兼容**：若已有用户依赖旧的 `christian-pinto/...` 镜像，考虑在文档或代码中保留别名或说明迁移步骤。  
2. **依赖管理**：新增的 `imagehash`、`pywavelets` 等仅在测试中使用，最好通过 `extras_require["test"]` 或 separate `requirements-dev.txt` 来隔离，防止生产环境不必要的膨胀。  
3. **CI 资源**：`terratorch>=1.2.2` 与 `datasets>=3.3.0` 拉取的二进制体积较大，确认 CI 缓存或镜像已更新，以免导致超时。  
4. **文档同步**：示例脚本和 README 中的模型名称、启动参数（如 `--skip-tokenizer-init`）应保持一致，避免用户复制旧参数导致启动失败。  
5. **安全性**：去除 `trust_remote_code=True` 是正确的安全强化，确保所有自定义模型在 CI 中通过官方 `torch.hub` 或 `accelerate` 加载；如仍需远程代码，建议在文档中提供显式开启的指南。  

总体而言，本次改动提升了 Prithvi 插件的可验证性并规整了依赖，唯一需要关注的是兼容性与 CI 资源的提升。

---

### [CI] Heavy refactoring of Voxtral multimodal audio model tests (#34294)
**SHA**: `de42abb` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/de42abb366032519bca073e057331ead6270e09f)

**🎯 变更类型**：功能增强 / 重构（测试层面和模型层面的兼容性提升）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. **依赖升级**：在 `requirements/rocm-test.txt` 中固定 `mistral-common[image,audio]==1.9.1`，为 Voxtral 多模态音频模型提供统一的音频/图像协议实现。  
2. **测试重构**：大幅改写 `tests/models/multimodal/generation/test_voxtral*`，引入离线‑在线双层验证、HF 参考实现对比以及音频‑文本 Prompt 的统一构造。  
3. **HF Runner 补丁**：新增 `voxtral_patch_hf_runner`，解决 `HfRunner` 在多音频、聊天模板以及输出截断方面的限制。  
4. **模型层改动**：  
   - `vllm/model_executor/models/voxtral.py` 增加仅音频处理的 `_apply_hf_processor_mm_only`，实现 audio → tensor 的直接包装。  
   - `whisper_causal.py` 扩展对 ROCm‑Aiter FlashAttention、RocmAttention、TritonBackend 的支持，并加入日志提示。  
5. **Tokenizer 细节**：将对特殊控制 token 的获取从 `get_control_token` 改为 `get_special_token`，统一语义。  
6. **其它**：提升 `gpu_memory_utilization`、在实时测试中使用 `SpecialTokenPolicy.IGNORE`，以及对 `reasoning` 与 `tokenizers` 模块的轻微修正。

**🎯 影响范围**  
- **测试套件**：`tests/models/multimodal/...` 系列全部受影响，新增的工具函数和补丁会影响所有依赖 `HfRunner` 的多模态模型测试。  
- **模型加载路径**：Voxtral、Whisper‑Causal 在 ROCm 环境下的注意力后端选择逻辑改变，可能触发不同的实现路径。  
- **依赖链**：`mistral-common` 的新版本引入，可能与旧版 `mistral-common` 或其他音频处理库产生冲突。  
- **用户代码**：直接使用 `MistralTokenizer` 的 `get_control_token` 已被弃用，需迁移到 `get_special_token`。  

**💡 关注建议**  
1. **CI 环境**：确认 ROCm CI 镜像已经包含 `mistral-common` 以及 `soundfile`、`base64` 等运行时依赖，避免因缺失导致测试卡死。  
2. **向后兼容**：若项目中仍有代码使用旧的 `get_control_token`，建议加入兼容层或在升级日志中明确迁移指南。  
3. **性能验证**：`gpu_memory_utilization` 从 0.4 提升至 0.9，可能导致显存占用激增，建议在资源受限的机器上做显存回归测试。  
4. **日志监控**：Whisper‑Causal 在非 FlashAttention 后端会输出信息级日志，确保生产环境的日志过滤策略不会因频繁 INFO 输出而被淹没。  
5. **HF Runner 补丁**：`voxtral_patch_hf_runner` 改写了 `get_inputs` 与 `generate`，请在自定义 HFRunner 使用前进行单元测试，防止因输入结构变化导致意外错误。  
6. **文档同步**：更新 vLLM 文档中关于 **Multimodal（音频）** 的使用说明，特别是 `limit_mm_per_prompt`、`tokenizer_mode="mistral"` 与 `config_format="mistral"` 的组合要求。  

整体来看，此次提交提升了 Voxtral 与 Whisper 在 ROCm 环境下的多模态兼容性和测试覆盖，但引入了新依赖和部分 API 改动，建议在升级前完成上述检查，以保障现有工作流的平稳过渡。

---

### fix: use `__annotations__` instead of `get_type_hints()` for dynamic `kwargs` detection (#34527)
**SHA**: `0ef5b91` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/0ef5b9147bb1f37c9a90ab2a3ee2a85cf9e84e30)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将动态 `kwargs` 检测中对 `kwargs_cls` 的属性解析从 `typing.get_type_hints()` 改为直接读取 `__annotations__`，并在异常路径加入日志记录。此改动避免了因前向引用（如 `PILImageResampling`）未解析导致的 `NameError`，提升了在不完整依赖环境下的健壮性。  

**🎯 影响范围**：  
- `vllm/transformers_utils/processor.py` 中的 `_collect_dynamic_keys_from_processing_kwargs` 逻辑。  
- 依赖该函数的所有模型处理器（文本、图像、视频、音频）在构造 `processor_kwargs` 时的行为。  

**💡 关注建议**：  
1. **开发者**：确认已有单元测试仍然通过，尤其是包含前向引用的自定义 `Processor` 类；若有新增类型注解，确保其 `__annotations__` 能被正确继承（MRO 合并已实现）。  
2. **用户**：在使用自定义 Processor 或在轻量化部署（缺少 Pillow 等可选依赖）时，升级后不再出现 `NameError`，无需额外安装缺失的库。  
3. **日志**：`logger.exception` 已引入，确保项目日志初始化配置不影响性能；如不需要日志，可在调用方捕获并忽略返回的空集合。  

总体而言，此次改动提升了动态 kwargs 收集的容错能力，影响面局限于处理器模块，建议在升级后跑一遍完整的集成测试以验证功能一致性。

---

### [Feature][Perf] Support Selective CPU Weight Offloading (#34535)
**SHA**: `b37b679` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b37b679770aade27f33d20c93bf467c6a7fba65d)

**变更类型**：功能增强（Selective CPU 参数 Offload）  
**核心改动**  
1. **CacheConfig** 增加 `cpu_offload_params: set[str]`，用于显式指定需 CPU‑offload 的参数片段。  
2. **EngineArgs / CLI** 把该字段暴露为 `--cpu‑offload‑params` 参数，并在 `create_engine_config` 中向底层 `CacheConfig` 传递。  
3. **model_executor.utils**  
   - 新增全局变量 `_CPU_OFFLOAD_PARAMS` 与 `set_cpu_offload_params`。  
   - `maybe_offload_to_cpu` 在遍历 `named_parameters()` 时，依据 `".{param}." in ".{name}."` 的全段匹配决定是否 offload。  
   - 记录并在 `make_layers` 完成后打印已 offload 的总 GB。  
4. **gpu_model_runner** 在实例化时同步设置最大 offload 大小与匹配集合。

**影响范围**  
- 配置/启动入口 (`vllm/config/cache.py`, `engine/arg_utils.py`)  
- 参数 offload 逻辑 (`model_executor/models/utils.py`)  
- GPU 运行器初始化 (`v1/worker/gpu_model_runner.py`)  

**关注建议**  
- **CLI 参数类型**：`--cpu-offload-params` 需要 `action='append'` 或自定义 `type=set`，否则默认会解析为字符串而非集合。  
- **匹配准确性**：当前采用 `".{param}." in ".{name}."` 能确保完整段匹配，但对首/尾段仍有效；仍建议加入单元测试覆盖 `w2_weight` 与 `w2_weight_scale` 等边界。  
- **向后兼容**：空集合保持原有 “随容量 offload” 行为，确保不影响已有部署。  
- **日志与监控**：已在 `make_layers` 打印 offloaded GB，建议在 `maybe_offload_to_cpu` 也记录每个参数的大小，便于排查内存异常。  
- **文档更新**：说明 `cpu_offload_params` 的使用方式及匹配规则，尤其是段必须完整匹配的限制。  

总体来说，改动实现了细粒度的 CPU‑offload 控制，提升了大模型部署的灵活性；关注参数解析与匹配测试即可平滑落地。

---

### [Bugfix]: Fix structured output in multi-turn gpt-oss (#34454)
**SHA**: `fd267bc` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/fd267bc7b7cd3d001ac5a893eacb9e56ff256822)

**🎯 变更类型**：Bug 修复  

**⚡ 重要程度**：🟡 中。该修复针对 GPT‑OSS 多轮对话结构化输出的错误检测，避免在历史消息中误判为“reasoning end”，提升了推理解析的准确性。  

**📋 变更摘要**  
1. 在 `gptoss_reasoning_parser.py` 中加入对 `<|end|>`（eom） token 的检测，并在 `is_reasoning_end` 循环中提前返回，防止向后遍历到上一轮对话的结束标记导致误判。  
2. 为单元测试的 mock tokenizer 增添 `vocab` 条目 `"<|end|>": 6`，使其能够提供 `eom_token_id`。  
3. 在测试用例中补充了多轮会话的示例 `MULTI_TURN_CONTENT`，并相应调整常量组合方式，以验证新逻辑。  

**🎯 影响范围**  
- 核心解析模块 `vllm/reasoning/gptoss_reasoning_parser.py`（新增属性 `eom_token_id` 与判断逻辑）。  
- 单元测试目录 `tests/entrypoints/openai/`、`tests/reasoning/`、`tests/v1/structured_output/`（mock tokenizer、额外测试数据）。  

**💡 关注建议**  
- **开发者**：确认使用的 tokenizer 实例在真实部署中确实包含 `<|end|>` token；若自定义 tokenizer 可能需要显式加入该词条。  
- **性能**：提前返回的分支仅在检测到 `<|end|>` 时触发，整体时间复杂度保持 O(N)；若对超长对话有严格延迟要求，可在 `reasoning_max_num_between_tokens` 之外继续限制搜索窗口。  
- **回归测试**：运行完整的结构化输出与推理解析测试，确保旧的单轮案例仍然通过，同时多轮会话不再出现误判。  
- **文档**：更新 `gptoss_reasoning_parser` 的实现说明，标注 `<|end|>` 在多轮对话中的作用，帮助使用者理解 tokenizer 必备的特殊 token。  

整体来看，此次提交修正了多轮对话场景下的逻辑错误，影响范围局限在解析器及对应测试，风险低，建议合并后在实际服务中做一次全链路放量验证。

---

### Revert "[Bugfix] Fix fused MoE IMA (sans chunking) by using int64 for strides" (#34530)
**SHA**: `bfaa559` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bfaa5593050ec9bf60e2361b3b9dc575efeee83f)

**🎯 变更类型**：回滚（Revert）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交将之前为 fused‑MoE kernel 添加的 `tl.int64` 类型约束全部撤回，恢复为隐式推断的参数形式。对应的两个 kernel（`fused_moe_kernel_gptq_awq` 与 `fused_moe_kernel`）中所有 stride 参数的显式 `tl.int64` 声明被删除，保持与 Triton 默认的 integer 类型兼容。  

**🎯 影响范围**：  
- `vllm/model_executor/layers/fused_moe/fused_moe.py` 中的两段 Triton kernel 定义。  
- 受影响的模块主要是 MoE（Mixture‑of‑Experts）相关的前向计算路径，尤其是使用 fused‑MoE 加速的 GPTQ/ AWQ 量化及普通推理路径。  

**💡 关注建议**：  
1. **类型安全**：撤回 `tl.int64` 可能导致在某些平台上 stride 采用 32‑bit 整型，进而触发整数溢出或对齐错误。建议在单元测试中加入大模型/大 batch‑size 场景，确保 stride 仍能正确表示。  
2. **性能回归**：`tl.int64` 在 Triton 中会强制使用 64‑bit 寄存器，可能略有性能损耗。回滚后应对比基准测试（尤其是显存/计算密集的 MoE 场景），确认没有出现明显的吞吐下降。  
3. **文档与注释**：原来的 commit 注释提到 “Fix fused MoE IMA (sans chunking) by using int64 for strides”。回滚后请在代码或 changelog 中说明原因，防止后续维护者误以为已使用 64‑bit。  
4. **兼容性验证**：确保其他依赖此 kernel 的调用方（如 `model_executor` 中的调度逻辑）在传入 stride 时仍使用整数而非 Tensor，避免因隐式类型推断导致的运行时异常。  

总体而言，此次回滚恢复了之前的参数签名，降低了对 Triton 版本的耦合度，但需通过针对性测试验证在极端张量尺寸下的正确性与性能表现。

---

### [Feature] Support CPU Offloading without Pytorch Pinned Memory that leads to doubled allocation (#32993)
**SHA**: `59d5306` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/59d53066d8daed5c2e39e3bce38ac308bc80a9ae)

**🎯 变更类型**：功能增强 & Bug修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 CUDA/XPU 后端实现了 **“CPU Offload + UVA + 可选 pinned‑memory”**，当原始 CPU 张量未使用 PyTorch pinned memory 时，会自动在 CUDA 端分配映射的 pinned 缓冲区并拷贝数据，从而避免因缺少 pinned memory 而导致的双倍显存占用。  
2. 新增环境变量 `VLLM_WEIGHT_OFFLOADING_DISABLE_PIN_MEMORY`、`VLLM_WEIGHT_OFFLOADING_DISABLE_UVA`，可分别关闭 pinned‑memory 与 UVA 支持，以便在不满足条件的机器上回退到 eager 模式。  
3. 相应的单元测试覆盖了四种配置组合，确保功能兼容性。  

**🎯 影响范围**：  
- **csrc/cuda_view.cu**（底层 CUDA 视图实现）  
- **vllm/utils/torch_utils.py**（统一入口）  
- **vllm/model_executor/model_loader/utils.py**（加载时恢复 CPU 参数）  
- **vllm/model_executor/models/utils.py**（CPU offload 逻辑）  
- **vllm/envs.py**（新增 env 配置）  
- **tests/basic_correctness/test_cpu_offload.py**（新增参数化测试）  

**💡 关注建议**：  
- **开发者**：确认平台支持 `cudaHostAllocMapped`（UVA）后再开启该特性；若使用自定义 allocator，请确保在异常路径（`cudaHostAlloc`、`cudaMemcpy` 失败）中及时释放 `cudaFreeHost`，防止内存泄漏。  
- **用户**：在显存紧张或机器未启用 pinned memory 时，可通过设置 `VLLM_WEIGHT_OFFLOADING_DISABLE_PIN_MEMORY=1` 或 `VLLM_WEIGHT_OFFLOADING_DISABLE_UVA=1` 让 vLLM 自动回退到 eager（不使用 UVA），保持原有行为。  
- **回归测试**：关注 `test_cpu_offload` 在不同 env 组合下的运行时显存占用，尤其在禁用 UVA 时确保 `--enforce-eager` 被正确添加。  

总体而言，此次改动提升了 CPU offload 在普通服务器上的可用性，同时保持向后兼容，风险主要在平台不支持 UVA 时的异常处理，需要在生产环境做好 env 配置验证。

---

#### 🟢 低重要度变更 (9)

### [ROCm][CI] Guard sparse MLA backend imports for ROCm compatibility in tests (#34538)
**SHA**: `b3c1422` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/b3c14229b032a8bbf93d450a52c9a404ddaea429)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在稀疏 MLA 后端测试中加入对平台的判定，若非 CUDA（如 ROCm）则直接 skip，以避免不兼容的导入错误。

---

### [Bugfix] Fix Qwen3.5 config loading (#34554)
**SHA**: `2f18663` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/2f186635cbcb38fd85e718a5b7ff9ec698cbb4f8)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Qwen3.5 与 Qwen3.5‑MoE 配置类中，将 `pad_token_id`、`bos_token_id`、`eos_token_id`、`tie_word_embeddings` 的赋值移到 `super().__init__()` 之后，防止 Transformers v4 的默认参数覆盖自定义值。

---

### Add explicit validation error for tool calls. (#34438)
**SHA**: `60ca798` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/60ca7981bce1bd6e2155df1a58bc9f916f7c4093)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `mistral.py` 中为工具调用的验证添加显式错误处理，使用 `pydantic.ValidationError` 捕获并抛出更易理解的 `ValueError`，同时修正空 `tool_calls` 的处理逻辑。

---

### [bug] Make sure get_modality_with_max_tokens is deterministic (#34533)
**SHA**: `ed24265` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/ed242652d7f9cb4222e8840311b5229295b5d266)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `get_modality_with_max_tokens` 中的 `max` 比较改为 `(tokens, modality)`，确保在 token 数相同的情况下返回结果 deterministic。

---

### [Bugfix] Fix ROCm UVA CPU weight offloading broken by #32993 (#34543)
**SHA**: `a0638d0` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/a0638d052db74ba28eada4768b9bbf98720b44a4)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `get_accelerator_view_from_cpu_tensor` 中将 CUDA 判定扩展为 `is_rocm()`，修复 ROCm 环境下 CPU 权重 offloading（UVA）失效的问题。

---

### [Hybrid] Enable spec decoding in mamba cache align mode (#33705)
**SHA**: `c027541` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/c027541eaf05c6ca1e9a544804afe28caef671fc)

**🎯 变更类型**：代码重构/其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：解除 Mamba cache “align” 模式对 speculative decoding 的限制，更新测试以新进程运行并在结束后清理显存及分布式环境。

---

### [Misc] vLLM's --enforce-eager should turn off compile and cudagraphs only (#34523)
**SHA**: `87789c8` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/87789c836422cf3b666ddf3eca9ede8e03f735ee)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `--enforce-eager` 的处理改为直接禁用 `torch.compile` 与 CUDAGraph，等同于 `-cc.mode=none -cc.cudagraph_mode=none`，并给出警告提示。

---

### [Bugfix] Replace c10::optional with std::optional in topk kernel (#34467)
**SHA**: `bcd65c1` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/bcd65c1f6a25ab76be325fbc0766eb074519a4fc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `c10::optional` 替换为标准库 `std::optional`，用于 top‑k kernel 接口 `large_context_topk`，并相应更新默认值为 `std::nullopt`。此改动提升了代码的可移植性和与标准 C++ 的一致性。

---

### [Bugfix] Add quant_config in ViT of Kimi-K2.5 (#34501)
**SHA**: `4a9952e` | 🔗 [查看提交](https://github.com/vllm-project/vllm/commit/4a9952ec1b15453053f4ec443d2d81505d344075)

**🎯 变更类型**：代码重构/Bugfix  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 Kimi‑K2.5 的 ViT 实现中新增 `quant_config` 参数并向 Vision Tower、投影层及相关子模块传递，加入 `_maybe_ignore_quant_config` 以在使用 CompressedTensors 时忽略量化配置，提升量化兼容性。

---

