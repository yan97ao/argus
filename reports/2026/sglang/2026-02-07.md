# 每日更新报告（2026-02-07）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-07 22:08:42 | wxy | [diffusion] feat: support saving videos directly on the server to avoid the overhead of tensor transfer (#18253) |
| 2026-02-07 20:56:04 | Mick | [diffusion] fix: respect dist_timeout option (#18386) |
| 2026-02-07 20:42:47 | Mohammad Miadh Angkad | [Doc] Fix outdated `--fp4-gemm-backend` documentation (#18350) |
| 2026-02-07 20:35:12 | Hao Jin | [diffusion] fix: remove unnecessary norm_type argument from GLM-Image dits (#18382) |
| 2026-02-07 19:06:53 | Baizhou Zhang | [CI] Skip some flaky subtests for test_multi_lora_backend.py (#18408) |
| 2026-02-07 18:04:37 | Mohammad Miadh Angkad | [Doc] Update CUDA 13 install guide to install torch first (#18404) |
| 2026-02-07 17:30:01 | Shangming Cai | chore: bump mooncake version to 0.3.9 (#18316) |
| 2026-02-07 17:28:42 | Xiaoyu Zhang | [Diffusion] Apply fused_norm_scale_shift to LTX2/MOVA (#18257) |
| 2026-02-07 16:38:05 | Hexq0210 | [NPU] update npu doc (#18344) |
| 2026-02-07 12:33:29 | 赵晨阳 | Support execute_shell_command for env var support (#18390) |
| 2026-02-07 11:59:42 | hlu1 | [Qwen3Next] Optimize fused_sigmoid_gating_delta_rule_update_kernel (#18271) |
| 2026-02-07 09:19:07 | Baizhou Zhang | Revert "[Build] Enable full kernel in aarch64 wheel" (#18385) |
| 2026-02-07 07:32:33 | Alison Shao | Merge stage-c-test-large-4-gpu suites into partitioned suites (#18325) |
| 2026-02-07 07:05:36 | Neal Vaidya | add hybrid model PD to NIXL connector (#16229) |
| 2026-02-07 05:17:51 | Rishit Shivam | [Docs] Add Falcon H1, Hunyuan-Large, Qwen3-Omni support and update Diffusion usage (#17888) |
| 2026-02-07 01:19:02 | Baizhou Zhang | Add CI permission for Shunkangz, dongjiyingdjy, samuellees (#18377) |
| 2026-02-07 00:28:44 | Prozac614 | [diffusion] CI: update perf baseline (#17512) |

### 📊 统计摘要
> 本日共 17 个提交 | 🔴高 0 | 🟡中 9 | 🟢低 8
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🟡 中重要度变更 (9)](#-🟡-中重要度变更-9)
    - [[diffusion] feat: support saving videos directly on the s...](#64950d8)
    - [[diffusion] fix: respect dist_timeout option (#18386)](#31d4cd2)
    - [[Doc] Fix outdated `--fp4-gemm-backend` documentation (#1...](#fddef76)
    - [[Diffusion] Apply fused_norm_scale_shift to LTX2/MOVA (#1...](#baec650)
    - [[NPU] update npu doc (#18344)](#e834b85)
    - [Merge stage-c-test-large-4-gpu suites into partitioned su...](#bedade1)
    - [add hybrid model PD to NIXL connector (#16229)](#f1ff697)
    - [[Docs] Add Falcon H1, Hunyuan-Large, Qwen3-Omni support a...](#c850a8a)
    - [[diffusion] CI: update perf baseline (#17512)](#e13b727)
  - [🟢 低重要度变更 (8)](#-🟢-低重要度变更-8)
    - [[diffusion] fix: remove unnecessary norm_type argument fr...](#d792aa7)
    - [[CI] Skip some flaky subtests for test_multi_lora_backend...](#eb4cf1d)
    - [[Doc] Update CUDA 13 install guide to install torch first...](#c47c2f9)
    - [chore: bump mooncake version to 0.3.9 (#18316)](#52401be)
    - [Support execute_shell_command for env var support (#18390)](#1552aab)
    - [[Qwen3Next] Optimize fused_sigmoid_gating_delta_rule_upda...](#4637970)
    - [Revert "[Build] Enable full kernel in aarch64 wheel" (#18...](#9fbec79)
    - [Add CI permission for Shunkangz, dongjiyingdjy, samuellee...](#f2e0048)
#### 🟡 中重要度变更 (9)

### [diffusion] feat: support saving videos directly on the server to avoid the overhead of tensor transfer (#18253)
**SHA**: `64950d8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/64950d8f9735e0e0f652092b695d21e3ca34f310)

**🎯 变更类型**：功能增强（在服务器端直接保存视频/音频文件，避免张量回传）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为 `SamplingParams` 新增 `return_file_paths_only`（默认 `True`），并在 CLI 中提供 `--return-file-paths-only` 开关。  
2. 在 `utils.py` 中抽象出 `save_outputs`（统一保存并可选返回 `samples/audio/frames`），并新增 `attach_audio_to_video_sample` 处理视频‑音频配对。  
3. 调整 `diffusion_generator`, `http_server`, `openai/utils` 等入口，使在 `req.save_output && req.return_file_paths_only` 时仅返回文件路径。  
4. GPU worker 在生成完毕后若请求开启该模式，会直接写文件并把 `output` 置空、填充 `output_file_paths`。  
5. `OutputBatch` 新增 `output_file_paths` 字段；`Req` 中 `fps` 字段恢复至日志输出。  

**🎯 影响范围**：  
- `sglang/multimodal_gen/configs/sample/sampling_params.py`  
- `sglang/multimodal_gen/runtime/entrypoints/*`（diffusion、http、openai）  
- `sglang/multimodal_gen/runtime/utils.py`（核心保存逻辑）  
- `sglang/multimodal_gen/runtime/managers/gpu_worker.py`、`schedule_batch.py`（调度/日志）  

**💡 关注建议**  

| 方向 | 建议 |
|------|------|
| **兼容性** | `return_file_paths_only` 默认 `True` 会导致现有调用在不修改代码的情况下得到 `samples=None`。建议在发布说明中明确该默认行为，或提供 `--return-file-paths-only false` 以保留旧行为。 |
| **依赖** | `save_outputs` 中使用了 `torch`，但本次 diff 未在 `utils.py` 顶部显式 `import torch`，可能触发 `NameError`。请确认 `torch` 已被导入或改为延迟导入。 |
| **序列化** | `OutputBatch.output_file_paths` 需在所有 RPC（scheduler ↔ client）路径上序列化。检查 `OutputBatch.model_dump()`、`AsyncSchedulerClient` 和 HTTP 返回体是否已包含该字段。 |
| **错误处理** | 当 `save_output` 为 `True` 且 `return_file_paths_only` 为 `True`，但磁盘写入失败时，`output` 被置空，调用方只能收到空 `samples`。建议在 `save_outputs` 抛出异常或在 `gpu_worker` 捕获后填充 `error`。 |
| **文档/示例** | 更新 CLI 帮助、OpenAI/HTTP 接口文档，增加 “仅返回文件路径” 示例，说明 `samples/audio/frames` 为 `None` 的情况。 |
| **性能验证** | 在多卡、长序列生成场景下对比原始 “回传张量” 与新 “直接写文件” 的延时与显存占用，确保提升符合预期并未引入额外 I/O 争用。 |
| **单元/集成测试** | 添加对 `return_file_paths_only=True/False` 两种路径的测试，尤其是视频‑音频配对逻辑（`attach_audio_to_video_sample`）以及 `save_outputs` 的多输出、`output_file_paths` 填充情况。 |

总体来说，此次改动通过在服务端提前落盘实现了显著的带宽节省，代码结构也更清晰。但需要注意默认行为的兼容、`torch` 导入以及 RPC 序列化的完整性，确保在所有调用链上都能平滑使用。

---

### [diffusion] fix: respect dist_timeout option (#18386)
**SHA**: `31d4cd2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/31d4cd2ffd9662568765efbc6a4a3939c613ad9a)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在分布式初始化链路中加入 `dist_timeout` 参数，使用户能够显式控制 `torch.distributed.init_process_group` 的超时时间。`ServerArgs` 默认改为 3600 秒（1 h），并将该值向下传递至 `maybe_init_distributed_environment_and_model_parallel` → `init_distributed_environment` → `torch.distributed.init_process_group`。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/runtime/distributed/parallel_state.py`（分布式环境初始化）  
- `python/sglang/multimodal_gen/runtime/managers/gpu_worker.py`（GPU worker 启动）  
- `python/sglang/multimodal_gen/runtime/server_args.py`（CLI 参数与默认值）  

**💡 关注建议**  

1. **兼容性**：  
   - 之前默认 `timeout=None`，Torch 会使用内部默认（≈30 min）。现在默认改为 3600 s，可能导致长时间卡住的场景恢复正常，但也会改变已有部署的超时行为。若需要保持原行为，请在启动脚本中显式 `--dist-timeout 0`（或 None）并在代码中容忍 `0` 转为 `None`。  
   - `init_distributed_environment` 新增 `timeout` 参数，外部调用若未更新可能出现 **TypeError**。当前已在所有内部入口同步传参，建议在项目文档中标明该签名变更。  

2. **实现细节**：  
   - 使用 `datetime.timedelta(seconds=timeout)` 传递给 `torch.distributed.init_process_group`，符合 PyTorch 2.x 的 API。  
   - `extra_args` 直接注入 `timeout`，若后续再添加其他关键字参数，需要确保不因覆盖而产生冲突。  
   - 新增的 `import datetime` 位于文件顶部，未对已有导入顺序产生影响。  

3. **测试建议**：  
   - 在单机多卡、跨机器两种场景下验证：① `dist_timeout` 为 `None`（使用默认）② 设置较小值（如 10 s）导致超时，确认异常被合理抛出。  
   - 增加单元测试覆盖 `maybe_init_distributed_environment_and_model_parallel` 参数传递路径，确保 `dist_timeout` 能正确记录在日志并被 `init_process_group` 使用。  

4. **文档/CLI**：  
   - 更新 README/CLI 说明，解释默认 1 h 的含义以及在长时间 idle（如维持服务）时如何通过 `--dist-timeout` 调整。  
   - 若项目有配置文件（JSON/YAML），同步添加对应字段。  

总体而言，此次改动解决了分布式初始化在长时间空闲后因默认超时过短而导致的连接异常，对分布式部署的可靠性提升显著。只需关注默认值的行为变化以及在自定义启动脚本中显式传递该参数即可。

---

### [Doc] Fix outdated `--fp4-gemm-backend` documentation (#18350)
**SHA**: `fddef76` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fddef766199e85c8200e6b150419e2643ac46a2c)

**🎯 变更类型**：文档/功能修正  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 FP4‑GEMM 的默认后端从 `auto`（自动在 flashinfer_cudnn / flashinfer_cutlass 之间切换）改为 `flashinfer_cutlass`，并同步到 CLI 帮助、`ServerArgs` 默认值以及文档说明；在 `modelopt_quant.py` 中简化了后端映射逻辑，去掉了 “auto 时强制使用 cutlass” 的特殊分支。  

**🎯 影响范围**  
- `python/sglang/srt/server_args.py`（默认参数、CLI 帮助）  
- `python/sglang/srt/layers/quantization/modelopt_quant.py`（FP4 GEMM 调用路径）  
- `docs/advanced_features/server_arguments.md`（用户文档）  

**💡 关注建议**  
1. **兼容性检查**：`flashinfer_cutlass` 在 CUDA 12 上表现最佳，旧版 CUDA 11/12‑beta 可能没有对应实现。请在 CI 中加入不同 CUDA/cuDNN 组合的跑通测试，确认在不支持 FlashInfer 时仍能回退到 sgl‑kernel CUTLASS。  
2. **`get_fp4_gemm_runner_backend()` 实现**：现在 `is_auto()` 不再影响返回值，确保该函数在 `auto` 情况下仍返回合适的 FlashInfer backend（或在不可用时返回 `cutlass`），否则会出现 “backend == None” 报错。  
3. **文档同步**：CLI 帮助已更新，但 `--fp4-gemm-backend` 的默认值仍在代码中写死为 `flashinfer_cutlass`。若后续想恢复 `auto`，请统一修改 `ServerArgs.fp4_gemm_runner_backend` 与文档。  
4. **回退路径**：在 `flashinfer_fp4_gemm` 调用前若 FlashInfer 未安装，代码会使用 sgl‑kernel CUTLASS。建议在日志中明确提示用户当前使用的后端，以免产生 “性能不如预期” 的误解。  

总体来看，此次改动提升了默认行为的确定性，但需确保在所有受支持的硬件/软件环境下仍能安全回退，避免因默认后端不可用导致启动失败。

---

### [Diffusion] Apply fused_norm_scale_shift to LTX2/MOVA (#18257)
**SHA**: `baec650` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/baec650462e351bc338098efdc21726e66328651)

**🎯 变更类型**：功能增强（融合层归一化与缩放/平移以提升推理效率）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 MOVA‑Audio 与 MOVA‑Video 模型新增 `norm3 → self_attn_norm` 的权重映射，兼容新的归一化实现。  
2. 引入 `LayerNormScaleShift`、`ScaleResidualLayerNormScaleShift` 与 `MulAdd`，实现 **Self‑Attn → Cross‑Attn → MLP** 三阶段的 **LayerNorm‑Scale/Shift‑Residual** 融合。  
3. 将原始 `nn.LayerNorm` 与手写的残差加法替换为统一的融合算子，并在 Patch‑Head 中同样使用融合层。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/configs/models/dits/*`（权重映射）  
- `python/sglang/multimodal_gen/runtime/models/dits/mova_video_dit.py`（核心 Block 与 Head）  
- `sglang/multimodal_gen/runtime/layers/layernorm.py`（新增融合层实现）  

**💡 关注建议**  
1. **兼容性**：确认新权重映射在已有 checkpoint 上能够成功加载，尤其是 `norm3` → `self_attn_norm` 的改名。  
2. **数值稳定性**：`LayerNormScaleShift` 默认 `dtype=torch.float32`，在混合精度下请确保在调用前后 `to(orig_dtype)`，防止精度漂移。  
3. **梯度检查**：融合算子涉及自定义前向/反向路径，建议在 `torch.autograd.gradcheck` 上跑一次，确保梯度无误。  
4. **编译兼容**：代码已写入 `torch.compile` 友好的模式，但仍需在不同硬件（CUDA、CPU）上跑完整训练/推理基准，验证速度提升是否如预期且没有回归。  
5. **单元测试**：补充对应的层归一化、残差融合单元测试，尤其是 `ScaleResidualLayerNormScaleShift` 的四参数调用（residual, output, coeff, shift, scale）。  

总体来看，此次改动在保持模型结构不变的前提下，引入了更高效的融合实现，预计能显著提升 MOVA 系列的推理吞吐。务必完成兼容性、数值与性能回归验证后再合并。

---

### [NPU] update npu doc (#18344)
**SHA**: `e834b85` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e834b85ab677971ffcdf60edb373f4472fe37224)

**🔧 变更类型**：文档更新（功能特性说明 & 模型支持表）  
**⚡ 重要程度**：🟡 中（不涉及代码逻辑，但直接影响用户使用体验）  

**📋 变更摘要**  
1. 新增 `docs/platforms/ascend_npu_support_features.md`，系统化列出 Ascend NPU 在 SGLang 中各类启动参数、默认值、可选值及对应的 A2/A3 服务器兼容性。  
2. 更新 `docs/platforms/ascend_npu_support_models.md`，为 NPU 增加了数个新模型的兼容标记（如 OLMo‑2‑7B、MiniMax‑M2、Solar‑10.7B、StarCoder2、Trinity‑Mini 等），并在 Reward/Multimodal 表中做了排版微调。  

**🎯 影响范围**  
- **用户文档**：所有阅读 Ascend NPU 支持说明的用户（特别是部署 A2/A3 机器的开发者）会直接受益。  
- **CI/文档构建**：新增的 Markdown 文件会被 `mkdocs` 或类似工具重新渲染，可能触发文档构建时间及链接检查。  
- **代码层面**：本次提交未改动实现，仅依赖已有参数实现。如文档列出的参数在代码中缺失或命名不一致，将导致使用者困惑。  

**💡 关注建议**  

| 建议 | 说明 |
|------|------|
| **核对参数实现** | 确认 `sglang/server/args.py`（或等价入口）中已完整实现文档列出的所有 `--*` 参数，尤其是 NPU‑专属的 `--attention-backend=ascend`、`--dtype`、`--enable-hierarchical-cache` 等。 |
| **保持同步** | 后续若在代码中添加/下沉新特性，及时同步到该文档；建议在 PR 模板中加入 “文档同步检查” 步骤，防止文档与实现脱节。 |
| **链接有效性** | 文档中引用的外部链接（如 Server Arguments、issue 页面）应在 CI 中加入链接检查，防止死链影响新手体验。 |
| **多语言/多模态模型** | 新增的模型在实际 NPU 上的兼容性需在内部测试报告中说明（如是否需要 `--enable-multimodal`、`--dtype=bfloat16` 等），可在表格的 “注释” 列补充简要说明。 |
| **文档渲染** | 由于表格较大，建议在 CI 中加入 Markdown lint 检查，避免因表格格式错误导致构建失败。 |
| **使用指引** | 在 “快速上手” 或 “常见问题” 页面加入示例命令，帮助用户快速定位 NPU 需要的关键参数（如 `--device=ascend`、`--tp-size`、`--kv-cache-dtype=auto`）。 |

> **总结**：本次改动提升了 Ascend NPU 的可用性说明，主要风险在于文档与实际实现不一致。请在后续功能迭代时同步更新文档，并在 CI 中加入文档完整性检查，以确保用户获取的参数信息始终准确。

---

### Merge stage-c-test-large-4-gpu suites into partitioned suites (#18325)
**SHA**: `bedade1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bedade1ef050eaae4cd2232e229b1d267efb66fd)

**🎯 变更类型**：重构 / CI 调整  
**⚡ 重要程度**：🟡 中  

**📋 变更概述**  
- 在 `.github/workflows/pr-test.yml` 中删除了 `stage-c-test-large-4-gpu` 与 `stage-c-test-large-4-gpu-b200` 两个 4‑GPU 大模型测试任务，相关的依赖下载、安装和运行步骤全部移除。  
- 同步在 `scripts/ci/utils/slash_command_handler.py`、`test/run_suite.py`、各测试注册文件以及 `test/README.md` 中把原套件名替换为新的 `stage-c-test-4-gpu-h100`、`stage-c-test-4-gpu-b200`、`stage-c-test-4-gpu-gb200` 等。  
- 只保留了针对 H100/B200（4‑GPU）以及 8‑GPU 的新套件，避免了重复或不必要的 CI 资源占用。

**🎯 影响范围**  
- CI 工作流（GitHub Actions）  
- 测试注册脚本 `test/registered/*`  
- 文档 `test/README.md`  
- 通过 `/rerun` 指令触发的 CI 逻辑

**💡 关注建议**  
1. **确认无残余引用**：在项目其它位置（如自定义 CI 脚本、第三方 CI）搜索 `stage-c-test-large-4-gpu`，防止遗漏导致构建失败。  
2. **验证新套件映射**：确保 `stage-c-test-4-gpu-h100`、`-b200` 等套件的硬件标签、超时、依赖安装与原套件保持一致，防止因资源不匹配导致不稳定。  
3. **CI 资源评估**：删除的两项任务原本占用 4‑GPU H100，若有针对该配置的专项回归需求，请在新套件中补充相应测试。  
4. **文档同步**：README 已更新，但 CI 变量（如 `RUNNER_LABELS`）的说明也应同步，以免新贡献者产生误解。  

总体而言，此次改动精简了 CI 流水线，降低了冗余资源消耗，只要完成以上检查即可安全合并。

---

### add hybrid model PD to NIXL connector (#16229)
**SHA**: `f1ff697` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f1ff697494c59bd0a4d298d5a1411f9eed563982)

**🎯 变更类型**：功能增强（为 NIXL 连接器加入 Hybrid（Mamba）模型的状态（state）传输）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为 `TransferInfo`、`KVArgsRegisterInfo`、`TransferStatus` 增加 `state` 相关字段（state indices、state data ptrs、接收状态等）。  
2. 在 `register_buffer_to_engine` 中完成对 state 缓冲区的 VRAM 注册。  
3. 实现 `_send_mamba_state`，在 Prefill‑>Decode 过程中通过 RDMA 将 Mamba 状态从前置 rank 复制到目标 rank。  
4. 发送/接收流程在 `add_transfer_request`、`update_transfer_status`、`send`、`init` 中注入 state 信息，并在 `TransferStatus` 中加入 `expects_state` 与 `received_state_per_pp` 的判定。

**🎯 影响范围**  
- `python/sglang/srt/disaggregation/nixl/conn.py`（核心网络层）  
- 相关数据结构：`TransferInfo`、`KVArgsRegisterInfo`、`TransferStatus`  
- 运行时路径：Prefill → Decode 的 KV/aux/state 传输、内存注册、状态完成判定  

**💡 关注建议**  

| 关注点 | 建议 |
|--------|------|
| **向后兼容** | 解析 `msg` 时硬编码了字段下标（如 `msg[8]`、`msg[9]` 等），若旧版 NIXL 仍发送 8‑字段消息会导致 `IndexError`。建议在 `from_zmq` 中使用显式的协议版本或在缺少字段时回退到默认值。 |
| **字段同步** | `KVArgsRegisterInfo` 新增 `dst_state_data_ptrs`，但在 `_register_kv_args` 中只打包 `state_data_ptrs`，未同步 `state_data_lens`（对应 `state_item_lens`）到远端。若远端依赖长度信息，可能导致越界访问。请同时发送对应长度数组或在文档中说明长度固定。 |
| **状态长度信息** | `TransferInfo` 只携带 `dst_state_indices`，而 `_send_mamba_state` 通过 `self.kv_args.state_item_lens[i]` 取得每条状态的长度。若 `state_item_lens` 未初始化（例如在非 Mamba 场景），会抛 `AttributeError`。建议在 `init` 时显式校验 `state_item_lens` 是否存在或提供默认值。 |
| **单一 State Index 断言** | `assert len(prefill_state_indices) == 1` 假设 Mamba 只能有一个 state index。若后续模型演进支持多段状态，此断言会阻塞。可以改为 per‑layer 循环，而不是全局断言。 |
| **TP 大小不匹配** | 当 `attn_tp_size != decode_tp_size` 时直接 `RuntimeError`。此限制在文档中应明确标记，或在后续实现中加入跨‑TP 的状态拆分/合并逻辑。 |
| **状态完成判定** | `TransferStatus.is_done` 现在会检查 `received_state_per_pp` 与 `expects_state`，但 `expects_state` 仅在 `init` 时根据 `state_indices` 是否为 `None` 设置，若后续出现 “state 只在部分 Chunk” 的情况，完成判定可能提前或延迟。建议在每次收到 `state` 通知后打印调试日志，以便排查。 |
| **异常路径** | 多处使用 `raise Exception(...)`，但未捕获并回滚已注册的内存资源。建议在关键路径（如 `register_buffer_to_engine`、`_send_mamba_state`）加入 `try/except`，在失败时调用对应的 `deregister_memory`，防止资源泄漏。 |
| **测试覆盖** | 1) 老版（无 state）和新版（有 state）两个注册路径的单元测试。<br>2) Mamba‑state 传输成功/失败的端到端模拟。<br>3) `TransferStatus.is_done` 在仅收到 aux、仅收到 state、两者皆收到的三种组合下的断言。 |
| **日志可观测性** | `register_buffer_to_engine` 已加入 `logger.debug`，建议在 `_send_mamba_state`、`update_transfer_status` 中同样记录 src/dst 地址、长度、rank，以便在集群排障时快速定位状态复制异常。 |

**总结**  
本次改动为 NIXL 支持 Hybrid（Mamba）模型提供了必要的状态传输能力，但涉及协议字段、长度信息以及跨 TP 兼容性的细节仍需在文档、错误处理和测试层面进一步完善。建议在合并前完成上述兼容性与异常安全检查，以保证现有 Prefill/Decode 流程在无‑state 场景下仍能平稳运行。

---

### [Docs] Add Falcon H1, Hunyuan-Large, Qwen3-Omni support and update Diffusion usage (#17888)
**SHA**: `c850a8a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c850a8a41af2d2f973a7cb033eef9ba9cacc0ca3)

**🎯 变更类型**：文档增强 / 结构重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 新增 **Diffusion** 与 **Diffusion LLM** 基础使用文档，展示 FLUX、Qwen‑Image、HunyuanVideo 等图像/视频生成模型的启动示例。  
- 将已有的模型列表拆分为 **文本生成、图像生成、检索‑排序、专用模型、扩展** 五大目录，并统一使用 `.. toctree::` 结构。  
- 调整多处链接路径（embedding、vision、frontend）以匹配新目录布局；更新模型支持表格的格式与排版。  

**🎯 影响范围**：  
- `docs/`（全部新增/重命名的 markdown 与 rst 文件）  
- 依赖文档链接的 CI / 自动化部署脚本  
- 对外使用 SGLang 文档的用户（尤其是查阅 Diffusion、Vision、Embedding 示例的用户）  

**💡 关注建议**：  
1. **链接完整性**：本次大量重命名（`supported_models/*`）后，需要在 CI 中运行 `sphinx-build`，确认所有交叉引用（如 `../supported_models/...`）均可解析，防止 404。  
2. **示例一致性**：`diffusion.md` 中的启动命令使用了 `sglang.launch_server`，检查项目根目录下的示例脚本是否已有对应参数（如 `--model-path`），若无需补充或在 README 中提示。  
3. **版本发布说明**：虽然未改动代码，但新增模型（Falcon‑H1、Hunyuan‑Large、Qwen3‑Omni、Step3‑VL）会影响用户对“支持模型”页面的期待，建议在 Release Notes 中加入对应的文档更新说明。  
4. **CI 文档检查**：加入一步 “检查文档中是否出现重复或失效的锚点” 的自动化测试，防止后续改动再次破坏结构。  

总体来看，此次提交提升了 Diffusion 相关功能的可见度，并对模型文档进行了更清晰的层级划分，对使用者的查找体验有明显好处；只要通过文档构建验证链接正确即可安全发布。

---

### [diffusion] CI: update perf baseline (#17512)
**SHA**: `e13b727` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e13b727e928354e53b1e5ba3f613d8bb447a5594)

**🎯 变更类型**：性能基准更新（CI perf baseline）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交对 `python/sglang/multimodal_gen/test/server/perf_baselines.json` 进行大幅度修改（+398 / –331 行），主要包括：  
1. 调整了多项图像生成任务（如 `qwen_image_t2i`、`flux_image_t2i`、`zimage_image_t2i`、`qwen_image_edit_ti2i` 等）的阶段耗时、去噪步时长以及对应的 `expected_e2e_ms`、`expected_avg_denoise_ms`、`expected_median_denoise_ms`。  
2. 为 `wan2_1_t2v_1.3b_teacache_enabled` 增加了全新的基线条目。  
3. 部分已有基线的数值被显著降低（如 `qwen_image_t2i` 的端到端期望从 74 500 ms 降至 14 959 ms），对应去噪步耗时也同步收敛。

**🎯 影响范围**  
- **CI 测试体系**：所有依赖该 JSON 的性能回归检查（`test_server_perf`、`benchmark` 等）将使用新的阈值。  
- **文档/报告**：任何对外发布的基准报告需同步更新，否则可能出现统计不符。  
- **开发者本地调试**：调试时若仍使用旧硬件/配置，可能出现 “超过期望” 的误报。

**💡 关注建议**  
1. **验证采集过程**：确保这些新数值是基于统一硬件、相同模型配置、相同随机种子得到的，避免因硬件差异导致 CI 产生波动。  
2. **防止回归**：在 CI 中加入容忍阈（如 ±10%）或使用统计区间，防止轻微波动触发不必要的失败。  
3. **文档同步**：更新 README/Perf‑Guide 中对应的基准说明，特别是新增的 `wan2_1_t2v_1.3b_teacache_enabled` 场景。  
4. **监控 flaky**：新增基线后观察 CI 运行日志，若出现频繁“超出期望”的情况，考虑放宽阈值或回退到旧基准。  
5. **回滚策略**：保留旧基线的提交记录，以便在硬件或模型版本变化导致大幅偏差时能够快速回滚。  

总体来看，此次更新是对 CI 性能阈值的全面调整，若采集方法可靠，将提升回归检测的准确性；但需留意 CI 稳定性并同步文档。

---

#### 🟢 低重要度变更 (8)

### [diffusion] fix: remove unnecessary norm_type argument from GLM-Image dits (#18382)
**SHA**: `d792aa7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d792aa761873c92b867c311b6e4dd2c060103171)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `glm_image.py` 中删除 `ScaleResidualLayerNormScaleShift` 初始化时多余的 `norm_type="layer"` 参数，保持其它配置不变。此改动简化了代码，提升可维护性。

---

### [CI] Skip some flaky subtests for test_multi_lora_backend.py (#18408)
**SHA**: `eb4cf1d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/eb4cf1dfc46bdc7a90bfe3954044dd35bc7d1637)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `test/lora_utils.py` 中屏蔽了几组不稳定的多 LoRA 子测试，并移除 `torch.use_deterministic_algorithms(True)`，以避免 CI 中的偶发失败。

---

### [Doc] Update CUDA 13 install guide to install torch first (#18404)
**SHA**: `c47c2f9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c47c2f9466dfcbec8256d28b8b6d300ebee22d5f)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/get_started/install.md` 中重新编写 CUDA 13 的安装流程，先安装支持 CUDA 13 的 PyTorch，再安装 sglang，随后提供 `sgl_kernel` wheel 的下载指令，并归纳常见问题的快速解决方案。整体步骤更清晰，指令更准确。

---

### chore: bump mooncake version to 0.3.9 (#18316)
**SHA**: `52401be` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/52401bec1d1c624977d6f9d7a9022e0c51880aad)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 Mooncake 版本从 0.3.8.post1 升级至 0.3.9，更新 Dockerfile 与 CI 安装脚本对应依赖，并对 flaky 的 mooncake‑ep 测试添加 CI 环境下跳过。

---

### Support execute_shell_command for env var support (#18390)
**SHA**: `1552aab` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1552aab741bf89c241b10e06297ee657597bd758)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：`execute_shell_command` 现支持前置 `KEY=VALUE` 环境变量的写法，并在 CI workflow 中增加 `synchronize`、`labeled` 触发类型。

---

### [Qwen3Next] Optimize fused_sigmoid_gating_delta_rule_update_kernel (#18271)
**SHA**: `4637970` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4637970dfb369d900936136c98cb5a6663385884)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `fused_sigmoid_gating_delta_rule_update` 中对 V 维度的块大小上限从 8 提升至 32，改善 GPU 线程块布局，潜在提升性能。

---

### Revert "[Build] Enable full kernel in aarch64 wheel" (#18385)
**SHA**: `9fbec79` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9fbec799063b23f9d75a7d81006d0612b8bbd547)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：撤回此前在 aarch64 wheel 中启用完整 kernel 的改动，删除 CI workflow、Dockerfile 与 build 脚本中针对 aarch64 的 `-DENABLE_BELOW_SM90` 与 `CMAKE_EXTRA_ARGS` 参数相关代码。无需影响其他平台构建。

---

### Add CI permission for Shunkangz, dongjiyingdjy, samuellees (#18377)
**SHA**: `f2e0048` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f2e0048d0688e929803f109bb633b327a9fb3d2c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中为 Shunkangz、dongjiyingdjy、samuellees 新增 CI 权限（可标记运行、重跑失败 CI、60 分钟冷却等）。

---

