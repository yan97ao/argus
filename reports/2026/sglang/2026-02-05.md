# 每日更新报告（2026-02-05）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-05 23:55:43 | Glen Liu | throw error if got adapter with added_tokens (#18046) |
| 2026-02-05 21:49:37 | pansicheng | [Kernel] Add JIT apply_rope_with_cos_sin_cache_inplace (#18155) |
| 2026-02-05 20:36:18 | 陈一涵 | [diffusion] fix: fix accuracy bug caused by #14717 (#18296) |
| 2026-02-05 18:26:15 | ishandhanani | docker: add patch to increase GPU deepep timeout (#18298) |
| 2026-02-05 17:38:09 | Shangming Cai | [PD] Minor code cleanup for mooncake backend (#18279) |
| 2026-02-05 15:24:38 | zhangheng | [piecewise graph]: support MiniMax-M2 (#18217) |
| 2026-02-05 15:10:26 | danielafrimi | [FIX] Always support TP > 4 for FP4 Gemm (#17300) |
| 2026-02-05 15:09:59 | Meng, Hengyu | [XPU] Integrate MoE and minor improvements in XPU attention backend (#13561) |
| 2026-02-05 13:16:07 | Xiaoyu Zhang | [Diffusion] Support layerwise offload for mova (#18272) |
| 2026-02-05 12:16:01 | Alison Shao | Fix test_return_routed_experts to use response-level sglext (#18274) |
| 2026-02-05 11:59:40 | Kun Lin | Support Markdown/Notebook-Friendly Documentation Export for Downstream Integration(convert rat files to md files and save) (#18278) |
| 2026-02-05 11:35:29 | rinbaro | [docs] fix misspellings & typos (#18276) |
| 2026-02-05 11:03:33 | Teng Ma | [PD] doc: Document SGLANG_MOONCAKE_CUSTOM_MEM_POOL and supported values (#18259) |
| 2026-02-05 10:43:23 | Ch3ngY1 | [PD] improve kv offset calculation for MHA model with different tp size (#18163) |
| 2026-02-05 09:22:26 | Mick | [diffusion] chore: prohibit Chinese characters usage (#18249) |
| 2026-02-05 08:59:01 | yinghui | fix kimi k2.5's moe gemm config init (#18064) |
| 2026-02-05 07:49:41 | zwang86 | fix: add SGLANG_IS_IN_CI env var to release-docs workflow (#18225) |
| 2026-02-05 05:59:58 | linhaifeng | [Bugfix] fix a obvious logic error (#18254) |
| 2026-02-05 05:36:35 | Mohammad Miadh Angkad | Add MoE fused config for Qwen3-Coder-Next-FP8 on H100 TP=2 (#18195) |
| 2026-02-05 05:18:02 | Zack Yu | fix: fix MockModelRunner in attention tests (#18240) |
| 2026-02-05 04:03:57 | Michael | [AMD] Add kimi mi35x nightly test, folder organization and several stability fixes (#17895) |
| 2026-02-05 00:23:31 | Mick | [diffusion] refactor: move model_stages into stages folder (#18248) |
| 2026-02-05 00:22:44 | RunningLeon | model: support interns1-pro (#18145) |

### 📊 统计摘要
> 本日共 23 个提交 | 🔴高 0 | 🟡中 12 | 🟢低 11
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🟡 中重要度变更 (12)](#-🟡-中重要度变更-12)
    - [[Kernel] Add JIT apply_rope_with_cos_sin_cache_inplace (#...](#2eb4359)
    - [[diffusion] fix: fix accuracy bug caused by #14717 (#18296)](#4aa03d9)
    - [[PD] Minor code cleanup for mooncake backend (#18279)](#afae4c7)
    - [[FIX] Always support TP > 4 for FP4 Gemm (#17300)](#3f1df32)
    - [[XPU] Integrate MoE and minor improvements in XPU attenti...](#368936a)
    - [[PD] improve kv offset calculation for MHA model with dif...](#f730c18)
    - [fix: add SGLANG_IS_IN_CI env var to release-docs workflow...](#bdaf3de)
    - [Add MoE fused config for Qwen3-Coder-Next-FP8 on H100 TP=...](#efbf395)
    - [fix: fix MockModelRunner in attention tests (#18240)](#2e87c2b)
    - [[AMD] Add kimi mi35x nightly test, folder organization an...](#6fd878b)
    - [[diffusion] refactor: move model_stages into stages folde...](#36a3e78)
    - [model: support interns1-pro (#18145)](#3e7ecb7)
  - [🟢 低重要度变更 (11)](#-🟢-低重要度变更-11)
    - [throw error if got adapter with added_tokens (#18046)](#3f32a58)
    - [docker: add patch to increase GPU deepep timeout (#18298)](#8f8c172)
    - [[piecewise graph]: support MiniMax-M2 (#18217)](#079fc8f)
    - [[Diffusion] Support layerwise offload for mova (#18272)](#dff3ba2)
    - [Fix test_return_routed_experts to use response-level sgle...](#c910829)
    - [Support Markdown/Notebook-Friendly Documentation Export f...](#e616d35)
    - [[docs] fix misspellings & typos (#18276)](#de6a032)
    - [[PD] doc: Document SGLANG_MOONCAKE_CUSTOM_MEM_POOL and su...](#c8212b9)
    - [[diffusion] chore: prohibit Chinese characters usage (#18...](#f218234)
    - [fix kimi k2.5's moe gemm config init (#18064)](#599c5f4)
    - [[Bugfix] fix a obvious logic error (#18254)](#c1d5cc3)
#### 🟡 中重要度变更 (12)

### [Kernel] Add JIT apply_rope_with_cos_sin_cache_inplace (#18155)
**SHA**: `2eb4359` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2eb4359ada96d94a9f4047afec0fae76722766f7)

**🧩 变更概览**  
本次 PR 在 `sglang/jit_kernel` 中新增了 **RoPE‑with‑cos/sin‑cache** 的 JIT CUDA 实现，并提供了 Python 包装、可选的 KV‑cache 写回、PDL（Programmable Data Layout）支持以及 `head‑parallelism` 的调度优化。对应的测试用例、benchmark 与 `rotary_embedding` 的导入路径也同步切换到新实现。

| 主要文件 | 作用 |
|----------|------|
| `python/sglang/jit_kernel/csrc/elementwise/rope.cuh` | 核心 CUDA kernel（两版：普通版 & `HeadParallelism` 版），实现向量化、交错/非交错、可选 KV‑cache 保存、PDL 控制、SM 占用自适应 launch。 |
| `python/sglang/jit_kernel/rope.py` | TVM‑JIT 编译入口、`apply_rope_pos_ids_cos_sin_cache_*` 两个 custom‑op（有/无 KV‑cache），以及高层函数 `apply_rope_with_cos_sin_cache_inplace`。 |
| `python/sglang/jit_kernel/tests/test_rope.py` | 参数化单元测试 + 基准，对齐 JIT vs. 原生 `sgl_kernel` 实现的数值与性能。 |
| `python/sglang/srt/layers/rotary_embedding.py` | 导入路径改为新 JIT 实现。 |

**⚡ 关键实现细节**  
1. **向量化加载**：`vec_size = max(16/sizeof(DType), HEAD_DIM/32)`，确保 16 Byte 对齐。  
2. **交错/非交错分支**：通过 `constexpr bool interleave` 在编译期决定取 cos/sin 的下标，避免运行时分支。  
3. **KV‑cache 写回**：`kv_buffer_saver::prepare/save` 负责读取原始 V、写回 K/V 到外部缓冲区；仅在 `save_kv_cache` 为 true 时实例化。  
4. **Head‑parallelism**：当 token 数量不足以撑满所有 SM，采用 `BatchQKApplyRotaryPosIdsCosSinCacheEnhancedHeadParallelismKernel` 按 head 维度并行，提升 occupancy。  
5. **PDL 控制**：在 kernel launch 中使用 `cudaLaunchAttributeProgrammaticStreamSerialization`，默认仅在 arch≥sm90 且用户显式开启。  
6. **兼容性检查**：`check_cuda_contiguous`、dtype 限制（fp32/fp16/bf16）以及 `head_dim >= rotary_dim` 的前置断言。  

**📦 影响范围**  
- **核心算子**：`apply_rope_with_cos_sin_cache_inplace`（被 SGLang 推理层 `rotary_embedding` 调用）。  
- **KV‑cache 相关**：当 `fused_set_kv_buffer_arg` 传入时，原来的 `sgl_kernel` 写回逻辑被新 JIT 替代。  
- **可选功能**：PDL 只在支持的 GPU（sm≥90）且显式开启时生效；否则保持原有行为。  
- **测试/CI**：新增大量参数化测试，覆盖 fp16/bf16/fp32、不同 head/rotary size、interleave、PDL、KV‑cache 等组合。  

**🔍 需要关注的潜在问题**  

| 问题 | 影响 | 建议 |
|------|------|------|
| **GPU 架构兼容性**：`griddepcontrol.wait/launch_dependents` 仅在 `__CUDA_ARCH__>=900` 有效，低于 sm90 时仍能编译但会产生未定义行为。 | 在不支持的 GPU 上运行会崩溃或性能异常。 | 在 kernel 入口添加 `static_assert(__CUDA_ARCH__ >= 900, "requires sm90+")` 或在运行时检测并回退到原实现。 |
| **PDL 与 KV‑cache 交叉**：`enable_pdl` 默认在 `save_kv_cache` 为 true 时才允许，但 API 没有显式限制，用户可能误传 `enable_pdl=True` 而未提供 KV‑cache。 | `cudaLaunchAttributeProgrammaticStreamSerialization` 在不支持的情况下仍会被设置，导致 launch 失败。 | 在 `apply_rope_with_cos_sin_cache_inplace` 中加入 `if (enable_pdl && !save_kv_cache) RuntimeCheck(false, "...")`。 |
| **Stride 检查不足**：只检查了 `contiguous`（最内维度 stride=1），但对 `head_dim` 与 `vec_size` 对齐没有显式校验。 | 可能在非 16 Byte 对齐的 tensor 上产生错误的向量加载（崩溃或错误结果）。 | 在 `run` 前添加 `RuntimeCheck(q.strides(-2) % vec_size == 0, ...)` 或在 kernel 中使用 `__restrict__` 防止未对齐访问。 |
| **异常信息**：大多数 `RuntimeCheck` 只给出 “Unsupported data type” 或 “head_dim < rotary_dim”。 | 调试时难以定位是哪一步的前置检查失败。 | 尽量在每个 `RuntimeCheck` 中加入上下文信息（如 `function=apply_rope...`, 参数名等）。 |
| **内存占用**：在 `save_kv_cache` 分支，`k_buffer`/`v_buffer` 被复制两遍（kernel 输入 + JIT wrapper），在大模型下可能导致显存峰值翻倍。 | OOM 报错。 | 在 wrapper 中对 `k_buffer`、`v_buffer` 使用 `torch.utils.checkpoint` 或提供 `inplace=True` 选项，避免不必要的拷贝。 |
| **测试覆盖**：虽然参数化丰富，但没有专门验证 **多卡并行**、**异常路径**（如非连续张量、错误 dtype）以及 **不同 SM 版本**。 | CI 在部分硬件/异常场景下可能出现隐藏 bug。 | 增加针对错误输入的负向测试；在 CI 中加入 nvcc `-arch=sm90` 与 `-arch=sm80` 双构建，确保回退路径可用。 |
| **代码重复**：`BatchQKApplyRotaryPosIdsCosSinCacheEnhanced` 与旧的 `BatchQKApplyRotaryPosIdsCosSinCache` 逻辑几乎相同，只是多了 `save_kv_cache`、`interleave`、`pdl` 等分支。 | 维护成本上升。 | 抽取公共子函数（如 cos/sin 读取、向量化 RoPE）到 header，使用模板特化统一实现；同时保留旧实现仅做兼容层。 |

**🚀 推荐的后续行动**  

1. **兼容性回退**：在 `sglang/jit_kernel/rope.py` 中检测 `torch.cuda.get_device_capability()`，若 `<9.0` 则直接调用原 `sgl_kernel`（保持 API 不变）。  
2. **显式 PDL 检查**：在 `apply_rope_pos_ids_cos_sin_cache` wrapper 增加 `if (enable_pdl && !is_arch_support_pdl()) RuntimeCheck(false, "PDL not supported on this GPU")`。  
3. **文档**：在 `README` 或 `docs` 中说明 **head‑parallelism** 的触发条件（`nnz` 与 `num_qo_heads+num_kv_heads` 的比值），以及何时开启 PDL 才能获得收益。  
4. **CI 扩展**：加入负向测试（非连续张量、错误 dtype）以及不同显卡模型的构建/跑测。  
5. **性能基准**：在 `test_bench_rope` 中记录 `sm90` 与 `sm80` 的对比，帮助用户判断是否值得开启 PDL。  

总体而言，此次改动为 RoPE 

---

### [diffusion] fix: fix accuracy bug caused by #14717 (#18296)
**SHA**: `4aa03d9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4aa03d91fd86da1dbd0272e7e2cee7f66d093383)

**🎯 变更类型**：Bug 修复（伴随小幅功能调整）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `diffusion/cutedsl/common/reduce.py` 中，将 CTA 归约的共享内存 `acc` 大小从 `num_warps` 扩展到 `num_warps + 1`，并把最终累计值写入 `acc[num_warps]` 而不是 `acc[0]`，消除因写入冲突导致的数值误差。  
2. 测试文件 `test_fused_norm_scale_shift.py` 调整了 `SHAPES` 顺序，将原先的两组大模型放在后面，确保在修复后仍能覆盖全部维度。  
3. `layernorm.py` 中 `self.weight` 的 dtype 从显式 `dtype` 改为默认 `torch.float32`（去掉 `dtype` 参数），统一权重的存储类型，避免在不同精度下出现隐式转换误差。  

**🎯 影响范围**  
- `python/sglang/jit_kernel/diffusion`（CTA 归约实现）  
- `python/sglang/jit_kernel/tests`（相关单元测试）  
- `python/sglang/multimodal_gen/runtime/layers/layernorm.py`（LayerNorm 权重初始化）  

**💡 关注建议**  
- **兼容性**：`self.weight` 现在强制为 `float32`，若上层模型期望 `float16/bfloat16` 权重，需要在后续代码中显式 cast，避免精度回退。  
- **性能**：额外的共享内存槽位几乎不影响占用（每个 warp 多 4 B），但请在大规模并行时确认 SMEM 使用仍在上限内。  
- **回归验证**：运行完整的 JIT‑kernel 测试套件，尤其是 diffusion 相关的数值回归测试，确认归约结果与前一版在高精度下保持一致。  
- **文档**：在相应的 API 文档或注释中说明 `cta_reduce_sum` 现在写入 `acc[num_warps]`，防止后续维护者误用旧的写法。  

总体而言，此次改动修复了 #14717 引入的数值偏差，影响范围局部且可控，建议在 CI 中加入跨 dtype 的数值对比，以防止类似的精度回退问题再次出现。

---

### [PD] Minor code cleanup for mooncake backend (#18279)
**SHA**: `afae4c7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/afae4c7178f57a60f45c0685f41e1ac42ae4b607)

**🎯 变更类型**：代码清理 / 轻微功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `mooncake/conn.py` 中加入更严格的错误检查并统一返回值；将预填/解码 KV 索引的数组类型从 `int64` 降为 `int32`，简化了地址计算逻辑，去除冗余变量并对日志信息进行了整理。  

**🎯 影响范围**  
- `sglang.srt.disaggregation.mooncake.conn._send_kvcache_generic`  
- `sglang.srt.disaggregation.mooncake.conn.send_kvcache_slice`  
- 可能被上述函数调用的上层 KV‑cache 迁移模块（如 Prefill/Decode 调度器）。  

**💡 关注建议**  
1. **返回码处理**：`_send_kvcache_generic` 现在在层数越界时返回 `-1`，请确认调用方已检查该返回值，否则可能出现未捕获的错误。  
2. **数据类型兼容**：接口参数从 `np.int64` 改为 `np.int32`，如果外部代码仍使用 `int64` 数组，需自行 `astype(np.int32)`，否则会触发类型不匹配。建议在函数入口处加入显式类型检查或兼容转换，以免破坏向后兼容。  
3. **空层快速返回**：新增 `if not src_addr_list: return 0` 防止空转移导致异常，这在层数为 0 或索引列表为空的边界情况尤为重要，建议补充相应单元测试。  
4. **日志信息**：日志从 f‑string 改为固定前缀 + 参数拼接，保持一致性。若需要更详细的调试信息，可考虑在 `logger.debug` 中加入完整上下文。  
5. **性能**：使用 `int32`、去除不必要的 reshape 与 dtype 转换，可略微降低内存占用和计算开销，建议在大规模推理场景下进行基准对比，确保实际收益。  

总体来说，这次提交提升了代码的健壮性和可读性，唯一需留意的是对返回码和数据类型的兼容性做充分验证。

---

### [FIX] Always support TP > 4 for FP4 Gemm (#17300)
**SHA**: `3f1df32` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3f1df322f9df60e228a567bbd2fa3064d4b5f269)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 FP4 GEMM 添加统一的对齐处理：在加载权重时根据后端（CUTLASS/FlashInfer‑TRTLLM）对 N、K 维度进行 32/128 的填充。  
2. 新增 `pad_nvfp4_weight`、`pad_nvfp4_activation_for_cutlass`、`slice_nvfp4_output` 等工具函数，实现权重、激活的填充以及前向输出的裁剪。  
3. 在 `process_weights_after_loading` 与 `apply` 中分别记录原始输出尺寸、填充列数，并在执行 GEMM 前后对应处理，解决 TP > 4 时的维度不满足对齐要求的问题。  

**🎯 影响范围**  
- `python/sglang/srt/layers/quantization/modelopt_quant.py`（权重加载、前向 Q‑FP4 计算）  
- 受影响的后端实现：CUTLASS、FlashInfer、TRTLLM（尤其是 FlashInfer‑TRTLLM 路径）  

**💡 关注建议**  
- **性能**：填充会额外占用显存并增加计算量，请在多卡（TP > 4）场景对比填充前后吞吐率。  
- **正确性**：确保 `weights_padding_cols` 在权重重新加载或模型切换时被恢复，否则可能导致激活维度不匹配。建议添加单元测试覆盖：① 权重/scale 不满足对齐时的填充；② 前向结束后 `slice_nvfp4_output` 正确裁剪。  
- **兼容性**：其他后端（如纯 CUDA 实现）仍使用原始维度，保持 `pad_*` 参数默认 32，以免意外影响已有部署。  
- **文档**：更新 FP4 GEMM 使用说明，明确 TP > 4 时的对齐要求以及可能的显存开销。  

总体来看，此次改动提升了在大规模并行（TP > 4）下的 FP4 量化支持，风险主要在内存开销和未同步的 padding 状态，建议通过 CI 加入相应回归测试。

---

### [XPU] Integrate MoE and minor improvements in XPU attention backend (#13561)
**SHA**: `368936a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/368936a62bdd533cdfe919d8a055c73cc2e34712)

**🎯 变更类型**：功能增强（在 XPU 上加入 MoE 与后端加速）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `sglang` 的 MoE 实现中新增对 Intel XPU（`use_intel_xpu_backend`）的支撑，加入 XPU‑specific kernel 调用（`moe_sum_reduce`、`silu_and_mul`、`fused_experts` 等）。  
- 相应地在 `utils/common.py`、`layers/moe/*`、`layers/quantization/unquant.py` 中加入 `is_xpu` 检测与环境变量 `SGLANG_USE_SGL_XPU` 控制。  
- 更新测试套件，新增 XPU 场景下的 DeepSeek‑OCR 示例，确保两种后端（sgl‑kernel‑xpu 与原 Triton）都能通过。

**🎯 影响范围**  
- **核心模块**：`sglang/srt/layers/moe/*`（fused_moe、moe_align_block_size、moe_runner/triton、topk），`sglang/srt/layers/quantization/unquant.py`，`sglang/srt/utils/common.py`。  
- **自定义 OP 注册**：`direct_register_custom_op` 新增 XPU 设备分支。  
- **测试**：`test/srt/xpu/test_deepseek_ocr.py`，以及 `run_suite.py` 的 XPU 测例列表。

**💡 关注建议**  

1. **变量重复**：`topk.py` 中出现两次 `_is_xpu = is_xpu()`，保持单一定义以免混淆。  
2. **环境变量统一**：文档应明确 `SGLANG_USE_SGL_XPU` 与 `use_intel_xpu_backend()` 的对应关系，避免与 `SGLANG_USE_XPU` 等潜在命名冲突。  
3. **后端兼容性**：`fused_moe` 里仅在 `_use_sgl_xpu` 为真时走 XPU 路径，其他情况下仍走 Triton；建议在异常情况下提供明确报错信息，而不是悄然回退。  
4. **自定义 OP 实现**：`direct_register_custom_op` 中对 XPU 使用 `"XPU"` 标记，确认已在 `torch.utils.cpp_extension` 中注册对应后端的编译库。  
5. **测试覆盖**：现有测试只覆盖 XPU/DeepSeek‑OCR 场景，建议补充 CPU 与 CUDA 上的 MoE 单元测试，防止改动引入跨平台回归。  
6. **依赖声明**：XPU 需要 `sgl_kernel` 包含 XPU 目标，确保 `requirements.txt` / CI 镜像已加入相应 wheel。  

总体而言，此次改动为 XPU 环境提供了完整的 MoE 加速路径，代码结构保持与 CUDA/ROCM 路径一致，风险主要在环境配置与自定义 OP 注册上。完成上述检查后即可安全合入。

---

### [PD] improve kv offset calculation for MHA model with different tp size (#18163)
**SHA**: `f730c18` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f730c186799d966a62531269ce46178364c85dc3)

**🔧 变更类型**：重构 / 性能优化  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `mooncake/conn.py` 中将原先的多层循环式 KV‑offset 计算改为基于 NumPy 向量化的批量计算，并统一为 `process_layer_tp_aware` 接口，提升不同 TP 大小的 MHA 模型在预填与解码阶段的 KV 拷贝效率。  
**🎯 影响范围**  
- `python/sglang/srt/disaggregation/mooncake/conn.py`（KV 缓存拷贝路径）  
- 依赖该模块的分布式推理管线（PP/TP 调度、Mooncake 后端）  

**💡 关注建议**  
1. **功能验证**：新增或补全单元测试，覆盖：① 空切片、② KV‑indices 长度不一致、③ 不同 `src_kv_item_len / dst_kv_item_len` 与 `page_size` 组合的边界情况。  
2. **类型安全**：确保 `prefill_kv_indices`、`dst_kv_indices` 在所有入口均为可安全转为 `int64`，避免溢出。  
3. **内存与性能**：`src_addr_list`/`dst_addr_list` 仍通过 `tolist()` 生成 Python 列表，若拷贝规模极大可能产生额外 GC 开销，可考虑直接传递 `np.ndarray` 给 `batch_transfer_sync`（若后端支持）。  
4. **并发行为**：`futures` 的构建顺序改为两次循环，保持原有 “K‑层 + V‑层” 的执行顺序；若后续对结果顺序有依赖，请确认业务不受影响。  
5. **文档与注释**：建议在函数头部添加简要说明，阐明 “TP‑aware offset 计算” 的数学公式，方便后续维护。  

总体而言，此次改动通过向量化显著减少循环次数，提升大规模 TP 场景下的 KV 拷贝吞吐，但需通过全面测试确认在极端配置下的正确性与资源占用。

---

### fix: add SGLANG_IS_IN_CI env var to release-docs workflow (#18225)
**SHA**: `bdaf3de` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bdaf3de9b3ece4d81b6bb297b75272e08f819720)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为 `execute-notebook.yml` 与 `release-docs.yml` 两个 GitHub Actions 工作流统一加入环境变量 `SGLANG_IS_IN_CI=true`。该变量在项目代码中会被用来判断当前是否运行在 CI 环境，从而控制某些仅在 CI 中才需要执行的逻辑（如跳过本地依赖下载、使用特定的缓存路径等）。

**🎯 影响范围**  
- CI/CD 流程：所有触发上述工作流的 CI 运行都会多出该环境变量。  
- 代码路径：任何依赖 `os.getenv("SGLANG_IS_IN_CI")` 的模块会在 CI 环境下进入对应分支。  
- 文档发布：`release-docs.yml` 在生成并部署文档时会感知到该标记，避免因本地特有配置导致构建失败。

**💡 关注建议**  
1. **代码兼容性**：检查项目中是否已有默认值或 fallback（如 `None`）的处理，防止在本地运行时因缺失该变量导致异常。  
2. **安全性**：虽然变量值为 `true`，但仍应确认未误将其作为敏感信息写入日志。  
3. **条件判断**：若已有 `if: ${{ env.SGLANG_IS_IN_CI }}` 类的条件步骤，建议同步更新以保持行为一致。  
4. **本地调试**：开发者可在本地手动导出 `SGLANG_IS_IN_CI=true` 进行测试，确保 CI 与本地的行为差异在可控范围。  
5. **文档说明**：在 CONTRIBUTING 或 CI 说明文档中标注此环境变量的作用，避免后续贡献者因不知情而产生困惑。  

总体而言，此次改动对功能影响有限，但涉及 CI 环境感知，需确保代码在有无该变量的两种环境中均能正常工作。

---

### Add MoE fused config for Qwen3-Coder-Next-FP8 on H100 TP=2 (#18195)
**SHA**: `efbf395` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/efbf39583e7a716e0204b071db687145392e41b2)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：新增 `triton_3_5_1/E=512,N=256,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json`，为 Qwen3‑Coder‑Next‑FP8（TP=2）在 H100 上提供 MoE fused kernel 的调优配置。  

**🎯 影响范围**：  
- `python/sglang/srt/layers/moe/fused_moe_triton/configs/` 目录下的配置加载模块。  
- 使用 MoE fused 实现且在 H100（FP8）上运行的推理/微调脚本。  

**💡 关注建议**：  
1. 确认配置读取路径支持新文件名，避免因路径硬编码导致加载失败。  
2. 在 CI 中加入针对 H100 FP8 的单元/集成测试，验证 `BLOCK_SIZE_*`、`num_warps`、`num_stages` 能够与 Triton 3.5.1 正常协同。  
3. 更新文档或 README，说明该配置仅在 NVIDIA H100 80GB HBM3（FP8）且 TP=2 场景下使用，防止在不兼容的硬件上误选。  
4. 若项目提供配置自动选择逻辑，需在匹配表中加入对应的 `device_name`、`dtype` 与 `block_shape` 条目，保持向后兼容。  

以上措施可确保新配置平滑落地，提升在最新 H100 环境下的 MoE 推理性能。

---

### fix: fix MockModelRunner in attention tests (#18240)
**SHA**: `2e87c2b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2e87c2bd5e43bfad57150ff878761bc6cffc0ab8)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交针对 `test_flashattn_backend.py` 与 `test_flashattn_mla_backend.py` 中的 `MockModelRunner` 实例化过程进行补全。新增 `kv_cache_dtype、is_hybrid_swa、attention_chunk_size、is_encoder_decoder、is_local_attention_model` 等属性，并在创建 `model_config` 时显式调用 `()` 生成对象，确保与真实模型配置结构保持一致。

**🎯 影响范围**  
- `python/sglang/test/attention/` 中的两套注意力后端单元测试。  
- `MockModelRunner`（测试专用的轻量模型包装）实现细节。  
- 可能间接触及 `AttentionArch`、`MHATokenToKVPool` 等核心注意力模块的初始化参数检查。

**💡 关注建议**  
1. **属性兼容性**：新增属性均采用默认值 (`float16`、`False`、`None`) 与真实模型保持一致，避免在其他未更新的测试或工具中触发 `AttributeError`。若项目中还有其他使用 `MockModelRunner` 的地方，请确认它们不依赖旧的属性集合。  
2. **实例化方式**：原先直接传入字典后缺少 `()` 导致返回字典而非配置对象，此次改为 `()()` 生成实例，确保后续代码按照 `ModelConfig` 接口访问属性。建议在项目文档或注释中说明此调用约定，以防以后误用。  
3. **回归测试**：运行完整测试套件（包括非注意力相关的单元/集成测试），确认新增字段不会影响其他模块的行为。  
4. **代码整洁**：如果 `MockModelRunner` 在正式代码库中仍保留，考虑将这些默认属性抽取到基类或配置帮助函数中，减少每个测试文件的重复声明。  

总体而言，此次修改仅影响测试层面的模型模拟对象，风险可控，只需确保全套测试通过即可。

---

### [AMD] Add kimi mi35x nightly test, folder organization and several stability fixes (#17895)
**SHA**: `6fd878b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6fd878b41df0153bd28f0185920e1b2d9dcc7480)

**🎯 变更类型**：功能增强 / 稳定性修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 AMD CI 中新增 `nightly‑8‑gpu‑mi35x‑kimi‑k2` 准确度任务，并相应扩展 GitHub Actions workflow（新增 job、延长 timeout、安装 `tabulate`）。  
2. 统一 `nightly‑test‑amd` 的 profiling 参数：在 `nightly_utils.py` 中新增 `enable_profile` 开关，默认在 NVIDIA 环境开启、AMD 环境关闭；各 AMD 性能/准确度测试均显式关闭 profiling。  
3. 关闭 `SGLANG_USE_AITER`（从 1 → 0）以规避 MI35x 上的 SWA eviction bug；相应调整部分模型的估算时间、阈值（如 Grok‑2 由 0.915 降至 0.90）。  
4. 为 Kimi‑K2 添加完整的评测脚本 `test_kimi_k2_eval_mi35x.py`（≈60 min），并在 CI 步骤中写入 GitHub Summary。  
5. 其它细节：延长 `nightly‑amd‑accuracy‑8‑gpu‑mi35x‑grok1‑int4` 超时至 90 min，更新性能测试中对 profiling 的禁用。

**🎯 影响范围**  
- CI 工作流：`.github/workflows/nightly-test-amd.yml`、AMD 容器启动/依赖脚本。  
- 测试套件：`python/sglang/test/nightly_utils.py`、所有 AMD 相关 `test/registered/amd/*`（准确度、性能）。  
- 运行时配置：`SGLANG_USE_AITER`、profiling 参数。  
- 文档/报告：GitHub Step Summary 中将出现 Kimi‑K2 的准确度表。

**💡 关注建议**  
1. **CI 资源**：新增 8‑GPU Kimi‑K2 任务耗时约 3 h，确保对应的 AMD 8‑GPU runner 有足够配额；若频繁超时，可考虑拆分或降低 `est_time`。  
2. **Profiling 开关**：确认所有 AMD 测试均已显式传 `enable_profile=False`，防止意外生成大量 profiling 文件导致磁盘耗尽。  
3. **环境变量**：`SGLANG_USE_AITER=0` 已在模型配置里硬编码，建议在项目文档中注明该限制及对应的 Issue（#17220），防止后续误把默认值改回 1。  
4. **阈值与时间**：Grok‑2 阈值下降及部分 `est_time` 调整已通过现有回归，但请在下次发布前进行一次全量 Nightly（包含 NVIDIA 与 AMD）跑分，以验证阈值是否仍合理。  
5. **依赖安装**：MI35x 容器缺失 `tabulate`，已在 workflow 中补装，建议把此依赖写入容器镜像的 `requirements.txt`，降低 CI 维护成本。  

总体来看，变更提升了 AMD 平台的模型覆盖与报告可读性，且通过统一 profiling 开关避免了之前的性能回退风险。后续关注资源消耗和阈值回归即可。

---

### [diffusion] refactor: move model_stages into stages folder (#18248)
**SHA**: `36a3e78` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/36a3e78af93b93bfa8e6a7c654c935e5f90a63f1)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将原先位于 `runtime/models/model_stages` 的模型阶段实现迁移到 `runtime/pipelines_core/stages/model_specific_stages`，并相应更新了 `glm_image.py、mova_pipeline.py、qwen_image.py` 等管线文件的导入路径。文件本身未改动，仅完成了组织结构的清晰化。

**🎯 影响范围**  
- `python/sglang/multimodal_gen/runtime/pipelines/*`（GLM、MoVA、Qwen）  
- `python/sglang/multimodal_gen/runtime/pipelines_core/stages/model_specific_stages/*`（新加入的三个 stage 文件）  
- 任何直接使用旧路径 `sglang.multimodal_gen.runtime.models.model_stages.*` 的内部代码或第三方插件。

**💡 关注建议**  

1. **兼容性检查**：确认项目其余模块（包括测试、示例、文档）是否仍引用旧路径，若有需统一改为新路径或在旧路径下提供兼容性转发（`import … as …`）。  
2. **包声明**：确保 `pip`/`setuptools` 的 `MANIFEST.in` 与 `setup.cfg`（或 `pyproject.toml`）已包括 `pipelines_core/stages/model_specific_stages` 目录，防止发布时文件缺失。  
3. **运行时验证**：执行完整的单元/集成测试，特别是涉及多模态生成的 pipeline，验证模型加载、去噪、解码等阶段仍按预期工作。  
4. **文档同步**：更新开发者文档中关于 “模型阶段” 的导入示例，避免新手因路径变更产生导入错误。  
5. **回退方案**：若出现兼容性问题，可在旧目录中放置轻量的包装模块，转发到新实现，给外部依赖留出迁移窗口。  

总体而言，此次结构化迁移提升了代码可维护性，但请务必在全仓库范围内完成路径统一并通过 CI，以防止因导入错误导致的运行时异常。

---

### model: support interns1-pro (#18145)
**SHA**: `3e7ecb7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3e7ecb78a60f8e1d889cfe25c88006577783d903)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 `model_config.py` 中加入 `InternS1ProForConditionalGeneration`，实现对 InternS1‑Pro 模型的识别。  
2. 新增 `FourierRotaryEmbedding`（FOPE）实现，支持 `rope_scaling` 中的 `use_fope`、`fope_init_factor`、`fope_sep_head`、`num_inv_freq` 等参数。  
3. 新增 `interns1pro.py`，实现基于 Qwen3‑MoE 框架的 InternS1‑Pro 文本/多模态模型，包括自定义注意力、路由以及权重加载。  
4. `qwen3_vl_moe.py` 支持可配置的 `decoder_layer_type`，便于复用 InternS1‑Pro 解码层。  
5. 增加多模态处理器 `processors/interns1pro.py`，完成图像/视频输入的预处理并接入模型。  
6. `rotary_embedding.py` 中加入 FOPE 参数缓存、`cos/sin` 系数的计算与填充逻辑。  

**🎯 影响范围**：  
- **模型加载/执行**：`sglang.srt.models.*`（InternS1Pro、Qwen3MoE）以及对应的注意力、解码层。  
- **位置编码**：`FourierRotaryEmbedding` 替代或补充原有 RoPE，实现更灵活的频率裁剪。  
- **配置系统**：`model_config.is_generation_model` 与 `rope_scaling` 参数扩展。  
- **多模态管线**：新增 `interns1pro` 处理器，影响 `schedule_batch` 与 `multimodal` 相关代码。  

**💡 关注建议**  
1. **兼容性**：确认未使用 `rope_scaling` 的模型仍走原有路径，防止因 `use_fope` 默认缺失导致错误。  
2. **性能验证**：FOPE 引入额外矩阵乘法和 `einsum`，建议在不同 KV‑head、batch‑size 下跑基准，确保吞吐未出现回退。  
3. **权重加载**：`_load_fope_weights` 对 TP‑并行的处理稍显复杂，建议加入单元测试覆盖 head‑数不整除 TP‑size 的情况。  
4. **文档/示例**：更新模型列表、`rope_scaling` 参数说明及示例（尤其是 `num_inv_freq` 与 `fope_sep_head` 的取值），帮助使用者快速上手。  
5. **异常处理**：在 `FourierRotaryEmbedding` 的 `forward` 中加入对 `positions` 越界或 `offsets` 维度不匹配的检查，提升鲁棒性。  

总体来看，此次提交为 InternS1‑Pro（含 FOPE 位置编码）添加了完整的模型实现和多模态支持，改动集中在模型层、位置编码和配置解析，需重点关注兼容性与性能回归。

---

#### 🟢 低重要度变更 (11)

### throw error if got adapter with added_tokens (#18046)
**SHA**: `3f32a58` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3f32a5831d329320972403931819ca037b071830)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `lora_manager.py` 的 `validate_new_adapter` 中新增对 `lora_added_tokens_size` 的检查，若大于 0 则抛出 `ValueError`，提示 LoRA 当前不支持添加词表的适配器。

---

### docker: add patch to increase GPU deepep timeout (#18298)
**SHA**: `8f8c172` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8f8c1724ae2f4dd4ba40b18d246766da3545e4eb)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 Dockerfile 中为 DeepEP 添加了 `sed` 替换，将 `NUM_TIMEOUT_CYCLES` 的默认值从 `200000000000ull` 提升至 `2000000000000ull`，从而延长 GPU 推理的超时时间。

---

### [piecewise graph]: support MiniMax-M2 (#18217)
**SHA**: `079fc8f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/079fc8f3c591a43316d98fae6f108ce03d0eeeb3)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `fp8_kernel.py` 中加入 `torch._dynamo.is_compiling()` 检测，避免编译期间的非张量操作；在 `minimax_m2.py` 中使用 `nullcontext` 条件性地绕过 `expert_distribution_recorder`，兼容 `enable_piecewise_cuda_graph` 开关。

---

### [Diffusion] Support layerwise offload for mova (#18272)
**SHA**: `dff3ba2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/dff3ba202ad034630a8faa53ae6550a06b981e90)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_args.py` 中完善帮助文案并扩展自动开启层级 offload 的判断逻辑，加入对 MOVA 模型的支持。

---

### Fix test_return_routed_experts to use response-level sglext (#18274)
**SHA**: `c910829` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c910829708c7b71f82e646393c6503b17501e396)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `test_return_routed_experts` 中对 OpenAI 响应的解析从 `choices[0].sgl_ext` 调整为直接读取响应级别的 `sglext`，并相应更新错误提示。

---

### Support Markdown/Notebook-Friendly Documentation Export for Downstream Integration(convert rat files to md files and save) (#18278)
**SHA**: `e616d35` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e616d3584737686f6d221ce3f21c67b98a936827)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/Makefile` 中新增 Markdown 导出步骤，复制 `.md` 文件并将 `.rst` 转为 GitHub‑Flavored Markdown，另外将所有 `.ipynb` notebook 转换为 `.md`，便于下游集成。

---

### [docs] fix misspellings & typos (#18276)
**SHA**: `de6a032` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/de6a03260f59fd33a9eeb8f67e7e6e2cf235a70f)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：修正多个文档中的拼写、语法和用词错误，统一表述，提升可读性。

---

### [PD] doc: Document SGLANG_MOONCAKE_CUSTOM_MEM_POOL and supported values (#18259)
**SHA**: `c8212b9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c8212b9fac11d7ad3a2aa088946e1a815a618a97)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 PD Disaggregation 与 Mooncake 文档中新增 `SGLANG_MOONCAKE_CUSTOM_MEM_POOL` 环境变量说明，并在环境变量表中加入该变量及其支持的取值。仅文档改动，无代码影响。

---

### [diffusion] chore: prohibit Chinese characters usage (#18249)
**SHA**: `f218234` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f218234e4f19323d09f10c03505a89a82d52ebd4)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.pre-commit-config.yaml` 新增检查中文字符的 hook 并限定文件范围；同时删除 `t5.py` 中未使用的 `_seen_keys` 变量。

---

### fix kimi k2.5's moe gemm config init (#18064)
**SHA**: `599c5f4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/599c5f4922579742a0c65a4c2fb4503dd63f7ae3)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `scheduler.py` 中，初始化 MoE GEMM 配置时改为先检查 `hf_config.text_config`（若不存在则回退到 `hf_config`），解决 Kimi K2.5 模型的 MoE 参数未被识别的问题。

---

### [Bugfix] fix a obvious logic error (#18254)
**SHA**: `c1d5cc3` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c1d5cc3b24ada6857bc32af13e0a0528a01fcb70)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 `pyproject.toml` 与 `pyproject_xpu.toml` 中 `av` 包的依赖条件，将错误的逻辑运算符改为 `or`，确保在 Linux 上的 `aarch64`、`arm64` 和 `armv7l` 架构均能正确安装。

---

