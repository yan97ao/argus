# 每日更新报告（2026-02-15）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-15 23:28:43 | Douglas Yang | feature: adding build commit to sgl kernel workflow (#18853) |
| 2026-02-15 22:17:51 | Ratish P | [diffusion]: Improve layerwise offload buffer reuse and shared-storage handling (#18611) |
| 2026-02-15 20:43:15 | chenxu214 | change npu.dockerfile (#18835) |
| 2026-02-15 20:43:00 | Mick | [diffusion] quant: add support for svdquant and nunchaku (#18549) |
| 2026-02-15 20:36:47 | Michael | [AMD] Fix nightly 1-GPU test failures and bench_serving regression (#18761) |
| 2026-02-15 18:42:57 | fzyzcjy | Add missing dumper tests (#18859) |
| 2026-02-15 18:33:23 | fzyzcjy | Extract dumper and prefill delayer tests common utils (#18857) |
| 2026-02-15 16:32:13 | haowen-han | fix: fix bug on kimi2.5 with dp2 and tp4 (#18604) |
| 2026-02-15 15:54:27 | Ratish P | [diffusion] fix: enable torch.compile for UlyssesAttention (#18840) |
| 2026-02-15 15:46:56 | zhangxiaolei123456 | perf: add minimax-2.5 fused_moe tuning config for h20 (#18833) |
| 2026-02-15 15:30:52 | jackey hua | [Perf] Tune MiniMax M2 fused moe kernel on H100 GPU (#18851) |
| 2026-02-15 14:09:44 | andyluo7 | Fix/qwen3 5 amd rope cutedsl fallback (#18753) |
| 2026-02-15 12:14:39 | muse-coder | [FIX] Correct JIT kernel compilation on newer GPUs with outdated driver metadata. (#18496) |
| 2026-02-15 11:24:41 | Bhavneek Singh | Model: Support IBM Granite (Dense/Mamba + MoE)  (#18040) |
| 2026-02-15 10:18:56 | shuwenn | [Doc] Convert the speculative decoding notebook to markdow (#18395) |
| 2026-02-15 10:12:32 | Lianmin Zheng | [Auto Sync] Update grpc_request_manager.py, tokenizer_manag... (20260214) (#18838) |
| 2026-02-15 09:39:53 | Guangda Liu | Fix model loading for DeepSeek-V3.2-AWQ (#16907) |
| 2026-02-15 09:19:30 | Lianmin Zheng | [Auto Sync] Update test_deterministic.py (20260214) (#18839) |
| 2026-02-15 08:24:36 | Mohammad Miadh Angkad | Add CI permissions (#18847) |

### 📊 统计摘要
> 本日共 19 个提交 | 🔴高 3 | 🟡中 8 | 🟢低 8
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[diffusion] quant: add support for svdquant and nunchaku ...](#3feb481)
    - [Model: Support IBM Granite (Dense/Mamba + MoE)  (#18040)](#1ce3420)
    - [[Auto Sync] Update grpc_request_manager.py, tokenizer_man...](#b337697)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [[AMD] Fix nightly 1-GPU test failures and bench_serving r...](#88010e9)
    - [Add missing dumper tests (#18859)](#90555a0)
    - [Extract dumper and prefill delayer tests common utils (#1...](#4c7f986)
    - [fix: fix bug on kimi2.5 with dp2 and tp4 (#18604)](#b992828)
    - [perf: add minimax-2.5 fused_moe tuning config for h20 (#1...](#ad1bdb9)
    - [[Perf] Tune MiniMax M2 fused moe kernel on H100 GPU (#18851)](#922fbc2)
    - [[Doc] Convert the speculative decoding notebook to markdo...](#4cf4f08)
    - [[Auto Sync] Update test_deterministic.py (20260214) (#18839)](#8b20205)
  - [🟢 低重要度变更 (8)](#-🟢-低重要度变更-8)
    - [feature: adding build commit to sgl kernel workflow (#18853)](#4ef8ece)
    - [[diffusion]: Improve layerwise offload buffer reuse and s...](#ddfe147)
    - [change npu.dockerfile (#18835)](#4e162d4)
    - [[diffusion] fix: enable torch.compile for UlyssesAttentio...](#274bf66)
    - [Fix/qwen3 5 amd rope cutedsl fallback (#18753)](#944a9f6)
    - [[FIX] Correct JIT kernel compilation on newer GPUs with o...](#91230dc)
    - [Fix model loading for DeepSeek-V3.2-AWQ (#16907)](#190fa82)
    - [Add CI permissions (#18847)](#b1b69ae)
#### 🔴 高重要度变更 (3)

### [diffusion] quant: add support for svdquant and nunchaku (#18549)
**SHA**: `3feb481` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3feb48139e5e0c79201a08ec9c8343ed5bbe6672)

**🎯 变更类型**：功能增强 / 架构变更  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 为 Diffusion 系列模型（以 Qwen‑Image 为代表）新增 **SVDQuant（Nunchaku）** 低位宽（W4A4）后训练量化支持，增加 `--enable-svdquant`、`--quantized-model-path`、`--quantization-precision`、`--quantization-rank` 等 CLI 参数。  
2. 在运行时引入全新 **QuantizationConfig** 子类 `NunchakuConfig` 与对应的线性算子 `NunchakuSVDQLinearMethod` / `NunchakuAWQLinearMethod`，实现 **Fuse‑QKV、Fuse‑Add‑QKV** 的 `MergedColumnParallelLinear`，并在模型构造时自动注入。  
3. 扩展模型加载流水线：根据 `--quantized-model-path` 自动切换权重加载路径、自动推断精度/秩、在加载后对 `wtscale` 与 `wcscales` 进行补丁。  
4. 兼容性提升：在没有 rotary‑embeddings（量化模型缺失该张量）的情况下返回 `freqs_cis=None`；在 `image_encoding` 流程中规避 FSDP offload 冲突。  
5. 完善文档、CLI 参数、日志、错误检查，新增 `quantization.md` 与安全检查。

**🎯 影响范围**  
- `python/sglang/multimodal_gen/configs/models/dits/qwenimage.py`、`pipeline_configs/qwen_image.py`、`pipeline_configs/zimage.py`  
- `python/sglang/multimodal_gen/runtime/layers/quantization/*`（新增 `nunchaku_config.py`、`nunchaku_linear.py`、`configs/base_config.py` 重构）  
- `python/sglang/multimodal_gen/runtime/loader/transformer_loader.py`、`loader/fsdp_load.py`、`loader/utils.py`  
- `python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py`（线性层、FFN、Attention 适配）  
- `python/sglang/multimodal_gen/runtime/server_args.py`（CLI 参数、配置校验）  
- 相关文档 `docs/quantization.md`、日志与工具函数。

**🔍 技术洞察**  

- **架构影响**  
  - 引入 **QuantizationConfig 体系**，`NunchakuConfig` 通过 `base_config` 统一进模型构造过程，实现 **“可选量化插件”** 的架构化。  
  - 新增 **MergedColumnParallelLinear**（融合 Q、K、V 或 Add‑QKV 的列并行线性），改变了原有三路 `ColumnParallelLinear` 的实现路径，兼容 TP/PP 并保持 FSDP 兼容。  
  - `TransformerLoader` 现在会优先读取 `--quantized-model-path`，并在 `maybe_load_fsdp_model` 中把 `param_dtype` 交由 checkpoint 自身决定，打破了“一刀切的 BF16”假设。  
  - `fsdp_load` 中对非浮点权重的 `requires_grad` 进行显式控制，防止 `int8/FP8` 权重在 FSDP 参数同步时触发错误。  

- **性能影响**  
  - 采用 **SVDQuant（W4A4）** 可将显存占用降低约 **3.6×**，且在 Ampere/SM12x GPU 上推理速度提升 **3.0‑8.7×**（依据 FP4 或 INT4）。  
  - 额外的 **custom kernels**（`svdq_gemm_w4a4_cuda`、`awq_gemv_w4a16_cuda`）在 CUDA 上实现 SIMD‑style 低位宽乘加，理论上比 PyTorch 原生 FP16/ BF16 更快，但引入了 **GPU 架构依赖**（仅支持 SM8x、SM12x）。  
  - 对非量化路径保持原有路径不变，因 `quant_config` 为 `None` 时仍走标准 `ColumnParallelLinear`，不产生额外开销。  

- **安全考虑**  
  - 新增对 **Nunchaku 库** 的运行时检测 (`is_nunchaku_available`) 与显式错误提示，降低因环境缺失导致的隐蔽崩溃。  
  - 加载外部 `.safetensors`（量化权重）时会读取文件元信息，若文件来源不可信，仍可能出现 **供应链风险**（恶意修改的量化权重）。建议在生产环境对模型文件进行校验（hash/签名）。  
  - CLI 参数 `--enable-svdquant` 默认 `false`，且在不满足 GPU 条件或路径缺失时会在 `validate()` 中抛异常，防止误用。  

**⚠️ 潜在风险**  

| 风险点 | 可能后果 | 缓解措施 |
|--------|----------|----------|
| **GPU 兼容性**：仅支持 Ampere (SM8x) 与 SM12x，Hopper (SM90) 明确排除。<br>在不满足条件的机器上执行会抛 `ValueError`，但若代码路径被硬编码绕过可能导致 **非法 kernel launch**。 | 程序崩溃、显存异常、错误的推理结果。 | 在 `ServerArgs.adjust_quant_config()` 中提前检测 `current_platform.is_cuda()` 与 `torch.cuda.get_device_capability`，并在 `main` 入口层面阻止启动。 |
| **依赖 Nunchaku 版本不匹配**：`nunchaku.ops.*` 接口若升级或拆分，可能导致 `ImportError` 或运行时 `AttributeError`。 | 启动失败、部分层回退为未量化（性能下降）。 | Pin 依赖版本，或在 `except ImportError` 中给出升级提示；单元测试应覆盖 `is_nunchaku_available` 与 kernel 调用。 |
| **`requires_grad` 与非浮点权重**：在 FSDP 参数同步时，如果 `requires_grad=True` 并且 dtype 为 `int8`，会触发 **torch** 断言。 |

---

### Model: Support IBM Granite (Dense/Mamba + MoE)  (#18040)
**SHA**: `1ce3420` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1ce3420784c26997a01380c8738fcdc0981fc961)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：在 sglang 中新增对 IBM Granite 4.0 系列（Hybrid Mamba‑MoE 与 Dense）模型的原生支持。实现了完整的配置类、模型结构、权重加载、服务器启动时的特化处理以及文档与测试的同步。  
**🎯 影响范围**：  
- `sglang.srt.configs`（新增 GraniteMoeHybridConfig）  
- `sglang.srt.models`（新增 GraniteMoeHybridModel 系列）  
- `sglang.srt.model_executor`（model_runner 中的 mamba2 配置检测）  
- `sglang.srt.server_args`（针对 GraniteMoeHybrid 的缓存/后端默认配置）  
- 文档 `docs/supported_models/...`、模型注册测试 `test/registered/models/test_generation_models.py`  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | • 引入 **Hybrid Mamba‑Attention** 层，使模型在同一 Transformer 中交替使用 Mamba2 状态空间层和标准多头注意力层。<br>• 通过 `layer_types` 参数决定每层类型，默认每 6 层插入一次 Attention（可自定义）。<br>• 支持 **MoE**（稀疏专家）和 **共享 MLP** 两种可选子层，保持与已有 `GraniteMoeShared` 实现复用。<br>• 采用现有的分布式 PP/TP 框架：Embed、层、Norm 均使用 `PPMissingLayer`/`Parallel` 实现，保持向前兼容。 |
| **性能影响** | • **Mamba2** 层在长序列上比纯注意力更高效，但会占用额外的 **state缓存**（`mamba2_cache_params`），导致显存需求上升约 `state_size × chunk_size`。<br>• 引入 **MoE** 时，计算瓶颈转移到稀疏路由和专家前向，可能出现 **负载不均**（取决于 `num_local_experts / num_experts_per_tok`）。<br>• `server_args` 为该模型关闭了 `mamba_cache_extra_buffer`，避免无谓的缓存扩容，提升显存利用率。<br>• 默认后端仍使用 **triton**，对 GPU 兼容性友好，但在 SM100 以上的卡上可能受限于 Triton 对 Mamba 的实现效率。 |
| **安全考虑** | • 新增的模型权重通过 `trust_remote_code=True` 加载（已在测试中），增加了 **代码执行风险**；必须确保模型仓库可信（IBM 官方）。<br>• 配置解析中增加了 `layer_types` 校验，防止恶意或错误的层定义导致异常行为。<br>• 无新的网络请求或外部依赖，安全面基本不变。 |
| **可维护性** | • 将配置、模型、权重加载全部放在独立文件，遵循现有 `granitemoe` 模块风格，易于后续扩展。<br>• 权重映射逻辑相对复杂，复制了 vLLM 的实现；若 HuggingFace 未来更改 MoE 权重命名，需要同步更新。<br>• 文档、注册测试同步完成，覆盖率得到提升。 |

---

### ⚠️ 潜在风险

1. **层类型长度不匹配**  
   - `GraniteMoeHybridConfig` 会在 `__init__` 中检查 `len(layer_types) == num_hidden_layers`；如果模型库提供的 `layer_types` 与默认不匹配会抛异常，导致启动失败。  
2. **显存超出预期**  
   - Mamba2 状态缓存 + MoE expert 参数在大模型（如 389B）上会显著增加显存，用户若未调整 `max_position_embeddings` 或 `mamba_chunk_size` 可能 OOM。  
3. **权重加载不完整**  
   - MoE 权重的切分与映射依赖 `expert_params_mapping` 的固定命名规则；如果 IBM 未来改动权重结构（例如改为 `gate_proj`/`up_proj` 新前缀），当前加载器将无法匹配，导致未加载的参数错误。  
4. **后端兼容性**  
   - `server_args` 中对 SM100 系列默认使用 Triton，若用户在不支持 Triton 的环境（如老旧 CUDA）上运行，会回落到 CUDA kernels，性能可能出现回退。  
5. **安全信任模型**  
   - 通过 `trust_remote_code=True` 加载 IBM 模型代码，若模型仓库被恶意篡改，可能执行任意代码。需要在生产环境中使用 **签名校验** 或 **离线镜像**。  

---

### 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | 1. 在新增模型的 CI 中加入 **显存基准测试**，确保 `mamba_chunk_size` 与 `max_position_embeddings` 的组合不会导致 OOM。<br>2. 为 `GraniteMoeHybridConfig` 增加 **可选的 `layer_types` 自动推断**（如读取 HF 配置中的 `layer_type` 列表），减少手动错误。<br>3. 把权重映射逻辑抽象为通用的 MoE 映射工具，以便未来支持其他 MoE 变体。 |
| **用户** | 1. 启动时务必使用 `--trust-remote-code` 明示确认，或自行下载模型到本地后通过 `--model-path` 加载。<br>2. 评估目标硬件的显存：Granite‑Hybrid 对显存需求约 **1.5‑2×** 于同等参数的纯注意力模型（尤其在长序列 > 64k 时）。<br>3. 若仅使用 **Dense** 版本（`granite-4.0-micro`），可在 `model_args` 中显式禁用 MoE（`num_local_experts=0`），以节省显存。 |
| **运维** | 1. 在发布镜像时加入 **IBM Granite** 的许可证（Apache‑2.0）声明，确保合规。<br>2. 对使用 `trust_remote_code=True` 的容器加硬——仅允许来自可信 registry 的镜像。<br>3. 监控 `model_runner.mamba2_config` 的返回值，确认在不包含 Mamba 层的模型（如纯 Dense）不会误触 `mamba2_config`。 |
| **后续扩展** | - 考虑在 `GraniteMoeHybridConfig` 中加入 **混合比例** 参数（如 `attention_ratio`），让用户直接控制 Mamba 与 Attention 的占比，无需手动编辑 `layer_types`。<br>- 为 MoE 路由器提供 **负载均衡统计**（top‑k 命中率），以便调优 `num_experts_per_tok`。 |

--- 

**结论**：本次提交为 sglang 引入了重要的企业级模型族（IBM Granite 4.0），在架构上实现了 Mamba‑MoE‑Attention 混合，在功能上提升了对长序列与稀疏专家模型的支持。只要在显存资源、权重映射和可信代码使用上做好充分验证，即可安全、稳健地将其投入生产。

---

### [Auto Sync] Update grpc_request_manager.py, tokenizer_manag... (20260214) (#18838)
**SHA**: `b337697` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b33769786fe9079565b23c06ad467e873e9f79bd)

**🎯 变更类型**：重构 / 功能增强 / Bug修复  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 调整 `grpc_request_manager.py` 与 `tokenizer_manager.py` 中的信号处理逻辑，仅在 **主线程** 添加 `SIGTERM` / `SIGQUIT` 处理器，去除非主线程下的警告日志。  
2. 删除 `utils/common.py` 中对 `SIGCHLD` 的默认处理，避免在子进程退出时产生无意义日志。  
3. 大幅重构 `scripts/code_sync/` 系列脚本：抽取公共常量与函数到 `utils.py`，改进获取最近同步提交的方式，新增对新增代码行与 OSS 代码行的比对统计，强化补丁应用冲突处理并自动生成 PR。  

**🎯 影响范围**  
- `python/sglang/srt/grpc/grpc_request_manager.py`  
- `python/sglang/srt/managers/tokenizer_manager.py`  
- `python/sglang/srt/utils/common.py`  
- `scripts/code_sync/`（`check_commits.py`, `copy_from_oss.py`, `copy_to_oss.py`, 新增 `utils.py`）  

**🔍 技术洞察**  

- **架构影响**  
  - **信号处理统一化**：将信号处理的添加统一限制在主线程，防止在子线程中调用 `loop.add_signal_handler` 抛出 `RuntimeError`，提升系统启动安全性。  
  - **代码同步脚本模块化**：通过 `utils.py` 将文件路径列表、同步提交前缀、Git 操作封装，降低重复代码，便于以后维护或迁移到其它项目。  
  - **Patch 应用流程改进**：在 `copy_to_oss.py` 中引入 **两阶段** Patch 应用（clean → 3‑way），若出现冲突仍会提交并创建 PR，保证同步流程不中断，提升协作效率。  

- **性能影响**  
  - 信号处理逻辑本身开销极低，改动对运行时性能几乎无影响。  
  - `check_commits.py` 新增对每个相关提交的 **增量行比对**（`git show` + 行集合比较），在大量提交时会产生额外 I/O，建议在 CI 环境或单次同步时使用；对本地开发者影响不大。  

- **安全考虑**  
  - 移除 `SIGCHLD` 的默认处理不会降低安全性，只是去掉了不必要的日志噪声。  
  - 新增的 Git 操作均通过 `subprocess.run(..., check=True)` 捕获错误，避免因返回码未检查导致潜在的未预期行为。  
  - 脚本现在会在 **非 CI 环境**（`GITHUB_STEP_SUMMARY` 不存在）时自动忽略写入步骤摘要，防止误写本地文件。  

**⚠️ 潜在风险**  
1. **信号处理缺失风险**：若项目在非主线程启动 `grpc_request_manager` 或 `tokenizer_manager`（例如在某些测试框架或自定义进程池中），将不再注册 `SIGTERM`/`SIGQUIT`，导致进程在收到终止信号时无法执行优雅的资源清理。  
2. **同步脚本对 Git 环境的依赖**：`check_commits.py` 与 `copy_to_oss.py` 强制使用 `cwd=repo_root`，在子模块或非标准工作目录下执行时可能出现 “not a git repository” 错误。  
3. **行比对的误判**：增量行比对仅比较 **新增行的文本**，若 OSS 端同一文件对同一行做了格式化（如空格、换行）导致文本不匹配，可能误报 “未同步”。  
4. **冲突补丁自动推送**：在冲突情况下仍会创建分支并推送，可能在 OSS 主仓库留下带冲突标记的临时分支，若未及时处理会增加仓库杂乱度。  

**💡 关注建议**  
- **主线程约束**：在项目启动入口（如 `main.py`）明确确保 `GrpcRequestManager` 与 `TokenizerManager` 在主线程实例化；若确实需要在子线程启动，请在外层自行注册信号处理或使用 `signal.pthread_sigmask` 进行兼容。  
- **监控日志**：在 CI/生产环境加入对 “Signal handler is not added” 警告的监控，防止因误置线程导致的不可预期关机行为。  
- **代码同步脚本 CI**：为 `scripts/code_sync/` 增加单元测试，可使用临时 Git 仓库模拟 OSS 与私有仓库，确保：  
  - `get_last_sync_commit` 能正确定位最近同步提交；  
  - 行比对统计 (`synced_lines/total_added_lines`) 与实际差异相符；  
  - 冲突补丁路径的 `apply_patch_and_push` 正确返回 `False` 并保留冲突标记。  
- **清理临时分支**：在 PR 合并后，自动删除由 `copy_to_oss.py` 创建的临时分支（可在 PR 合并触发的 GitHub Action 中完成），防止仓库中残留大量未使用的同步分支。  
- **文档更新**：在项目的贡献指南或部署文档中补充 **信号处理仅在主线程** 的约束说明，以及 **代码同步脚本的使用前置条件**（Git 环境、OSS Repo 克隆路径等），帮助新贡献者避免误用。  

---

#### 🟡 中重要度变更 (8)

### [AMD] Fix nightly 1-GPU test failures and bench_serving regression (#18761)
**SHA**: `88010e9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/88010e9601bccca6f816984b34f7373ec80778ac)

**🎯 变更类型**：功能增强 / Bug 修复 / CI 稳定性提升  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 统一 AMD ROCm 7.2 Nightly CI：将 1‑GPU 测试超时从 60 → 90 min，提升 `timeout‑per‑file` 为 900 s；新增 MI35x‑8‑GPU Kimi‑K2.5 准确度任务并补全依赖（`tabulate`）。  
2. 将原 Kimi‑K2（8‑GPU）相关作业迁移为 Kimi‑K2.5（准确度）作业，更新工作流名称、触发条件、suite 参数。  
3. `bench_serving.py` 中对 `compute_random_lens` 的返回值改为 `np.array`，显式 `int()` 转换，防止 Tensor/ndarray 与标量混用导致的回归。  
4. 调整 AMD‑OSS 评估的 `accuracy_threshold`（0.79 → 0.75），放宽对 GPT‑OSS‑120B 的容错。  
5. `run_suite.py` 为 AMD 添加了 `"nightly-amd-1-gpu"` 与 `"nightly-amd-1-gpu-mi35x"` 两套子集，使 CI 调度更明确。

**🎯 影响范围**  
- **CI 工作流**：`.github/workflows/nightly-test-amd*.yml`（AMD ROCm 7.2、MI35x）  
- **测试套件**：`test/run_suite.py`、`test/registered/amd/accuracy/mi30x/test_gpt_oss_eval_amd.py`  
- **核心代码**：`python/sglang/bench_serving.py`（采样函数）  
- **文档/监控**：可能涉及 CI 结果展示页面与 benchmark 用例说明。

**💡 关注建议**  
- **开发者**  
  1. 确认所有新增 suite 名称在 `scripts/ci/amd/` 相关脚本、模型配置文件中均已同步，否则易出现 “suite not found” 错误。  
  2. `bench_serving.sample_generated_shared_prefix_requests` 现在返回 `np.ndarray`，若后续有函数直接使用 `list` 接口，请使用 `tolist()` 或 `int()` 包装以保持兼容。  
  3. 关注新增 8‑GPU Kimi‑K2.5 任务的运行时长（180 min），建议在 CI 机器资源紧张时适当调低 `timeout-per-file` 或拆分模型规模。  
  4. 由于 `accuracy_threshold` 降低，确保对应评测报告已更新阈值解释，防止误判回归。  

- **用户**  
  1. 若在本地复现 Nightly 1‑GPU 或 Kimi‑K2.5 场景，请使用相同的 `timeout‑per‑file` 参数（900 s）以避免超时。  
  2. 使用 `bench_serving` 时，若出现 “type error: int() argument must be a string, a bytes‑like object or a number, not ‘numpy.int64’” 的报错，请升级到本次提交后代码或自行在调用处加 `int()`。  
  3. 关注 CI 公告板块中新增的 MI35x‑8‑GPU Kimi‑K2.5 结果，若对模型精度有严格要求，可自行在 `test_gpt_oss_eval_amd.py` 中调回更高阈值。  

总体而言，此次改动提升了 AMD 环境的 Nightly 稳定性，修复了 `bench_serving` 的类型回归，并对测试阈值作了合理放宽。建议在下一个发布周期中加入对应文档更新，确保 CI 与 benchmark 使用者都能快速了解新作业与参数变化。

---

### Add missing dumper tests (#18859)
**SHA**: `90555a0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/90555a022801c822888b12609f6804d211f3edc8)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `sglang.srt.debug_utils.dumper` 添加缺失的 `torch_save` 单元测试，包括正常保存、参数降级保存以及错误容错（静默跳过）。  
2. 增加分布式环境下文件内容完整性的校验，确保保存的 `.pt` 文件能够正确恢复张量。  
3. 引入 `save=False` 参数的测试，验证在关闭保存时不会产生任何文件。  

**🎯 影响范围**  
- `sglang/srt/debug_utils/dumper.py`（核心调试、tensor dump 功能）  
- 分布式调度相关的测试工具 `test/registered/debug_utils/test_dumper.py`  
- 相关辅助函数 `_find_dump_file`、`_get_filenames`。

**💡 关注建议**  
- 确认 `dumper._torch_save` 在异常路径（如 `BadParam`）仍保持 `torch.save` 的兼容性，避免因自定义 `Parameter` 导致进程崩溃。  
- 验证 `save=False` 的实现是否在所有代码路径（包括 `on_forward_pass_start`、`dump`）统一生效，防止意外写文件。  
- 考虑为 `dump` 方法增加返回值或日志级别控制，以便在生产环境中更灵活地关闭调试保存，而不依赖环境变量。  
- 注意分布式测试中 `torch.load(..., weights_only=True)` 可能在未来的 PyTorch 版本中弃用，可提前准备兼容方案。  

总体来看，本次提交提升了调试工具的可靠性和异常容错，测试覆盖面得到显著扩展，建议合并。

---

### Extract dumper and prefill delayer tests common utils (#18857)
**SHA**: `4c7f986` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4c7f986c6b579335ffff9dcc6dd07fef4befe917)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 `sglang/test/test_utils.py` 中新增 `run_distributed_test` 与配套的 `_distributed_worker`，统一分布式单元测试的启动、初始化、异常收集与清理逻辑。  
2. 将原有的 `debug_utils`、`prefill_delayer` 测试从 `unittest`‑style 重写为 `pytest`‑style，删除 `CustomTestCase` 依赖，改用新工具函数执行多进程测试。  
3. 同时把原本散落在测试文件中的分布式启动代码抽取到公共实现，去除重复的 `_run_*`、`_run_worker` 等函数。

**🎯 影响范围**  
- **测试框架**：`test/registered/debug_utils/test_dumper.py`、`test/registered/scheduler/test_prefill_delayer.py` 以及可能的其他自定义测试文件（若仍引用 `CustomTestCase`）。  
- **CI/CD**：CI 脚本中调用 `python -m unittest` 的方式需要改为 `pytest`，否则新加入的分布式入口不会被触发。  
- **运行时依赖**：`torch.distributed`（NCCL、GLOO）和 `torch.multiprocessing`，以及 `find_available_port`（新增的端口查找函数）在测试节点上必须可用。

**💡 关注建议**  

1. **兼容性检查**  
   - 确认项目中仍有使用 `CustomTestCase` 的测试是否已全部迁移，否则会因缺少基类而失效。  
   - `run_distributed_test` 默认使用 `nccl`，但 `prefill_delayer` 仍需要 `gloo`；确保调用方显式传递 `backend` 参数（如本次提交已在 `test_prefill_delayer.py` 中完成）。

2. **资源与端口管理**  
   - `run_distributed_test` 会在每次调用时通过 `find_available_port(29500)` 动态分配端口，避免硬编码冲突。若 CI 环境对端口有额外限制，请在 CI 脚本中显式设置 `MASTER_PORT`。  
   - 进程数 (`world_size`) 直接映射到可用 GPU 数量，建议在 CI 中提前检查 `torch.cuda.device_count()`，防止因 GPU 不足导致 `torch.cuda.set_device(rank)` 报错。

3. **异常传播与调试**  
   - 通过 `result_queue` 传回异常字符串，已保留完整的 traceback，便于定位分布式子进程错误。开发者在本地调试时可直接跑 `pytest -s` 观察子进程输出。  
   - 若后续需要在测试中捕获特定异常，建议在子函数内部使用 `assert` 或 `pytest.raises`，而不是手动抛出。

4. **性能与可维护性**  
   - 每个分布式测试都会重新 `spawn` 一组子进程，启动开销不容忽视。若同类测试数量较多，可考虑将 `world_size`、`backend` 固定在 fixture 中复用进程池。  
   - `run_distributed_test` 已抽离公共逻辑，后续新加入的分布式测试只需实现 `func(rank, **kwargs)`，降低代码重复度。

5. **CI 配置**  
   - 确认 CI 镜像已安装 NCCL（GPU）或 Gloo（CPU）依赖，并开启相应的 GPU 资源。  
   - 由于 `test_dumper.py` 仍会向本地 `http://localhost:40000/dumper` 发起请求，确保该端口在 CI 环境未被占用，或在测试前启动相应的 mock server。

**总结**：本次改动把分布式测试的底层实现抽象为通用工具函数，统一了异常收集与进程清理，提高了测试可读性与可维护性。迁移到 `pytest` 需要同步更新 CI 调用方式，并确保所有旧的 `CustomTestCase` 用例已完成改造。后续若继续扩展分布式测试，直接复用 `run_distributed_test` 即可，避免重复代码。祝测试顺利通过！

---

### fix: fix bug on kimi2.5 with dp2 and tp4 (#18604)
**SHA**: `b992828` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b992828ad272dbea38446b1b41c45fb78d049547)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `trtllm_mla_backend.py` 中，`forward_extend` 分支原先使用 `torch.empty` 创建输出张量导致未初始化内存，在 Kimi 2.5 模型搭配 DP=2、TP=4 场景下出现异常。现改为 `torch.zeros` 初始化为全 0，确保输出值合法。  

**🎯 影响范围**：  
- `python/sglang/srt/layers/attention/trtllm_mla_backend.py`（注意力层后端实现）  
- 受 Kimi 系列模型、使用 TensorRT‑LLM 多卡并行（dp2 / tp4）推理的用户影响。  

**💡 关注建议**：  
1. **功能验证**：在多卡（DP=2、TP=4）环境下跑一次完整的推理基准，核对输出与单卡/旧版一致，确保没有数值偏差。  
2. **性能检查**：`torch.zeros` 会有一次显存写入开销，确认对吞吐量影响可忽略（一般仅一次初始化），若有显著下降可考虑改为 `torch.empty` + `out.copy_(src)` 的显式填充方式。  
3. **回归测试**：在 CI 中加入对应的多卡配置测试，防止后续改动再次引入未初始化错误。  
4. **文档/注释**：在代码处添加注释说明为何必须使用 `zeros`（避免未初始化导致的 NaN/inf），帮助后续维护者快速理解。  

总体来看，此次改动范围小、风险低，但涉及并行推理的关键路径，建议在多卡环境做充分回归验证后再合入主线。

---

### perf: add minimax-2.5 fused_moe tuning config for h20 (#18833)
**SHA**: `ad1bdb9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ad1bdb93df029bb0c6ce145cfca8a0bc62d925d2)

**🎯 变更类型**：性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/` 目录下新增了 `E=256,N=384,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128, 128].json`，为 H20（NVIDIA H20 GPU）在 fp8_w8a8 精度下的 MoE fused‑triton kernel提供了细粒度的调优参数（BLOCK_SIZE、GROUP_SIZE、num_warps、num_stages），覆盖 1‑4096 的不同 expert 数目。

**🎯 影响范围**  
- `sglang/srt/layers/moe/fused_moe_triton`：配置加载逻辑、kernel 调度会读取此文件。  
- 与 `triton` 3.5.1 兼容的 GPU 运行时。  
- 可能影响 `sglang` 的 benchmark、性能回归测试以及使用默认配置的用户。

**💡 关注建议**  
1. **配置完整性**：确认键名与代码中读取的字段保持一致（如 `BLOCK_SIZE_M/N/K`、`GROUP_SIZE_M`、`num_warps`、`num_stages`），避免因拼写或缺失导致运行时 KeyError。  
2. **回退机制**：若在运行时未检测到 H20 或 fp8_w8a8，确保系统能够安全回退到通用配置，防止因找不到匹配配置导致崩溃。  
3. **文档更新**：在 README/CONFIG 章节加入该新配置的适用硬件、数据类型说明以及如何手动指定。  
4. **测试覆盖**：新增或扩展单元/集成测试，验证在 H20 环境下加载该 JSON 并触发相应 kernel 参数；同时加入性能基准对比，确保调优实际带来吞吐或延迟提升。  
5. **兼容性检查**：该配置依赖 Triton 3.5.1，若项目升级到更高版本，需要审查 `BLOCK_SIZE` 与 `num_stages` 是否仍然有效，防止潜在的编译错误。  

总体而言，新增的 H20‑特化调优文件为大模型 MoE 推理提供了细粒度的加速潜力，关键在于加载路径的健壮性和文档、测试的同步更新，以免引入运行时不兼容或误用风险。

---

### [Perf] Tune MiniMax M2 fused moe kernel on H100 GPU (#18851)
**SHA**: `922fbc2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/922fbc21e25dd0b609834e98ceb9e07a62b66928)

**🎯 变更类型**：性能优化  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 MiniMax M2 fused MoE kernel 新增了针对 NVIDIA H100（FP8 w8a8、E=256、N=384）的调优配置文件 `triton_3_5_1/E=256,N=384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json`，提供了不同 `num_experts` 下的 block‑size、warps、stages 等参数。  

**🎯 影响范围**：  
- `python/sglang/srt/layers/moe/fused_moe_triton/` 读取配置的路径逻辑  
- 自动调度/核选择器在 H100 环境下的行为  
- 可能的单元测试或 benchmark 脚本  

**💡 关注建议**  
1. **配置加载**：确认 `config` 读取代码能够根据设备名称、dtype、E/N 自动匹配到该文件，避免 fallback 到不适配的参数导致性能回退或错误。  
2. **兼容性回退**：在非 H100（或不支持 FP8）的机器上应有明确的 fallback 配置或警告，防止误用此 JSON。  
3. **验证覆盖**：补充针对 H100、FP8 精度的 CI / benchmark，验证 `BLOCK_SIZE_*` 与 `num_stages` 的组合在实际运行时达标，并检查数值准确性（FP8 可能产生溢出）。  
4. **文档更新**：在 `README` 或配置说明中标明此文件的适用条件（GPU、triton 3.5.1、FP8），并给出如何手动覆盖的方式。  
5. **后续维护**：若未来 Triton/CUDA 版本或 H100 驱动升级，建议保留旧配置并提供迁移指南，以免出现不可预期的回归。  

整体来看，此提交为 H100 环境提供了细粒度的调参，若配套加载和回退机制完善，可显著提升 MoE 推理吞吐，风险主要在配置匹配和跨硬件兼容性。

---

### [Doc] Convert the speculative decoding notebook to markdow (#18395)
**SHA**: `4cf4f08` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4cf4f0859fb0ece74a882ce972a2512bf7a258aa)

**🔧 变更概述**  
本次提交将 `docs/advanced_features/speculative_decoding.ipynb` 完全删除，改为以纯 Markdown 形式提供同等内容的文档 `docs/advanced_features/speculative_decoding.md`（约 592 行）。核心信息保持不变，仅将交互式 notebook 转为静态文档，便于阅读、搜索和 CI 检查。

**📦 受影响模块**  
- **文档体系**：所有引用该特性文档的链接、搜索索引以及 `mkdocs` 构建流程都会从 `.ipynb` 切换到 `.md`。  
- **自动化检查**：CI 里可能有对 Jupyter notebook 的 lint 检查或 `nbconvert` 步骤，需要相应删减或更新。  
- **开发者使用示例**：原 notebook 中的代码块仍保留在 markdown 中，保证示例可直接复制运行。

**💡 关注建议**  
1. **CI/构建脚本**：检查项目的文档生成流程（如 `mkdocs.yml`），确保已将 `.ipynb` 移除的路径删除，避免构建时报错。  
2. **链接一致性**：搜索项目中是否还有指向 `speculative_decoding.ipynb` 的内部链接或 README，全部改为指向 `speculative_decoding.md`，防止 404。  
3. **代码块可执行性**：虽然 markdown 中的代码块保持原样，但缺少 notebook 的 cell 顺序信息。建议在文档开头加入 **“完整示例文件下载”** 或 **“直接运行脚本”** 的提示，降低新手复制出错的概率。  
4. **文档同步**：后续若对 speculative decoding 参数或示例进行更新，只需修改单一的 markdown 文件，避免出现 notebook 与 markdown 内容不一致的情况。  
5. **搜索友好度**：markdown 更易被搜索引擎索引，建议在关键参数表格前后添加适当的锚点或关键词，以提升在官方文档站点的可检索性。

**⚠️ 潜在风险**  
- 若项目仍依赖 `nbconvert` 将 notebook 转为 HTML，删除后会导致找不到源文件，需要同步更新构建脚本。  
- 代码块中使用的相对路径（如 `sglang.test.doc_patch`）在 markdown 中直接复制粘贴时可能失效，需在文档中说明运行环境或提供完整的 `pip install -e .` 步骤。

**结论**  
此改动提升了文档的可维护性和可访问性，对运行时代码没有影响，只要同步更新文档引用与 CI 配置，即可安全合并。整体风险低，建议尽快合并。

---

### [Auto Sync] Update test_deterministic.py (20260214) (#18839)
**SHA**: `8b20205` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8b2020584c6053d95627d6c5ad31e8c20b8e11e9)

**🎯 变更类型**：其他（测试增强）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `python/sglang/test/test_deterministic.py` 中，`compare()` 的日志不匹配分支由仅打印尾部 10 条日志改为 **定位首个差异位置**，并从该点展示前后 5 条日志及差值；若未定位则回退到原来的尾部打印。  
**🎯 影响范围**：仅 `test_deterministic.py`（测试模块），不涉及库运行时代码。  

**💡 关注建议**  
1. **容错性**：当前通过 `la != lb` 判断差异，直接比较浮点数可能因微小数值误差触发“首个差异”。建议使用 `math.isclose`（或自定义 tolerance）避免误报。  
2. **长度不一致**：`zip(a.logprobs, b.logprobs)` 会截断较长序列，若两者长度不同但前部相同，首个差异会被误认为不存在。可以先比较长度，或在遍历结束后补充 “长度差异”。  
3. **输出对齐**：`label_width` 只对 `Diff` 行做了对齐，A/B 行可能仍产生不对齐的日志，建议统一使用格式化字符串（`f"{label:<{width}}"`）。  
4. **回退路径**：回退到尾部打印的分支代码基本保持原有行为，但仍使用 `len(a.logprobs)` 而非 `len(b.logprobs)`，若两者不等会产生误导信息，建议使用统一的 `max_len`。  
5. **性能与可维护性**：定位首差的 O(N) 遍历对测试仅有微量开销，接受度高。代码可抽取为独立函数以提升可读性。  

总体而言，此次改动提升了调试信息的可读性，对生产代码无影响，只需在上述细节上做些健壮性检查即可。

---

#### 🟢 低重要度变更 (8)

### feature: adding build commit to sgl kernel workflow (#18853)
**SHA**: `4ef8ece` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4ef8ece08ab35acbfab7908888e416c3c15878ac)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `release-whl-kernel.yml` 中新增 `pr_number` 参数，并在 checkout 步骤使用该参数切换至对应 PR 的代码，实现基于 PR 的内核构建。

---

### [diffusion]: Improve layerwise offload buffer reuse and shared-storage handling (#18611)
**SHA**: `ddfe147` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ddfe147377b701dda8a656d0dff718c5423a720f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `layerwise_offload` 中新增 GPU 缓冲池复用、层级常驻窗口、强制释放参数及池清理，实现显存复用并提升加载/释放效率。

---

### change npu.dockerfile (#18835)
**SHA**: `4e162d4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4e162d4b1bf51ef66f21b66ca7ae6eaaa8b91a26)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 NPU 镜像的内核标签从 `2026.01.28` 更新至 `2026.02.01.post2`，并同步修改 Dockerfile 下载 URL、文件名及 pip 安装命令，以匹配新发布的 `sgl‑kernel‑npu` 包。

---

### [diffusion] fix: enable torch.compile for UlyssesAttention (#18840)
**SHA**: `274bf66` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/274bf6607a1ff183caeb70b1753823debd98b8cd)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `UlyssesAttention` 及其子类 `UlyssesAttention_VSA` 中 `forward` 方法的 `@torch.compiler.disable` 装饰，以恢复对 `torch.compile` 的支持。

---

### Fix/qwen3 5 amd rope cutedsl fallback (#18753)
**SHA**: `944a9f6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/944a9f6fcfe6a56f682da9e0644abf569cb4ca2c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Qwen3.5 配置中统一处理 `rope_parameters` 与 `rope_scaling`，保持兼容性。为 CuTe‑DSL GDN 解码添加 `try/except`，在缺少 `cuda.bindings` 时安全回退至 FLA 实现，并在日志中提示。

---

### [FIX] Correct JIT kernel compilation on newer GPUs with outdated driver metadata. (#18496)
**SHA**: `91230dc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/91230dcca890252018e04f838db7f19995a62aa1)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `utils.cuh` 中为非 ROCm 环境增加对 `__CUDA_ARCH__ >= 900` 的检查，避免在旧驱动元数据上误触发；在 `utils.py` 中临时设置 `TVM_FFI_CUDA_ARCH_LIST` 为当前 GPU 的实际架构，确保 JIT 编译正确，并在结束后恢复环境变量。

---

### Fix model loading for DeepSeek-V3.2-AWQ (#16907)
**SHA**: `190fa82` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/190fa8246fbc021034b8b2725ce30df11a96915e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 DeepSeek‑V3.2‑AWQ 模型加载时的属性访问错误。将原先直接访问 `weight` 改为在 `weight` 不存在且 `weight_packed` 存在时再赋值，防止 `AttributeError`，提升兼容性。

---

### Add CI permissions (#18847)
**SHA**: `b1b69ae` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b1b69ae0a993dc98e89480cf10df6488abdedd2b)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中为用户 **mmangkad** 新增 CI 权限，允许使用标签触发 CI、重跑失败的 CI、零冷却间隔以及重跑阶段。

---

