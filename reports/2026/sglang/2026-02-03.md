# 每日更新报告（2026-02-03）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-03 20:32:49 | elvischenv | [Bugfix] Fix Mistral Large 3 NVFP4 TRTLLM MoE (#18065) |
| 2026-02-03 17:47:52 | Lewis | [PD] feat: support mooncake intra-node nvlink kv transfer (#17866) |
| 2026-02-03 17:45:23 | Xiaowei Wang | Warmup before profiling prefill latency for dynamic chunk sizing (#17198) |
| 2026-02-03 15:29:14 | Zhaoyi Li | AMD PD/D PR ci (#17183) |
| 2026-02-03 15:22:15 | Mohammad Miadh Angkad | [Docker] Remove hardcoded `America/Los_Angeles` timezone, default to UTC (#18121) |
| 2026-02-03 15:21:13 | Mohammad Miadh Angkad | [Perf] Use safetensors `load_file` in multithread loader (#18124) |
| 2026-02-03 14:54:56 | fatSheep | [HiCache] fix: apply extra_backend_tag in Mooncake batch_exists (#17265) |
| 2026-02-03 14:28:34 | Viacheslav | Gigachat 3 tool parser and tests (#14765) |
| 2026-02-03 14:17:53 | Kaixi Hou | [NVIDIA] Add --top-k argument to run_eval.py (#18025) |
| 2026-02-03 14:06:50 | Glen Liu | [TestFix] use unit tests for LoRA overlap loading tests (#18140) |
| 2026-02-03 13:43:20 | Kun Lin |  Support Markdown/Notebook-Friendly Documentation Export for Downstream Integration (#18131) |
| 2026-02-03 12:45:14 | b8zhong | MoE Refactor: Refactor `modelopt_quant.py` -> `flashinfer_trllm.py` (#16685) |
| 2026-02-03 12:17:16 | Xiaoyu Zhang | [Diffusion] fix serving image_edit get input image bug (#18109) |
| 2026-02-03 12:08:23 | Hank Han | Add triton_fused_moe config for GLM-4.7-FP8 tp8 H20 H20-3e (#18091) |
| 2026-02-03 10:49:17 | Linyu Wu | [Move sgl-kernel Kernel to JIT] Add JIT concat MLA kernels (#17889) |
| 2026-02-03 10:35:05 | Mick | [diffusion] UX: improve logging (#18122) |
| 2026-02-03 10:07:37 | zhangheng | [HiCache]: Support DeepSeek v32 cpu offloading (#17415) |
| 2026-02-03 10:03:17 | Xiaoyu Zhang | [Diffsuion & JIT_kernel] QKNorm cross heads kernel (#18073) |
| 2026-02-03 09:44:20 | EkiRui | [Performance] Optimize radix cache eviction performance (#14339) |
| 2026-02-03 09:11:28 | Douglas Yang | feature: adding gpt-oss 120b nightly test (#18134) |
| 2026-02-03 06:57:45 | Alison Shao | Fix HF hub race condition in CI by coordinating model downloads across TP ranks (#17787) |
| 2026-02-03 06:38:42 | Alison Shao | Re-enable test_mla_int8_deepseek_v3.py after HF token fix (#18123) |
| 2026-02-03 06:38:15 | cctry | [Fix] data race in req_to_token pool (#17850) |
| 2026-02-03 06:03:57 | TZHelloWorld | [MiMoV2Flash] [feat]: support two batch overlap (#17634) |
| 2026-02-03 03:34:12 | khalilzhk | [NPU] support dsv32 radixcache on ascend (#17964) |
| 2026-02-03 01:32:20 | Yongfei Xu | [DeepSeek V3.2] [Bugfix] slice indexer and padding fa3 when can not run cuda graph (#17076) |
| 2026-02-03 00:40:07 | Yuhao Yang | model: support Step-3.5-Flash (#18084) |
| 2026-02-03 00:13:30 | Sugar920 | [NPU] update nightly tests (#17952) |

### 📊 统计摘要
> 本日共 28 个提交 | 🔴高 2 | 🟡中 18 | 🟢低 8
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [model: support Step-3.5-Flash (#18084)](#980d293)
    - [[NPU] update nightly tests (#17952)](#c781db0)
  - [🟡 中重要度变更 (18)](#-🟡-中重要度变更-18)
    - [[Bugfix] Fix Mistral Large 3 NVFP4 TRTLLM MoE (#18065)](#99fab2c)
    - [AMD PD/D PR ci (#17183)](#8e933e1)
    - [Gigachat 3 tool parser and tests (#14765)](#74f716d)
    - [[TestFix] use unit tests for LoRA overlap loading tests (...](#fe57a88)
    - [Support Markdown/Notebook-Friendly Documentation Export f...](#f032c4f)
    - [MoE Refactor: Refactor `modelopt_quant.py` -> `flashinfer...](#78bf13d)
    - [Add triton_fused_moe config for GLM-4.7-FP8 tp8 H20 H20-3...](#e484c90)
    - [[Move sgl-kernel Kernel to JIT] Add JIT concat MLA kernel...](#9b1619c)
    - [[diffusion] UX: improve logging (#18122)](#62004fd)
    - [[HiCache]: Support DeepSeek v32 cpu offloading (#17415)](#1805943)
    - [[Diffsuion & JIT_kernel] QKNorm cross heads kernel (#18073)](#a1bbc89)
    - [[Performance] Optimize radix cache eviction performance (...](#fd983b0)
    - [feature: adding gpt-oss 120b nightly test (#18134)](#c8da307)
    - [Fix HF hub race condition in CI by coordinating model dow...](#28e2340)
    - [[Fix] data race in req_to_token pool (#17850)](#027f314)
    - [[MiMoV2Flash] [feat]: support two batch overlap (#17634)](#cbf1500)
    - [[NPU] support dsv32 radixcache on ascend (#17964)](#b0a6d52)
    - [[DeepSeek V3.2] [Bugfix] slice indexer and padding fa3 wh...](#677f3c4)
  - [🟢 低重要度变更 (8)](#-🟢-低重要度变更-8)
    - [[PD] feat: support mooncake intra-node nvlink kv transfer...](#a45647b)
    - [Warmup before profiling prefill latency for dynamic chunk...](#cc69ac9)
    - [[Docker] Remove hardcoded `America/Los_Angeles` timezone,...](#25508d1)
    - [[Perf] Use safetensors `load_file` in multithread loader ...](#6f6b9c6)
    - [[HiCache] fix: apply extra_backend_tag in Mooncake batch_...](#7a9d9c7)
    - [[NVIDIA] Add --top-k argument to run_eval.py (#18025)](#4181290)
    - [[Diffusion] fix serving image_edit get input image bug (#...](#eedd472)
    - [Re-enable test_mla_int8_deepseek_v3.py after HF token fix...](#812fd47)
#### 🔴 高重要度变更 (2)

### model: support Step-3.5-Flash (#18084)
**SHA**: `980d293` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/980d2936cd9a94a6346fb678a8134d5776fdf996)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：  
- 为 SGLang 新增对 **Step‑3.5‑Flash**（Step3p5）模型族的完整支持，包括配置、权重加载、前向算子以及多层 EAGLE 推理路径。  
- 新增 `Step3p5Config`、`Step3p5Model`、`Step3p5ForCausalLM`、`Step3p5MTP` 等实现文件，并在模型注册、解析器、服务器参数等入口统一接入。  
- 拓展了 MoE、SWIGLU、混合 SWA、分层缓存等底层组件，以适配 Step3p5 的滑动注意力、注意力组、MTP/next‑n 机制。

**🎯 影响范围**：  
- `sglang/srt/configs/*`（新增 `step3p5.py`，更新 `__init__` 与 `model_config.py`）  
- 模型实现：`sglang/srt/models/step3p5.py`、`sglang/srt/models/step3p5_mtp.py`  
- MoE 相关：`fused_moe_triton/fused_moe.py`、`moe_runner/triton.py`（新增 swiglu 变体）  
- 解析器：`function_call_parser.py`、`reasoning_parser.py`、`hf_transformers_utils.py`（注册新模型）  
- 服务器与推理调度：`server_args.py`、`multi_layer_eagle_worker_v2.py`、`model_runner.py`、`loader.py`  
- 其它：`speculative`、`utils` 等小幅调整。

---

### 🔍 技术洞察

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | • 在 SGLang 的模型抽象层新增 Step3p5 系列，包含 *普通*（Flash）与 *MTP*（多层预测）两种实现。<br>• 通过 `model_config._config_draft_model` 自动把 `Step3p5ForCausalLM` 转为 `Step3p5MTP`，使其能够在草稿模型路径下使用。<br>• 通过 `is_hybrid_swa_model` 与 `get_hybrid_layer_ids` 增加对 Step3p5 的 hybrid‑SWA 兼容。<br>• 在 MoE 及 Swiglu 实现中加入两套激活路径：<br> - `swiglu_with_alpha_and_limit`（GPT‑OSS 旧版）<br> - 新增 `swiglu_gpt_oss_sigmoid_alpha` 与 `swiglu_silu_clamp_mul`，兼容仅 `limit` 的场景，提升代码复用与编译友好性。 |
| **性能影响** | • **滑动注意力**（`layer_types == "sliding_attention"`）在 Step3p5 中通过 `attention_other_setting` 动态配置 KV 组数，避免全局复制，降低显存占用。<br>• **多层 EAGLE**：在 `server_args` 中为 Step3p5 自动打开 `enable_multi_layer_eagle`，并强制开启 Spec‑V2，预计在高并发推理时可显著提升 token‑throughput。<br>• **层级缓存**：针对层级缓存的 Step3p5 自动把 `swa_full_tokens_ratio` 设为 `1.0`，并禁用 hybrid‑SWA memory，以避免缓存冲突，确保推理速度稳定。<br>• 新增的 `torch.compile` 包装的 Swiglu 变体，使 JIT 编译器可以更好地内联计算，理论上在 CUDA 环境下提升 **5‑10%** 的算子执行效率。 |
| **安全考虑** | • 代码仅在 **模型权重加载** 处引入了 `draft_model_idx` 透传，未涉及外部输入的解析，安全风险极低。<br>• 新增的 `FunctionCallParser` 与 `ReasoningParser` 直接映射 `"step3p5"` 为已现有 detector，无新增代码执行路径，未引入注入风险。<br>• 环境变量 `SGLANG_ENABLE_SPEC_V2` 被强制开启，若用户在生产环境中未评估 Spec‑V2 的安全/稳健性，可能导致未捕获的异常，建议在发布前进行充分验证。 |
| **可维护性** | • 大量新增文件（≈ 1 k 行），代码风格与现有模型保持一致，易于后续迭代。<br>• `step3p5_mtp.py` 中对 **next‑n** 权重的过滤逻辑较为复杂，建议抽离为通用的 “checkpoint filter” 工具函数，以降低重复。<br>• `model_config.get_hybrid_layer_ids` 中硬编码的 `"Step3p5ForCausalLM"` 与 `"Step3p5MTP"` 需要在后续模型加入时同步维护，推荐使用统一的 **模型特征注册表**。 |

---

### ⚠️ 潜在风险

1. **权重加载不完整**  
   - `Step3p5ForCausalLM.load_weights` 对 `nextn` 层进行跳过，如果 HF checkpoint 包含未实现的 next‑n 模块，加载时会直接 **skip**，导致模型行为与原始不一致。  
   - `Step3p5MTP.load_weights` 中的 `_rewrite_spec_layer_name` 采用了多阶段字符串替换，若 checkpoint 命名规则改动（如新增前缀），可能导致权重未匹配而产生随机初始化。

2. **混合 SWA 与层级缓存冲突**  
   - `server_args` 为 Step3p5 自动关闭 hybrid‑SWA memory，但仍保留部分旧代码路径（如 `get_hybrid_layer_ids`）会返回 `None`，在特殊组合（开启 `hierarchical_cache` + 关闭 `hybrid_swa_memory`）下可能出现 **未定义行为**。

3. **多层 EAGLE 逻辑改动**  
   - `multi_layer_eagle_worker_v2` 将 `draft_logits_output` 结构从 `draft_logits_output` → `draft_logits_output.logits_output`，若其他分支仍旧使用旧属性，将触发 **AttributeError**，影响非 Step3p5 场景的兼容性。

4. **环境变量强制开启**  
   - `SGLANG_ENABLE_SPEC_V2` 被强制设为 `True`，可能对依赖旧 Spec‑V1 行为的模型产生副作用（如不同的采样策略或并行调度）。

5. **编译器兼容性**  
   - `torch.compile` 包装的 Swiglu 变体在 PyTorch 2

---

### [NPU] update nightly tests (#17952)
**SHA**: `c781db0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c781db0f6c236e357a0652a962fdf7d3f528d752)

**🎯 变更类型**：功能增强 / 架构变更 / 测试扩展  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- CI 工作流 `.github/workflows/nightly-test-npu.yml` 大幅扩展，新增 **nightly‑8‑npu‑a3**、**nightly‑16‑npu‑a3** 以及对应的依赖安装、模型缓存、日志打印等步骤。  
- 移除 `torch_npu==2.8.0` 安装，仅保留通用依赖，统一通过内部脚本 `scripts/ci/npu/npu_ci_install_dependency.sh` 完成 NPU 环境准备。  
- 在 `python/sglang/test/ascend/` 中新增 **test_ascend_utils.py**，集中声明所有模型权重路径常量，供新旧测试统一引用。  
- 大量 **test/registered/ascend/** 测试新增或改写：  
  - 新增接口/参数类测试（`enable_thinking`、`log-level`、`warmups`、`chunked‑prefill`、`no‑overlap‑scheduler`、`no‑chunked‑prefill`、`logprobs` 对齐等）。  
  - 多模型的 GSM8K、MMMU 基准测试覆盖从 1‑B 到 480‑B 超大模型，涉及 LLM、VLM、embedding、reward、rerank 等多模态/多任务模型。  
  - 为部分测试配置了更高的 `timeout_for_server_launch`，防止长模型启动超时导致 CI 失效。  
- `test/run_suite.py` 中的 NPU 任务列表加入 **stage‑a‑test‑1**、**stage‑b‑test‑1‑npu‑a2**、**stage‑b‑test‑2‑npu‑a2**、**stage‑b‑test‑4‑npu‑a3**、**stage‑b‑test‑16‑npu‑a3**。  
- 多个模型测试的 `register_npu_ci` 调用里加入 `disabled="run failed"` 标记，以便在资源不足时自动跳过。  

---

### 🎯 影响范围
| 受影响模块/组件 | 影响说明 |
|----------------|----------|
| `.github/workflows/nightly-test-npu.yml` | CI 调度、镜像、依赖、缓存、日志、模型下载全部改动。 |
| `scripts/ci/npu/npu_ci_install_dependency.sh` | 统一 NPU 依赖安装方式，移除显式 `torch_npu` 包。 |
| `python/sglang/test/ascend/` | 新增 `test_ascend_utils.py`，所有 Ascend 测试统一引用模型路径常量。 |
| `test/registered/ascend/*` | 新增约 30+ 测试文件，覆盖模型、参数、接口功能、日志、warm‑up、原始 logprob 对齐等。 |
| `test/run_suite.py` | NPU 任务列表扩展，导致 CI 调度逻辑更新。 |
| 代码库整体 | 增加约 **1.7k 行** 代码（主要为测试与配置），对业务运行代码无直接影响，但对 CI 资源消耗和维护成本产生显著影响。 |

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 将 NPU 依赖集中在 `npu_ci_install_dependency.sh`，降低工作流中硬编码依赖的风险。<br>- 新增 `test_ascend_utils.py` 将模型权重路径抽象为常量，提升代码复用，减少硬编码路径导致的同步问题。<br>- CI 流程中引入多机多卡（8、16 卡）配置，要求调度系统能够正确识别 `linux-aarch64-a3-8`、`linux-aarch64-a3-16` 节点；若调度标签缺失，将导致 CI 卡住或错误。 |
| **性能影响** | - 新增大量大模型（如 Qwen3‑235B‑A22B‑W8A8、Qwen3‑Coder‑480B‑A35B）以及多卡并行测试，单次 CI 耗时将显著增长（部分 Suite `timeout‑minutes: 240`）。<br>- 通过内部 `cache-service` 加速 apt/pypi 拉取，降低网络 IO，但模型文件仍需从 ModelScope / HF 镜像下载，磁盘 I/O 与网络带宽仍是瓶颈。<br>- `--disable-cuda-graph` 与 `--mem-fraction-static` 参数在 NPU 上保持一致，可避免图构建开销导致的波动。 |
| **安全考虑** | - 引入内部 `cache-service.nginx-pypi-cache`，若该服务被滥用或未正确隔离，可能成为供应链攻击入口。建议在 CI 环境中对该内部域名做白名单，并启用 TLS（目前使用 `http`）。<br>- `HF_ENDPOINT` 指向 `https://hf-mirror.com`，仍为可信的镜像站点；若镜像被污染，可能导致模型被篡改。<br>- `SGLANG_RETURN_ORIGINAL_LOGPROB` 环境变量在 logprob 对齐测试中使用，可能泄露模型原始 logits，需确认仅在受信任的 CI 环境下开启。 |
| **可维护性** | - 测试文件数量激增（>200 条），若没有统一的测试基类或标签管理，后期维护成本会大幅上升。<br>- `register_npu_ci(..., disabled="run failed")` 方式软禁用失败的测试，虽能防止 CI 失效，但会隐藏潜在回归，建议配合 flaky‑test 标记或自动重试机制。 |
| **资源消耗** | - 估算每个 nightly‑NPU‑a3 Suite（8 / 16 卡）约 2‑4 GB GPU 显存 + ~30 GB 系统内存，累计磁盘约 200 GB（模型缓存）。<br>- CI 时长预计从原来的 ~30 min 拉升至 **2‑4 h**（取决于模型大小），对 GitHub Actions 付费额度影响明显。 |

---

### ⚠️ 潜在风险

| 风险点 | 描述 | 严重程度 | 缓解措施 |
|--------|------|----------|----------|
| **CI 超时 / 资源耗尽** | 大模型启动时间长，`timeout_for_server_launch` 未调高时会导致任务被杀。 | 高 | 为每个大模型统一设置 `timeout_for_server_launch`（已在部分测试中加入），并在 CI 中使用 `timeout-minutes` 足够大的阈值。 |
| **调度标签缺失** | 新增的 `linux-aarch64-a3-8/16` 运行器若未在自建集群中提供，会导致 workflow 卡住。 | 高 | 在 CI 节点管理平台提前定义对应标签，并在 PR 合并前跑一次单机验证。 |
| **缓存服务安全** | 使用内部未加密的 HTTP 缓存可能被中间人篡改。 | 中 | 为缓存 URL 使用 HTTPS（或在内部网络中启用 TLS），并在 CI 中加入 checksum 校验。 |
| **测试 flaky** | 部分模型在 NPU 上受限于算子实现差异，可能出现间歇性错误导致 CI 频繁失败。 | 中 | 为这些测试加入 `pytest.mark.flaky` 或 GitHub Actions `continue-on-error`，并在失败时记录详细日志供后续排查。 |
| **模型路径硬编码** | 虽已抽象为常量，但路径仍是硬编码的本地缓存路径（`/root/.cache/modelscope/hub/models/…`），在不同 CI 镜像或用户自定义目录时会失效。 | 中 | 将模型根路径抽成环境变量（如 `MODELSCOPE_CACHE_DIR`），并在 `test_ascend_utils.py` 中读取。 |
| **并发两套日志级别冲突** | `--log-level` 与 `--log-level-http` 同时使用，可能导致日志过量影响 CI 输出解析。 | 低 | 在文档中说明两者的互斥使用场景

---

#### 🟡 中重要度变更 (18)

### [Bugfix] Fix Mistral Large 3 NVFP4 TRTLLM MoE (#18065)
**SHA**: `99fab2c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/99fab2ce673eeae87736cee44844777a3c9ae304)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `CompressedTensorsMoEMethod` 增加对 **flashinfer‑TRTLLM** MoE 后端的支持，新的代码路径在 `use_flashinfer_trtllm=True` 时走 `trtllm_fp4_block_scale_moe`。  
- 同时在测试套件中加入 **Mistral‑Large‑3‑NVFP4** 模型的跑分/精度验证，并把 CI 超时时间从 1800 s 提升至 3000 s。  

**🎯 影响范围**  
- `python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py`（MoE 量化与调度核心）  
- `test/registered/8-gpu-models/test_mistral_large3.py`（模型启动、性能基准）  
- 相关配置字段：`moe_runner_backend`、`apply_router_weight_on_input`、`routed_scaling_factor` 等。  

**💡 关注建议**  
1. **依赖与环境**：`flashinfer`（含 `fp4_quantize`、`trtllm_fp4_block_scale_moe`）必须在所有目标机器上安装，且版本需兼容 `torch.float8_e4m3fn`。建议在 README/CI 脚本中显式声明。  
2. **回退路径**：在 `use_flashinfer_trtllm=False` 时仍然使用原 Cutlass 实现，确保 `topk_weights/ids` 仍然被正确传递，避免因变量未定义导致运行时错误。  
3. **dtype 与 scaling**：`router_logits` 在 DeepSeekV3 路由时被强制转成 `float32`，请确认其它路由方法（如 `TopK`）不受影响；`hs_scale` 通过 `torch.float8_e4m3fn` 转换后应保持与后端一致。  
4. **对称内存**：`use_symmetric_memory` 包裹的代码仅在 `is_allocation_symmetric()` 为真时生效，建议加入单元测试覆盖对称/非对称两种分配场景，防止出现未初始化的 `symm_output`。  
5. **测试可靠性**：新增的 NVFP4 变体使用更长的 CI 超时，建议在本地跑一次完整的 `nightly-8-gpu-common`，确认 `flashinfer_trtllm` 路径在不同 GPU（B200/H200）上均能成功加载。  
6. **文档更新**：说明何时需要在 `ModelLaunchSettings` 中加上 `--moe-runner-backend=flashinfer_trtllm`，以及对应的模型文件命名约定（`-NVFP4`）。  

总体而言，改动为大模型（Mistral‑Large‑3‑NVFP4）引入了更高效的 FP4 MoE 实现，核心逻辑已保持向后兼容。但请重点检查环境依赖、dtype 对齐以及对称内存分配的两条路径，以免在不同硬件/配置下出现运行时错误。

---

### AMD PD/D PR ci (#17183)
**SHA**: `8e933e1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8e933e1914b6b335e58bb1442b3931e2303192bb)

**变更类型**：功能增强（为 AMD‑GPU Disaggregation 场景新增 CI 流水线、容器启动脚本以及完整的端到端测试）  

**核心改动**  
1. **GitHub Actions**：在 `pr-test-amd.yml` 中新增 `stage-b-test-large-8-gpu-35x-disaggregation-amd` 作业，加入 RDMA 环境检测、容器启动、依赖安装以及两类测试（RMSNorm 预检查 + `test_disaggregation`）。  
2. **容器启动脚本** `scripts/ci/amd/amd_ci_start_container_disagg.sh`：  
   - 自动从 Git tag 获取最新 SGLang 版本并拼装对应的 ROCm‑mi30x/mi35x 镜像标签。  
   - 根据 runner 主机名识别 GPU 架构，回退至 mi30x。  
   - 按天查找本地缓存或远端镜像，找不到时使用硬编码 fallback。  
   - 挂载 RDMA 相关库（`libionic`、`libnl-3`、`libmnl`）及 InfiniBand 系统路径，开启 `--privileged`、`--network=host`、`--cap-add=RDMA` 等。  
3. **测试套件**：新增 `test/registered/amd/disaggregation/` 下三个文件，分别覆盖：  
   - 基础准确性、日志概率、结构化输出、首 token 结束等基本功能。  
   - 失败注入（`DISAGGREGATION_TEST_FAILURE_PROB`）和重试恢复。  
   - Pipe‑Parallel (PP) + 动态 Chunk 场景的前向/解码准确性。  
4. **CI 注册**：在 `test/run_suite.py` 加入新 stage 名称，使 CI 调度能识别该作业。  

**影响范围**  
- **CI 基础设施**：新增大量 Docker 启动与挂载逻辑，可能影响所有 AMD‑CI 运行者（尤其是 mi35x 系列）。  
- **测试时长**：每个 Disaggregation 作业约 60 min，整体 CI 时长将显著提升。  
- **运行环境**：依赖 RDMA 设备、`libionic` 动态库以及系统路径的可访问性，缺失时会导致测试报错或降级为非 RDMA。  

**建议**  
- **镜像策略**：确认 `rocm/sgl-dev` 镜像 tag 规则与默认 `v0.5.5-rocm700-mi35x-YYYYMMDD` 匹配；若项目升级版本，需要同步更新脚本里 `SGLANG_VERSION` 默认值或改为读取 `git describe --tags`.  
- **容错处理**：当前若找不到 RDMA 库仅打印警告，后续代码仍会尝试挂载，建议在没有 libionic 时直接跳过相关测试或显式 `exit 0`。  
- **资源清理**：CI 结束后未看到容器/镜像清理步骤，长期运行可能导致磁盘膨胀，可在 `pr-test-amd-finish` 加入 `docker rm -f ci_sglang && docker system prune -f`.  
- **安全性**：`--privileged` 与 `--network=host` 暴露宿主机，若不是专用 Runner，建议改为最小化权限（仅 `--device=/dev/kfd --device=/dev/dri --cap-add=IPC_LOCK,SYS_PTRACE,RDMA`).  
- **测试可靠性**：`DISAGGREGATION_TEST_FAILURE_PROB=0.05` 可能导致偶发 flaky，CI 里应捕获并重试，或把容错阈值调低。  
- **文档更新**：在项目 README/CI 指南中说明 “Disaggregation CI 需要 RDMA 设备 & libionic”，并给出本地调试的 docker run 示例。  

整体来看，此次改动为 AMD‑RDMA Disaggregation 引入完整的端到端 CI，提升功能覆盖度，但对 Runner 环境依赖较强，需做好镜像/tag 管理、容器清理及错误容忍的细节，以免导致 CI 失稳或资源泄漏。

---

### Gigachat 3 tool parser and tests (#14765)
**SHA**: `74f716d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/74f716dbd711bef3bd7bc9d6eab7ebc675e47e65)

**变更概览**  
- **功能增强**：在 SGLang 中加入对 *Gigachat‑3* 工具调用格式的解析器 `GigaChat3Detector`。  
- **文档更新**：`--tool-call-parser` 参数说明新增 `gigachat3` 选项。  
- **核心代码**：`function_call_parser.py` 中的 `parser_map` 新增 `"gigachat3": GigaChat3Detector`。  
- **单元测试**：在 `test_function_call_parser.py` 中添加 `TestGigaChat3Detector`，覆盖非流式、流式、错误容错等 30+ 场景。

**影响范围**  
- **解析层**：所有使用 `FunctionCallParser`、`--tool-call-parser=gigachat3` 的后端服务或 demo。  
- **文档/CLI**：用户在启动服务器时可通过新选项激活该解析器。  
- **测试套件**：新增约 500 行测试，提升整体覆盖率。

**关注要点**  

| 关注点 | 建议 |
|--------|------|
| **正则表达式鲁棒性** | `REGEX_FUNCTION_CALL`、`REGEX_CONTENT_PATTERN`、`NAME_REGEX`、`ARGS_REGEX` 均使用 `DOTALL`，但未对嵌套 `<|...|>` 标记做防冲突检测。建议在匹配前先做一次安全裁剪或对 `function call` 前后空白做更严格的边界检查，防止误匹配普通文本。 |
| **增量解析状态** | `parse_streaming_increment` 通过 `_buffer` 累积文本，状态标记 (`tool_started`, `tool_name_sent`, `prev_tool_call_arr`) 未在解析结束后复位。若同一个 `FunctionCallParser` 实例用于多轮对话，后续解析会残留上一次的状态。建议在 `has_tool_call` 为 `False` 且已完成一次完整工具调用后手动重置，或在 `detect_and_parse` 调用前提供 `reset()` 接口。 |
| **异常路径** | 当 JSON 解码失败直接返回 `normal_text=model_output`，但仍保留 `calls=[]`。如果在流式场景中出现中间块无效 JSON，后续块可能仍触发 `tool_started=True`，导致后续解析混乱。可在 `except` 中加入 `self._buffer = ""` 并重置状态。 |
| **兼容性** | 新增解析器不会影响已有 parsers，但 `--tool-call-parser` 默认值仍是 `None`，用户若误写 `gigachat3`（大小写、下划线）会报错。推荐在 CLI 参数解析阶段加入大小写容错与提示。 |
| **性能** | 正则搜索在每次增量调用中都对完整 `_buffer` 进行，若流式文本很长（千行）会产生 O(N²) 的成本。可以在首次匹配到 `function call` 标记后，仅在后续块上继续搜索 `ARGS_REGEX`，或使用 `re.search` 的 `pos` 参数定位增量区域。 |
| **测试** | 测试覆盖丰富，但全部基于 `GigaChat3Detector` 的内部实现。建议再加一个集成测试，使用 `FunctionCallParser` 并通过 CLI 启动服务，验证 `--tool-call-parser=gigachat3` 在真实推理流中能够正确返回 OpenAI 兼容的 `tool_calls`。 |

**总体评价**  
此 PR 为 SGLang 引入了对新模型 Gigachat‑3 的工具调用支持，代码结构清晰，单元测试覆盖面广，文档同步更新。若按上述建议处理状态复位、正则效率与异常容错，可进一步提升稳定性并避免在长对话或多轮推理场景中出现残留状态导致解析错误。整体风险低，建议合并。

---

### [TestFix] use unit tests for LoRA overlap loading tests (#18140)
**SHA**: `fe57a88` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fe57a887b110a52670fcaa5c616cd0a8dce12615)

**🎯 变更类型**：其他（单元测试补充）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `test/registered/lora/test_lora_overlap_loading.py` 中新增 127 行单元测试，重点验证 `LoRAOverlapLoader` 的完整生命周期、容量限制以及批量校验逻辑。通过 `unittest.mock` 对 `torch`、CUDA 事件/流以及 `LoRAManager` 进行隔离模拟，并相应调整 CI 注册的预估执行时间（从 100 s 降至 75 s）。

**🎯 影响范围**  
- `sglang/srt/lora/lora_overlap_loader.py`（核心加载器逻辑）  
- `sglang/srt/lora/lora_manager.py`（验证与拉取 LoRA 的接口）  
- CI 相关注册 (`sglang/test/ci/ci_register.py`)  

**💡 关注建议**  
1. **测试可靠性**：确保 `torch` 的 patch 路径与实际模块保持一致，防止在不同环境下出现 `ImportError`。  
2. **容量阈值**：单元测试假设最大并发为 3（由内部 event 列表决定），若未来改动 `LoRAOverlapLoader` 的并发策略，需要同步更新对应断言。  
3. **CI 时长**：修改的 `est_time` 为 75 s，建议监控实际运行时长，防止因机器负载波动导致 CI 超时。  
4. **回归检查**：本次只增加测试，不改变业务代码，建议在合并前跑全套 `pytest`，确认新测试不会因 mock 配置不完整导致隐藏的功能缺陷。  

总体来说，此次提交提升了 LoRA 重叠加载模块的覆盖率，对功能本身没有直接影响，只需关注测试环境的正确模拟即可。

---

### Support Markdown/Notebook-Friendly Documentation Export for Downstream Integration (#18131)
**SHA**: `f032c4f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f032c4f3d66605edd3177ac7d242c3d9704888bf)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 docs 构建流程中新增 `make markdown` 目标，实现对所有 Jupyter Notebook 的无执行直接 Markdown 导出，并在 CI `release-docs.yml` 中同步执行，使下游项目可直接复用 Markdown 文档。  

**🎯 影响范围**  
- `docs/Makefile`（新增 markdown 目标及并行转化逻辑）  
- `.github/workflows/release-docs.yml`（CI 流程新增 `make markdown` 步骤）  
- `docs/README.md`（文档说明更新）  

**💡 关注建议**  
1. **依赖环境**：确保 CI 镜像预装 `jupyter`, `nbconvert`, `parallel`，并验证 `jupyter nbconvert --to markdown` 在无交互环境下不会因缺少 LaTeX、pandoc 等导致失败。  
2. **并行度**：当前硬编码 `-j3`，在资源受限的 runner（如 GitHub 2‑core）可能导致 CPU 争用或 OOM，建议改为 `$(nproc)` 或通过变量可配置。  
3. **路径兼容**：`$(BUILDDIR)/html/markdown/$$REL_DIR` 会在 Windows 环境下产生不一致的路径分隔符，若项目计划支持跨平台 CI，需要使用 `$(shell cygpath -u ...)` 或统一使用 POSIX 路径。  
4. **增量构建**：`make markdown` 每次都会遍历全部 `.ipynb`，可考虑加入文件时间戳判断，仅重新导出自上次构建后变更的 Notebook，以缩短 CI 时长。  
5. **文档发布流程**：新增的 Markdown 产物会推送到 `sgl-project.io`，请检查该仓库的部署脚本是否已适配 `markdown/` 目录的同步，防止出现 404。  
6. **测试覆盖**：建议在本地 CI 环境跑一次完整 `make compile && make html && make markdown`，确认生成的 Markdown 与 HTML 内容一致（尤其是图表、数学公式的渲染），并在 PR 检查中加入 markdown 目标的成功校验。  

总体来看，此次改动提升了文档的可复用性，但需留意并行执行、跨平台路径以及 CI 依赖的完整性，适当加入配置项和增量构建可以进一步降低构建成本。

---

### MoE Refactor: Refactor `modelopt_quant.py` -> `flashinfer_trllm.py` (#16685)
**SHA**: `78bf13d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/78bf13db4447b98eb9d8169c400448d1dcad12a3)

**🎯 变更类型**：重构 + 功能增强（新增 FlashInfer TRT‑LLM 对 FP8/FP4 MoE 的完整支持）

**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 将原 `modelopt_quant.py` 中与 FlashInfer TRT‑LLM 相关的权重对齐、量化、scale 计算等逻辑抽取到 `flashinfer_trtllm.py`，并实现 **FP8 → FlashInfer** 与 **FP4 → FlashInfer** 两套统一接口。  
- 新增 `align_fp8_moe_weights_for_flashinfer_trtllm`（可选交换 W13 半块）与 `align_fp4_moe_weights_for_flashinfer_trtllm`，分别完成权重 reorder、shuffle、scale 预计算，并在 `process_weights_after_loading` 中统一调用。  
- 引入 `FlashInferTrtllmFp8MoeQuantInfo`、`FlashInferTrtllmFp4MoeQuantInfo` 两个 `dataclass`，保持量化信息的结构化。  
- `fused_experts_none_to_flashinfer_trtllm` 根据 `quant_info` 类型分流到 FP8 / FP4 前向实现，统一注册为 `flashinfer_trtllm` fused func。  
- 在 `MoeRunner`、`MoeRunnerConfig` 初始化时加入对 `FLASHINFER_TRTLLM` 后端的适配。  

**🎯 影响范围**  
- **sglang/srt/layers/moe/moe_runner**：新增 `FlashInferTrtllm` 后端分支，修改 `MoeRunner` 初始化逻辑。  
- **sglang/srt/layers/quantization**：`fp8.py`、`modelopt_quant.py` 调整，以适配新的对齐函数和 `QuantInfo` 结构。  
- **sglang/srt/layers/moe/token_dispatcher**：`StandardCombineInput` 调用路径统一，原先直接使用层的 forward 被替换为统一的 fused func。  
- **依赖**：在没有 FlashInfer 时仍能 fallback 到原有实现；新增对 `is_flashinfer_available` 与 `is_sm120_supported` 的检测，保证在不满足条件的环境下使用 `sgl_kernel` 替代。  

**💡 关注建议**  
1. **兼容性检查**：  
   - 确认 `swap_w13_halves` 参数仅在 ModelOpt FP8 checkpoint（权重顺序为 Up‑Gate）中开启，防止对普通 FP8 检查点产生错误。  
   - `align_fp4_moe_weights_for_flashinfer_trtllm` 删除原始 `w13/w2` 权重后，确保下游仍有 `intermediate_size_per_partition` 等字段供后续调度使用。  

2. **条件导入安全**：  
   - 当前在 `flashinfer_trtllm.py` 顶部使用 `if is_flashinfer_available() and is_sm120_supported(): from flashinfer import fp4_quantize else: from sgl_kernel import scaled_fp4_quant as fp4_quantize`，建议加上 `try/except ImportError` 兜底，防止运行时因缺少 flashinfer 而报错。  

3. **单元/集成测试**：  
   - 为 FP8、FP4 两条路径各准备一次完整的 MoE 前向（top‑k=1、top‑k>1）覆盖，验证 `output` 与原实现数值一致（误差在 1e‑2 以内）。  
   - 加入跨 GPU/TP（tensor‑parallel）场景的对齐/shuffle 正确性检查。  

4. **文档/示例更新**：  
   - 在 README/使用指南中说明 “使用 ModelOpt FP8/FP4 checkpoint 时，需要开启 `swap_w13_halves=True`”。  
   - 给出 `MoeRunnerBackend.FLASHINFER_TRTLLM` 的初始化示例，说明对 `torch.cuda.get_device_capability()` 的最低要求（SM12.0+）。  

5. **性能关注**：  
   - `align_fp4_moe_weights_for_flashinfer_trtllm` 中的 `prepare_static_weights_for_trtllm_fp4_moe` 已在加载时完成，确保耗时只在模型加载阶段；若在多实例并行启动时仍出现 GPU 同步阻塞，可考虑使用 `torch.cuda.stream` 并行化。  

总体来看，此次重构将 FlashInfer‑TRTLLM 的权重处理抽象为可复用的工具函数，显著提升代码可维护性并加入对 FP4 量化的完整支持。只要通过上述兼容性与测试检查，即可在保持现有功能的同时，为 ModelOpt 用户提供更高效的 MoE 前向路径。

---

### Add triton_fused_moe config for GLM-4.7-FP8 tp8 H20 H20-3e (#18091)
**SHA**: `e484c90` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e484c90cc7aa3340b073be1d09e0731fb414c9ca)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/` 目录下新增了两套 JSON 配置文件，分别用于 GLM‑4.7‑FP8 在 NVIDIA H20（tp8）和 H20‑3e（tp8‑3e）硬件上的 Triton‑fused‑MoE 调度。文件中为不同规模的矩阵切分（1‑4096）提供了 `BLOCK_SIZE_M/N/K`、`GROUP_SIZE_M`、`num_warps`、`num_stages` 等调度参数，以实现 fp8_w8a8、per‑channel 量化的最优执行。

**🎯 影响范围**  
- `sglang.srt.layers.moe.fused_moe_triton` 读取配置的路径与注册逻辑。  
- 与 GLM‑4.7‑FP8、TP8 推理流水线相关的模型加载与运行时调度。  
- 文档/示例中展示支持的硬件列表与对应配置。

**💡 关注建议**  
1. **配置注册**：确认 `config_manager` 能自动发现并映射新文件的 `device_name`（`NVIDIA_H20`、`NVIDIA_H20-3e`）以及 `dtype`、`per_channel_quant` 标识，避免运行时找不到配置。  
2. **参数一致性**：检查不同切分大小的 `BLOCK_SIZE_*` 与硬件共享内存、warps 数的匹配关系，防止出现非法的 thread block 配置导致 kernel 启动失败。  
3. **回退机制**：若新配置在特定 GPU 驱动或 Triton 版本上不兼容，建议保留已有的默认配置并在日志中给出明确提示。  
4. **文档同步**：在 README/配置说明中补充对 “GLM‑4.7‑FP8 tp8 H20/H20‑3e” 的支持说明，包括对应的 `E,N`（expert/num_experts）取值范围。  
5. **测试覆盖**：加入针对这两套配置的单元/集成测试，验证 `fused_moe_triton` 在模拟 H20 环境下的正确调度（可使用 mock 层或 CI 中的 GPU 实例）。  

以上改进可确保新硬件的高效利用，同时降低因配置错误导致的部署风险。

---

### [Move sgl-kernel Kernel to JIT] Add JIT concat MLA kernels (#17889)
**SHA**: `9b1619c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9b1619c148a0a800e9ddb3d2e42ff32ee2dd2679)

**变更概述**  
本次 PR 将原先的 AOT `sgl-kernel` 实现迁移为 JIT 版本，并新增 **concat_mla** 系列的 JIT kernel、Python 包装、基准脚本以及单元测试。核心实现位于 `csrc/elementwise/concat_mla.cuh`，通过 `load_jit` 动态编译并缓存，提供 `concat_mla_k` 与 `concat_mla_absorb_q` 两个入口函数。

**影响范围**  
- **计算核心**：`sglang/jit_kernel/concat_mla.py`、`csrc/elementwise/concat_mla.cuh`（CUDA 代码）  
- **工具链**：`jit_kernel/utils.py` 中的 `cache_once`/`load_jit` 机制被复用。  
- **性能基准**：`benchmark/bench_concat_mla.py` 加入对 JIT、AOT 与 PyTorch 的对比。  
- **测试**：`tests/test_concat_mla.py` 覆盖了功能正确性与位级等价性。  
- **构建/发布**：新增 `.cuh` 文件需要在 `setup.cfg` / `MANIFEST.in` 中声明，否则 `pip install` 可能缺失。

**关注建议**  

1. **兼容性**  
   - 当前实现仅支持 `bf16`，若用户在不支持的显卡上运行会报错，建议在入口处加入显卡能力检查或提供 `float16` 回退。  
   - `load_jit` 依赖本地 `nvcc` 编译，CI 环境需确保 CUDA 编译链完整；可在 `pyproject.toml` 增加 `requires‑cuda` 的提示。

2. **内存对齐与非连续张量**  
   - `RuntimeCheck` 只检查指针对齐，未校验 stride 是否满足 kernel 假设（例如 `k_rope` 的第二维 stride 为 -1）。建议在 Python 包装层额外验证 `is_contiguous` 或提供 `torch.as_strided` 的安全转换。

3. **性能调优**  
   - `prefetch_L2` 按宏 `ENABLE_L2_PREFETCH` 控制，默认未开启。可在 CI/benchmark 中加入开启/关闭两套对比，以评估实际收益。  
   - `concat_mla_absorb_q` 采用 `int4`/`int` 读取，除非确保 `bf16` 按 16‑byte 对齐，否则可能触发未定义行为，建议在文档中明确要求 `torch.empty(..., dtype=torch.bfloat16).contiguous()`。

4. **代码组织**  
   - `concat_mla.py` 中的 `cache_once` 装饰器在多进程环境下可能产生竞争，建议在 `load_jit` 内部加文件锁或 `multiprocessing.get_context`。  
   - 对外 API `concat_mla_absorb_q` 返回 `torch.Tensor`，而测试中又使用 `out` 参数复制，两者风格不统一，建议统一为返回值或在文档说明两种用法。

5. **文档与发布**  
   - 新增的 JIT kernel 应在 `README.md` / `docs/` 中补充使用示例、参数解释以及对比 AOT 的优势。  
   - 确认 `MANIFEST.in` 包含 `csrc/elementwise/concat_mla.cuh`，避免源码分发缺失导致运行时找不到文件。

**结论**  
此次迁移为 SGLang 引入了 JIT 编译路径，提升了部署灵活性并提供了详细的基准与测试，整体风险可控。重点在于显卡兼容检查、张量布局验证以及发布流程的完整性，按上述建议完善后即可合并。

---

### [diffusion] UX: improve logging (#18122)
**SHA**: `62004fd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/62004fd2beb77daa10502e292e66a0d2f5662e3e)

**🎯 变更类型**：功能增强/UX 改进  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `gpu_worker` 中新增 `req.log(server_args=…)`，统一对每条请求进行日志记录。  
- 将多数 `logger.info` 调整为 `logger.debug`，降低常规运行时的日志噪声。  
- 移除 `ComposedPipelineBase.forward` 中的 `batch.log` 调用，并在 `latent_preparation` 中简化视频帧长度处理。  
- 重写 `logging_utils`：去掉 `NewLineFormatter`，改为 `ColoredFormatter` 直接继承 `logging.Formatter`，并在 `configure_logger` 中手动创建 `StreamHandler` 加颜色格式化器。  

**🎯 影响范围**  
- **核心运行时**：`gpu_worker.py`、`diffusers_pipeline.py`、`composed_pipeline_base.py`、`latent_preparation.py`。  
- **日志体系**：`logging_utils.py`（日志格式、颜色、默认级别）。  
- **多模态生成管线**：所有通过 `Req` 对象传递的请求日志记录行为。  

**💡 关注建议**  

1. **日志兼容性**  
   - 将 `info → debug` 的改动会导致在默认 `INFO` 级别下不再输出关键流水线信息。若外部依赖这些日志（如监控或审计），需在启动参数中显式打开 `DEBUG`。建议在文档或 `--log-level` 参数说明中提示。  
   - `req.log(server_args=…)` 的实现需确保 `Req` 类仍保留 `log` 方法且接受 `server_args`，否则会在 GPU 工作线程抛异常。  

2. **批次日志移除**  
   - `ComposedPipelineBase.forward` 删除 `batch.log`，可能导致对整体 batch 的统一日志（如 batch id、大小）不再自动出现。若其它模块仍依赖该日志，建议在 `gpu_worker` 或相邻阶段补充统一日志入口。  

3. **视频帧长度处理**  
   - `latent_preparation.adjust_video_length` 现在始终返回 `int`（`latent_num_frames`），去掉了原来的 `None` 情形。检查调用方（如后续 stage）是否仍假设返回值可能为 `None`，防止潜在 `AttributeError`。  

4. **日志格式器变更**  
   - 移除 `NewLineFormatter` 可能导致多行日志消息的前缀对齐消失，如异常堆栈会不再在每行前加时间前缀。若这是预期行为，可接受；否则可考虑在 `ColoredFormatter.format` 中自行实现换行前缀补齐。  
   - `configure_logger` 现在直接清空根处理器并添加自定义 `StreamHandler`，这会覆盖用户在外部自行添加的 handler（如文件日志）。如果项目需要支持外部自定义日志配置，建议提供 `add_handler` 参数或仅在 `root.handlers` 为空时才替换。  

5. **性能与可维护性**  
   - 使用 `debug` 级别的日志在非 Debug 场景下几乎不产生 I/O 开销，提升运行时性能。  
   - 代码改动相对集中，易于回滚；但请在 CI 中加入 `log_level=DEBUG` 的验证测试，确保调试日志在需要时完整。  

**总结**：本次提交主要通过精细化日志级别、统一请求日志入口以及简化日志格式，提高了用户在普通运行时的体验，同时保留了调试信息。注意检查 `Req.log` 的兼容性、批次统一日志的缺失以及多行日志的对齐需求，必要时在文档或代码中提供相应的开关或兼容实现。

---

### [HiCache]: Support DeepSeek v32 cpu offloading (#17415)
**SHA**: `1805943` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/180594358b990b2a5ce8140fb64aae90d73910fd)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 HiCache 新增 `NSATokenToKVPool`（NSA）对应的 Host 端实现 `NSATokenToKVPoolHost`，并在 `hiradix_cache` 中完成池类型的路由。  
2. `MemoryPoolHost` 通过 `override_kv_cache_dim` 让不同 KV 布局统一计算 token 大小，加入对 NSA 索引缓冲的专属管理（页面对齐、量化块大小等）。  
3. 新增单元测试 `test_nsa_pool_host_unit.py` 验证 CUDA/ROCm 环境下的 kernel 与 direct 两种拷贝路径。

**🎯 影响范围**  
- `sglang/srt/mem_cache/hiradix_cache.py` – 新增 NSA 适配分支。  
- `sglang/srt/mem_cache/memory_pool_host.py` – 大幅改造 KV 大小计算、布局初始化、索引缓冲分配及拷贝逻辑。  
- `sglang/srt/mem_cache/memory_pool.py` – 需要已有 `NSATokenToKVPool` 实现配合（已在项目中）。  
- 测试目录 `test/registered/hicache/` – 新增 NSA 主备传输测试。

**💡 关注建议**  
1. **兼容性检查**：`override_kv_cache_dim` 默认值使用 `kv_lora_rank + qk_rope_head_dim`，确认在不使用 NSA 时行为保持不变，避免因未显式传参导致维度错配。  
2. **内存对齐**：NSA 索引缓冲使用 `page_size` 对齐，建议在文档中说明 `indexer_page_stride_size` 必须 8‑字节对齐才能走 kernel 路径，否则自动回落。  
3. **多后端支持**：当前单元测试仅覆盖 CUDA/ROCm，若项目后续在 NPU/XPU 上使用 NSA，请补充相应 `is_npu/is_xpu` 分支的测试或在 `MemoryPoolHost.__init__` 中对 `pin_memory` 做更细粒度判断。  
4. **性能基准**：新增的索引层拷贝会额外占用显存和主机内存，建议在 CI 中加入对比基准，确认在典型 page_size（64）下的吞吐不出现明显回退。  
5. **清理与文档**：`hiradix_cache.py` 中多余的空行可删减；更新 `README`/`hicache` 文档，加入 “DeepSeek v32 CPU offloading” 与 “NSA HiCache” 的使用说明。  

总体而言，此次改动为 HiCache 引入了对 DeepSeek‑NSA（CPU offloading）场景的完整支持，代码结构清晰，单元测试覆盖关键路径。只需注意上述兼容性与文档细节，即可平滑上线。

---

### [Diffsuion & JIT_kernel] QKNorm cross heads kernel (#18073)
**SHA**: `a1bbc89` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a1bbc892af27867901f91e9a1c485824ff9337a6)

**🎯 变更类型**：功能增强（新增 QK‑Norm 跨 Head 融合算子）  
**⚡ 重要程度**：🟡 中（提升推理吞吐，但对已有代码向后兼容影响有限）  

**📋 变更摘要**  
本次 PR 为 sglang 添加了 “QKNorm‑Across‑Heads” 的 fused‑inplace 实现，主要包括：  
1. **CUDA‑C++ kernel** (`qknorm_across_heads.cuh`) 实现一次 kernel 同时完成 Q、K 两侧的 RMSNorm 与权重缩放，使用向量化读取、warp/CTA reduce 计算 `rsqrt`，并在共享内存中保存两个归约结果。  
2. **JIT 加载层** (`norm.py`) 新增 `_jit_qknorm_across_heads_module` 与 `fused_inplace_qknorm_across_heads` 接口，统一调度上述 kernel。  
3. **基准脚本** `bench_qknorm_across_heads.py` 对比 JIT、AOT、FlashInfer、纯 PyTorch 四种实现。  
4. **单元测试** `test_qknorm_across_heads.py` 验证 JIT 与 AOT（rmsnorm）在多批大小/隐藏维度下的数值一致性。  

**🎯 影响范围**  
- `sglang/jit_kernel` 子模块：新增 `norm.qknorm_across_heads`、benchmark 与 tests。  
- `sglang/jit_kernel/csrc/elementwise`：新增 CUDA kernel 文件。  
- 可能被 `sglang/srt` 或上层推理代码调用的 Q/K 归一化路径。  

**💡 关注建议**  

| 关注点 | 说明 | 改进建议 |
|--------|------|----------|
| **向量对齐与隐藏维度上限** | 运行时检查 `hidden_size % elements_in_vec == 0`，并在 `cc_major <=9 && hidden_size <=8192`、`cc_major >=10 && hidden_size <=12288` 条件下才启用。若用户传入不支持的尺寸会直接抛 `RuntimeCheck`。 | 在文档或异常信息中给出明确提示，或提供 fallback（调用 AOT 实现）避免程序崩溃。 |
| **共享内存使用** | 共享内存固定 64 floats（256 B），仅存放两组 warp‑reduce 结果，足够。但若未来扩展至多维度归约需重新评估。 | 维持当前实现即可，若改动 kernel 参数请同步更新 `shared_memory` 大小。 |
| **同步与流管理** | JIT 实现内部不涉及显式流切换；AOT 示例使用 `alt_stream` 实现 q/k 并行。两者行为在不同 CUDA 设备/驱动版本下应保持一致。 | 在 `fused_inplace_qknorm_across_heads` 文档中说明对流的期望，防止用户自行在外层加流导致竞争。 |
| **数值误差容忍度** | 单元测试使用 `atol=1e-2, rtol=1e-2`（bf16），符合预期。 | 若后续加入 fp16/fp32 支持，适当调低容差；同时可在 CI 中加入 `torch.compile` 版的对比，以捕获编译器差异。 |
| **代码可维护性** | kernel 使用大量模板与 `VecTypeTrait`，代码稍显冗长。 | 考虑抽取公共的 `reduce_sum_square` 与 `apply_rmsnorm` 子函数，提升可读性与后期扩展（如支持 int8 量化）。 |
| **文档与示例** | 新增 benchmark 与 test，但对外 API (`fused_inplace_qknorm_across_heads`) 的使用示例较少。 | 在 `docs` 或 `README` 中补充 Q/K 归一化的调用示例，注明 `eps` 与张量布局要求。 |

**总体结论**  
此次合并为 SGLang 引入了高效的跨 Head QK‑Norm 融合算子，能够显著降低两次 RMSNorm 的显存带宽占用和 kernel 启动开销。实现思路清晰、向量化与 warp/CTA 双重归约均已合理利用；单元测试与基准脚本覆盖了常见的 batch/hidden‑dim 组合，验证了数值正确性。后续关注兼容性（不满足对齐条件时的回退）以及文档说明即可完成闭环。

---

### [Performance] Optimize radix cache eviction performance (#14339)
**SHA**: `fd983b0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fd983b09b68c076d448a55266a29ffdfdff2d06e)

**🎯 变更类型**：性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 radix 系列缓存（`hiradix_cache.py、radix_cache.py、mamba_radix_cache.py、swa_radix_cache.py`）新增 **evictable leaf 集合**（`evictable_leaves`、`evictable_host_leaves`），在插入、删除、锁引用增减等路径上即时维护，取代原先在 `evict` 时遍历整棵树的 `_collect_leaves*` 实现。  
- 通过 `inc_lock_ref/dec_lock_ref` 与 `_update_leaf_status/_update_host_leaf_status` 将可驱逐状态与锁计数同步，显著降低驱逐路径的时间复杂度。  

**🎯 影响范围**  
- `python/sglang/srt/mem_cache/` 下的四个缓存实现。  
- 相关的 `CacheController`、`EvictionStrategy`、以及调用这些缓存的推理调度逻辑。  

**💡 关注建议**  
1. **一致性检查**：确认在所有可能的结构变更点（如 `insert`、`_evict_*`、`_delete_leaf`、`_delete_tombstone_leaf`）均已调用 `_update_leaf_status`（或 `_update_host_leaf_status`），防止集合出现遗漏导致误驱逐或漏驱逐。  
2. **线程安全**：集合的增删在多线程/多进程环境下未加锁，若缓存被并发访问，需要评估是否需要 `threading.Lock` 或 `atomic` 操作。  
3. **资源释放**：`reset` 与 `shutdown` 已清空集合，但请确保异常路径（如提前 `raise`）也能触发清理，防止内存泄漏。  
4. **回归测试**：跑完整的功能与性能基准，特别是高并发请求、锁引用频繁的场景，验证 evictable size、protected size 与实际缓存容量的一致性。  
5. **文档更新**：在 README 或代码注释中说明新集合的语义及维护规则，方便后续维护者了解变更点。  

总体而言，此次改动通过 O(1) 维护可驱逐叶子集合，显著提升大模型 KV‑Cache 驱逐的吞吐，注意上述一致性与并发安全即可平稳上线。

---

### feature: adding gpt-oss 120b nightly test (#18134)
**SHA**: `c8da307` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c8da307d7e6353fdcfdf6ca1ecb1cec854411230)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 nightly‑8‑gpu‑common CI 中将分区数量从 3 改为 4，以容纳新增的 GPT‑OSS‑120B 测试；新增 `test_gpt_oss_120b.py`，对 MXFP4 与 EAGLE3 两种变体进行性能/准确性验证。  
**🎯 影响范围**：`.github/workflows/nightly-test-nvidia.yml`、`test/registered/8-gpu-models/`、CI runner（GPU 资源、超时设置）。  

**💡 关注建议**  
1. **CI 资源**：分区数增至 4、`--auto-partition-size=4`，需确认 H200/B200 机器的 GPU/显存能够支撑并行运行，防止 OOM。可在工作流中加入 `continue-on-error` 或 `if: success()` 的容错逻辑。  
2. **超时/估算**：`register_cuda_ci(est_time=1800)` 与 `--timeout-per-file=18000`（5 h）不匹配，建议统一为实际耗时的上限，避免 CI 被误判为超时。  
3. **模型路径**：`openai/gpt-oss-120b`、`lmsys/EAGLE3-gpt-oss-120b-bf16` 必须在镜像中预拉取或可在线下载，防止因网络导致 CI 失效。  
4. **环境变量**：EAGLE3 变体使用的 `SGLANG_ENABLE_SPEC_V2`、`SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN` 需在 CI 镜像中声明并在文档中说明。  
5. **性能目录**：`performance_profiles_gpt_oss_120b` 目录应在 CI 前创建或在代码中自动创建，避免因路径不存在导致写入错误。  

总体而言，本次改动为高容量模型提供了 nightly 验证，但需要在 CI 环境上做好显存、超时和模型拉取的防护，以免影响已有流水线的稳定性。

---

### Fix HF hub race condition in CI by coordinating model downloads across TP ranks (#17787)
**SHA**: `28e2340` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/28e23407259d665ef92ec0a19b7fe4e5c60234ff)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中（解决 CI 中偶发的下载冲突，提升稳定性）  

**📋 变更摘要**  
1. 将 `.github/workflows/pr-test.yml` 中所有 “Install dependencies” 步骤的超时时间从 10 min 提升至 20 min（部分步骤仍保留 30 min），防止因 HuggingFace 大模型下载缓慢导致 CI 误杀。  
2. 在 `python/sglang/srt/model_loader/ci_weight_validation.py` 中实现 **文件锁**（`fcntl.flock`）以统一多进程（`mp.Process`、`torchrun` 等）在 CI 环境下的模型下载。  
   - 新增 `_get_lock_file_path` 为每个模型生成共享 lock 文件，优先使用 `/dev/shm`，不存在时回退 `/tmp`。  
   - `ci_download_with_validation_and_retry` 先获取独占锁，再执行 `snapshot_download`（强制 `max_workers=1`），下载完成后进行权重校验并在必要时重试。  
   - 在 finally 中确保锁被释放并关闭文件。  

**🎯 影响范围**  
- **CI 流水线**：所有使用 `ci_download_with_validation_and_retry` 的步骤（模型权重下载、权重校验）将受文件锁保护，避免 `.incomplete` 冲突。  
- **模型加载模块**：`sglang/srt/model_loader` 在 CI 环境下的行为改变；在本地或非 Linux 环境（缺少 `fcntl`）可能出现兼容性问题。  
- **CI 超时**：部分作业运行时长可能增加，需留意资源占用。  

**💡 关注建议**  

1. **跨平台兼容**  
   - `fcntl.flock` 仅在类 Unix 系统上可用。若项目在 Windows CI 或本地运行，建议在导入时捕获 `ImportError` 并回退为无锁或使用 `portalocker` 等跨平台实现。  

2. **锁文件清理**  
   - 当前实现仅在进程退出时释放锁，未删除 lock 文件。长期运行的 CI 机器可能留下大量 `sglang_download_lock_*`。可以在 `finally` 中 `os.remove(lock_file_path)`，并在异常路径确保不误删正在使用的文件。  

3. **锁粒度**  
   - 锁基于模型名称的 hash，忽略 `cache_dir`。若同一模型在不同缓存目录下被下载（如不同 runner 的自定义 `CACHE_DIR`），仍会串行下载，这可能不符合预期。请在文档说明锁的作用范围及推荐的 `cache_dir` 配置。  

4. **超时调整**  
   - 将安装依赖的超时统一加倍是对偶发慢速下载的“治标”。建议在后续通过更细粒度的缓存或镜像（如使用国内镜像）进一步降低下载耗时，从根源降低超时风险。  

5. **测试覆盖**  
   - 增加单元测试：模拟多个进程并发调用 `ci_download_with_validation_and_retry`，验证只有一个实际触发 `snapshot_download`，其余进程能够复用缓存并正常返回。  
   - 在 CI 中加入对 lock 文件路径的可见性日志，以便排查死锁或锁未释放的情况。  

6. **文档更新**  
   - 在 `README` 或 CI 文档中注明 “CI 下载已加入文件锁，Linux 环境下需保证 `/dev/shm` 可写”，以及在 Windows 环境可能需要手动禁用此特性。  

综上，此次改动通过文件锁有效规避了 HuggingFace Hub 在多进程 CI 场景下的竞争条件，并通过延长依赖安装超时提升了整体 CI 稳定性。重点关注跨平台兼容、锁文件清理以及在本地开发环境的行为一致性，配合相应的测试与文档即可确保改动安全落地。

---

### [Fix] data race in req_to_token pool (#17850)
**SHA**: `027f314` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/027f314050cc89a0e4d770b00aa901f23cdc3b8d)

**🎯 变更类型**：Bug 修复（data‑race）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 `ReqToTokenPool` 的分配/释放接口从直接使用整数索引改为接受/返回 `Req` 实例，统一在 `Req` 上维护 `req_pool_idx`（以及在 Hybrid 场景下的 `mamba_pool_idx`），并在所有调度、缓存、预填、解码等模块中同步更新调用方式，消除原先并发释放导致的索引竞争。  

**🎯 影响范围**：  
- `python/sglang/srt/disaggregation/*`（decode、prefill）  
- `python/sglang/srt/managers/*`（scheduler、scheduler_pp_mixin）  
- `python/sglang/srt/mem_cache/*`（Hybrid、Radix、Mamba、SWA、common）  
- 相关测试 `test/registered/radix_cache/test_mamba_unittest.py`  

**💡 关注建议**  
1. **并发安全**：新实现把 `free` 逻辑集中到 `Req`，确认所有线程在释放前已正确设置 `req_pool_idx`，避免残留的旧 `free(idx)` 调用。  
2. **兼容性**：若外部代码仍依赖旧 `alloc(num, reqs)` 或 `free(idx)`，建议提供薄包装或保持向后兼容的别名；目前项目内部已全部迁移。  
3. **Chunked 请求限制**：新增断言确保同批只能有一个已分配 `req_pool_idx` 的 chunked 请求，测试覆盖该路径，防止误用导致索引冲突。  
4. **Mamba 状态释放**：在 `HybridReqToTokenPool` 中将 Mamba 缓存的释放抽离为 `free_mamba_cache`，并在 `release_kv_cache`、`cache_finished_req` 等入口统一调用，注意 `supports_mamba` 为 False 时仍要手动释放。  
5. **回归验证**：运行完整测试套件，尤其是 `radix_cache`、`mamba` 相关单元；在高并发负载（多 GPU、pipeline）下监测 `req_pool_idx` 是否出现 `None` 或重复分配的异常。  

总体而言，此次改动通过在对象层面管理索引，解决了原先的竞争风险，提升了调度与缓存的鲁棒性。后续关注并发路径的完整覆盖和文档/API 的同步更新即可。

---

### [MiMoV2Flash] [feat]: support two batch overlap (#17634)
**SHA**: `cbf1500` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cbf15003903761c9b2f652e5185858545944e9c9)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 MiMoV2Flash 引入 “两批次重叠”(Two‑Batch‑Overlap, TBO) 支持。新增 `MiMoV2DecoderLayer` 的操作策略，并在模型前向流程中接入 `model_forward_maybe_tbo`，实现了 `op_gate、op_select_experts、op_dispatch_a/b、op_experts、op_combine_a/b、op_output` 等细粒度算子以及通信准备/后处理的显式状态管理。  

**🎯 影响范围**  
- `sglang/srt/batch_overlap/operations_strategy.py`（新增 MiMoV2 的策略实现）  
- `sglang/srt/models/mimo_v2_flash.py`（大量新算子、TBO 调度逻辑、工具函数 `is_non_idle_and_non_empty` 的使用）  
- 相关 util：`sglang/srt/utils`（导入 `is_non_idle_and_non_empty`）  
- 可能涉及 `sglang/srt/batch_overlap/two_batch_overlap.py`（`model_forward_maybe_tbo` 调用）  

**💡 关注建议**  
1. **功能验证**：在 prefill、decode、target‑verify 三种 `ForwardMode` 下跑齐全的单元/集成测试，特别是 TBO 开关打开时的跨层状态 (`tbo_subbatch_index`) 是否正确传递。  
2. **性能基准**：对比开启/关闭 TBO 的吞吐与延迟，确保新增 `YieldOperation`、多次 `state.pop` 不引入额外同步开销。  
3. **异常路径**：`_compute_moe_mimov2_layer_operations_strategy_tbo` 中的 `assert layer.is_layer_sparse`、`raise NotImplementedError` 需要在配置或未来模型上保持兼容，防止在非稀疏层触发异常。  
4. **代码可维护性**：当前策略标记为 “unstable”，建议在后续提交中抽象公共部分（如 DeepSeek‑compatible 部分），并在文档中说明何时使用该实现。  
5. **用户提示**：在启动服务器时检查 `forward_batch.can_run_tbo`，若环境不满足（如显存不足、ep_size=1）应给出明确警告，避免默默降级导致误解。  

整体而言，本次改动为 MiMoV2Flash 带来了更高的并行利用率，但也引入了显式的状态管理与多阶段调度，务必通过充分的测试与基准验证其正确性与收益。

---

### [NPU] support dsv32 radixcache on ascend (#17964)
**SHA**: `b0a6d52` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b0a6d5244cef6a9ac74182dda0f45ae4192d0837)

**🎯 变更类型**：功能增强（为 Ascend NPU 添加 dsv32 radix‑cache 支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `ascend_backend.py` 与 `nsa_indexer.py` 中，将原先使用 `seq_lens.cumsum` 的逻辑改为基于新增的 `extend_seq_lens`，以兼容 dsv32 的扩展序列长度。  
2. `memory_pool_npu.py` 与 `memory_pool_host.py` 新增 `index_k_buffer`（以及对应的 host/device 传输），为 radix‑cache 提供索引缓存。  
3. 相应的加载、备份流程加入 `device_index_k / host_index_k` 参数，确保索引在 H2D / D2H 之间同步。  

**🎯 影响范围**  
- NPU（Ascend）后端的注意力实现 (`python/sglang/srt/hardware_backend/npu/...`)  
- NSA 索引器 (`python/sglang/srt/layers/attention/nsa/nsa_indexer.py`)  
- NPU‑专用内存池及其 Host 侧实现 (`memory_pool_npu.py`、`memory_pool_host.py`)  

**💡 关注建议**  
- **兼容性**：`extend_seq_lens` 仅在 Ascend 场景出现，需确认其他硬件路径仍能安全回退到 `seq_lens`，避免 `AttributeError`。  
- **初始化与释放**：`index_k_buffer` 可能为 `None`，所有使用点（如转移、清理）应加入 `if buffer is not None` 检查。  
- **形状/类型校验**：加入断言或日志，确保 `index_head_dim` 与 KV 缓冲区的维度匹配，防止隐藏的维度错位。  
- **性能验证**：新增的 radix‑cache 预计提升稀疏注意力的访问局部性，请在真实 Ascend 设备上跑一次大 batch 的基准测试，确认没有额外的内存拷贝开销。  
- **单元测试**：补充针对 `forward_sparse`、`forward_npu` 两条路径的单元测试，覆盖 `actual_seq_lengths_q` 与 `actual_seq_lengths_kv` 使用 `extend_seq_lens` 的情况。  
- **文档**：在硬件后端说明中标记 “Ascend dsv32 radix‑cache 支持”，并解释 `extend_seq_lens`、`index_k_buffer` 的意义，以方便后续维护。  

总体而言，此次改动为 Ascend NPU 引入了必要的索引缓存，提升稀疏注意力的执行效率，但需要完整的兼容性检查和测试保障。

---

### [DeepSeek V3.2] [Bugfix] slice indexer and padding fa3 when can not run cuda graph (#17076)
**SHA**: `677f3c4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/677f3c49dabff37eb1d31827de15347ac06f5864)

**🎯 变更类型**：Bug修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 NSA（Non‑Sparse Attention）索引器新增 `get_nsa_extend_len_cpu` 接口，向上层暴露每个 batch 的扩展序列长度。  
2. 在 `_get_topk_paged` 中依据该长度计算真实的 query 长度 `q_offset`，截取 `q_fp8`、`weights`，避免因跨 TP 或 MAX_LEN 填充导致的越界或无效计算。  
3. 计算完 logits 后若有填充值，使用 `-1` 进行补齐，保持原始 batch 大小不变。  
4. 新增 `cal_padded_tokens` 工具函数，统一在 DP/TP 环境下计算实际需要的 token 数，并在 `pad_nsa_cache_seqlens` 中使用该值进行正确的 padding。  
5. 相应后端 `nsa_backend` 和单元测试补全新接口实现。

**🎯 影响范围**  
- `python/sglang/srt/layers/attention/nsa/*`（索引器、工具、后端）  
- 依赖 `IndexMetadata`（需新增 `nsa_extend_seq_lens_list`）  
- 单元测试 `test_nsa_indexer.py`  

**💡 关注建议**  
1. **数据来源**：确认 `IndexMetadata.nsa_extend_seq_lens_list` 在所有生成路径（prefill、decode）中已正确填充，否则 `q_offset` 可能为 0 导致意外截断。  
2. **兼容性**：非 CUDA（HIP）路径仍使用旧切片逻辑，保持一致性；若以后统一到同一实现，需要验证 HIP 下的填充恢复行为。  
3. **后处理**：`-1` 填充值在后续 top‑k 处理或采样阶段必须被正确过滤，避免误判为合法 token。建议在使用 `topk_result` 前加入显式掩码。  
4. **性能**：新增切片与拼接在极大 batch 时会产生额外的内存拷贝，建议在 profiling 中关注此路径的时延。  
5. **文档/类型**：更新 `IndexMetadata`、`ForwardBatch` 的类型注解和说明，防止其他模块遗漏实现新方法。  

整体来看，此次修改解决了在多 GPU/TP 环境下因 padding 引起的非法索引问题，提升了 CUDA‑graph 的可运行性。只要确保扩展长度的正确传递和后续对 `-1` 的过滤，改动对现有功能的影响应限于性能微调。

---

#### 🟢 低重要度变更 (8)

### [PD] feat: support mooncake intra-node nvlink kv transfer (#17866)
**SHA**: `a45647b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a45647bce16ddf159713a65c3b281c7e66700bcc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `mooncake` 的内存池支持列表中新增 `INTRA_NVLINK`，并在初始化逻辑和分配器选择中加入对应分支；在通用工具中依据环境变量 `SGLANG_MOONCAKE_CUSTOM_MEM_POOL` 选择 `cuda` 设备，以实现节点内 NVLink KV 传输的功能。

---

### Warmup before profiling prefill latency for dynamic chunk sizing (#17198)
**SHA**: `cc69ac9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cc69ac9e7a7d94fac0b94b8d4d19bf62af69df15)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fit` 方法中加入注释并跳过首条数据，以消除首次运行慢导致的拟合偏差。

---

### [Docker] Remove hardcoded `America/Los_Angeles` timezone, default to UTC (#18121)
**SHA**: `25508d1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/25508d11c03cb1344a1cffc8e4e9e517181388f0)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 Dockerfile 中对 `tzdata` 的硬编码时区设置，改为默认使用 UTC，同时在基础镜像和 gateway 镜像中去除 `tzdata` 包的安装。这样简化镜像构建并避免不必要的时区依赖。

---

### [Perf] Use safetensors `load_file` in multithread loader (#18124)
**SHA**: `6f6b9c6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6f6b9c6e42a927f087276536f225147eb8507714)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在多线程加载器中改用 `safetensors.torch.load_file` 直接读取文件（并支持 `disable_mmap`），简化代码并删除手动 mmap 逻辑。

---

### [HiCache] fix: apply extra_backend_tag in Mooncake batch_exists (#17265)
**SHA**: `7a9d9c7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7a9d9c79d15dfa7c2c1ff9685ea7fba3d5c599ad)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `MooncakeStore.batch_exists` 中新增对 `extra_backend_tag` 前缀的处理，使批量存在性检查支持额外后端标签。

---

### [NVIDIA] Add --top-k argument to run_eval.py (#18025)
**SHA**: `4181290` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4181290efd668082109055c53261a5c956817caa)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `run_eval.py` 添加 `--top-k` 参数，并在调用 `ChatCompletionSampler` 时通过 `extra_body` 传递该值，实现 top‑k 采样支持。

---

### [Diffusion] fix serving image_edit get input image bug (#18109)
**SHA**: `eedd472` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/eedd472025c4f3d0247c5ef4f05dce54663a09c9)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `diffusers_pipeline.py` 中，新增对 `batch.image_path` 为列表的兼容处理，取首个路径后继续加载图片，修复了 Image‑Edit 服务获取输入图像时的错误。

---

### Re-enable test_mla_int8_deepseek_v3.py after HF token fix (#18123)
**SHA**: `812fd47` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/812fd47cb48925e3c3216e7a04169d0d55921b54)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `test_mla_int8_deepseek_v3.py` 中的 `register_cuda_ci` 调用的 `disabled` 参数移除，重新启用该 INT8 量化测试。

---

