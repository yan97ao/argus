# 每日更新报告（2026-02-19）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-19 23:27:23 | Mick | [diffusion] CI: enable warmup as default (#19010) |
| 2026-02-19 21:59:49 | Mick | [diffusion] chore: improve memory usage on consumer-level GPU (#18997) |
| 2026-02-19 21:03:19 | satyamk7054 | Move lora request validation to tokenizer_manager from server (#18962) |
| 2026-02-19 20:33:51 | Makcum888e | [Diffusion] [NPU] Enable profiler on NPU (#17807) |
| 2026-02-19 20:12:24 | Prozac614 | [diffusion] fix: fix rank used in parallel executor when enable_cfg_parallel is false (#18975) |
| 2026-02-19 17:03:44 | Xiaoyu Zhang | [diffusion] refactor: refactor diffusion triton kernels (#18966) |
| 2026-02-19 17:03:15 | pansicheng | [RadixTree][4/N Refactor]: Move available_and_evictable_str to individual radix cache classes (#17852) |
| 2026-02-19 16:56:06 | shaharmor98 | Feat/add fi selective state update kernel call (#18070) |
| 2026-02-19 16:48:06 | Yuwei An | Fix PCG MoE Error (#17739) |
| 2026-02-19 12:24:05 | hlu1 | [Qwen3.5] Enable nvfp4 checkpoint (#18937) |
| 2026-02-19 08:31:02 | hxie | Add batched zero copy to NIXL backend (#18850) |
| 2026-02-19 08:23:17 | Bingxu Chen | [AMD] Fix mi35x dsv32 mtp nightly (#18978) |
| 2026-02-19 07:55:16 | Alison Shao | Fix flaky Qwen3-Next KL divergence tests by reverting mamba slot release (#18910) |
| 2026-02-19 03:45:47 | Mohammad Miadh Angkad | [Doc] Add `flashinfer_deepgemm` to `--fp8-gemm-backend` (#18982) |
| 2026-02-19 03:24:07 | Mengyang Liu | [Feature] Implement update_weights_from_disk for SGLang-D (Diffusion … (#18306) |
| 2026-02-19 00:44:30 | Tamir Baydasov | [4/N] Quantization Refactor: Quark MoE schemes (#18252) |
| 2026-02-19 00:34:51 | Ethan (Yusheng) Su | [Fix] Add lora tied lm head support (for Qwen2.5, Gemma, etc model need) (#18634) |

### 📊 统计摘要
> 本日共 17 个提交 | 🔴高 3 | 🟡中 8 | 🟢低 6
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[diffusion] refactor: refactor diffusion triton kernels (...](#19aa19b)
    - [[Feature] Implement update_weights_from_disk for SGLang-D...](#4f980f6)
    - [[4/N] Quantization Refactor: Quark MoE schemes (#18252)](#150ed88)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [[diffusion] CI: enable warmup as default (#19010)](#3207427)
    - [Move lora request validation to tokenizer_manager from se...](#963def7)
    - [[RadixTree][4/N Refactor]: Move available_and_evictable_s...](#48642d5)
    - [Feat/add fi selective state update kernel call (#18070)](#82a0baf)
    - [Fix PCG MoE Error (#17739)](#0be30d4)
    - [[Qwen3.5] Enable nvfp4 checkpoint (#18937)](#bba2fc4)
    - [Add batched zero copy to NIXL backend (#18850)](#443b1a8)
    - [[Fix] Add lora tied lm head support (for Qwen2.5, Gemma, ...](#9c5aae4)
  - [🟢 低重要度变更 (6)](#-🟢-低重要度变更-6)
    - [[diffusion] chore: improve memory usage on consumer-level...](#d73f06f)
    - [[Diffusion] [NPU] Enable profiler on NPU (#17807)](#d07e8aa)
    - [[diffusion] fix: fix rank used in parallel executor when ...](#e21fc78)
    - [[AMD] Fix mi35x dsv32 mtp nightly (#18978)](#4622679)
    - [Fix flaky Qwen3-Next KL divergence tests by reverting mam...](#e2fccb2)
    - [[Doc] Add `flashinfer_deepgemm` to `--fp8-gemm-backend` (...](#2f592c3)
#### 🔴 高重要度变更 (3)

### [diffusion] refactor: refactor diffusion triton kernels (#18966)
**SHA**: `19aa19b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/19aa19b1113e905c754e6496aaf225f99e0390de)

**🎯 变更类型**：重构 / 性能优化  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 将原本庞大的 `python/sglang/jit_kernel/diffusion/triton/norm.py` 拆分为专职的子模块：`rotary.py`、`scale_shift.py`、`rmsnorm_onepass.py` 与 `npu_fallback.py`。  
- 为 NPU（Ascend）平台提供纯 PyTorch 回退实现，避免因 Triton 在 NPU 上的已知 bug 导致运行错误。  
- 引入单次遍历的 RMSNorm 实现 (`triton_one_pass_rms_norm`) 以降低内存访问次数，提升算子吞吐。  
- 更新运行时层（`elementwise.py`、`layernorm.py`、`rotary_embedding.py`、`qwen_image.py`）的导入路径，使其指向新模块。

**🎯 影响范围**  
- **核心算子**：scale‑shift、rotary embedding、RMSNorm（两种实现）  
- **平台适配**：新增 NPU 回退路径  
- **模型实现**：DiT 系列（`qwen_image.py`）以及所有使用 `MulAdd`、`LayerNorm`、`RotaryEmbedding` 的模块  
- **构建/部署**：依赖 `triton` 的自动调优配置保持不变，Python 包层级结构发生变化  

**🔍 技术洞察**  

- **架构影响**  
  - **模块化**：原先的单文件实现混杂多种 kernel，拆分后每个文件职责单一，便于单元测试、代码审查及未来功能扩展。  
  - **平台抽象**：通过 `npu_fallback.py` 将 NPU 专用回退实现解耦，避免在主 kernel 中散布 `if current_platform.is_npu(): …` 的分支逻辑，提升代码可读性。  
  - **导入统一**：所有 runtime 层均改为 `sglang.jit_kernel.diffusion.triton.*`，对外保持同一种 API，降低因路径变更导致的隐藏错误风险。  

- **性能影响**  
  - **RMSNorm 单遍历**：`triton_one_pass_rms_norm` 在一次遍历中完成均值平方、rsqrt 与 scaling，较原先两次遍历实现 (`triton_one_pass_rms_norm` existed but was previously embedded in `norm.py`) 减少了全局内存读取，预计在长序列（> 1k）上提升 10%~20% 的吞吐。  
  - **Scale‑Shift / Gate‑Select Kernel**：功能保持不变，拆分后编译缓存更细粒度，首次调用时可能略有编译开销但随后可重复使用。  
  - **NPU 回退**：在 Ascend 上转为纯 PyTorch 实现，虽然牺牲了 Triton 的加速，但能够保证模型在 NPU 上不崩溃，提供了可用的最低性能基线。  

- **安全考虑**  
  - 代码仅涉及数值运算和平台检测，没有引入外部 I/O、文件访问或网络请求，安全风险基本为 **无**。  
  - 唯一需要关注的是 `current_platform.is_npu()` 的实现可靠性，确保在非 NPU 环境不会误判导致回退执行，进而产生性能退化或隐藏的数值差异。  

- **可维护性**  
  - 新增的 `__all__` 或显式导出未提供，若未来外部直接 `from sglang.jit_kernel.diffusion.triton import *` 可能暴露内部实现细节。建议在每个子模块添加 `__all__` 限定公开接口。  
  - `triton.autotune` 配置保持不变，但拆分后每个 kernel 的 autotune 参数分别存在，需在 CI 中验证所有配置均能在主流 GPU（A100、RTX 4090）上编译通过。  

**⚠️ 潜在风险**  

1. **导入遗漏**  
   - 只有显式列出的四个运行时层被更新，项目中若还有其他模块仍引用旧路径（如 `sglang.multimodal_gen.runtime.layers.triton_ops`），会导致 `ImportError`。  
2. **NPU 回退数值差异**  
   - 回退实现使用 `x * (1 + scale) + shift`，与 Triton kernel 中的 `scale_constant`（默认为 1）保持一致，但若未来在 NPU 上引入其他 `scale_constant`（如 0），回退路径需同步更新。  
3. **Triton 兼容性**  
   - 新增 `rmsnorm_onepass.py` 中使用 `torch.get_device_module().device(x.device)`（原文为 `torch.library.wrap_triton`），如果目标环境的 Torch 版本不支持 `get_device_module`，可能抛出属性错误。  
4. **编译缓存**  
   - 拆分后每个 kernel 的编译缓存路径不同，可能在多进程训练（如 `torch.distributed`) 中出现 “kernel already defined” 的警告，需要确保 `torch.cuda.set_device` 前后保持一致。  

**💡 关注建议**  

1. **回归测试**  
   - 在 GPU、CPU、以及 NPU 三个平台跑完整的模型推理/训练基准，验证数值一致性（尤其是 RMSNorm 以及 gate‑select 分支）。  
2. **CI 扩展**  
   - 增加对 `sglang.jit_kernel.diffusion.triton.*` 模块的单元测试，确保每个 kernel 能在不同硬件上成功编译并产生预期输出。  
3. **文档同步**  
   - 更新开发者文档，说明新模块的 import 路径以及 NPU 回退的使用场景，避免新人在阅读旧文档时产生困惑。  
4. **显式导出**  
   - 为每个子模块添加 `__all__ = [...]`，限制外部仅能访问 `fuse_scale_shift_kernel`、`apply_rotary_embedding`、`triton_one_pass_rms_norm` 等公共 API。  
5. **兼容层**  
   - 考虑保留一个轻量的兼容层（例如 `sglang.multimodal_gen.runtime.layers.triton_ops`）内部仅做一次性 `from sglang.jit_kernel.diffusion.triton.xxx import *`，以免遗漏旧代码路径。  

---  

---

### [Feature] Implement update_weights_from_disk for SGLang-D (Diffusion … (#18306)
**SHA**: `4f980f6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4f980f6f233f7fff264006c2db5a7467cdb15a01)

**🎯 变更类型**：功能增强 / 重构 / 安全修复  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 为 SGLang‑Diffusion（SGLang‑D）实现了 `POST /update_weights_from_disk` 接口，支持在不重启服务器的情况下直接从磁盘加载新权重。  
2. 新增 **全量‑回滚**、**offload‑aware**、**DTensor‑aware** 三大机制，保证在层级显存 offload 与分布式张量并行环境下仍能安全、原子地完成权重更新。  
3. 同时提供 `GET /get_weights_checksum` 用于基于 SHA‑256 的权重校验，配套实现 `compute_weights_checksum`、`iter_materialized_weights` 等工具函数，并在文档、HTTP server、调度器、GPU worker 中完成集成。  
4. 大量新增单元与集成测试（包括 CI 随机模型对、offload 场景、损坏权重回滚），确保 API 的正确性与鲁棒性。  

**🎯 影响范围**  
- **文档**：`docs/advanced_features/sglang_for_rl.md`（新增 API 说明、请求/响应格式）。  
- **HTTP Server**：`http_server.py`、`post_training` 包（路由注册、请求/响应结构）。  
- **调度器 & GPU worker**：`scheduler.py`、`gpu_worker.py`（调度请求、权重校验、回滚逻辑）。  
- **加载与更新模块**：`weight_utils.py`、`weights_updater.py`、`layerwise_offload.py`（权重校验、offload 更新、DTensor 分发、迭代器）。  
- **测试套件**：`test_update_weights_from_disk.py`、相关 CI 调整。  

**🔍 技术洞察**  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • 新增 **post‑training** 子系统，形成 “调度器 ⇆ GPU worker ⇆ WeightsUpdater” 三层调用链。<br>• `WeightsUpdater` 负责收集可更新的 `nn.Module`，在 **layerwise offload** 与 **DTensor** 环境下分别走不同写入路径，保持与原有 **ModelRunner.update_weights_from_disk** 行为一致。<br>• 通过 `async_scheduler_client.forward(req)` 把请求转发给调度器，使得权重更新仍走统一的调度路径，兼容已有的 “RL 交互” 工作流。 |
| **性能影响** | • **IO**：首次加载权重会读取所有 safetensors 文件，可占用数 GB 磁盘 IO，但仅在调用时触发，对常规推理无影响。<br>• **显存**：更新过程不产生额外的 GPU 张量拷贝（采用直接写入 offload 管理器的 CPU 缓冲区），仅在 **prefetched** 层会执行一次 `param.data.copy_`，内存波动 ≤ 1 GB。<br>• **吞吐**：在单请求调度模型（SGLang‑D 当前实现的 scheduler 为串行）下，权重更新期间阻塞后续请求，符合设计预期；对并发场景需在未来实现 “hot‑refit”。 |
| **安全考虑** | • 接口直接接受任意文件系统路径 `model_path`，若服务器暴露在不可信网络，攻击者可尝试加载恶意模型导致代码执行或资源耗尽（Safetensors 虽有安全校验，但仍可能触发 OOM）。<br>• 回滚失败会抛出异常，可能把服务置于 **不一致** 状态。<br>• `flush_cache` 默认开启，会清空 TeaCache，若缓存中存有敏感信息，需确保不泄露。 |
| **可维护性** | • 通过 `dataclass` 定义请求/响应结构，代码可读性提升。<br>• 权重更新逻辑全部抽象到 `WeightsUpdater`，后续可以在 LLM 引擎或其它管线复用。<br>• 增加了大量端到端测试（包括 offload、损坏文件、回滚），降低后续改动回归风险。 |

**⚠️ 潜在风险**  

1. **回滚不完整**：若在回滚阶段再次触发 IO 错误或显存 OOM，异常会向上抛出，导致服务器进入不可用状态。  
2. **offload 管理器状态不一致**：`update_cpu_weights` 只在 `enabled` 的 manager 上操作，若某层在更新前已被手动 disabled，可能出现“局部更新 + 全量回滚”不匹配的情况。  
3. **DTensor 分片错误**：`distribute_tensor` 依赖当前 `device_mesh` 与 `placements`，若模型在更新前后重新初始化了分片策略（例如混合并行改动），可能导致写入错误的 shard。  
4. **并发请求冲突**：虽然调度器当前是串行处理，但未来如果开启并行调度，两个并发的 `update_weights_from_disk` 可能相互覆盖，导致不可预知的状态。  
5. **路径泄露**：`model_path` 直接暴露在请求体中，日志若未脱敏可能泄露磁盘结构信息。  
6. **资源泄漏**：`WeightsUpdater.update_weights_from_disk` 结束后仅调用 `gc.collect()` 与 `torch.cuda.empty_cache()`，若权重文件极大或频繁更新，仍可能出现显存碎片。  

**💡 关注建议**  

| 建议 | 说明 |
|------|------|
| **请求校验与授权** | - 对 `model_path` 做白名单或安全根目录限制（如只能位于 `~/sglang_models/` 下面）。<br>- 在生产环境加入 API‑Key、OAuth 或内部网络访问控制，防止外部恶意调用。 |
| **回滚容错** | - 为回滚过程再套一层“二次回滚”兜底：若第一次回滚失败，尝试重新加载最初的 checkpoint（通过 `maybe_download_model(initial_path)`），并记录不可恢复状态。 |
| **并发防护** | - 在调度器加入 **权重更新锁**（如 `asyncio.Lock`），防止同时的 `update_weights_from_disk` 互相冲突。<br>- 在 UI/CLI 层返回 “更新进行中” 状态码 409。 |
| **显存/CPU 缓冲区监控** | - 在 `WeightsUpdater` 前后记录 `torch.cuda.memory_allocated()`、`torch.cuda.memory_reserved()` 与 CPU buffer 大小，若出现异常波动即告警。 |
| **日志脱敏** | - 对外部请求日志统一过滤 `model_path`，或只记录模型名称而非完整路径。 |
| **安全审计** | - 确认 `safetensors` 加载前已使用 `safe_open` 进行校验，防止恶意的自定义解码器。<br>- 加入对权重文件大小的上限检测（如 > 200 GB 则直接拒绝）。 |
| **未来特性对齐** | - 为支持 **hot‑refit**（推理期间权重更新），需要在 `scheduler

---

### [4/N] Quantization Refactor: Quark MoE schemes (#18252)
**SHA**: `150ed88` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/150ed881be2cf9a9e64b636226b3c6189d341a7d)

**🎯 变更类型**：架构变更 / 重构 / 功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交对 Quark 量化框架在混合专家（MoE）层的实现进行了一次重大重构。核心改动包括：  
1. 将原有的 `QuarkW4A4MXFp4MoEMethod` 等类抽象为统一的 **Scheme**（`QuarkLinearScheme`、`QuarkMoEScheme`），并在 `quark/schemes` 包中实现对应的线性和 MoE 方案。  
2. 为 MoE 层新增 `QuarkFusedMoEMethod`，统一调用 `layer.scheme` 完成权重创建、后处理以及前向计算。  
3. 更新所有调用点（如 `ep_moe/layer.py`）从 `quant_method` 判断切换为 `scheme`，并加入对新 Scheme 类的类型检查。  
4. 整理 import、`__all__` 与类型检查结构，减少循环依赖并提升可读性。  

**🎯 影响范围**：  
- `sglang/srt/layers/quantization/quark/`（核心量化实现）  
- `sglang/srt/layers/quantization/quark/schemes/`（新 Scheme 定义）  
- `sglang/srt/layers/quantization/quark/quark.py`（QuantConfig 与层绑定逻辑）  
- MoE 相关层：`sglang/srt/layers/moe/ep_moe/layer.py`、`sglang/srt/layers/moe/fused_moe_triton/*.py`  

**🔍 技术洞察**  

- **架构影响**  
  - **职责分离**：原先的 MoE 量化方法混合了权重创建、后处理、前向计算等多职责，现在通过 `QuarkMoEScheme`/`QuarkLinearScheme` 完全解耦，层只持有 `scheme` 实例，QuantConfig 负责返回对应的 Method（Linear 或 FusedMoE）。  
  - **统一入口**：`QuarkFusedMoEMethod` 继承自 `FusedMoEMethodBase`，对外只暴露 `create_weights`、`process_weights_after_loading`、`create_moe_runner`、`apply`（实际为 `apply_weights`），保持了旧接口兼容性。  
  - **可扩展性提升**：新增 MoE scheme（如 `QuarkW8A8FP8MoE`）只需实现 `QuarkMoEScheme` 抽象，无需再次修改层级调度逻辑。  

- **性能影响**  
  - **算子保持不变**：前向计算仍然使用 `aiter.fused_moe` 或 Triton 实现，量化/反量化路径未改变，理论上不引入额外算力开销。  
  - **一次性预处理**：在 `process_weights_after_loading` 中完成权重与尺度的 shuffle 与 e8m0 转换，避免在每次前向时重复处理，可能略微提升推理吞吐。  
  - **轻微的对象检查成本**：每次前向会执行 `isinstance(self.scheme, QuarkW4A4MXFp4MoE)`，该检查成本极低，可忽略。  

- **安全考虑**  
  - 变更仅涉及模型内部的权重量化与调度，无网络 I/O、文件系统或权限提升操作。  
  - 新增的 `process_weights_after_loading` 直接写入 `layer` 参数的 `.data`，若出现异常可能导致权重损坏，但不会导致安全漏洞。  
  - 代码路径增加了 `TYPE_CHECKING` 条件导入，防止运行时循环导入导致的意外错误。  

**⚠️ 潜在风险**  

| 风险点 | 说明 | 可能影响 |
|--------|------|----------|
| **方案实例未赋值** | 新增的 `layer.scheme` 必须在 `QuarkConfig.get_quant_method` 中正确设置。若某层未走到相应分支（例如自定义层未继承 `LinearBase` 或 `FusedMoE`），`scheme` 为 `None`，前向会抛 `ValueError("A scheme must be defined for each layer")`。 | 运行时崩溃，影响模型加载或推理。 |
| **兼容性回退** | 旧的 checkpoint 仍然使用 `quant_method` 字段，如果未迁移配置文件或加载旧模型，`isinstance(self.quant_method, ...)` 判断会失效。 | 旧模型加载失败。 |
| **权重后处理** | `process_weights_after_loading` 对权重做了 **shuffle** 与 **e8m0_shuffle**，若 GPU 架构不支持（如不在 gfx9.5 系列），`_is_shuffle_moe_mxfp4` 为 `False`，但仍会执行 `shuffle_weight`（受条件保护），需确保条件判断完整。 | 权重顺序错误导致数值偏差或显存访问错误。 |
| **类型注解与运行时** | `QuarkLinearScheme`、`QuarkMoEScheme` 采用 `TYPE_CHECKING` 导入 `StandardDispatchOutput`，若实际运行时误用未导入的对象，可能触发 `NameError`。 | 程序异常。 |
| **多卡 / 多进程** | 权重加载后直接修改 `.data`，在多进程（fork）或分布式环境下若复制权重后仍执行 `process_weights_after_loading`，可能导致非预期的二次 shuffle。 | 显存浪费或数值不一致。 |

**💡 关注建议**  

1. **向后兼容**  
   - 在 `QuarkConfig.get_quant_method` 中加入对旧 `quant_method` 字段的兼容适配（如在缺失 `scheme` 时自动创建对应 Scheme），或在模型转换脚本中统一写入 `scheme`。  
   - 在文档或 Release Note 中明确说明新版检查 `layer.scheme`，并提供迁移指南。  

2. **单元测试 & 回归测试**  
   - 为每种 Scheme（`QuarkW4A4MXFp4MoE`、`QuarkW8A8FP8MoE` 等）新增 **权重加载 → process_weights → 前向** 的完整链路测试，覆盖 GPU（HIP、CUDA）和 CPU 两种环境。  
   - 在多卡/分布式训练环境下验证 `process_weights_after_loading` 只运行一次。  

3. **异常捕获**  
   - 在 `QuarkFusedMoEMethod.apply` 前加入 `if not hasattr(layer, "scheme") or layer.scheme is None` 的友好错误提示，避免空指针异常。  
   - 对 `process_weights_after_loading` 中的 shuffle 操作加 `try/except`，若底层库不支持（如非 ROCm 环境），回退到未 shuffle 的实现并记录警告日志。  

4. **性能基准**  
   - 对比重构前后相同模型的推理吞吐与显存占用，重点关注 `process_weights_after_loading` 带来的一次性预处理是否显著降低整体延迟。  

5. **代码审查点**  
   - 检查所有新加入的 `__all__` 与实际导出对象是否一致，防止遗漏导致 `import *` 行为不符。  
   - 确认 `quark/quark.py` 中的 `get_linear_scheme` 与 `get_moe_scheme` 对于非法或缺失配置抛出明确异常，而不是返回 `None` 导致后续 `NoneType` 错误。  

通过上述措施，可最大化本次架构重构带来的可维护性提升，同时降低因迁移或环境差异导致的潜在回归风险。祝开发顺利 🚀.

---

#### 🟡 中重要度变更 (8)

### [diffusion] CI: enable warmup as default (#19010)
**SHA**: `3207427` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3207427d6def80c182d418fe09b0ee3aac8797d7)

**🎯 变更概述**  
- 将 Diffusion Server 的 `--warmup` 参数设为默认打开，去除 `DiffusionServerArgs.warmup` 选项。  
- 相应地删除了大量测试用例中显式的 `warmup=True` 配置。  
- 调整了 CI 脚本以及 `perf_baselines.json` 中的基准数据，使其匹配新默认的 warm‑up 行为。

**⚡ 影响范围**  
- `python/sglang/multimodal_gen/test/*`（所有 diffusion 测试用例）  
- `core` 的服务器启动参数构建 (`_build_server_extra_args`、`test_server_common`)  
- 性能基准文件 `perf_baselines.json`（CI 用的期望时延值）  

**💡 关注建议**  
1. **兼容性**：确保服务器实现始终接受 `--warmup`；若未来需要关闭 warm‑up，需提供显式 `--no-warmup` 或在代码中做去重防止重复参数。  
2. **文档同步**：更新 README/CLI 文档，说明 warm‑up 现在是默认行为，原先的 `warmup` 字段已废弃。  
3. **基准校验**：CI 依赖的 `expected_*` 数值已被修改，建议在本地跑一遍完整的 perf 基准，确认新基准与实际硬件匹配，防止误报。  
4. **代码审查**：检查是否还有遗漏的 `.warmup` 访问（如老插件或外部脚本），避免 AttributeError。  

整体来看，此改动统一了测试环境的 warm‑up 行为，提升了 CI 稳定性，但需留意上面兼容性与文档同步的细节。

---

### Move lora request validation to tokenizer_manager from server (#18962)
**SHA**: `963def7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/963def7f26a9595e4713756e07aed4db12a72727)

**变更概览**  
本次提交把 LoRA 适配器的启用校验从 `serving_*` 基类迁移到 `TokenizerManager`，并在请求生成前统一通过新方法 `_validate_and_resolve_lora` 完成“是否开启 + 路径解析”。原有的 `_validate_lora_enabled` 被删除，相关调用在四个 OpenAI 接口（chat、completions、embedding、serving_base）中同步剔除；相应单元测试也做了删减和补充，确保 `Engine.encode` 在未开启 LoRA 时抛出期望的 `ValueError`。

**核心影响**  
- **请求流路径**：LoRA 校验现在在 `TokenizerManager.generate_request` 的模型更新读锁内部完成，保证与后续 `await self._resolve_lora_path` 在同一锁范围，避免并发竞争。  
- **错误信息**：保持原有提示（包含 adapter 名、`--enable-lora` 建议），但统一由管理层抛出，调用方不再需要自行检查。  
- **向后兼容**：所有公开的 OpenAI API 仍然接受 `model:adapter`、`lora_path` 等写法，唯一行为变化是校验时点从服务层提前到 tokenization 前。  
- **测试覆盖**：新增 `test_engine_encode_validates_enable_lora`，删除了原 `TestValidateLoraEnabled` 与若干对 `_validate_lora_enabled` 的直接调用，确保新实现被触达。

**审查要点 & 建议**  

1. **搜索残留调用**：确认项目其他模块（如自定义插件或旧版脚本）没有直接引用已删除的 `serving_base._validate_lora_enabled`，否则会在运行时出现 `AttributeError`。  
2. **异常类型**：目前仍使用通用 `ValueError`。若业务需要区分，可考虑定义 `LoRAConfigError` 并在文档中说明。  
3. **文档同步**：更新 README / API 文档，明确 LoRA 校验已在 tokenization 阶段完成，并给出 `--enable-lora` 与 `--lora-paths` 的使用示例。  
4. **性能影响**：校验仅涉及一次布尔检查和一次适配器名提取，几乎无额外开销；但在高并发下会在 `model_update_lock.reader_lock` 中执行，建议在性能基准中确认锁竞争仍在可接受范围。  
5. **日志记录**：建议在 `_validate_and_resolve_lora` 抛异常前加入 `self.logger.debug`，方便排障。  
6. **扩展兼容**：若未来引入其他请求类型（如 tool / vision），记得在对应 `obj` 结构体中加入 `lora_path` 并让该方法适配。

总体而言，迁移实现更聚合、职责更清晰，代码量减少且测试覆盖保持完整。按上述检查点逐项确认后即可合并。

---

### [RadixTree][4/N Refactor]: Move available_and_evictable_str to individual radix cache classes (#17852)
**SHA**: `48642d5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/48642d5384431cff6ea3eb697b141e98fe6aa106)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 `available_and_evictable_str` 的实现从公共函数迁移到各个 radix 缓存类（`BasePrefixCache`、`MambaRadixCache`、`SWARadixCache`），统一入口改为 `tree_cache.available_and_evictable_str()`。相应地删掉了在 `MambaRadixCache`、`SWARadixCache` 中的冗余 “expensive” 统计函数，并在单元测试中加入了该方法的打印与校验。

**🎯 影响范围**  
- `python/sglang/srt/mem_cache/base_prefix_cache.py`（新增实例方法）  
- `python/sglang/srt/mem_cache/common.py`（函数改为直接调用实例方法）  
- `python/sglang/srt/mem_cache/mamba_radix_cache.py`、`swa_radix_cache.py`（删除旧的 `*_lru_list_evictable_size` 方法，新增对应的 `available_and_evictable_str`）  
- 相关单元测试文件均加入了调用和打印。

**💡 关注建议**  

1. **接口兼容性**：`BasePrefixCache` 现在提供默认实现，确保所有子类（包括未来新增的缓存实现）都继承或自行覆盖该方法，否则在 `common.available_and_evictable_str(tree_cache)` 调用时会触发 `AttributeError`。建议在基类文档中明确该方法是公共接口。  

2. **性能说明**：新实现仍会调用 `sanity_check_evictable_size()`（遍历 LRU 链），在高频路径上可能带来额外开销。请在注释中标明该方法仅用于调试/日志，避免在生产代码的热点路径直接打印。  

3. **测试覆盖**：单元测试只做了打印，没有断言返回内容。建议补充断言，验证返回字符串中包含预期的 token 数量，防止未来改动导致信息缺失。  

4. **代码清理**：旧的 `full_lru_list_evictable_size`、`swa_lru_list_evictable_size` 已被删除，确认没有其它模块仍在直接引用这些私有方法，避免运行时 `AttributeError`。  

5. **日志统一**：如果项目已有统一的日志模块，考虑把 `available_and_evictable_str` 的返回值交由日志系统输出，而不是在测试中直接 `print`，保持代码风格一致。  

总体而言，此次重构提升了职责划分，使缓存内部状态的描述更具面向对象语义，代码可维护性得到改善；只要注意上述兼容性和性能提示，即可安全合并。

---

### Feat/add fi selective state update kernel call (#18070)
**SHA**: `82a0baf` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/82a0bafc1c3a0680b6389f5ae9c48def7800dcbd)

**🎯 变更类型**：功能增强（为 Mamba SSM Select‑State‑Update 增加可插拔后端）  
**⚡ 重要程度**：🟡 中  

**📋 变更概要**  
1. 在 `sglang/srt/layers/attention/mamba/ops` 新增 `ssu_dispatch.py`，实现统一的调度层 `selective_state_update`，并提供 Triton 与 FlashInfer 两种后端实现。  
2. `__init__.py` 重新导出 `PAD_SLOT_ID` 与新的 `selective_state_update`，并加入 `initialize_mamba_selective_state_update_backend`。  
3. `Scheduler` 在构造阶段调用 `init_mamba_backend()` 完成后端实例化。  
4. `ServerArgs` 增加 `--mamba-backend` CLI 选项及默认值 `triton`，并在参数处理流程中加入 `_handle_mamba_backend` 检查 FlashInfer 可用性。  
5. 测试层面添加后端初始化 fixture，并新增使用 FlashInfer 后端的 Nemotron‑3‑Nano‑30B 测试。  

**🎯 影响范围**  
- **注意力层（Mamba SSM）**：所有调用 `selective_state_update` 的位置现统一走调度层。  
- **Scheduler**：首次调度时会额外执行一次后端实例化。  
- **CLI / ServerArgs**：用户可通过 `--mamba-backend` 选项切换实现。  
- **测试套件**：conftest 与新增测试文件会在 CI 中触发 FlashInfer 的可用性检查。  

**💡 关注建议**  

1. **后端初始化时序**  
   - `selective_state_update` 断言后端已初始化；若在 Scheduler 之外（如单元测试或外部脚本）直接调用，需要显式调用 `initialize_mamba_selective_state_update_backend`。建议在 `ssu_dispatch.selective_state_update` 中做一次懒加载，避免显式初始化错误。  

2. **兼容性 & 命名冲突**  
   - `__all__` 同时导出 `selective_state_update`，而原始实现仍在 `mamba_ssm.py`。确保外部 `from … import selective_state_update` 仍指向调度层，避免因旧路径而产生不一致行为。可以在旧模块内部添加别名警告或直接删除旧导出。  

3. **错误信息与依赖提示**  
   - 当用户指定 `flashinfer` 但未安装对应包，当前会在 `_handle_mamba_backend` 抛 `ValueError`。建议改为 `ImportError` 并在错误信息中提供 `pip install flashinfer` 的安装建议，提升可用性。  

4. **线程安全**  
   - `_mamba_ssu_backend` 为全局单例，若在多进程/多线程环境下多次创建 Scheduler，可能出现重复初始化。可在 `initialize_mamba_selective_state_update_backend` 加锁或检查已有实例后直接返回。  

5. **功能差异文档**  
   - FlashInfer 后端目前不支持 `retrieve_parent_token`，而 Triton 支持。应在 README 或 CLI help 中明确标注该限制，防止用户在 EAGLE‑tree 场景下误选。  

6. **测试覆盖**  
   - 现有测试仅覆盖 FlashInfer 后端的启动路径，建议补充：  
     * Triton 后端的正常调用与异常路径（如缺少 CUDA）  
     * `disable_state_update`、`intermediate_state_indices` 等可选参数在两种后端的行为一致性  
   - CI 环境需确保 FlashInfer 包可用或提供跳过标记，以免因缺少依赖导致整个套件失败。  

7. **性能监控**  
   - 新增后端切换可能影响 KV‑cache 与 Mamba 的吞吐量。建议在 `Scheduler` 启动日志中记录实际后端名称，并在 profiling 中对比两者的延迟/内存占用。  

综上，这次改动为 Mamba SSM 引入了后端抽象，提升了灵活性与跨平台扩展性。只要在初始化、错误提示和文档上补齐细节，即可稳妥上线。

---

### Fix PCG MoE Error (#17739)
**SHA**: `0be30d4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0be30d4b0d5468daf4ea34b01df1e142463a08aa)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 将 `torch._dynamo.mark_dynamic` 替换为宽容的 `maybe_mark_dynamic`，避免在非‑torch‑compile 场景下抛异常。  
- 在 `piecewise_context_manager` 中使用 `TYPE_CHECKING` 延迟导入，防止运行时循环依赖。  
- `custom_all_reduce` 新增对 Piecewise CUDA‑Graph 分割算子执行阶段的检测，改为真实 all‑reduce 而不是 warm‑up 的占位零张量。  
- 为 `pynccl` 添加 `outplace_all_reduce` 接口，并在 `ParallelState._all_reduce_out_place` 中根据 `is_in_piecewise_cuda_graph` 选择该实现。  
- `PiecewiseCudaGraphRunner` 在重新播放前对多余缓存进行 zero‑clear，避免残留数据导致错误。  

**🎯 影响范围**  
- 编译子系统 (`compile.py`, `piecewise_context_manager.py`)  
- 分布式通信层 (`custom_all_reduce.py`, `pynccl.py`, `parallel_state.py`)  
- CUDA‑Graph 驱动的模型执行 (`piecewise_cuda_graph_runner.py`)  
- 相关 CI 测例 `test_piecewise_cuda_graph_2_gpu.py`  

**💡 关注建议**  
1. 确认运行环境使用的 PyTorch 版本支持 `torch._dynamo.maybe_mark_dynamic`；若仍在低版本，需要回退兼容路径。  
2. `pynccl.outplace_all_reduce` 直接在不同指针对 NCCL 调用，建议在 CI 中加入跨卡同步校验，防止指针冲突。  
3. `is_in_piecewise_cuda_graph` 的全局状态在多线程/多进程环境下是否安全需评审，必要时加锁或线程局部存储。  
4. 完善单元测试，覆盖 warm‑up、piecewise split、以及普通 torch.compile 三种路径，确保 all‑reduce 行为一致。  

总体而言，此次改动解决了 PCG MoE 在分段 CUDA‑Graph 中的错误触发，风险主要集中在新加入的 pynccl out‑place 路径和动态标记函数的兼容性，建议在发布前进行充分的跨卡、不同 Torch 版本回归测试。

---

### [Qwen3.5] Enable nvfp4 checkpoint (#18937)
**SHA**: `bba2fc4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bba2fc49a1700d82ac9f4f79bf6e05b120d424e7)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 Qwen‑3.5 系列模型新增对 `modelopt_fp4`（nvfp4）检查点的兼容处理。  
- 在 RoPE 缩放路径中加入更明确的错误信息。  
- 调整 `linear_attn`、`qkv_proj`、`o_proj` 等子层在加载 fp4 检查点时不使用量化配置，并在 MTP 变体中直接排除 fp4 量化。  

**🎯 影响范围**  
- `python/sglang/srt/layers/rotary_embedding.py`（RoPE 错误提示）  
- `python/sglang/srt/models/qwen3_5.py`（注意力层、QKV、专家权重加载）  
- `python/sglang/srt/models/qwen3_5_mtp.py`（多模态并行模型的量化配置）  

**💡 关注建议**  
1. **兼容性测试**：在不同量化模式（fp4、GPTQ、无量化）下跑完整的推理/微调脚本，确保 `quant_config=None` 不会导致未初始化的层或参数报错。  
2. **权重加载**：关注 `load_weights`、`load_fused_expert_weights` 中对 `ignore_suffixes` 的过滤逻辑，确认在 fp4 检查点中不再出现被错误跳过的参数。  
3. **RoPE 失效**：新错误信息会泄露 `rope_scaling` 内容，确保日志或异常处理不会暴露敏感配置。  
4. **文档/示例**：更新 README 与模型转换脚本，明确说明 fp4 检查点只能在非量化模式下加载，或需要使用 `modelopt_fp4` 标识。  
5. **回归检测**：对已有的 `modelopt`（非 fp4）模型运行回归测试，防止因为 `quant_config` 被错误置为 `None` 而导致精度下降或性能回退。  

总体来看，此次改动为 nvfp4 检查点提供了必要的兼容层，影响范围局限于 Qwen‑3.5 系列模型加载路径。请在多种量化配置下完成回归验证，防止意外破坏已有的量化模型行为。

---

### Add batched zero copy to NIXL backend (#18850)
**SHA**: `443b1a8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/443b1a88d105d6d3a322cc9436eb9651089d00f5)

**🎯 变更类型**：功能增强（为 NIXL 后端加入批量 zero‑copy 接口）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. `CacheController.attach_storage_backend` 支持新的后端类型 `nixl`。  
2. `backend_factory` 为 NIXL 后端注入 `mem_pool_host` 参数，以便后续 zero‑copy。  
3. `hicache_nixl.py` 大幅扩展：  
   - 新增 `batch_get_v1`、`batch_set_v1`、`batch_exists` 等 zero‑copy 与非 zero‑copy 双路实现。  
   - 引入 `HostKVCache` 的页面布局检测，自动开启 zero‑copy。  
   - 加入 `clear`、`register_mem_pool_host`、内部预处理/后处理函数以及带宽统计。  
4. `nixl_utils.py`  
   - `get_backend_initparams` 将所有配置值转成字符串，避免类型不匹配。  
   - `FileManager` 新增 `clear` 方法用于清理持久化文件。  

**🎯 影响范围**  
- `python/sglang/srt/managers/cache_controller.py`  
- `python/sglang/srt/mem_cache/storage/backend_factory.py`  
- `python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py`（核心缓存实现）  
- `python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py`（文件管理）  

**💡 关注建议**  

1. **零拷贝注册**：`register_buffers` 在 `READ/WRITE` 分支中被显式调用，确保所有 buffer 已在 NIXL 侧注册；若后续新增 buffer 类型，需要同步更新。  
2. **页面布局依赖**：zero‑copy 仅在 `page_first` / `page_first_direct` 布局下启用，若用户自行修改 `mem_pool_host.layout`，需确认 `is_zero_copy` 状态能及时刷新（可考虑在 `register_mem_pool_host` 后调用一次检测）。  
3. **MLA 模型兼容**：针对 MLA（key/value 交叉）模型，key 列表只生成 `*_k`，但后续 `batch_get_postprocess` 仍假设 `k/v` 成对；建议在文档中明确限制或在代码里加入断言防止误用。  
4. **初始化参数字符串化**：`get_backend_initparams` 统一转为 `str`，防止 C++ 后端解析错误，但也可能隐藏数值型配置错误，建议在注释或日志中提醒用户检查配置类型。  
5. **文件清理**：`FileManager.clear` 直接遍历删除文件，未使用锁或异常回滚，生产环境中若有并发写入，可能出现文件瞬时不存在导致错误，建议加文件锁或在清理前确保所有缓存已关闭。  
6. **回测与基准**：新增的带宽日志 (`logger.debug`) 方便性能分析，但默认 `DEBUG` 可能产生大量日志，建议在 CI 中打开一次进行基准对比，确认 zero‑copy 相比原实现的吞吐提升是否达标（尤其在大 batch 场景）。  

**对开发者**：在新增的 `*_v1` 接口上编写单元测试，覆盖 zero‑copy 与非 zero‑copy 两条路径、MLA 与非 MLA 两种模型、文件与对象两种 `mem_type`。  
**对用户**：如果使用 NIXL 后端并开启了 page‑first 布局，可直接切换到 `batch_get_v1`/`batch_set_v1`，预计可获得 2‑3× 的带宽提升；若使用自定义布局或在多进程环境下，请确认 `register_mem_pool_host` 已在每个进程中调用。

---

### [Fix] Add lora tied lm head support (for Qwen2.5, Gemma, etc model need) (#18634)
**SHA**: `9c5aae4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9c5aae4df55236214a72d9b60db2250c85583e27)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中

**📋 变更摘要**  
1. 为 LoRA 适配器增加对 **tied lm_head**（即 `tie_word_embeddings=True`）的支持：在模型层级上为 `lm_head` 生成独立的 `ParallelLMHead`，从而在 `named_modules()` 中能被正确包装。  
2. 处理 PEFT 导出的权重键名 `unembed_tokens` → `lm_head`，并在目标模块集合为 `{"all"}` 时放宽加载检查。  
3. 在 `lora_manager` 中加入对 PEFT 简写 `"all-linear"`、`"all"` 的显式校验，提示用户在启动服务时必须通过 `--lora‑target‑modules` 指定具体模块。  
4. `mem_pool` 与 `utils.get_normalized_target_modules` 也同步支持 `"all"` 哨兵值。  
5. 新增完整的单元测试 `test_lora_tied_lm_head.py`，对 Qwen2.5 等模型的 LoRA‑lm_head 进行 HF‑SGLang log‑prob 比对，确保数值一致且不产生 NaN。

**🎯 影响范围**  
- `python/sglang/srt/lora/`：`lora.py`, `lora_manager.py`, `mem_pool.py`, `utils.py`  
- 新增测试文件 `test/registered/lora/test_lora_tied_lm_head.py`  
- 相关日志与错误提示逻辑。

**💡 关注建议**  
- **用户侧**：使用包含 `lm_head` 或 PEFT 简写（如 `"all"`）的 LoRA 适配器时，务必在服务器启动命令中显式提供 `--lora-target-modules`（如 `--lora-target-modules=all`），否则会抛出明确提示。  
- **开发者侧**：检查 `ParallelLMHead` 的共享权重实现是否在不同 dtype、量化或多 GPU 环境下保持一致；若后续加入 DeepSpeed、FSDP，需要确保该 “复制‑共享” 机制不破坏参数同步。  
- **性能**：新增的 `ParallelLMHead` 只共享底层张量，理论上不增加显存，但在大量 LoRA 适配器同时加载时仍需关注 `mem_pool` 判定逻辑的成本。  
- **回归**：运行全套 `test_lora_*`，尤其是新加入的 `test_lora_tied_lm_head`，确认在 CPU 与 CUDA 两种设备上均能通过。若出现 `target_modules` 解析错误，检查 `get_normalized_target_modules` 对字符串返回的 `{ "all" }` 是否被上层正确拦截。  

总体而言，此次改动为集成了 **tied lm_head** 的模型（如 Qwen2.5、Gemma）提供了可靠的 LoRA 支持，兼容性提升明显，但需要在部署脚本中明确目标模块集合。

---

#### 🟢 低重要度变更 (6)

### [diffusion] chore: improve memory usage on consumer-level GPU (#18997)
**SHA**: `d73f06f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d73f06f09149b137bb23ac1dcd3966acce06dc49)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `utils.py` 中统一 `BYTES_PER_GB` 常量，并在 `server_args.py` 根据 GPU 总内存（< 30 GB）自动开启全部 offload，提升消费级显卡的内存使用效率，同时修正 `dit_cpu_offload` 的判断逻辑。

---

### [Diffusion] [NPU] Enable profiler on NPU (#17807)
**SHA**: `d07e8aa` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d07e8aa4a31520c0ead8b8062d6958dbc4e6ea98)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `profiler.py` 中加入 NPU 平台适配，导入 `torch_npu` 并打补丁；在活动列表、trace 处理、停止时加入 NPU 相关逻辑，确保在 NPU 上启用性能分析。

---

### [diffusion] fix: fix rank used in parallel executor when enable_cfg_parallel is false (#18975)
**SHA**: `e21fc78` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e21fc78dbd0576a6bada7f6657786604d42d63f7)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `parallel_executor.py` 中，修正了在 `enable_cfg_parallel=False` 时获取执行器 rank 的方式；原先始终使用 `get_classifier_free_guidance_rank()`，现改为在关闭 CFG 并行时改用 `get_world_rank()`，确保并行执行逻辑正确。

---

### [AMD] Fix mi35x dsv32 mtp nightly (#18978)
**SHA**: `4622679` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/462267982bd8d587f30d12fce56ee8e15261c467)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `nsa_backend.py` 中加入 `_is_hip` 检测，AMD HIP 环境下强制关闭融合元数据复制，以修复 mi35x dsv32 mtp nightly 的兼容性问题。

---

### Fix flaky Qwen3-Next KL divergence tests by reverting mamba slot release (#18910)
**SHA**: `e2fccb2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e2fccb2ee0a0211b75961e40009fe02133fa9983)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `maybe_release_mamba_cache` 的调用及实现，避免在调度失败时误释放 mamba 槽导致内存泄漏检测错误，修复 Qwen3‑Next KL 散度测试的不稳定性。

---

### [Doc] Add `flashinfer_deepgemm` to `--fp8-gemm-backend` (#18982)
**SHA**: `2f592c3` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2f592c3b181b7d75dbab8cc8f95806ee78e6ff01)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_arguments.md` 中为 `--fp8-gemm-backend` 增加 `flashinfer_deepgemm` 选项并更新说明。

---

