# 每日更新报告（2026-02-02）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-02 21:45:41 | sglang-bot | docs: move deepseek_ocr to popular model usage and add cookbook reference (#18120) |
| 2026-02-02 18:38:08 | Byron Hsu | [CI] Add logs to debug `TestOpenAIServer.test_completion_stream` (#17471) |
| 2026-02-02 17:52:49 | Xuhao Zhang | [NPU]mindspore model support moe (#15363) |
| 2026-02-02 17:24:19 | CHEN Xi | [diffusion] fix: remove accelerate dependency for device mapping (#18026) |
| 2026-02-02 17:18:50 | zhangxin81 | support smem in per_token_quant_fp8 kernel (#16725) |
| 2026-02-02 17:06:51 | Xiaoyu Zhang | [Diffusion] Fix Ring Parallel bug with FA4 (#18062) |
| 2026-02-02 16:28:30 | HAI | [AMD] enable MoRI to release and nightly builds (#18101) |
| 2026-02-02 15:16:08 | 陈一涵 | [diffusion] CI: deprecate WarmupRunner in CI (#18038) |
| 2026-02-02 13:50:21 | Xiaoming Liu | fix: correct weight loading prefix mapping for Qwen3-VL (#18024) |
| 2026-02-02 13:27:27 | RangerCD | fix: zmq_to_tokenizer encoder transfer when host listens to 0.0.0.0 (#17929) |
| 2026-02-02 13:15:23 | Kangyan-Zhou | Improve Per Commit Test job filtering for sglang-kernel (#18054) |
| 2026-02-02 11:37:32 | siyu | [EPD][refactor]: introduce BaseMMReceiver for gRPC transport integration (#17921) |
| 2026-02-02 11:11:52 | Cheng Wan | Refine logprob logic for request handling (#17986) |
| 2026-02-02 11:00:38 | YC Tseng | [AMD] Fix aiter version in rocm image (#18076) |
| 2026-02-02 10:43:50 | Simon (Jiyou) Li | Add bootstrap_room validation to detect metadata corruption in PD disaggregation (#17430) |
| 2026-02-02 09:48:17 | Mick | [diffusion] fix: fix missing component names for VAELoader (#18069) |
| 2026-02-02 09:47:40 | Mick | [diffusion] cli: introduce generic attention backend configuration in ServerArgs (#18036) |
| 2026-02-02 09:44:32 | Yuhao Yang | fix: avoid double reduce in VLM dp attention (#17991) |
| 2026-02-02 07:34:09 | Yuan Luo | [Fix] Remove no use code in MiMo-V2-Flash (#18051) |
| 2026-02-02 06:52:08 | Glen Liu | [TestFix] rewrite LoRA overlap loading tests (#18047) |
| 2026-02-02 06:49:00 | Koushik Dutta | [BUGFIX]: using language-only should not reserve space for the vision encoder (#18011) |

### 📊 统计摘要
> 本日共 21 个提交 | 🔴高 1 | 🟡中 8 | 🟢低 12
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (1)](#-🔴-高重要度变更-1)
    - [fix: avoid double reduce in VLM dp attention (#17991)](#d11ccc0)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [support smem in per_token_quant_fp8 kernel (#16725)](#e3021b6)
    - [[diffusion] CI: deprecate WarmupRunner in CI (#18038)](#86117df)
    - [fix: correct weight loading prefix mapping for Qwen3-VL (...](#522e13b)
    - [fix: zmq_to_tokenizer encoder transfer when host listens ...](#a480ca7)
    - [[EPD][refactor]: introduce BaseMMReceiver for gRPC transp...](#f1824a9)
    - [Add bootstrap_room validation to detect metadata corrupti...](#8ed35df)
    - [[diffusion] cli: introduce generic attention backend conf...](#977096a)
    - [[TestFix] rewrite LoRA overlap loading tests (#18047)](#99dad10)
  - [🟢 低重要度变更 (12)](#-🟢-低重要度变更-12)
    - [docs: move deepseek_ocr to popular model usage and add co...](#c971852)
    - [[CI] Add logs to debug `TestOpenAIServer.test_completion_...](#5636d16)
    - [[NPU]mindspore model support moe (#15363)](#0537232)
    - [[diffusion] fix: remove accelerate dependency for device ...](#aa780a6)
    - [[Diffusion] Fix Ring Parallel bug with FA4 (#18062)](#a0757c9)
    - [[AMD] enable MoRI to release and nightly builds (#18101)](#750ad0d)
    - [Improve Per Commit Test job filtering for sglang-kernel (...](#cd31540)
    - [Refine logprob logic for request handling (#17986)](#ab8b99e)
    - [[AMD] Fix aiter version in rocm image (#18076)](#ea04bc1)
    - [[diffusion] fix: fix missing component names for VAELoade...](#c84cd4b)
    - [[Fix] Remove no use code in MiMo-V2-Flash (#18051)](#9227d4f)
    - [[BUGFIX]: using language-only should not reserve space fo...](#993ec17)
#### 🔴 高重要度变更 (1)

### fix: avoid double reduce in VLM dp attention (#17991)
**SHA**: `d11ccc0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d11ccc0a0aa86baa465072e1969b6fc9438cc04a)

**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：本次提交针对 Vision Language Model（VLM）在数据并行（DP）注意力模式下的实现，去除了在投影后对 Tensor 进行的二次全梯度归约（all‑reduce），并通过 `use_dp_attention_reduce` 标识统一控制是否在 DP‑Attention 中进行归约。同时在 Kimi‑K25 模型初始化时显式读取 DP‑Attention 开关，以确保两者行为保持一致。相关测试用例也相应更新，新增对 DP=2、TP=4 场景的 VLM 生成验证。

**🎯 影响范围**  
- `python/sglang/srt/layers/attention/vision.py`（核心 VLM 注意力实现）  
- `python/sglang/srt/models/kimi_k25.py`（模型构造入口）  
- 分布式测试 `test/registered/distributed/test_dp_attention*.py`（DP/TP 组合的回归）  

**🔍 技术洞察**  

- **架构影响**  
  - `vision.py` 原先在投影层（`self.proj`）后总是执行 `all_reduce`，即使在 `use_dp_attention_reduce` 已经在内部完成归约时仍会再次归约，导致 **双重归约**。本次改动通过删除显式的 `all_reduce`，让归约职责统一交给 `dp_attention` 层实现，从而简化了调用链，提升代码可维护性。  
  - `kimi_k25.py` 通过 `is_dp_attention_enabled()` 将全局 DP‑Attention 开关注入模型构造，使得同一套代码在不同部署配置（单机/多机、DP/TP）下行为一致，降低了配置漂移风险。  

- **性能影响**  
  - **通信开销下降**：在 TP>1 时，`all_reduce` 是跨进程/机器的同步通信，去掉冗余一次可直接把通信次数从 2↓1，理论上可减少约 30%–50%（取决于模型大小与网络带宽）的延迟。对大模型（如 Qwen3‑VL‑30B）尤为明显。  
  - **计算流水线更顺畅**：去除不必要的同步点后，GPU 计算可以更连续地进行，提升整体吞吐率。  
  - **内存使用**：无额外内存分配变化，甚至略有降低（去掉了中间 `output` 的复制），对显存占用几乎无影响。  

- **安全考虑**  
  - 变更仅涉及内部通信和张量归约逻辑，不涉及外部输入的验证或权限控制，未引入新的安全风险。  
  - 通过统一的 `use_dp_attention_reduce` 标识，避免了在某些情况下因遗漏归约而导致的 **不一致输出**，从而间接提升了结果的可预测性与一致性，降低了因输出差异导致的业务风险。  

**⚠️ 潜在风险**  
1. **归约遗漏**：如果 `use_dp_attention_reduce` 为 `False`（例如在某些自定义模型或实验性路径中），而外层不再执行 `all_reduce`，可能导致跨 TP 进程的结果不一致。  
2. **兼容性**：老版本的模型或脚本如果直接依赖 `vision.py` 中的手动 `all_reduce` 行为，升级后可能得到不同的数值结果。  
3. **测试覆盖**：当前仅在 DP=2、TP=4 的 VLM 场景下添加了回归测试，其他组合（如 DP=4、TP=2）未直接验证，需确保在 CI 中覆盖全部配置。  

**💡 关注建议**  
- **验证配置**：在部署前确认 `--enable-dp-attention` 与 `--tp` 参数的组合是否符合预期，尤其是 `use_dp_attention_reduce` 的返回值。  
- **回归测试**：尽快在 CI 中加入更多 DP/TP 组合的 VLM 生成测试，以捕获潜在的归约遗漏。  
- **文档更新**：在项目的 DP‑Attention 使用说明中说明 “在 DP‑Attention 模式下 **不再需要手动 all_reduce**”，防止用户自行添加类似代码导致再次出现双重归约。  
- **监控指标**：上线后监控分布式训练/推理的网络通信量（如 NCCL all_reduce 时间），验证预期的通信下降是否生效。  

---  

*综上，此次修复通过删除冗余的全梯度归约显著简化了 VLM DP‑Attention 的实现，提升了分布式推理的性能与可维护性，风险可通过配置检查与测试覆盖进行有效控制。*

---

#### 🟡 中重要度变更 (8)

### support smem in per_token_quant_fp8 kernel (#16725)
**SHA**: `e3021b6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e3021b65fe79113be7642cca4074946e0a11fd26)

**🎯 变更类型**：功能增强（为 `per_token_quant_fp8` warp‑local kernel 增加可选共享内存路径）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 为 `per_token_quant_fp8_kernel` 增加模板参数 `USE_SMEM`，在 warp‑local 计算时可把输入数据先写入共享内存并在第二遍读取，以降低全局内存访问压力。  
2. 在 `sgl_per_token_quant_fp8` 中根据 hidden‑dim 大小和动态共享内存需求决定是否启用该路径，并统一通过 `launch_per_token_quant_fp8_warp_kernel` 完成 kernel 调用。  
3. 新增默认共享内存阈值 `DEFAULT_SHARED_MEM_THRESHOLD_KB = 48`，防止超过单块 48 KB 限制导致 launch 失败。  

**🎯 影响范围**：  
- `sgl-kernel/csrc/gemm/per_token_quant_fp8.cu`（核心 kernel、包装函数）  
- 相关的 Python 接口 `sgl_per_token_quant_fp8`（张量类型调度、共享内存大小计算）  

**💡 关注建议**  

| 领域 | 建议 |
|------|------|
| **共享内存计算** | 目前使用 `warp_smem_stride` 按 32‑字节对齐计算，但未显式检查 `dynamicSmemSz` 是否超过设备属性 `maxSharedMemoryPerBlock`（部分卡仅 48 KB）。建议在运行时 `cudaDeviceGetAttribute` 验证，并在超出时自动回退 `USE_SMEM = false`。 |
| **阈值硬编码** | `DEFAULT_SHARED_MEM_THRESHOLD_KB` 固定为 48 KB，若未来支持 SM≥80（共享内存可达 164 KB）会限制潜在性能提升。建议改为 `int get_default_smem_threshold()`，读取设备属性并给出合适阈值。 |
| **同步机制** | 只用了 `__syncwarp()`，但写入共享内存后仍有跨 warp 的隐式依赖（如同一 CTA 内不同 warp 读取同块共享内存的情况）。目前每个 warp 只读自己写的数据，安全；若后续修改为跨 warp 共享，需要 `__syncthreads()`. |
| **异常检测** | kernel launch 后缺少 `cudaGetLastError` 检查，建议在 `sgl_per_token_quant_fp8` 结束前加入错误捕获，防止因共享内存不足导致的 silent failure。 |
| **性能验证** | 共享内存路径在 hidden‑dim < 2K 时才启用，建议加入基准测试（不同 batch、hidden‑dim、GPU 架构）来量化提升，并在 CI 中保留对比基准。 |
| **代码可维护性** | `launch_per_token_quant_fp8_warp_kernel` 参数顺序较多，建议使用结构体或 `std::optional` 包装可选参数，提升可读性。 |

整体来看，新增的共享内存路径为大部分小 hidden‑dim 场景（常见的 512‑1024）提供了潜在的带宽优化，代码实现保持了向后兼容性。只需在上述细节上做少量防御性检查与文档说明，即可稳妥上线。

---

### [diffusion] CI: deprecate WarmupRunner in CI (#18038)
**SHA**: `86117df` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/86117dfe0e2d596a071324486c51b1203f77feff)

**🛠️ 变更类型**：重构 / CI 改进  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交在 CI 环境中彻底废弃 `WarmupRunner`，改为在服务器启动时通过 `--warmup`（布尔）标志控制是否进行预热；同时在 `GPUWorker` 中避免将预热请求的性能日志写入持久记录。为配合新逻辑，相关测试脚本、配置结构以及基准 JSON 均做了大量删减和字段改名。  

**🎯 影响范围**  
1. `sglang/multimodal_gen/runtime/managers/gpu_worker.py` – 性能日志过滤。  
2. CI 测试脚本 `gen_perf_baselines.py`、`test_server_common.py`、`test_server_utils.py` – 删除 `WarmupRunner` 实现，改用 `--warmup` 参数。  
3. 配置模型 `testcase_configs.py` – 将原来的 `warmup`（int）/ `enable_warmup`（bool）字段统一为 `warmup: bool`，并在多数用例中默认开启。  
4. 基准文件 `perf_baselines.json` – 移除 `warmup_requests` 节点，更新部分阶段耗时数据以匹配不再计入预热的统计。  

**💡 关注建议**  
- **功能兼容**：原来依赖 `WarmupRunner` 手动发送若干预热请求的 CI/本地脚本已全部改为自动在服务器启动时完成，确保新标志 `--warmup` 的默认值与历史行为一致（默认开启）。如果项目中仍有自定义 WarmupRunner 调用，需要手动迁移或在代码中加入兼容层。  
- **日志与监控**：`GPUWorker` 现在通过 `req.is_warmup` 判断过滤日志，确认 `Request` 对象在所有入口（包括编辑、文本）正确标记 `is_warmup`，防止误把正常请求当作预热丢失。  
- **基准数据**：`perf_baselines.json` 已被大幅修改，运行 CI 前请核实基准生成脚本仍能成功读取并对比；若有自定义基准，需要同步更新。  
- **测试验证**：执行完整的 CI（包括 `test_server_common`）以确认所有用例在新标志下仍能通过；特别留意涉及图片编辑的用例，原来的 `download_image_from_url` 与 `is_image_url` 已被删除，路径解析必须在外部准备。  
- **回滚方案**：若后续发现预热对某些模型的启动时延影响显著，可临时在 `DiffusionServerArgs` 中恢复 `warmup: int` 并在 `GPUWorker` 中放宽过滤条件，或在 CI 脚本中显式调用 `WarmupRunner`（已保留旧实现的源码，只是未被引用）。  

总体来看，改动清晰地把预热职责从测试工具迁入服务器本身，降低了 CI 脚本的维护成本，但需要确保所有请求正确标记 `is_warmup`，并在迁移后跑一遍完整测试以避免遗漏的性能回归。

---

### fix: correct weight loading prefix mapping for Qwen3-VL (#18024)
**SHA**: `522e13b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/522e13b4d2c1de1cfb44ecbbca374ec8f312de52)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `qwen3_vl` 权重加载时，新增对 `config.tie_word_embeddings` 的判断，仅在启用词嵌入共享时才把 `embed_tokens.weight` 复制到 `lm_head.weight`。避免了在 `tie_word_embeddings=False`（如 8B 规格）时误写共享权重导致的错误。  

**🎯 影响范围**：  
- `python/sglang/srt/models/qwen3_vl.py` 的权重加载逻辑。  
- 与 `Qwen3-VL` 系列模型的 checkpoint 加载相关的全部下游代码（如推理、微调脚本）。  

**💡 关注建议**：  
1. **功能验证**：分别在 `tie_word_embeddings=True`（小模型）和 `False`（大模型）两种配置下加载同一 checkpoint，确认 `lm_head.weight` 是否符合预期（共享或独立）。  
2. **回归测试**：新增单元测试，覆盖上述两种情形，防止未来改动再次引入相同错误。  
3. **文档更新**：在模型描述或配置文档中明确说明 `tie_word_embeddings` 对权重加载的影响，提醒用户在自定义 checkpoint 时保持一致。  
4. **其他模型检查**：项目中若还有类似的 “复制 embed_tokens 到 lm_head” 实现，建议同步加入 `tie_word_embeddings` 判定，以保持行为一致。  

整体来看，此修改修正了大模型在权重加载时的错误路径，风险较低，建议通过上述测试后合并。

---

### fix: zmq_to_tokenizer encoder transfer when host listens to 0.0.0.0 (#17929)
**SHA**: `a480ca7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a480ca7eadc7db55afdc7e512006c525cd3d5bcb)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
将 `EncodeReceiver` 中对 `self.host` 的赋值由直接使用 `server_args.host` 改为 `get_local_ip_auto(server_args.host)`，解决在服务监听 `0.0.0.0` 时 ZMQ‑to‑Tokenizer 的地址解析错误，使其自动获取本机可达 IP。

**🎯 影响范围**  
- `python/sglang/srt/disaggregation/encode_receiver.py`  
- 与编码器传输相关的 ZMQ 通信路径（`encoder_transfer_backend` 为 `mooncake` 时）  
- 依赖 `EncodeReceiver.host` 的配置、日志与监控模块  

**💡 关注建议**  
1. 确认 `get_local_ip_auto` 在无网络或多网卡环境下的回退策略，避免返回 `None` 导致后续连接失败。  
2. 运行跨机器的端到端测试，尤其是 `host=0.0.0.0` 与实际客户端通过局域网访问的场景，确保 ZMQ 端点能够正确绑定/连接。  
3. 检查是否有其他组件直接读取 `server_args.host`（未走包装函数）导致不一致，必要时统一使用 `get_local_ip_auto`。  
4. 在 CI 中加入网络模拟测试，防止未来改动再次引入类似 IP 解析问题。  

此改动提升了在多租户/容器化部署下的鲁棒性，但需关注 IP 解析的边界情况与跨模块一致性。

---

### [EPD][refactor]: introduce BaseMMReceiver for gRPC transport integration (#17921)
**SHA**: `f1824a9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f1824a957b2e43bf6388fc38edfe6586744a25da)

**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `encode_receiver.py` 中引入抽象基类 `MMReceiverBase`（原 `MMReceiver`），并实现其 HTTP 具体子类 `MMReceiverHTTP`。随后在调度器 `scheduler.py` 与分词管理器 `tokenizer_manager.py` 中，将原先直接实例化 `MMReceiver` 的位置全部替换为 `MMReceiverHTTP`，为后续 gRPC 传输层的接入做准备。  

**🎯 影响范围**  
- `sglang/srt/disaggregation/encode_receiver.py`（核心接收逻辑）  
- `sglang/srt/managers/scheduler.py`、`sglang/srt/managers/tokenizer_manager.py`（调度与分词入口）  
- 任何依赖 `MMReceiver` 的插件或自定义实现（若存在）  

**💡 关注建议**  
1. **实现抽象方法**：当前 `MMReceiverBase` 的 `__init__`、`process_waiting_requests`、`recv_mm_data`、`send_encode_request` 仅保留 `pass`，确保子类（尤其是后续的 gRPC 版）完整实现这些接口；否则运行时会抛出 `NotImplementedError`。  
2. **兼容性检查**：外部代码如果直接导入 `MMReceiver`，需更新为 `MMReceiverHTTP`（或新的 gRPC 子类）。建议在文档或发布说明中明确迁移路径。  
3. **单元测试**：新增抽象基类后，原有行为仍由 `MMReceiverHTTP` 保持，务必运行全部自动化测试，特别是涉及跨进程（zmq / ZMQ‑to‑scheduler）和语言‑only 模式的用例。  
4. **日志与错误处理**：`_recv_mm_data` 注释已改为 “Bypass MMReceiverHTTP”，确认在非 HTTP 场景下仍然满足功能需求，避免误删或遗漏后续的 gRPC 逻辑。  
5. **代码风格**：抽象基类建议改名为 `BaseMMReceiver` 与提交信息保持一致，提升可读性；同时在 `__all__` 中导出相应类，防止意外的导入冲突。  

总体而言，此次重构为未来的 gRPC 传输集成奠定了框架基础，但务必尽快补全抽象方法实现并做好向后兼容的迁移说明，防止生产环境出现 “未实现方法” 的运行时错误。

---

### Add bootstrap_room validation to detect metadata corruption in PD disaggregation (#17430)
**SHA**: `8ed35df` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8ed35df2043ea78aa71a0d0e8146cf4609a23533)

**🎯 变更类型**：功能增强 / 稳定性修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `disaggregation` 的解码路径中新增对 `bootstrap_room` 的校验，检测 PD（Persistent Disaggregation）元数据冲突导致的上下文破坏。  
- 为 `MetadataBuffers` 增加 `bootstrap_room` 缓冲区并在 `set_buf` 时写入，`_commit_transfer_to_req` 根据实际与期望的 room 值决定：  
  1. `actual_room == 0` → 元数据尚未就绪，保留请求；  
  2. `actual_room != expected_room` → 判定为真实冲突，记录错误并中止请求；  
  3. 匹配则正常提交。  

**🎯 影响范围**  
- `python/sglang/srt/disaggregation/decode.py`（请求提交、队列处理）  
- `python/sglang/srt/disaggregation/utils.py`（元数据缓冲区结构）  
- 相关的调度器、指标收集以及 KV‑cache 释放逻辑  

**💡 关注建议**  
1. **回退路径**：若用户在低版本部署仍依赖旧的 `metadata_buffer_index` 机制，需提供兼容开关或回滚说明。  
2. **监控**：开启 `scheduler.metrics_collector.increment_transfer_failed_reqs` 以捕获冲突频率，评估是否还有隐藏的索引碰撞。  
3. **测试**：新增单元/集成测试覆盖以下场景：元数据未就绪、业务正常、room 不匹配导致中止。  
4. **文档**：在 API 文档中说明 `bootstrap_room` 参数的意义及在 PD‑disaggregation 场景下的必填性。  

总体来看，此改动提升了分布式解码的鲁棒性，防止因元数据索引冲突导致的结果错位，风险主要在新增缓冲区的内存占用和对已有请求流程的兼容性。建议在生产环境开启前做灰度验证。

---

### [diffusion] cli: introduce generic attention backend configuration in ServerArgs (#18036)
**SHA**: `977096a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/977096ae03ac4d4148842ad7a9a052c8258bf790)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 SGLang‑diffusion 引入统一的 “attention‑backend‑config” 参数，取代原有的 `STA_mode`、`skip_time_steps`、`VSA_sparsity`、`moba_config`、`mask_strategy_file_path` 等分散字段。新增 `addict` 依赖并实现文件、JSON、键值对三种配置解析。文档同步更新，CLI 参数、示例和平台兼容表均已改动。  
**🎯 影响范围**：  
- `python/sglang/multimodal_gen/runtime/server_args.py`（参数结构、解析、默认值）  
- `runtime/entrypoints/utils.py`、`pipelines_core/*`（所有对旧字段的引用已改为 `attention_backend_config`）  
- `runtime/layers/attention/backends/sliding_tile_attn.py`（读取配置文件路径）  
- CLI 文档 `attention_backends.md`、`cli.md`，以及 `pyproject` 中新增 `addict` 包。  

**💡 关注建议**  
1. **向后兼容**：旧版脚本仍会因缺少 `--attention-backend-config` 而使用默认空配置，请在 CI 中加入测试，确保 `fa/torch_sdpa` 等不依赖额外参数的后端保持原有行为。  
2. **配置格式**：`--attention-backend-config` 支持三种形式；建议在使用说明中明确示例，尤其是键值对的布尔/数值自动转换规则，以免出现类型错误。  
3. **依赖管理**：`addict` 已加入 `requirements`，确认所有镜像、Dockerfile、CI 环境同步更新。  
4. **代码路径**：注意 `get_global_server_args().attention_backend_config` 的调用已遍布多个模块，后续新增后端时请统一放入该 dict，避免再次出现散落字段。  
5. **单元/集成测试**：加入包含 JSON 文件、YAML 文件、键值对三种方式的解析测试，以及 STA、VSA、VMoBA 等后端在有/无配置情况下的行为对比，确保异常路径（路径不存在、JSON 解析错误）能够给出友好提示。  

总体而言，此次改动提升了注意力后端的可扩展性与使用体验，关键在于验证新配置解析的可靠性并保证旧脚本的兼容性。

---

### [TestFix] rewrite LoRA overlap loading tests (#18047)
**SHA**: `99dad10` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/99dad105fd85a5c6fc46e22c88e702f2511c63be)

**🎯 变更类型**：Bug 修复 / 测试改进  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `python/sglang/test/lora_utils.py` 中为 LoRA 相关的测试函数新增 `enable_lora_overlap_loading` 参数，向下传递给 `SRTRunner`，以便在同一套测试入口统一控制 “重叠加载” 行为。  
- 在 `test/registered/lora/test_lora_overlap_loading.py` 中删除了原先的混合‑batch 对比实现（原本对比 overlap / non‑overlap 结果），改为直接调用新的统一测试函数 `run_lora_batch_splitting_equivalence_test` 与 `run_lora_multiple_batch_on_model_cases`，并始终打开 `enable_lora_overlap_loading=True`。  
- 同时为 CI 注册了 AMD 平台的测试入口，统一了 CI 配置。

**🎯 影响范围**  
- LoRA 迁移加载相关的单元/集成测试（`test_lora_overlap_loading.py`）。  
- `lora_utils.py` 中的公共测试工具函数，被所有 LoRA 多批次、批拆分测试调用。  
- CI 配置（CUDA 与 AMD），可能影响 CI 运行时间和 flaky 测试的出现频率。

**💡 关注建议**  
1. **功能完整性**：新参数默认 `None`，在 `SRTRunner` 中仍需确认其默认行为与旧实现保持一致，防止在未显式指定时产生意外差异。  
2. **回归风险**：原先对比 overlap 与非 overlap 的逻辑被移除，若后续需要确保两者一致性，需要单独保留对应的回归测试或在文档中说明已不再验证。  
3. **CI 资源**：新增 AMD CI 会增加资源消耗，建议监控运行时长和 flaky 率，必要时对 `est_time` 做适当调优。  
4. **代码可维护性**：`run_lora_*` 系列函数参数较多，可考虑使用 dataclass 包装配置，提升可读性与调用安全。  

总体来看，此次改动旨在消除原测试的 flaky 问题，提高 CI 稳定性，影响局限于 LoRA 相关测试层面，风险可控。确保 `SRTRunner` 对新参数的默认处理与旧行为一致即可。

---

#### 🟢 低重要度变更 (12)

### docs: move deepseek_ocr to popular model usage and add cookbook reference (#18120)
**SHA**: `c971852` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c971852ffc7a9c754d194532ec7b27fb43fc0451)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `deepseek_ocr.md` 从首页索引迁入 “Popular Model Usage”，并在该章节加入指向 SGLang Cookbook 的链接；同步更新 README 中的新闻条目顺序。  

（<100字）

---

### [CI] Add logs to debug `TestOpenAIServer.test_completion_stream` (#17471)
**SHA**: `5636d16` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5636d16dec9a7986424030ad3816d7cfdfbbaada)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_openai_server.py` 中的 `run_completion_stream` 添加了多条 `print` 调试日志，输出函数入参及生成的响应对象，以帮助定位 `TestOpenAIServer.test_completion_stream` 的问题。

---

### [NPU]mindspore model support moe (#15363)
**SHA**: `0537232` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0537232b05ca60299d29c6d65c50b1906d15d260)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 MindSpore Runner 中加入 `logging` 并使用 `logger.info` 替代 `print`；将模型架构获取逻辑抽取到 `_get_arch_from_config`，统一在 `mindspore.py` 中使用并加入对专家位置配置的查询；修正变量名 `casual_mask` 为 `causal_mask`。

---

### [diffusion] fix: remove accelerate dependency for device mapping (#18026)
**SHA**: `aa780a6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/aa780a6258e506a0cbbe8d069b04a62902693289)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：去除 `accelerate` 依赖的 `device_map`，改为统一使用 `get_local_torch_device()` 进行设备分配，简化加载流程并删除相关冗余代码。

---

### [Diffusion] Fix Ring Parallel bug with FA4 (#18062)
**SHA**: `a0757c9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a0757c9624db12767b5abe66d5ec0329873bec7a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `flash_attention` 的 `forward` 调用中加入 `return_lse=True` 参数，并在上下文中保存对应的 `lse`，修复 Ring Parallel 与 FA4 组合时的错误。

---

### [AMD] enable MoRI to release and nightly builds (#18101)
**SHA**: `750ad0d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/750ad0d29010ce8712420469261e47a5c6ece8e3)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 AMD Docker 镜像的 release 与 nightly 工作流中，新增 `--build-arg ENABLE_MORI=1` 与 `--build-arg NIC_BACKEND=ainic` 参数，以开启 MoRI 支持。

---

### Improve Per Commit Test job filtering for sglang-kernel (#18054)
**SHA**: `cd31540` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cd31540fd73341125168a4ac31503e97c9cf556c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：优化 PR 测试工作流的文件过滤，仅在对应平台（CUDA、ROCm、CPU、XPU）修改时触发 sgl‑kernel 相关作业，并排除非目标配置、文档及无关代码。

---

### Refine logprob logic for request handling (#17986)
**SHA**: `ab8b99e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ab8b99eb23c30948da470269784b5c1dfdaabe40)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：完善了 `logprob` 处理逻辑，只有在需要返回 logprob 且未提供 `token_ids_logprob` 时，才默认使用输出 token 的 logprob；防止在已指定 token 列表时错误覆盖。

---

### [AMD] Fix aiter version in rocm image (#18076)
**SHA**: `ea04bc1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ea04bc1dd68b4a4034f179821135747dbfaab687)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 ROCm 镜像的 Dockerfile 中，新增对 `SETUPTOOLS_SCM_PRETEND_VERSION` 环境变量的处理，防止其泄漏至 AITER 及其他子项目，仅在 SGLang 的 pip 安装阶段使用，从而修正 AITER 版本获取。

---

### [diffusion] fix: fix missing component names for VAELoader (#18069)
**SHA**: `c84cd4b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c84cd4b5ff2ff0b906e2d1854430c04502f4ea0f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `VAELoader` 中补全组件名称列表，新增 `"video_vae"`，修复因缺失导致的加载错误。

---

### [Fix] Remove no use code in MiMo-V2-Flash (#18051)
**SHA**: `9227d4f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9227d4f748834dd3d472f8fb188c131b09a379a1)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：移除 MiMo‑V2‑Flash 中未使用的代码和参数，包括 `captured_last_layer_outputs`、层捕获相关接口以及相应的导入，简化 `forward` 流程。

---

### [BUGFIX]: using language-only should not reserve space for the vision encoder (#18011)
**SHA**: `993ec17` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/993ec178ef2c9efa5b4e87a9ebeee179ac1a51a3)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `server_args.py` 中，将内存调整逻辑从仅检查模型是否多模态改为在模型为多模态且非 `language_only` 模式下才进行，避免在仅语言模式下为视觉编码器预留内存。

---

