# 每日更新报告（2026-02-10）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-10 21:59:13 | Hexq0210 | [NPU] update npu doc (#18474) |
| 2026-02-10 21:52:27 | husf | [NPU][docs] improve docs for Best Practice on Ascend NPU (#18360) |
| 2026-02-10 21:31:30 | Xiaoyu Zhang | [sgl-kernel] upgrade deepgemm (#18362) |
| 2026-02-10 20:22:20 | Mick | [diffusion] fix: fix fsdp (#18187) |
| 2026-02-10 19:53:43 | Makcum888e | [NPU] [CI] Enable run multimodal NPU CI when changes only in multimodal_gen  (#18523) |
| 2026-02-10 18:32:00 | wxy | [diffusion] feat: support parallel wan-vae decode (#18179) |
| 2026-02-10 16:54:15 | Zehuan Li | [DLLM] Basic dLLM scheduling strategy and implementation (#17484) |
| 2026-02-10 16:53:00 | shuwenn | [HiCache] fix: StorageMetricsCollector was initialized twice (#18354) |
| 2026-02-10 16:49:35 | YC Tseng | [AMD] Turn on aiter-prebuild (#18425) |
| 2026-02-10 15:20:10 | Xinyuan Tong | Support GlmMoeDsaForCausalLM (#18521) |
| 2026-02-10 14:50:40 | Xinyuan Tong | Deepseekv32 compatibility with transformers v5 (#18297) |
| 2026-02-10 11:52:54 | siyu | [EPD] Add notification mechanism to fix server hang and add timeout env var (#18229) |
| 2026-02-10 10:36:39 | maocheng23 | Make bench_one_batch_server compatible for more backends (#18512) |
| 2026-02-10 10:29:13 | Qiaolin Yu | Fix idle batch predict dtype in spec v2 (#18379) |
| 2026-02-10 10:03:04 | Teng Ma | [HiCache][PP] add test case for compatibility (#16395) |
| 2026-02-10 08:23:41 | Liangsheng Yin | Tiny fix wrong metric collect key name `forward_prefill` -> `forward_extend` (#18506) |
| 2026-02-10 08:09:09 | Kartik Ramesh | Add cache_config_info metric. (#17273) |
| 2026-02-10 07:09:52 | Zack Yu | docs: expand and update modelopt documentation (#18479) |
| 2026-02-10 06:49:41 | Lianmin Zheng | [Auto Sync] Update cache_init_params.py (20260209) (#18502) |
| 2026-02-10 06:33:02 | Lianmin Zheng | [Auto Sync] Update logits_processor.py (20260209) (#18503) |
| 2026-02-10 04:39:15 | ishandhanani | feat(kv-events): Add medium field to KV event types for storage tier tracking (#18205) |
| 2026-02-10 01:04:54 | Bingxu Chen | [AMD] add amd ci monitor (#17476) |
| 2026-02-10 00:27:59 | Zheng Li | model: support Qwen3.5 (#18489) |

### 📊 统计摘要
> 本日共 23 个提交 | 🔴高 3 | 🟡中 8 | 🟢低 12
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[DLLM] Basic dLLM scheduling strategy and implementation ...](#26f2b37)
    - [feat(kv-events): Add medium field to KV event types for s...](#01e3f46)
    - [model: support Qwen3.5 (#18489)](#27c4476)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [[diffusion] fix: fix fsdp (#18187)](#efcdda0)
    - [[diffusion] feat: support parallel wan-vae decode (#18179)](#47978ee)
    - [Support GlmMoeDsaForCausalLM (#18521)](#398b81f)
    - [Deepseekv32 compatibility with transformers v5 (#18297)](#e8a2c13)
    - [[EPD] Add notification mechanism to fix server hang and a...](#0b15f19)
    - [Make bench_one_batch_server compatible for more backends ...](#1d366f1)
    - [[HiCache][PP] add test case for compatibility (#16395)](#df733c5)
    - [[AMD] add amd ci monitor (#17476)](#316f9cb)
  - [🟢 低重要度变更 (12)](#-🟢-低重要度变更-12)
    - [[NPU] update npu doc (#18474)](#d0d387d)
    - [[NPU][docs] improve docs for Best Practice on Ascend NPU ...](#99101ce)
    - [[sgl-kernel] upgrade deepgemm (#18362)](#bec7fe9)
    - [[NPU] [CI] Enable run multimodal NPU CI when changes only...](#49cbb46)
    - [[HiCache] fix: StorageMetricsCollector was initialized tw...](#8da14ae)
    - [[AMD] Turn on aiter-prebuild (#18425)](#d94d0af)
    - [Fix idle batch predict dtype in spec v2 (#18379)](#4a1b50b)
    - [Tiny fix wrong metric collect key name `forward_prefill` ...](#2825f5d)
    - [Add cache_config_info metric. (#17273)](#26a006e)
    - [docs: expand and update modelopt documentation (#18479)](#54589a2)
    - [[Auto Sync] Update cache_init_params.py (20260209) (#18502)](#b027c5a)
    - [[Auto Sync] Update logits_processor.py (20260209) (#18503)](#ce95f20)
#### 🔴 高重要度变更 (3)

### [DLLM] Basic dLLM scheduling strategy and implementation (#17484)
**SHA**: `26f2b37` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/26f2b3798db8244ea52d1b781483ba9798b34752)

**🎯 变更类型**：功能增强、架构变更、性能优化  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 新增 **Diffusion LLM（dLLM）** 调度相关的 Mixin 与管理类，实现对 dLLM 请求的分阶段预填（prefill）与解码（decode）调度。  
- 在 `Scheduler`、`ScheduleBatch`、`SchedulePolicy` 等核心调度模块中加入 dLLM 调度路径，拆分了原有的 prefill 逻辑，并提供了独立的 `DllmManager` 用于等待/排队/分配资源的管理。  
- 对 `ServerArgs` 做了兼容性检查（禁用 LoRA、disaggregation、mixed‑chunk 等在 dLLM 场景下不支持的特性），并在 forward 模式中新增 `DLLM_EXTEND`。  
- 删除了原有的 `DllmStagingReqs` 实现，统一由 `DllmManager` 负责，同步了相关调用点，简化了代码路径。  

**🎯 影响范围**  
- **调度层**：`sglang/srt/managers/scheduler.py`、`schedule_policy.py`、`schedule_batch.py`  
- **请求对象**：`sglang/srt/managers/schedule_batch.py`（`Req` 继承 `ReqDllmMixin`）  
- **dLLM 管理**：新增 `sglang/srt/dllm/mixin/req.py`、`sglang/srt/dllm/mixin/scheduler.py`  
- **前向执行**：`sglang/srt/model_executor/forward_batch_info.py`（新增 forward mode）  
- **服务器参数**：`sglang/srt/server_args.py`（限制 LoRA、disaggregation、mixed‑chunk）  
- **测试**：删除了旧的批处理测试，更新了 `dllm/test_llada2_mini.py` 以适配新批次大小。  

---

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **模块化**：将 dLLM 相关的状态 (`dllm_phase`, `dllm_ids`, `dllm_config` 等) 从 `Req` 抽离到 `ReqDllmMixin`，实现了关注点分离，降低 `Req` 类的职责。<br>- **调度路径分离**：通过 `SchedulerDllmMixin` 与 `DllmManager`，原有的 `PrefillAdder` 仍负责普通请求，dLLM 请求走独立的 `get_new_batch_dllm` 流程，避免了在同一批次中混用普通与 dLLM 请求导致的资源竞争。<br>- **统一入口**：`Scheduler.get_next_batch_to_run` 根据 `dllm_config` 自动切换为 dLLM 批次生成，保持对外接口不变。 |
| **性能影响** | - **资源隔离**：dLLM 的 `max_running_requests` 与普通请求的 `max_running_requests` 分别计数，防止 dLLM 大块 token（block_size）抢占所有 KV 缓存，提升整体吞吐。<br>- **预填阶段划分**：将 dLLM 请求分为 `STAGING_PREFILL/INCOMING_PREFILL` 与 `STAGING_DECODE/INCOMING_DECODE`，在 `PrefillAdder` 中提前判断是否可以加入，避免在 decode 阶段才发现资源不足造成频繁回滚。<br>- **调度开关**：在 `ServerArgs._handle_dllm_inference` 中关闭 LoRA、disaggregation、mixed‑chunk，防止这些特性在 dLLM 场景下额外的内存/计算开销。<br>- **潜在瓶颈**：dLLM 仍依赖 `dllm_block_size`（默认 64）进行 token 批量填充，若 `max_running_requests` 设置过大，单个 batch 的 token 数可能激增，导致 KV cache 迁移或显存碎片。 |
| **安全考虑** | - **配置校验**：在 `ServerArgs._handle_dllm_inference` 加入显式警告并强制关闭不兼容特性，防止因 LoRA/Disaggregation 引入的未受控内存访问或跨进程共享导致的安全风险。<br>- **请求重复检测**：`DllmManager.add_waiting_reqs` 中加入了重复 request ID 检测，避免同一请求被多次调度产生状态不一致。<br>- **异常路径**：`add_dllm_staging_req` 会在 token 配额不足时返回 `NO_TOKEN`，防止在资源耗尽时继续执行导致 OOM。 |
| **可维护性** | - **Mixin 方式**：通过 `ReqDllmMixin` 与 `SchedulerDllmMixin` 将 dLLM 代码解耦，可在未来独立演进（如支持多种 diffusion 算法）。<br>- **代码精简**：删除了旧的 `DllmStagingReqs`，统一使用 `DllmManager`，减少冗余实现。<br>- **类型注解**：大量使用 `TYPE_CHECKING` 与 `Optional`，提升 IDE 自动补全与静态检查的可读性。 |

---

### ⚠️ 潜在风险  

1. **兼容性回退**  
   - 旧版用户仍可能在 `server_args` 中保留已废弃的 `dllm_algorithm` 以外的参数（如 `--enable-lora`），虽然代码会自动禁用，但如果用户在代码中自行开启 LoRA 相关的模型权重加载，可能出现 **未捕获的运行时错误**。  
2. **资源争用**  
   - `DllmManager` 的 `max_running_requests` 与全局 `max_running_requests` 同时受 `server_args` 控制，若两者不匹配（如 `max_running_requests` 很大，但 `dllm_config.max_running_requests` 较小），可能导致 **waiting_queue 永久堆积**，影响调度公平性。  
3. **批次切换逻辑**  
   - `Scheduler.get_next_batch_to_run` 先调用 `self.dllm_manager.filter_finished_reqs()`，随后根据 `dllm_config` 决定是否走 `get_new_batch_dllm`。若 `dllm_config` 为 `None` 而 `dllm_manager` 中仍残留旧请求（例如异常退出后未清理），可能导致 **死锁或空 batch**。  
4. **测试覆盖缺失**  
   - 删除了原 `test_dllm_batching.py`，而新加入的测试仅覆盖 `llada2_mini` 单模型，仍缺少 **多模型、多GPU、混合普通+ dLLM 请求的集成测试**。  
5. **ForwardMode 扩展**  
   - `ForwardMode.DLLM_EXTEND` 仅在调度层标记，若后续模型执行路径（如 `model_executor`）未针对该模式进行特殊处理，可能导致 **不匹配的 KV 缓存布局**，出现错误或性能回退。  

---

### 💡 关注建议  

| 建议 | 说明 |
|------|------|
| **统一资源上限** | 在 `ServerArgs` 初始化时校验 `max_running_requests` 与 `dllm_config.max_running_requests` 是否一致或在合理比例范围内，防止 waiting queue 长时间堆积。 |
| **完善清理机制** | 在 `Scheduler.shutdown` 或异常捕获路径中加入 `self.dllm_manager.waiting_queue.clear()` 与 `staging_queue.clear()`，确保进程退出后不会残留未处理请求。 |
| **细化 ForwardMode 处理** | 在 `model_executor` 的 forward 路径中对 `DLLM_EXTEND` 做显式分支（如使用 `dllm_block_size` 进行 KV cache 分配），并在日志中记录使用的 block 大小，方便排查。 |
| **增加集成测试** | 新增 **混合调度** 场景的 CI：普通请求 + dLLM 请求并发，验证 `dllm_manager` 与 `prefill adder` 的资源分配、preempt 行为以及 batch 合并的正确性。 |
| **监控指标** | 在 `metrics_collector` 中加入 `dllm_waiting_queue_len`、`dllm_staging_queue_len`、`dllm_token_remain` 等指标，便于运维观察 dLLM 资源使用情况。 |
| **文档与示例** | 更新 README 与部署文档，明确说明 **dLLM 不支持 Lo

---

### feat(kv-events): Add medium field to KV event types for storage tier tracking (#18205)
**SHA**: `01e3f46` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/01e3f4682e2071ae110a593a219685a3ef7e201f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：在 KV 缓存相关事件中新增 `medium` 字段，用于标记存储介质（GPU / CPU_PINNED），并在 RadixCache 的写入/删除路径统一填入 `MEDIUM_GPU`。此改动旨在实现对不同存储层级的追踪和后续分析。  

**🎯 影响范围**：`sglang/srt/disaggregation/kv_events.py`、`sglang/srt/mem_cache/radix_cache.py`，以及所有消费 KV 事件的监控、日志、持久化或调度模块。  

**🔍 技术洞察**：  
- **架构影响**：  
  - 为 KV 事件模型引入存储介质概念，提升事件的语义信息，为多层级缓存调度、资源监控、成本分析等提供数据支撑。  
  - 该字段为可选 (`Optional[str]`)，向后兼容旧代码；但若其他模块基于事件结构进行序列化（如 JSON、protobuf）或类型检查，需要同步更新对应 schema。  
- **性能影响**：  
  - 仅在事件对象创建时多传递一个字符串引用，开销极小；对热点路径（如每页写入一次）影响可以忽略。  
  - 若后续对 `medium` 做统计或过滤，可能带来额外的 I/O/计算，但目前仅是记录，不会改变现有缓存命中/淘汰逻辑。  
- **安全考虑**：  
  - 该字段仅用于内部监控，不涉及敏感信息或权限控制。  
  - 需要确保在跨进程/网络传输（如日志聚合）时，序列化格式保持兼容，防止因未知字段导致解析错误。  

**⚠️ 潜在风险**：  
1. **下游消费方未适配**：若已有消费者（日志、监控、审计）基于事件的 `__dict__` 或固定字段列表进行处理，新增字段可能被忽略或导致序列化失败。  
2. **序列化/持久化 schema 不同步**：使用自定义 JSON/MessagePack 等持久化时，可能需要手动更新 schema，否则 `medium` 会被丢失或出现兼容性警告。  
3. **遗漏其他写入路径**：目前仅在 `RadixCache._record_store_event/_record_remove_event` 添加了 `medium=MEDIUM_GPU`，若还有其他 KV 写入实现（如 CPU‑Pinned 缓存）未补齐，导致统计不完整。  
4. **默认值不明确**：`None` 代表“未知或非 GPU”。若后续依赖该字段进行资源分配决策，需要明确 `None` 的语义，以免误判。  

**💡 关注建议**：  
- **文档与规范**：在项目文档或代码注释中说明 `medium` 的取值约定（`GPU`、`CPU_PINNED`、`None`），并更新事件 schema（如 pydantic、protobuf）示例。  
- **向后兼容测试**：为关键下游消费者（日志、监控、持久化）补充单元测试，验证在缺失 `medium`（旧版本事件）和存在 `medium`（新版本事件）两种情况下均能正常解析。  
- **完整性覆盖**：审查项目中其他 KV 事件生成点（例如 CPU‑Pinned 缓存实现），确保在合适的路径上填入对应的 `MEDIUM_CPU`，避免统计盲区。  
- **监控仪表盘更新**：若已有基于 KV 事件的监控仪表盘，新增 `medium` 维度的过滤/聚合，以便观察 GPU 与 CPU‑Pinned 缓存的使用比例。  
- **回滚策略**：保持 `medium` 为可选字段，若出现严重兼容性问题，可通过在事件创建处不传递该参数（保持 `None`）快速回退。  

通过上述措施，可在确保功能增益的同时，最小化对现有系统的冲击。

---

### model: support Qwen3.5 (#18489)
**SHA**: `27c4476` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/27c447653d9cf0f63aea1c190b931be4875cbf86)

**🎯 变更类型**：功能增强（新增 Qwen3.5 系列模型＋多模态、Mixture‑of‑Experts、Multi‑Token‑Predictor 支持）  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 在 SGLang 中首次加入对 Qwen 3.5（含 Dense、MoE、MTP）模型的完整推理实现，包括配置、权重加载、前向路径、以及多模态（Vision）处理。  
2. 为 ‑​`ForwardBatch`、`LogitsProcessorOutput`、`LogitsMetadata` 等核心结构新增 `mm_input_embeds` 字段，使多模态特征能够在跨阶段（pipeline、speculative）间透明传递。  
3. 更新了多处公共工具（rope、视频读取、服务器参数、speculative worker）以兼容新模型和新增的 **rope_parameters** 配置。  

**🎯 影响范围**  
- `sglang/srt/configs/*`（新增 `qwen3_5.py`，导出 `Qwen3_5Config/…MoeConfig`）  
- `sglang/srt/models/qwen3_5*.py`（Dense、MoE、MTP实现）  
- `sglang/srt/layers/*`（roto‑embedding 兼容新模型前缀、LogitsProcessor 增加 `mm_input_embeds`）  
- `sglang/srt/model_executor/*`（`ForwardBatch`、`ModelRunner`、权重加载）  
- `sglang/srt/managers/mm_utils.py`（合并 multimodal embed 输出）  
- `sglang/srt/speculative/eagle_worker.py`（把 `mm_input_embeds` 传递给草稿模型）  
- `sglang/srt/utils/*`（视频读取支持 `list/tuple/torch.Tensor/np.ndarray`，新增 VideoReader 判定）  
- `sglang/srt/server_args.py`（模型列表扩展、量化路径兼容）  
- `benchmark/kernels/fused_moe_triton/common_utils.py`（新增模型名称）  
- 多模态处理器 `sglang/srt/multimodal/processors/qwen_vl.py`（识别 Qwen‑3.5 系列）  

**🔍 技术洞察**  

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | • 新增 **Qwen3_5ForCausalLM / Qwen3_5MoeForCausalLM** 与 **Qwen3_5AttentionDecoderLayer / Qwen3_5LinearDecoderLayer**，复用了现有 **LayerCommunicator、DP‑Attention、RadixAttention** 等基础设施。<br>• 引入 **GatedDeltaNet**（线性注意力）和 **MTP**（多 Token 预测）两种新解码路径，分别通过 `layers_block_type` 控制。<br>• 为多模态特征在 pipeline/Speculative 场景下保留 `mm_input_embeds`，避免在每层重新 embed，提升跨阶段一致性。 |
| **性能影响** | • **线性注意力（GatedDeltaNet）** 与原有 **RadixAttention** 在 GPU 上使用自定义 kernel，可显著降低长序列的显存占用，适用于超长上下文。<br>• **MTP** 在 `Qwen3_5MultiTokenPredictor` 中将上一轮的 hidden 状态与当前 token 的 embedding 拼接再一次性前向，理论上可提升 1‑2 token 的吞吐。<br>• 额外的 `mm_input_embeds` 复制成本极低（仅一次指针转移），对整体推理延迟影响可忽略。<br>• 新增的 `rope_parameters` 兼容旧的 `rope_scaling`，在启用 **mrope** 时保持原有缓存策略。 |
| **安全考虑** | • 代码路径中未出现外部输入直接参与计算图构建，安全风险基本等同于已有模型。<br>• 视频读取函数 `load_video` 已放宽输入类型，需确保上层对文件来源做安全校验，防止恶意构造的二进制/torch.Tensor 被当作视频流读取导致异常或资源耗尽。 |
| **可维护性** | • 新增模型文件体量大（>1k 行），但结构与已有 Qwen‑3、Qwen‑3‑VL 系列保持高度统一，便于后期统一升级。<br>• 对 `ForwardBatch` 添加字段后，所有构造该对象的调用点已同步更新，风险已降至最低。<br>• `qwen3_5.py` 中的权重加载逻辑复制自 Qwen‑3 系列，保持一致性，但额外引入了 **expert 参数映射**，建议抽象为公共函数以降低重复。 |
| **兼容性** | • 新增 `rope_parameters` 支持后，在旧模型仍会读取 `rope_scaling`，兼容性已保留。<br>• `server_args` 对模型列表做了扩展，确保在自动量化/FlashInfer 选择时能够识别 Qwen 3.5。 |

**⚠️ 潜在风险**  
1. **权重映射错误**：`load_weights` 中对 MoE 与非 MoE 参数的映射逻辑非常复杂，若 checkpoint 命名略有差异（如 `gate_up_proj` vs `gate_proj`），可能导致未加载或错误加载参数。  
2. **mm_input_embeds 传递遗漏**：`eagle_worker.forward_draft_extend` 已补丁，但若未来新增的 `ForwardBatch` 子类忘记复制该字段，可能导致多模态 token 在草稿模型阶段丢失。  
3. **视频输入安全**：`load_video` 现在接受任意 `list/tuple/torch.Tensor/np.ndarray`，若调用方未做尺寸/类型检查，可能触发 `VideoReader` 的异常或内存泄漏。  
4. **MTP 与 Speculative 冲突**：MTP 只在草稿模型使用，若用户在同一请求中同时开启 Speculative（draft + target）且使用 MTP，可能出现 `mm_input_embeds` 冲突或状态不匹配。  
5. **默认配置缺失**：`Qwen3_5Config` 中未显式声明 `partial_rotary_factor`、`rope_theta` 等默认值，若上游 checkpoint 未提供，使用默认 `None` 可能导致运行时异常。  

**💡 关注建议**  
- **单元/集成测试**：新增针对 Qwen 3.5（Dense、MoE、MTP）的一键推理脚本，特别是 **多模态+MTP** 的组合路径。  
- **权重加载统一抽象**：将 `stacked_params_mapping`、`expert_params_mapping` 抽成工具函数，避免在 Dense、MoE、MTP 三个类中重复实现。  
- **日志与警告**：在 `load_weights` 中对未匹配的参数给出 **WARNING**（当前已有），建议在 CI 中统计 “未匹配参数比例”，防止因 checkpoint 版本升级导致加载不全。  
- **安全审计**：对 `load_video` 增加文件头/尺寸校验，或在入口层限制只能接受 `str`（文件路径）或 `bytes`，对 `list/tuple/torch.Tensor` 的直接使用加上显式注释说明为内部调试

---

#### 🟡 中重要度变更 (8)

### [diffusion] fix: fix fsdp (#18187)
**SHA**: `efcdda0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/efcdda01766b5e5f3fd94781ad9860bb840d4102)

**🛠️ 变更类型**：功能增强 / Bug 修复（FSDP 兼容）  
**⚡ 重要程度**：🟡 中  

**🎯 变更摘要**  
1. 为 Z‑Image 模型新增 `is_zimage_layer` 判定函数，并在 `ZImageArchConfig`、`ZImageTransformer2DModel` 中暴露 `_fsdp_shard_conditions`，实现细粒度 FSDP 分片。  
2. `layernorm`、`triton_ops`、`lora/linear` 的前向实现已修正残差返回与权重切片，以兼容 FSDP/TP。  
3. `fsdp_load` 重构：统一排序参数名、检测 FSDP 环境、在 CPU‑offload 场景下强制转到 CPU；确保多卡同步加载。  
4. 运行时加入 `--use-fsdp-inference` 开关，GPU‑worker、denoising 阶段以及 OOM 提示相应调整。  
5. 测试层面添加 `extras` 参数、FSDP 基准数据及相应 JSON/脚本更新。

**🎯 影响范围**  
- `sglang/multimodal_gen/configs/models/dits/zimage.py`  
- `runtime/layers/*`（layernorm、triton_ops、lora/linear）  
- `runtime/loader/fsdp_load.py`、`runtime/models/dits/zimage.py`  
- `runtime/managers/gpu_worker.py`、`runtime/pipelines_core/stages/denoising.py`  
- 测试套件：`test/server/*`、`test/scripts/*`、`perf_baselines.json`

**💡 关注建议**  
1. **分片策略**：确认 `is_zimage_layer` 对所有 Z‑Image 子模块的匹配准确，避免误分片导致显存泄漏或性能下降。  
2. **残差返回**：`forward_cuda` 现在可能返回 `(out, residual_out)`，检查所有调用方是否已适配 tuple 返回。  
3. **LoRA 权重切片**：`slice_lora_a_weights` 直接返回 `A`，若模型在不同 device/ dtype 上运行，可能出现不匹配，建议加入 `to(self.base_layer.weight.device, dtype=self.base_layer.weight.dtype)`。  
4. **FSDP 加载**：多卡环境下请务必使用相同的 `torch.distributed` `backend` 并确保 `cpu_offload` 与 `strict` 参数符合预期；可在小规模实验后再推广到大模型。  
5. **CLI 参数**：新增 `extras` 通过 `--use-fsdp-inference` 等传入，确认主入口的 `argparse` 已配置 `parse_known_args` 或相应选项，否则可能被忽略。  
6. **回归测试**：运行新增的 `fsdp-inference` 基准，比较 `expected_e2e_ms` 与实际，确保性能提升且数值误差在合理范围。  

总体来看，此次提交完善了 FSDP 在 Diffusion（尤其 Z‑Image）上的支持，提升了大模型多卡推理的可用性与显存效率。但因前向返回结构及权重切片细节的修改，建议在多卡真实环境下做完整回归，以防潜在的类型/设备不匹配问题。

---

### [diffusion] feat: support parallel wan-vae decode (#18179)
**SHA**: `47978ee` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/47978ee85843e207479c1ea1cc5af1b42a02f511)

**🎯 变更类型**：功能增强（为 WanVAE 引入并行 encode / decode）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `configs/models/vaes/wanvae.py` 中新增 `use_parallel_encode / use_parallel_decode` 开关（默认 true）。  
- 大量重构 `wanvae.py`：原始实现的 `AvgDown3D、DupUp3D、WanCausalConv3d、WanRMS_norm、WanUpsample` 等迁移到 `runtime/models/vaes/parallel/wan_common_utils.py`，新增 `wan_dist_utils.py` 实现分布式切分、halo‑exchange、并行卷积、残差块等。  
- 在模型的 `__init__` 中根据分布式环境（`torch.distributed.is_initialized()`）与开关自动切换到并行版层（`WanDist*`），并在前向传播时完成高度切分、局部 padding、全局 gather / trim。  
- 新增 `bind_context` 用于在全局 contextvars 中绑定特征缓存变量。  
- 更新 `ParallelTiledVAE` 相关逻辑，使其在并行模式下能够正确统计 `downsample_count / upsample_count` 并在前向中调用 `split_for_parallel_*` / `gather_and_trim_height`。  

**🎯 影响范围**  
- `sglang/multimodal_gen/runtime/models/vaes/wanvae.py`（Encoder/Decoder 逻辑）  
- `sglang/multimodal_gen/runtime/models/vaes/wanvae.py` 之外的 `ParallelTiledVAE` 调用路径  
- 新增的 `runtime/models/vaes/parallel/wan_common_utils.py`、`wan_dist_utils.py`  
- 配置文件 `configs/models/vaes/wanvae.py`  

**💡 关注建议**  

| 建议 | 说明 |
|------|------|
|**分布式前置检查**|`forward` 中直接使用 `dist.is_initialized()` 决定 `world_size`。若在未初始化的进程中调用 `get_sp_world_size()`（内部假设已初始化），可能抛异常。建议在入口统一判断并在单机模式下返回 `1`，或在 `parallel` 包中加入安全包装。|
|**全局 Context 绑定顺序**|`bind_context(is_first_frame, …)` 在模块加载时即执行，依赖的 `ContextVar` 必须已经创建。若其他模块提前导入导致顺序冲突，可能出现 `RuntimeError`。建议将绑定放入 `forward_context` 初始化，或提供显式 `init_parallel_context()` 接口。|
|**默认开启的兼容性**|`use_parallel_encode/decode` 默认 `True` 会在单卡环境下仍走分布式代码路径（会产生不必要的切分、halo‑exchange），且可能因为 `world_size==1` 而产生额外 `gather` 调用。建议默认 `False`，或在 `__init__` 中判断 `world_size>1` 自动关闭。|
|**halo_exchange 死锁风险**|在 `halo_exchange` 中使用 `dist.isend/irecv`，但未对异常或超时做保护。若出现通信错误（例如 NCCL 初始化失败）会导致进程阻塞。建议在 `try/except` 中捕获异常并回退到单机实现。|
|**梯度检查点与分布式**|部分块仍保留 `self.gradient_checkpointing`，但在并行实现中没有包装 `torch.utils.checkpoint`. 如在训练时开启检查点，可能导致跨进程的激活值丢失。应在 `WanDist*` 前向中加入相同的检查点支持或在文档中说明不兼容。|
|**模型保存/加载**|并行版层的参数名称与原版相同，但 `state_dict` 中会出现 `WanDist*` 对象。如果已有 checkpoints 使用原版类，加载时会报 `Missing key`。建议在 `load_state_dict` 前统一映射（例如在 `ParallelTiledVAE` 中提供 `state_dict_compat()`），或在 README 中提醒升级步骤。|
|**单元测试/CI**|新增的大量分支未覆盖现有测试。建议在 CI 中添加：<br>1. 单卡模式下 `use_parallel_* = False` 的前向一致性测试；<br>2. 多卡（2‑4 卡）模拟分布式环境的前向‑后向梯度检查；<br>3. `halo_exchange` 边界条件的单元测试（首/尾 rank）。|
|**文档与示例**|并行 VAE 的使用方式（如何启动 `torchrun`、环境变量 `SP_PARALLEL_SIZE` 等）未在代码注释里说明。请在 `configs/models/vaes/wanvae.py` 或 `README` 中补充快速上手示例。|

**总体结论**  
本次 PR 为 WanVAE 引入了显式的序列并行（height 方向）实现，代码结构由单一实现拆分为通用 + 并行两套，提升了大尺度视频 VAE 的可扩展性。但并行路径依赖分布式通信，新增的全局 context 与默认启用方式可能导致单机运行出现隐藏错误或性能下降。建议根据上述风险点完成防护、兼容性迁移以及测试，以确保在单卡和多卡环境下均能稳定运行。

---

### Support GlmMoeDsaForCausalLM (#18521)
**SHA**: `398b81f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/398b81f78c939469f60f3f54f6526df95f4cab31)

**变更类型**：功能增强  
**重要程度**：🟡 中  

**变更摘要**：本次提交为 sglang 增添对 `GlmMoeDsaForCausalLM` 模型的全链路支持。核心做法是：①在 `model_config.py` 将其归入 DeepSeek‑NSA 判定并同步草稿模型替换逻辑；②在 `glm4_moe.py` 新建继承自 `DeepseekV2ForCausalLM` 的空壳类并加入入口列表；③在 `server_args.py` 扩展注意力后端、 speculative‑decoding、确定性推理等多处特化路径，使其享受 DeepSeek‑NSA、MLA 等优化。

**影响范围**  
- `python/sglang/srt/configs/model_config.py`（模型判定及形状推导）  
- `python/sglang/srt/models/glm4_moe.py`（新增模型类、EntryClass）  
- `python/sglang/srt/server_args.py`（注意力后端、推理参数自动选择）  

**关注建议**  
1. **权重兼容性**：`GlmMoeDsaForCausalLM` 直接复用 DeepseekV2 实现，确保模型权重格式与 DeepseekV2 完全匹配，否则加载会失败。  
2. **功能覆盖**：目前仅继承空实现，若后续出现 DSA‑专属算子（如特殊的 Q‑K 归一化或调度），需要在 `glm4_moe.py` 中补齐对应逻辑。  
3. **测试覆盖**：新增模型应加入单元测试，验证草稿模型切换、NSA + MLA 强制开启以及 speculative‑decoding 路径的正确性。  
4. **硬件依赖**：文档已提示在 SM100（Blackwell）上强制使用 MLA，建议在 CI 中加入相应 GPU 环境的回归，以防止在不支持的硬件上出现意外 fallback。  

总体来看，改动集中在模型识别与调度层，风险主要在权重兼容与未实现的 DSA 专属特性，建议在正式发布前完成全链路功能测试。

---

### Deepseekv32 compatibility with transformers v5 (#18297)
**SHA**: `e8a2c13` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e8a2c133807cc25d872be19ab35110c79df77efd)

**🎯 变更类型**：功能增强 / 兼容性修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `model_config._derive_model_shapes` 中统一了 rope‑scaling 的读取方式，兼容 Transformers v5（`rope_parameters`）并兼容 v4（`rope_scaling.type`）。  
2. 新增环境变量 `SGLANG_NSA_FORCE_MLA`，允许强制在 NSA 前置阶段使用 MLA（关闭 MHA‑ONE‑SHOT）。  
3. 为 `NSAIndexer` 与 DeepSeek‑V2 增加 `is_neox_style` 参数，使其能够根据配置中的 `rope_interleave` / `indexer_rope_interleave` 自动选择 NeoX‑style 或默认实现。  
4. 在 DeepSeek‑V2 初始化、rope 包装以及模型构造时改用 `rope_parameters`，并在缺省时回退到旧字段。  

**🎯 影响范围**  
- `python/sglang/srt/configs/model_config.py`（模型形状与 rope 处理）  
- `python/sglang/srt/environ.py`（新增 env）  
- `python/sglang/srt/layers/attention/nsa/*`（索引器、后端实现）  
- `python/sglang/srt/models/deepseek_v2.py`（模型初始化、rope 包装）  

**💡 关注建议**  
1. **兼容性验证**：在使用 Transformers v4 与 v5 的模型上分别跑一次启动和推理，确保 `rope_scaling` 与 `rope_parameters` 均被正确解析。  
2. **环境变量影响**：`SGLANG_NSA_FORCE_MLA` 关闭 MHA‑ONE‑SHOT 可能导致前置阶段的吞吐下降，建议在生产前做性能基准对比。  
3. **`is_neox_style` 默认**：当前默认 `True`（NeoX‑style），但部分模型（如 DeepSeek‑V2）可能需要 `False`；确保配置文件中 `rope_interleave` / `indexer_rope_interleave` 正确设置，否则会产生不匹配的 RoPE。  
4. **回归测试**：添加针对 `rope_parameters`、`rope_scaling.type`、`rope_type` 的单元测试，并覆盖 `SGLANG_NSA_FORCE_MLA` 的开关路径。  
5. **文档更新**：在模型配置文档、环境变量说明中补充 v5 兼容说明及 `SGLANG_NSA_FORCE_MLA` 的使用场景，防止用户误解导致性能回退。  

以上改动提升了对 Transformers v5 的兼容性，并提供了更细粒度的 NSA 前置控制，但需注意配置一致性和潜在的性能变化。

---

### [EPD] Add notification mechanism to fix server hang and add timeout env var (#18229)
**SHA**: `0b15f19` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0b15f19927bf703ab32891ce528521b66e076433)

**🎯 变更类型**：功能增强 + Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为图像嵌入的请求引入超时机制，避免因编码服务阻塞导致整个 SGLang Server 卡死。  
2. 新增环境变量 `SGLANG_ENCODER_RECV_TIMEOUT`、`SGLANG_ENCODER_SEND_TIMEOUT`，可自定义接收/发送超时时间（默认 180 s）。  
3. 在 `EncodeReceiver` 中记录请求起始时间并在超时后返回 `TIMEOUT` 状态；在 `EncodeServer` 中通过 `asyncio.Condition` 对接收端点数量的变化进行通知，配合新的发送超时控制。  
4. 统计日志中加入 `waiting-image-req`，便于监控等待图像请求的堆积情况。  

**🎯 影响范围**  
- `python/sglang/srt/disaggregation/encode_receiver.py`  
- `python/sglang/srt/disaggregation/encode_server.py`  
- `python/sglang/srt/environ.py`（新增 env 变量）  
- `python/sglang/srt/managers/scheduler_metrics_mixin.py`（日志展示）  

**💡 关注建议**  
1. **超时阈值**：默认 180 s 较长，若部署环境对响应时效有更严格要求，请在启动脚本或容器环境中调低 `SGLANG_ENCODER_RECV_TIMEOUT`、`SGLANG_ENCODER_SEND_TIMEOUT`。  
2. **异常处理**：超时分支会返回 HTTP 408，确保上层调用（如调度器或客户端）能够正确捕获并进行重试或降级。  
3. **并发安全**：`rid_to_cond` 与 `cond_dict_lock` 采用 `asyncio.Lock`，在高并发场景下应留意锁竞争对整体吞吐的影响；如有性能瓶颈，可考虑改用 `collections.defaultdict` + `asyncio.Event`。  
4. **监控告警**：`waiting-image-req` 计数写入日志后，建议在监控平台添加对应指标阈值报警，以及时发现图像处理异常堆积。  
5. **回退兼容**：如果旧版客户端未传递 `req_id` 或 `receive_count`，新增的 `Condition` 可能永不触发，建议在部署前验证所有调用路径均已适配。  

总体而言，此次改动显著提升了多模态编码阶段的鲁棒性，避免了因单张图片卡住而导致的服务不可用。后续关注超时阈值的业务适配和高并发下的锁竞争即可。

---

### Make bench_one_batch_server compatible for more backends (#18512)
**SHA**: `1d366f1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1d366f1206abf82fef7cc50d1ee7af749a1867ae)

**🎯 变更类型**：功能增强（在 `bench_one_batch_server_internal` 中加入对 vLLM 后端的兼容）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `BenchArgs` 新增 `backend` 参数，默认 `"sglang"`，支持 `"vllm"`。  
- 在 CLI 中加入 `--backend` 选项。  
- `_warmup_cache`、`run_one_case`、`run_benchmark_internal` 等核心函数根据 `backend` 构造不同的请求体和 URL（`/v1/completions` vs `/generate`），并在 vLLM 场景下使用 `reset_prefix_cache`、`model` 参数等。  
- 为 vLLM 添加模型名称获取路径（`/v1/models`），并相应地加载 tokenizer。  
- 对返回的流式数据解析做了分支处理，兼容 OpenAI‑compatible 的 vLLM 流式格式。  

**🎯 影响范围**  
- `python/sglang/test/bench_one_batch_server_internal.py`（全部改动）  
- 依赖的辅助函数：`_warmup_cache`、`run_one_case`、`run_benchmark_internal`、`launch_server_process` 等。  
- 与 tokenizer、服务器信息获取相关的分支逻辑也随之改变。  

**💡 关注建议**  
1. **环境准备**：使用 vLLM 时需确保 `VLLM_SERVER_DEV_MODE=1`（用于 `/reset_prefix_cache`）并在服务器侧暴露 `/v1/models`。  
2. **参数传递**：`--backend vllm` 时必须同时提供 `--model-name`（或让脚本自动从 `/v1/models` 读取），否则请求会缺失 `model` 字段。  
3. **兼容性检查**：原有的 SGLang 路径仍保持默认，可通过不指定 `--backend` 继续运行；若已有 CI/脚本依赖 `flush_cache`，请确认在 vLLM 场景切换为 `reset_prefix_cache`。  
4. **性能指标**：vLLM 不再返回 `last_gen_throughput`、`avg_spec_accept_length`，相关统计会被置为 `-1`，需要在后处理时考虑这一差异。  
5. **测试覆盖**：新增的分支未加入单元测试，建议在 CI 中加入一次 vLLM 启动的 smoke‑test，验证 tokenization、图片数据（mmmu）以及流式解析是否均能正常工作。  

总体来说，此次改动为 benchmark 脚本提供了透明的多后端支持，除了上述运行时要求外，对原有功能影响最小。若在生产基准中切换后端，请注意上述注意事项并逐步验证。

---

### [HiCache][PP] add test case for compatibility (#16395)
**SHA**: `df733c5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/df733c5ee5e8014d6c6f062827a0b887d3e4be29)

**🎯 变更类型**：功能增强（新增兼容性测试）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `test/manual/hicache` 中新增 `test_pp_with_hicache.py`，用于在 **PP+HiCache** 场景下启动 Mooncake 元数据服务、master 服务以及 SGLang 服务器，随后跑 GSM8K few‑shot 评测，验证① 初始推理准确率>0.6，② flush‑cache 后再次评测准确率波动 < 0.05。  

**🎯 影响范围**  
- **测试套件**：新增一个手动/慢速测试，涉及 `sglang.srt.utils.kill_process_tree`、`sglang.test.few_shot_gsm8k.run_eval`、端口分配等公共工具。  
- **运行时依赖**：需本地装有 `mooncake` 二进制（`mooncake_master`、`mooncake.http_metadata_server`），且能够打开多个 TCP 端口。  

**💡 关注建议**  
1. **CI 可靠性**：该测试依赖外部服务，可能在 CI 环境缺少 Mooncake 时频繁 **Skip** 或卡死。建议在 `pytest`/`unittest` 中加入 `@unittest.skipUnless` 检查 `which mooncake_master`，或提供一个轻量的 mock 实现。  
2. **资源控制**：启动两路 PP、两路 TP，且使用默认模型（可能是 13B/70B），在 CI 机器上会消耗大量显存与时间。可考虑把模型降为 **tiny** 版，仅用于功能验证，或在文档中明确标记为 **slow‑test**。  
3. **端口冲突**：`find_available_port` 在高并发 CI 中仍有概率冲突，建议在 `setUpClass` 捕获 `OSError` 并自动重试，或使用 `socket` 随机分配端口。  
4. **清理完整性**：`tearDownClass` 已调用 `kill_process_tree`，但若 `popen_launch_server` 抛异常，`process` 可能未定义。可在 `finally` 中统一调用 `_stop_mooncake_services()`，防止僵尸进程残留。  
5. **日志与超时**：`_wait_for_mooncake_ready` 采用轮询并硬编码 1.5 s 间隔，容易导致 flaky。建议使用指数回退或加入 `requests` 超时日志，便于调试。  

总体而言，此次提交为 **HiCache‑PP** 的兼容性验证提供了完整的端到端测试，但因依赖外部服务和大模型，建议将其标记为 **optional/slow**，并加强环境检查与清理逻辑，以免影响主 CI 稳定性。

---

### [AMD] add amd ci monitor (#17476)
**SHA**: `316f9cb` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/316f9cbb35840c944bcd1cc0b0722d1bb2b65c78)

**核心变更**  
- 新增 GitHub Actions workflow `.github/workflows/amd-ci-job-monitor.yml`，每日定时或手动触发，用矩阵读取 `pr-test-amd.yml` / `nightly-test-amd.yml` 中的 job 名称并调用 `scripts/ci/query_job_status.py` 生成状态报告。  
- 新增脚本 `scripts/ci/query_job_status.py`：通过 `gh` CLI 调用 GitHub API，分页获取最近 N 小时的 workflow runs、遍历 job 列表、支持前缀过滤、卡住/ghost job 检测、队列/执行时长计算，并以表格、CSV、JSON、Markdown 四种形式输出。  

**影响范围**  
- CI 环境：所有 AMD CI 相关的 PR、nightly 流程将额外消耗一次 `gh api`（最多 20 × 2 × 100 条记录）以及 `yq` 解析工作流。  
- 项目依赖：运行该 workflow 需要 `gh`、`yq`、`tabulate`，若 runner 未预装会导致 job 直接失败。  
- 安全/配额：脚本使用 `GITHUB_TOKEN` 访问 API，频繁调用可能触达速率限制。  

**关注建议**  
1. **依赖声明**：在 workflow 中显式安装 `gh`（`actions/setup-gh`）和 `yq`，或在 repo‑level `devcontainer`/`requirements.txt` 中注明。  
2. **容错与限流**：给 `run_gh_command` 增加重试/退避逻辑，捕获 403（rate‑limit）并给出提示。  
3. **分页安全**：当前硬编码 `page>20` / `page>5`，建议改为 “while 有数据且 page ≤ max_pages” 并在日志中记录实际请求次数。  
4. **过滤逻辑**：前缀匹配后仅检查下一个字符为 ` ` 或 `(`，可能漏掉 `-`、`_` 等合法后缀，考虑改为正则 `^filter_lower($|[-_ ])`。  
5. **跨平台**：脚本使用 `subprocess.run`，在 Windows runner 上 `gh`/`yq` 可能路径不同，建议使用 `shutil.which` 预检。  
6. **单元测试**：抽取 `parse_time`、`calculate_queue_time`、`process_results` 等函数独立测试，确保时间计算、stuck 判定在极端数据（缺失字段、负时差）下仍稳健。  
7. **文档**：在 `README` 或 `docs/ci.md` 中添加“AMD CI 监控使用说明”，并说明 `--summary` 参数在 GitHub Actions 中的实际效果。  

总体而言，此次新增为 AMD GPU CI 提供可视化监控，提升了问题定位效率，但在 CI 环境依赖、错误容忍和过滤细节上仍可进一步强化，以避免因外部工具缺失或 API 限额导致监控任务失效。

---

#### 🟢 低重要度变更 (12)

### [NPU] update npu doc (#18474)
**SHA**: `d0d387d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d0d387dea10dddfd67ef29a7814a01e90f18fbb8)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/platforms/ascend_npu_support.rst` 中新增对 `ascend_npu_support_features.md` 的引用，完善 Ascend NPU 支持特性文档。

---

### [NPU][docs] improve docs for Best Practice on Ascend NPU (#18360)
**SHA**: `99101ce` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/99101ce30b2de4ca25366d91f3dee8457f954670)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：完善 Ascend NPU 最佳实践文档，重新组织表格、统一 TPOT 与数据集描述、补充随机数据基准命令，提升可读性与使用指引。

---

### [sgl-kernel] upgrade deepgemm (#18362)
**SHA**: `bec7fe9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bec7fe9e65239f4118c1633e27650b1fdc47b3f8)

**🎯 变更类型**：代码重构 / 配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：升级 DeepGEMM 子模块至最新提交，修改 CMake 构建以输出 `_C` 扩展模块并显式链接 `torch_python`，并在构建脚本中加入 `USE_CCACHE` 参数；另添一行 CI 注释。

---

### [NPU] [CI] Enable run multimodal NPU CI when changes only in multimodal_gen  (#18523)
**SHA**: `49cbb46` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/49cbb469b437a9584e337368a56cbe2012873191)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/workflows/pr-test-npu.yml` 中新增 `changes_exist` 输出，并将 PR Gate 的执行条件改为判断该输出，以便在仅修改 `multimodal_gen` 时仍能触发多模态 NPU CI。

---

### [HiCache] fix: StorageMetricsCollector was initialized twice (#18354)
**SHA**: `8da14ae` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8da14aea8894c713ee4002809ca7cfce682f932d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 HiCache 中 `StorageMetricsCollector` 被重复实例化的问题，将其初始化逻辑改为只在首次创建时实例化，并在标签变化时保留已有实例，防止重复注册指标。与此同时简化并提前设置缓存配置属性。

---

### [AMD] Turn on aiter-prebuild (#18425)
**SHA**: `d94d0af` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d94d0af57351dcd4813f3a99151161d138630092)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docker/rocm.Dockerfile` 中将 `BUILD_AITER_ALL` 环境变量由 `0` 改为 `1`，开启 AITer 预编译支持。

---

### Fix idle batch predict dtype in spec v2 (#18379)
**SHA**: `4a1b50b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4a1b50bb2dfc30edcb59cd1b40eb98bd6419b117)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将空闲批次的 `predict` 张量 dtype 从 `torch.long` 修正为 `torch.int32`，保持与 spec v2 的数据类型一致。

---

### Tiny fix wrong metric collect key name `forward_prefill` -> `forward_extend` (#18506)
**SHA**: `2825f5d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2825f5d8e0aec3191a16ad886ddd0423dab752fe)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test/registered/metrics/test_metrics.py` 中将错误的度量键名 `forward_prefill` 修正为 `forward_extend`，确保测试收集的 GPU 执行时间指标类别正确。

---

### Add cache_config_info metric. (#17273)
**SHA**: `26a006e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/26a006e47f0bb764d1bda5e1af6631b12c0e7f4a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `Scheduler.init_model_worker` 中新增缓存配置上报；`MetricsCollector` 添加 `cache_config_info` Gauge 并实现 `emit_cache_config_info` 方法用于记录页面大小和页数。

---

### docs: expand and update modelopt documentation (#18479)
**SHA**: `54589a2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/54589a2f2d0801cfb5e3d6136d0601277ffd2a9a)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `quantization.md` 中细化 NVIDIA ModelOpt 章节，新增离线/在线量化模式说明、预量化模型与自建量化检查点的使用示例、安装指引及代码示例，提高文档可读性和实操性。

---

### [Auto Sync] Update cache_init_params.py (20260209) (#18502)
**SHA**: `b027c5a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b027c5aca656acf024e3b31f29c3984bd571b554)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `cache_init_params.py` 为 `CacheInitParams` 添加 `cache_ttl_seconds` 可选字段，用于控制缓存条目的存活时间（无则禁用）。同时稍微修正了 `kv_events.py` 中的注释格式。整体影响小，主要为功能扩展。

---

### [Auto Sync] Update logits_processor.py (20260209) (#18503)
**SHA**: `ce95f20` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ce95f203b054dae900f60d976fbe2298899b5bd5)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 logits_processor.py 中新增 `self.vocab_size`，统一使用该属性替代 `config.vocab_size`，修正多卡注意力并行和 logits 截断的实现。

---

