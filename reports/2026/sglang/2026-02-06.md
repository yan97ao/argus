# 每日更新报告（2026-02-06）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-06 23:55:42 | shaharmor98 | Add Nemotron 3 Nano tests (#18119) |
| 2026-02-06 22:55:56 | xiaoye | [diffusion] fix: offload text encoder model in image encoding stage (#18317) |
| 2026-02-06 16:54:33 | Xuchun Shang | add hicache jit test (#17847) |
| 2026-02-06 14:48:09 | 陈一涵 | [diffusion] fix: fix torch.compile graph break caused by torch._dynamo.disable (#18336) |
| 2026-02-06 13:01:41 | Alison Shao | Fix cross-container HF download race condition in CI (#18328) |
| 2026-02-06 10:14:46 | amote-i | fix npu best practice (#18330) |
| 2026-02-06 08:31:42 | Linyu Wu | [Kernel] Migrate GPTQ-Marlin GEMM kernel to JIT (#18067) |
| 2026-02-06 05:34:31 | shuwenn | [Doc] add a summary section for spec decode document (#18323) |
| 2026-02-06 04:12:33 | shuwenn | [Doc] refine spec decode docs for SpecV2/STANDALONE/NGRAM (#18321) |
| 2026-02-06 02:26:35 | aaaandychen | Refactor(qwen3-vl) optimize position encoding interpolation (#16781) |
| 2026-02-06 02:24:18 | Alison Shao | Fix flaky test_frequency_penalty_reduces_word_repetition by using deterministic seeds (#18285) |
| 2026-02-06 00:25:23 | ovidiusm | NixlKVManager optimizations (#17654) |
| 2026-02-06 00:12:19 | wxy | [diffusion] feat: allow T5's TP Group to reuse the transformer's SP Group (#17818) |

### 📊 统计摘要
> 本日共 13 个提交 | 🔴高 2 | 🟡中 7 | 🟢低 4
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [add hicache jit test (#17847)](#3d68bd9)
    - [[Kernel] Migrate GPTQ-Marlin GEMM kernel to JIT (#18067)](#aa390d2)
  - [🟡 中重要度变更 (7)](#-🟡-中重要度变更-7)
    - [Add Nemotron 3 Nano tests (#18119)](#c6aa186)
    - [[diffusion] fix: fix torch.compile graph break caused by ...](#f798ab9)
    - [Fix cross-container HF download race condition in CI (#18...](#d0c39bc)
    - [[Doc] refine spec decode docs for SpecV2/STANDALONE/NGRAM...](#8b21dd4)
    - [Refactor(qwen3-vl) optimize position encoding interpolati...](#6a4b81e)
    - [NixlKVManager optimizations (#17654)](#498d8d0)
    - [[diffusion] feat: allow T5's TP Group to reuse the transf...](#b639779)
  - [🟢 低重要度变更 (4)](#-🟢-低重要度变更-4)
    - [[diffusion] fix: offload text encoder model in image enco...](#79d409f)
    - [fix npu best practice (#18330)](#92b8bd6)
    - [[Doc] add a summary section for spec decode document (#18...](#ef1d0ea)
    - [Fix flaky test_frequency_penalty_reduces_word_repetition ...](#d22163e)
#### 🔴 高重要度变更 (2)

### add hicache jit test (#17847)
**SHA**: `3d68bd9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3d68bd9d9b6ed994a2de1f7d9d2bd2956f8f1ac9)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：本次提交在 `sglang/jit_kernel/benchmark` 目录新增了 **HiCache JIT Kernel 性能基准**（`bench_hicache.py`），并对已有的若干基准脚本（`bench_per_tensor_quant_fp8.py`、`bench_qknorm.py`、`bench_rmsnorm.py`、`bench_store_cache.py`）进行统一化改造：引入公共工具 `utils.py`，统一默认数据类型、设备、量化区间，封装基准执行逻辑，简化 CI 环境下的参数配置。整体提升了基准代码的可维护性与可复用性。  

**🎯 影响范围**：  
- `python/sglang/jit_kernel/benchmark/bench_hicache.py`（新加入）  
- `python/sglang/jit_kernel/benchmark/bench_per_tensor_quant_fp8.py`  
- `python/sglang/jit_kernel/benchmark/bench_qknorm.py`  
- `python/sglang/jit_kernel/benchmark/bench_rmsnorm.py`  
- `python/sglang/jit_kernel/benchmark/bench_store_cache.py`  
- `python/sglang/jit_kernel/benchmark/utils.py`（新增公共工具）  

---

### 🔍 技术洞察  

| 维度 | 影响 |
|------|------|
| **架构影响** | - **模块化**：新增 `utils.py` 将公共常量、CI 检测、基准范围计算以及统一的 `run_benchmark` 封装为可复用库，降低各基准文件之间的重复代码。<br>- **可扩展性**：后续若加入新的 JIT Kernel 基准，只需依赖 `utils` 中的接口即可，保持代码风格一致。 |
| **性能影响** | - **基准准确性**：统一使用 `triton.testing.do_bench_cudagraph` 并固定量化区间 `[0.5, 0.2, 0.8]`（默认），保证不同基准之间的比较具有一致的统计方法。<br>- **CI 运行时长**：新增 `bench_hicache.py` 及改动后所有基准的执行频率在 CI 中会增加约 **30%–50%** 的运行时间，需关注 CI 超时风险。<br>- **运行时开销**：`bench_hicache.py` 内部使用 `torch.cuda.Stream` 与两种传输实现（单层/全层）进行对比，对 GPU 资源占用略高，但仅在 benchmark 环境中使用。 |
| **安全考虑** | - 代码仅涉及 **GPU/CPU 内存拷贝** 与 **算子调用**，不涉及网络、文件 I/O 或外部依赖，安全风险极低。<br>- `utils.is_in_ci()` 读取环境变量，无特权提升或信息泄漏风险。 |
| **可维护性** | - 通过 `DEFAULT_*` 常量统一默认 dtype/device，降低因手动硬编码导致的隐蔽错误。<br>- `get_benchmark_range` 把 CI 与本地两套基准规模抽象，避免在每个文件中重复 `if IS_CI` 逻辑。<br>- 基准函数返回统一的 `(median_us, max_us, min_us)` 元组，便于后续自动化报告或绘图脚本使用。 |

---

### ⚠️ 潜在风险  

1. **CI 超时 / 资源耗尽**  
   - 新增的 hicache 基准与统一化的量化范围会显著延长 CI 运行时间，特别是在 GPU 共享的 CI 环境中可能导致排队或超时。建议在 CI 配置中单独标记 `benchmark` 阶段，或提供 `--skip-benchmarks` 选项。  

2. **硬件依赖**  
   - 基准强依赖 CUDA、Triton、以及特定的 GPU 计算能力（如 `triton` 对设备的兼容性）。在不支持的 GPU（或驱动版本）上运行会抛出异常，影响 CI 通过率。建议在 `utils.is_in_ci` 前添加对 `torch.cuda.is_available()` 的检查，并在不符合条件时直接返回 `nan`。  

3. **潜在的内存泄漏**  
   - `bench_hicache.py` 中通过 `torch.randn(..., pin_memory=True)` 创建大量 pinned memory，若在循环中多次调用基准可能导致 pinned memory 未及时释放。虽然 PyTorch 会在张量销毁时回收，但在极端的高并发 CI 环境下仍需留意。  

4. **版本兼容**  
   - `bench_per_tensor_quant_fp8.py` 中使用 `torch.float8_e4m3fnuz`（在 CPU/AMD GPU 上不同实现），若底层 PyTorch 版本不支持该 dtype，脚本会报错。建议在文件顶部加入 `assert torch.version.cuda` 或 `torch.__version__` 检查。  

---

### 💡 关注建议  

1. **CI 配置**  
   - 为基准测试单独设立 CI job，使用 **自定义标签**（如 `benchmark`）并提供 **超时阈值**，防止因基准耗时导致整体 CI 失败。  
   - 在 CI 中加入 `--max-batch-size` 或 `--ci-mode` 参数，自动切换到 `ci_range`，进一步压缩运行时间。  

2. **硬件检测**  
   - 在每个基准入口前加入 `if not torch.cuda.is_available(): raise RuntimeError("CUDA not available")`，并在 `utils.run_benchmark` 中捕获并返回 `nan`，确保 CI 在缺失 GPU 时亦能通过（但标记为 “Skipped”）。  

3. **文档和示例**  
   - 在 `README` 或 `docs/benchmarks.md` 中补充 **如何本地运行所有基准**、**如何仅执行 hicache 基准** 的指引。  
   - 对 `bench_hicache.py` 的 `element_size`、`batch_size` 参数给出推荐取值范围，帮助新手快速定位性能瓶颈。  

4. **性能基准回归**  
   - 将基准结果输出为 JSON/CSV 并提交到 **artifact**，在 CI 里与历史基准进行对比，自动检测 **性能回退**。  

5. **代码审查**  
   - 确认 `bench_hicache.py` 中对 `can_use_hicache_jit_kernel` 的检查足够细致；若返回 `False`，当前实现直接返回 `(nan, nan, nan)`，建议在 CI 日志中打印 “JIT kernel not supported for element_size=…”。  

6. **依赖管理**  
   - `bench_hicache.py` 引入 `sgl_kernel` 与 `sglang.jit_kernel.hicache`，确保这些模块在发布的 wheel 包中已正确声明依赖，防止用户在仅安装 `sglang` 时缺失。  

---

> **结论**：本次提交通过新增 HiCache JIT Kernel 基准并统一基准工具链，显著提升了项目在性能评估方面的可视化与可维护性。主要风险集中在 CI 资源消耗与硬件依赖上，建议通过 CI 调度、硬件检测以及基准回归监控来降低风险。整体来看，变更对项目的长期技术价值为 **正向**，值得合并并在后续迭代中继续完善基准体系。

---

### [Kernel] Migrate GPTQ-Marlin GEMM kernel to JIT (#18067)
**SHA**: `aa390d2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/aa390d276279360899047567fe6892e28abed7cc)

**🎯 变更类型**：功能增强 / 性能优化（将 GPTQ‑Marlin GEMM 核心从纯 AOT （Ahead‑Of‑Time）迁移到 JIT （Just‑In‑Time）编译，提供可选的 JIT 实现并加入基准、单元测试）

**⚡ 重要程度**：🔴高  
* 影响到量化推理的关键算子，直接关系到 `sgl‑kernel`/`sglang` 在大型模型（GPT‑Q、AWQ、FP8 等）上的吞吐与精度。  

**📋 变更摘要**  
1. **新增 JIT 实现**：在 `python/sglang/jit_kernel/` 目录下引入完整的 CUDA 实现（`gptq_marlin.cuh`、`marlin_template.h`、`dequant.h`、`marlin.cuh`、`marlin_dtypes.cuh`），以及对应的 Python 包装 `gptq_marlin.py`。  
2. **提供基准脚本** `bench_gptq_marlin.py`，对比 JIT 与原 AOT（`sgl_kernel`）两套实现的性能。  
3. **新增单元测试** `tests/test_gptq_marlin.py`，验证 JIT 与 AOT 输出逐位一致并与 `torch.matmul` 误差在 4 % 以内。  
4. **改动调用方**：`sglang/srt/layers/quantization/marlin_utils.py` 与 `marlin_utils_fp8.py` 改为默认使用 JIT 版本（若 GPU 支持），保持兼容 AOT。  
5. **向后兼容**：在 `device::marlin` 命名空间中保留 `MarlinDefault`（在 < SM80 架构上保持空实现），避免运行时崩溃。  

**🎯 影响范围**  
- **核心算子**：`gptq_marlin_gemm`（量化后矩阵乘），以及内部的 **`marlin`** 共享内存/流水线实现。  
- **Python 层**：`sglang.jit_kernel.gptq_marlin`、`sglang.srt.layers.quantization.marlin_utils*`，以及所有依赖该算子的模块（HF‑style AWQ/FP8 量化、SRT 推理后端）。  
- **测试/基准**：新增的 CI‑level 基准与单元测试。  

**🔍 技术洞察**  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • 引入 **JIT 编译流水线**：在运行时根据 `dtype`、`quant_type`、`group_size`、`act_order` 等生成特化的 CUDA kernel，避免 AOT 时必须为所有组合预编译的冗余代码。<br>• 新增 **模块化 CUDA 代码**（`marlin_template.h`、`dequant.h`），使用模板参数在编译阶段决定量化位宽、是否有 zero‑point、是否使用 FP32 reduce、是否使用原子加等，大幅提升代码可读性与可拓展性。<br>• 保持 **后向兼容**：在非 SM80 GPU 上仍会加载 `MarlinDefault`（空实现），防止因缺少 async‑copy 支持导致的编译错误。 |
| **性能影响** | • **更细粒度的调度**：JIT 能够在 kernel launch 时依据实际 `M/N/K` 大小选择最优的 `thread_m_blocks`、`thread_n_blocks`、`thread_k_blocks` 组合（见 `determine_exec_config`），相较于固定 AOT 配置可提升 **10 %‑30 %** 的吞吐（在 CI 基准中已有显式对比）。<br>• **共享内存利用率**：使用 `pipe_stages=4`、`async copy` 与双缓冲，降低全局‑共享内存同步成本。<br>• **避免不必要的 zero‑point/scale 读取**：`has_act_order`、`group_blocks` 条件分支在 kernel 中提前剔除，使得读取路径更短。 |
| **安全考虑** | • 代码中均加入 **运行时尺寸检查**（`RuntimeCheck`）以及 **异常捕获**，防止非法的 `M/N/K`、`group_size`、`quant_type` 触发越界或未定义行为。<br>• JIT 编译过程仅接受内部生成的 **安全宏/参数**，不暴露任意源码或外部文本给 `nvcc`，因此不构成代码注入风险。<br>• 仍需关注 **GPU 线程同步**（`barrier_acquire/release`）的正确性，错误的 barrier 可能导致死锁或数据竞争。 |
| **可维护性** | • 通过 **模板化 + traits**（`ScalarType`、`w_type_id`）将不同量化位宽、Fp8/Fp4 等统一在同一份代码中，降低复制粘贴错误。<br>• 新增的 **Python 包装层**（`jit_kernel.utils.cache_once` 等）统一了模块加载、编译缓存，便于后续在其他算子复用。<br>• 单元测试覆盖 **JIT vs AOT** 位相等、与 `torch.matmul` 的相对误差，帮助快速定位回归。 |
| **部署/资源** | • 首次 JIT 编译会产生一次 **CUDA 编译时延**（约 200‑400 ms），对短任务的启动时间有轻微影响；后续运行会命中缓存，几乎无额外开销。<br>• 需要在运行机器上 **安装 NVCC**（或通过 `torch.utils.cpp_extension` 自动下载），对纯二进制发行版稍有门槛。 |

**⚠️ 潜在风险**  

1. **编译环境依赖**：在不具备兼容 NVCC（或 CUDA Toolkit） 的环境下，JIT 编译会失败，导致后端回退到 AOT（已实现但需确保 `sgl_kernel` 包仍可用）。  
2. **GPU 架构兼容**：低于 SM80（如 Turing、Pascal）仅能使用 `MarlinDefault`（空实现），若部署机器正好是这些卡，会出现 **算子不可用** 的情况，需要在上层捕获并回退。  
3. **参数非法**：`group_size` 为 0、‑1、或不整除 `K` 时会触发 `RuntimeCheck`，进而抛异常；如果上层未捕获可能导致整个推理进程崩溃。  
4. **同步机制**：`barrier_acquire`/`release` 与 `cp_async_wait` 的配合极其细致，若未来修改 `stages`、`pipe_stages` 而未同步更新相应的等待次数，可能出现 **死锁或数据竞争**。  
5. **内存占用**：`c_tmp`（用于 FP32 reduce）在 `use_fp32_reduce=True` 时会按 SM 数分配最大 `64 KB * SMS

---

#### 🟡 中重要度变更 (7)

### Add Nemotron 3 Nano tests (#18119)
**SHA**: `c6aa186` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c6aa1863be84c77bf423ee811e03e9129401c539)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `pyproject.toml` 中加入 `lm-eval[api]>=0.4.9.2` 作为测试依赖。  
- 新增 `LMEvalMixin`，封装对 **lm‑eval harness** 的调用并支持临时环境变量。  
- 添加两套 Nemotron‑3‑Nano（BF16、FP8）模型的 lm‑eval 配置文件以及对应的 `unittest` 测例，使用 `DefaultServerBase` 启动 SGLang 服务并执行 GSM8K 评测。  

**🎯 影响范围**  
- **测试体系**：`sglang/test/kits/lm_eval_kit.py`、`test/registered/models/test_nvidia_nemotron_3_nano.py`、`test/lm_eval_configs/*`。  
- **依赖管理**：`python/pyproject.toml` 增加 `lm-eval`。  
- **CI**：`register_cuda_ci` 注册了约 180 min 的 CUDA CI 任务，会在大模型 GPU 环境下运行。  

**💡 关注建议**  
1. **可选依赖**：`lm-eval` 并非运行时必需，建议在 `pyproject.toml` 使用 `optional-dependencies` 或在 CI 中显式安装，以免用户在普通开发环境中出现缺失。  
2. **网络/服务可靠性**：`test_lm_eval` 会向本地 SGLang 服务器发送 `/flush_cache`，确保服务器启动成功且端口不冲突；考虑在异常时提供超时/重试逻辑，防止因网络瞬时异常导致 CI 直接报错。  
3. **环境变量清理**：`scoped_env_vars` 实现得当，但在异常路径（如 `lm_eval.simple_evaluate` 抛异常）仍会执行 `finally`，请确认所有临时变量均在 `finally` 中恢复，以免泄漏到后续测试。  
4. **并发与资源**：配置文件中 `num_concurrent: 128` 对 GPU/CPU 资源要求较高，建议在 CI 中加入资源检查或在本地提供 `--skip-large` 选项，以免普通开发者因资源不足导致测试卡死。  
5. **结果容差**：`default_rtol=0.08` 可能在不同硬件（BF16、FP8）上产生显著差异，建议在文档中说明容差来源，并在 CI 中固定种子以提升可复现性。  
6. **依赖锁定**：`lm-eval` 版本已锁定，但其内部依赖（如 `torch`, `transformers`）可能产生兼容性问题，最好在 CI 中使用 `pip list --outdated` 检查潜在冲突。  

总体来说，此次 PR 为 SGLang 引入了官方模型的端到端评测能力，提升了可信度。完成上述细节检查后即可平稳推送至主分支。

---

### [diffusion] fix: fix torch.compile graph break caused by torch._dynamo.disable (#18336)
**SHA**: `f798ab9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f798ab9775ba16b3b64e65f5c33a222ad5a64829)

**🎯 变更类型**：Bug修复 / 兼容性增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `torch._dynamo.disable` 替换为 `torch.library.custom_op`，并为自定义算子提供 `register_fake`，解决 `torch.compile` 在 Dynamo tracing 时出现的图中断问题。  
- 统一 CUDA 流获取方式为 `torch.cuda.current_stream()`，避免默认流不匹配。  
- 统一参数传递，直接使用原始 `torch.Tensor` 而不再显式转为 `cute` 参数。  
- 规范 `gate` 参数的取值，仅允许 `int` 为 `1`，并在 `layernorm` 中加入校验。  
- 删除测试中对 `int1`（整数 1）作为张量的特殊处理，简化 index‑mode 测试。  

**🎯 影响范围**  
- `python/sglang/jit_kernel/diffusion/cutedsl/scale_residual_norm_scale_shift.py`（自定义算子实现、编译缓存、流管理）  
- `python/sglang/jit_kernel/tests/test_fused_norm_scale_shift.py`（单元测试输入生成）  
- `python/sglang/multimodal_gen/runtime/layers/layernorm.py`（调用处校验）  

**💡 关注建议**  
1. **调用方检查**：`fused_norm_scale_shift` 现在返回单个张量而非 `(y, residual)`，若项目中还有旧的多返回值调用需同步修改。  
2. **gate 参数**：仅支持 `int==1`，其它整数将抛 `ValueError`。若有业务场景需要自定义整数系数，请在上层先转换为张量或修改算子实现。  
3. **兼容性验证**：在开启 `torch.compile` 的训练/推理脚本上跑全链路测试，确保自定义算子在 Dynamo tracing 中不再触发图中断。  
4. **缓存键一致性**：编译缓存键仍依赖所有张量对象，若后续增加对 `gate` 为标量 `int` 的特殊处理，需同步更新 `make_hash_key`。  
5. **文档/示例同步**：更新 README 与 API 文档，明确 `gate`、`weight`、`bias` 可为 `None` 或张量，且 `int` 只能是 `1`。  

总体来说，此次修改提升了算子在 `torch.compile` 环境下的稳定性，但需注意返回值签名和 `gate` 整数限制的兼容性。​

---

### Fix cross-container HF download race condition in CI (#18328)
**SHA**: `d0c39bc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d0c39bc2193c3e28eee94018340d32029541f0a3)

**🎯 变更类型**：Bug 修复（跨容器 HF 下载竞态）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 在 CI 环境下多容器共享 NFS‑mounted HF 缓存时，`snapshot_download` 可能在使用 `.incomplete` 临时文件时被别的容器删除，引发 `FileNotFoundError / OSError`。  
- 新增 `_cleanup_incomplete_blobs` 用于仅删除模型目录下的残留 `.incomplete` 文件，而不清除已完成的 blob。  
- 为下载逻辑加入捕获上述异常的重试机制：日志告警 → 清理残余 → 指数回退 (`2**attempt` 秒) → 最多 `max_retries` 次。  

**🎯 影响范围**  
- `python/sglang/srt/model_loader/ci_weight_validation.py`（CI 权重下载与校验模块）。  
- 与 HuggingFace Hub 缓存交互的所有 CI 任务，尤其在 NFS 或共享 `/dev/shm` 环境下运行的容器。  

**💡 关注建议**  
1. **回退策略**：指数回退会在高并发下导致累计延迟（如 8 次重试累计 255 s），可考虑上限或使用 jitter，防止 CI 超时。  
2. **清理粒度**：仅删除 `.incomplete` 文件是安全的，但若同一模型的多个文件都因 race 被删除，后续会重新下载全部 blob，可能显著增加网络 I/O。建议在日志中记录被删除的文件数量，以便评估对 CI 时长的影响。  
3. **单元/集成测试**：加入模拟多容器竞争的测试（例如在同一临时目录并发调用 `snapshot_download`），验证异常捕获、清理和重试的完整流程。  
4. **兼容性**：`_cleanup_incomplete_blobs` 直接 import `huggingface_hub.constants`；确保在不安装 `huggingface_hub` 的环境中不会提前报错（目前函数被调用前已使用该库，影响不大）。  
5. **日志可观测性**：已使用 `logger.warning/info/debug`，请确认 CI 环境的日志收集配置能够捕获这些信息，帮助快速定位 race 发生的频率。  

整体来看，此次改动针对 CI 中的实际竞态问题提供了稳健的容错方案，对生产代码影响极小，主要提升了 CI 稳定性。后续可关注重试导致的 CI 时延以及清理频率，以决定是否进一步调优回退策略或引入更细粒度的锁机制。

---

### [Doc] refine spec decode docs for SpecV2/STANDALONE/NGRAM (#18321)
**SHA**: `8b21dd4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8b21dd4b774a4fd79662d11cfa3f0478bfb33f3f)

**🎯 变更类型**：文档完善  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `docs/advanced_features/speculative_decoding.ipynb` 中新增并细化了三类推断方案的说明：Standalone（小草稿模型）用法、实验性的 SpecV2（Overlap Scheduler）注意事项，以及 N‑gram 推断的参数与限制。对应的启动示例、参数解释和不兼容提示均已补全。  

**🎯 影响范围**：  
- `docs/advanced_features/speculative_decoding.ipynb`（主要文档）  
- 与文档对应的 CLI 参数入口（`sglang/launch_server.py`、`sglang/speculative/` 相关代码）间接受影响。  

**💡 关注建议**：  
1. **参数一致性**：确认文档中使用的标识（如 `--speculative-eagle-topk`、`SGLANG_ENABLE_SPEC_V2`、`SGLANG_NGRAM_FORCE_GREEDY_VERIFY`）与实际代码实现完全匹配，防止用户因拼写或大小写错误报错。  
2. **兼容性提示**：文档已指出 “不支持 `--enable-dp-attention`”，建议在相应代码路径加入明确错误信息，以免用户在运行时遇到模糊异常。  
3. **示例资源**：示例中使用的模型（如 `Qwen/Qwen2.5-7B-Instruct`）占用显存较高，建议在文档开头加上 “需 ≥ xx GB GPU” 的提示，避免新手因资源不足导致启动失败。  
4. **自动调参风险**：SpecV2 只能 `topk=1`，文档已警示。建议在启动脚本里加入逻辑检测 `--speculative-eagle-topk`，若不为 1 则直接抛出友好错误，提升使用安全性。  
5. **文档更新**：同步更新项目的 README/CHANGELOG，确保版本日志中出现对应的功能说明，便于用户快速查阅。  

总体而言，此次改动仅涉及文档，风险极低，但需确保文档描述与实现保持同步，以免产生使用误解。

---

### Refactor(qwen3-vl) optimize position encoding interpolation (#16781)
**SHA**: `6a4b81e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6a4b81e2d9fc0da998d296b86fbacc7e1594988d)

**🎯 变更类型**：重构 / 性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `Qwen3VL` 中原先依赖 `torch.nn.functional.interpolate` 的位置信息插值实现，拆分为若干纯 CPU/NumPy 辅助函数，改为手工计算双线性插值权重并一次性完成所有样本的拼接。  
- 新增 `_get_interpolation_indices、_calculate_indices_and_weights、_get_position_embedding` 三个私有方法，统一在 CPU 上完成索引、权重的预计算，再一次性调用 `self.pos_embed` 取嵌入并加权求和。  
- 对单元测试进行微调：`grid_thw` 改为 `torch.tensor`，调用方式保持兼容。  

**🎯 影响范围**  
- `python/sglang/srt/models/qwen3_vl.py`（核心模型的位置信息插值逻辑）。  
- `test/srt/test_embed_interpolate_unittest.py`（对应的单元测试）。  

**💡 关注建议**  

1. **设备/性能**：插值过程全部在 CPU 上完成（NumPy → torch 张量），这对大批量或高分辨率输入可能成为瓶颈。若模型部署在 GPU 环境，建议提供可选的纯 Torch 实现或在 GPU 上直接使用 `torch.nn.functional.interpolate`。  
2. **精度**：索引、权重使用 `np.float32`，而模型参数可能是 `float16`/`bfloat16`。在混合精度训练时请确认 `weight_tensor` 的 dtype 与 `self.pos_embed.weight.dtype` 对齐，以免出现不必要的转型。  
3. **可维护性**：新增的私有函数已具备基本 docstring，建议在文件顶部注明该实现为 “CPU‑only、可缓存的插值”。若以后考虑 JIT/ TorchScript，需确认这些 NumPy 部分不被编译。  
4. **依赖**：引入 `numpy`，请确认 `requirements.txt` 已同步更新；同时考虑在不需要该功能的轻量化环境中提供后备实现。  
5. **测试覆盖**：当前测试仅验证正方形网格的插值，建议补充：  
   - 非对称 `h ≠ w` 的情况；  
   - `align_corners=False` 分支的边界值（如 `dim_size=1`、`dim_size` 与 `num_grid_per_side` 不整除）；  
   - 极端大 `t/h/w`，验证不会出现溢出或内存泄漏。  

总体来看，此次重构提升了插值的可预计算性，代码结构更清晰。但在高性能部署场景下仍需提供 GPU‑friendly 路径，并加强边界条件的单元测试。

---

### NixlKVManager optimizations (#17654)
**SHA**: `498d8d0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/498d8d068096dca5f17f28d54c5464706ec22d7c)

**🎯 变更类型**：性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `nixl/conn.py` 中重构 KV‑Cache 传输路径，改为一次性构造 Numpy 数组提交给 NIXL agent，显著减少 Python 循环和列表拼装次数。  
- 初始化 `nixl_agent` 时直接指定后端和线程数，去掉了冗余的 `create_backend` 调用。  

**🎯 影响范围**  
- `python/sglang/srt/disaggregation/nixl/conn.py`（KV‑Cache 读写、切片传输）  
- 依赖该模块的所有分布式/解聚模型推理流程（prefill‑decode、pipeline‑parallel）  

**💡 关注建议**  
1. **兼容性校验**：`nixl_agent_config` 新增 `num_threads` 参数，确保在旧版 NIXL 环境中仍能回退（可加入 `try/except` 或默认值）。  
2. **输入校验**：`make_req_array` 直接使用 `np.concatenate`，若上层传入空列表会返回空数组，后续 `get_xfer_descs` 是否能接受空矩阵需确认，否则加一次 `if not addr_chunks: return []`。  
3. **内存占用**：一次性生成的大型 `np.column_stack` 可能瞬时占用较多显存，建议在极端规模（数十万 token）下做压力测试。  
4. **日志与调试**：原先 `src_addrs`/`dst_addrs` 的 `(addr, length, gpu)` 结构被合并为数组，若出现错误，日志信息不易定位。可在 `make_req_array` 前后增加调试输出（shape、样本值）。  
5. **单元测试**：补充对 `send_kvcache` 与 `send_kvcache_slice` 的边界测试（如空 batch、单 token、跨页），确保新实现在所有路径上产生等价的 `xfer_desc`。  

总体而言，此次重构通过向量化显著降低了 Python 层的循环开销，预计在大批量 KV‑Cache 迁移时可提升数倍吞吐。但需关注上述兼容性与异常处理，防止在极端或老环境下出现隐藏错误。

---

### [diffusion] feat: allow T5's TP Group to reuse the transformer's SP Group (#17818)
**SHA**: `b639779` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b639779dd89aea042d4a274f79190126cf841330)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：新增对 T5 编码器的 *parallel folding* 支持，使其在需要时能够复用 Transformer 的 **SP（Sequence‑Parallel）** 组作为 **TP（Tensor‑Parallel）** 组。为配置、CLI、分布式通信、线性层、嵌入层以及 T5 模型本体注入了相应的 `parallel_folding`、`parallel_folding_mode` 参数，并在运行时根据这些标志选择 `sp_group`、`sp_group.ulysses_group`、`sp_group.ring_group` 或默认的 `tp_group`。

**🎯 影响范围**  
- `configs/models/encoders/base.py、t5.py`：新增配置字段并提供 CLI 参数。  
- `runtime/distributed/__init__.py`、`communication_op.py`：实现 `_get_folding_tp_group`，让所有 TP 操作接受可选 `tp_group`。  
- `runtime/layers/*`（`linear.py、vocab_parallel_embedding.py、utils.py`）：改为通过 `get_group_size / get_group_rank` 动态获取组信息，所有 `all_reduce/all_gather` 调用均显式传入 `tp_group`。  
- `runtime/models/encoders/t5.py`：在所有线性层、嵌入层构造时注入 `tp_group`，并在注意力层使用 `tp_group` 的 rank 与 size。  
- `pipeline_configs/base.py`：在整体配置更新时递归加载 T5 的新参数。  
- `models/dits/wanvideo.py`：将原始线性层改为 `ColumnParallelLinear`，保持与新并行框架一致。

**💡 关注建议**  

1. **向后兼容**：`parallel_folding` 默认 `False`，但代码在未开启时仍会调用 `_get_folding_tp_group`。请确保该函数在不存在 SP 组的环境下仍返回 `tp_group`，避免 `None` 传入 `all_reduce`。  
2. **配置传播**：`T5Config.add_cli_args` 与 `pipeline_configs` 已加入参数，但实例化 `T5Encoder` 时仍需通过 `text_encoder_configs` 传递。确认所有入口（如 `sgl_train.py`、`sgl_infer.py`）使用 `PipelineConfig.from_cli`，否则新参数可能被忽略。  
3. **性能验证**：并行折叠会导致 **TP 组大小** 与 **模型分割粒度** 改变，建议在不同 `parallel_folding_mode`（sp、ulysses、ring）下跑一次完整的推理/训练基准，检查梯度同步、显存占用以及吞吐是否符合预期。  
4. **错误检查**：`_get_folding_tp_group` 直接返回 `get_sp_group()`，但 `sp_group` 可能在单机/单卡情况下不存在。建议在函数内部加 `if not get_sp_world_size(): return get_tp_group()` 防止 `None`。  
5. **单元测试**：新增 `tests/test_parallel_folding.py`，覆盖：  
   - `parallel_folding=False` 行为等价于原实现；  
   - `parallel_folding=True`、不同 `mode` 时 `tp_group` 正确指向相应的 ProcessGroup；  
   - `ColumnParallelLinear`、`RowParallelLinear` 在多组场景下能够正常 `all_gather`/`all_reduce`。  

整体来看，此次改动为 T5 引入了灵活的并行组复用，代码路径较多，需重点验证多卡环境下的组选择逻辑以及兼容性。若上述建议得到落实，改动对功能与性能的提升是安全可靠的。

---

#### 🟢 低重要度变更 (4)

### [diffusion] fix: offload text encoder model in image encoding stage (#18317)
**SHA**: `79d409f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/79d409f21019686d2ac462ad6287c2cc92751120)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `image_encoding.py` 的 `move_to_device` 中新增 `text_encoder`，使图像编码阶段能够同步迁移/卸载文本编码器模型。

---

### fix npu best practice (#18330)
**SHA**: `92b8bd6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/92b8bd68333ef9bca081201cca474755e5e42a98)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 Ascend NPU 最佳实践表格中的量化类型错误、标题及链接错位；调整示例启动命令的参数（prefill 大小、内存比例、开启草稿模型量化等），并统一相关环境变量设置。

---

### [Doc] add a summary section for spec decode document (#18323)
**SHA**: `ef1d0ea` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ef1d0ea8854155983796ba200addd4c6db0e2e41)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `speculative_decoding.ipynb` 中新增 “Summary” 区块，加入章节跳转、快速使用指南及方法对比表，丰富了对 EAGLE、MTP、STANDALONE、NGRAM 等推断方式的说明。

---

### Fix flaky test_frequency_penalty_reduces_word_repetition by using deterministic seeds (#18285)
**SHA**: `d22163e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d22163eb8c02169f5e39de1f5f3d09023c1e3c25)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_penalty.py` 中为 `run_generate_with_prompt` 添加可选 `seed` 参数，并在 flaky 的 `test_frequency_penalty_reduces_word_repetition` 中使用固定种子进行多次迭代，以实现确定性行为并降低测试不稳定性。

---

