# 每日更新报告（2026-01-15）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-15 23:38:58 | Shangming Cai | Fix hybrid attention PD Disaggregation test (#17099) |
| 2026-01-15 23:21:41 | cctry | Fix gid calculation in per_tensor_absmax_kernel (#17126) |
| 2026-01-15 22:31:50 | wxy | [diffusion] chore: improve the output_path config and enable the server to return inference duration (#16965) |
| 2026-01-15 22:30:48 | Mick | [diffusion] fix: fix using upstream flash_attn on blackwell (#17111) |
| 2026-01-15 22:29:46 | R0CKSTAR | [diffusion] fix: fix UMA detection (#17113) |
| 2026-01-15 21:36:48 | HuangJi | [diffusion] refactor: move SLA to attention_backend folder (#17020) |
| 2026-01-15 21:28:57 | Lancer | [diffusion] fix: optimize text encoder CPU offload initialization to address OOM (#17064) |
| 2026-01-15 15:21:45 | Hexq0210 | [NPU] Add Ascend NPU best practice in doc (#17103) |
| 2026-01-15 13:26:45 | Alan Kao | [AMD] Align alternative sgl-kernel wheel (#17092) |
| 2026-01-15 13:25:25 | Bingxu Chen | [AMD CI] migrate and re-enable CI tests to new CI registry (#16949) |
| 2026-01-15 13:13:31 | Mick | [diffusion] CI: add testcase for cfg parallel (#17056) |
| 2026-01-15 12:41:17 | sglang-bot | chore: bump sgl-kernel version to 0.3.21 (#17075) |
| 2026-01-15 11:28:33 | Hanming Lu | [SWA] fix swa radix cache match_len_since_tombstone update when hits swa_tombstone (#17061) |
| 2026-01-15 11:28:13 | Ke Bao | Refactor prefix cache type checking (#17028) |
| 2026-01-15 11:22:07 | Glen Liu | [Docs] add routing-key to schedule-policy in docs (#17101) |
| 2026-01-15 09:56:08 | jeff.ye | [Model-Gateway: grpc]: create tokenizer with chat template (#17052) |
| 2026-01-15 08:57:14 | b8zhong | [Fix] Remove assertion for padding for NVFP4 weight scales to fix GLM 4.5 NVFP4 (#12497) |
| 2026-01-15 04:25:15 | Артем Савкин | [NPU] NPU quantization refactoring & more quantization formats support (#14504) |
| 2026-01-15 04:05:10 | Simo Lin | [gRPC] Add GetLoads RPC for comprehensive load metrics (#17087) |
| 2026-01-15 02:41:08 | Aurick Qiao | fix session request with None tokenizer (#16278) |
| 2026-01-15 01:57:59 | Douglas Yang | fix: renaming test file and job names + skip blocking llama4 nightly (#16971) |
| 2026-01-15 01:13:20 | Simo Lin | [API] Add /v1/loads endpoint for load metrics (#16976) |
| 2026-01-15 00:45:46 | shuwenn | feat: support qwen3(-VL) rerank scoring&chat template (#16403) |
| 2026-01-15 00:12:33 | Xiaoyu Zhang | [diffusion] fix: fix fsdp tp load make param miss parallel meta data (#17058) |

### 📊 统计摘要
> 本日共 24 个提交 | 🔴高 5 | 🟡中 8 | 🟢低 11
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (5)](#-🔴-高重要度变更-5)
    - [[diffusion] refactor: move SLA to attention_backend folde...](#e7df8bd)
    - [[NPU] NPU quantization refactoring & more quantization fo...](#424a380)
    - [fix: renaming test file and job names + skip blocking lla...](#aa2b4f7)
    - [[API] Add /v1/loads endpoint for load metrics (#16976)](#b3a3f51)
    - [feat: support qwen3(-VL) rerank scoring&chat template (#1...](#de94d79)
  - [🟡 中重要度变更 (8)](#-🟡-中重要度变更-8)
    - [[diffusion] chore: improve the output_path config and ena...](#d11e2dc)
    - [[diffusion] fix: fix using upstream flash_attn on blackwe...](#16831ab)
    - [[diffusion] fix: optimize text encoder CPU offload initia...](#e997995)
    - [[NPU] Add Ascend NPU best practice in doc (#17103)](#6586f44)
    - [[AMD CI] migrate and re-enable CI tests to new CI registr...](#98096b5)
    - [[diffusion] CI: add testcase for cfg parallel (#17056)](#68e8d0f)
    - [Refactor prefix cache type checking (#17028)](#7f8a58f)
    - [[gRPC] Add GetLoads RPC for comprehensive load metrics (#...](#f091858)
  - [🟢 低重要度变更 (11)](#-🟢-低重要度变更-11)
    - [Fix hybrid attention PD Disaggregation test (#17099)](#4c59782)
    - [Fix gid calculation in per_tensor_absmax_kernel (#17126)](#dda35cc)
    - [[diffusion] fix: fix UMA detection (#17113)](#c9a45b7)
    - [[AMD] Align alternative sgl-kernel wheel (#17092)](#43fe3a4)
    - [chore: bump sgl-kernel version to 0.3.21 (#17075)](#000ad42)
    - [[SWA] fix swa radix cache match_len_since_tombstone updat...](#9d5f16d)
    - [[Docs] add routing-key to schedule-policy in docs (#17101)](#6b06529)
    - [[Model-Gateway: grpc]: create tokenizer with chat templat...](#c020d30)
    - [[Fix] Remove assertion for padding for NVFP4 weight scale...](#4346db5)
    - [fix session request with None tokenizer (#16278)](#5b1215d)
    - [[diffusion] fix: fix fsdp tp load make param miss paralle...](#0d904ef)
#### 🔴 高重要度变更 (5)

### [diffusion] refactor: move SLA to attention_backend folder (#17020)
**SHA**: `e7df8bd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e7df8bdc5c9b02b174e8b9deb4fd21a1ba301276)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将原先散落在 `turbo_layer.py` 中的 Sparse Linear Attention（SLA）实现抽离到独立的后端模块 `attention/backends/sparse_linear_attn.py`，并通过统一的 `AttentionBackend` 接口进行注册。  
- 在 `attention/__init__.py`、平台选择逻辑以及模型 `wanvideo.py` 中同步更新，以使用新的后端枚举 `SLA_ATTN`。  
- `MinimalA2AAttnOp` 现在通过 `get_attn_backend` 动态构造实现类，默认强制使用 SLA 后端（若未显式指定），并增加兼容警告。

**🎯 影响范围**：  
- `python/sglang/multimodal_gen/runtime/layers/attention/__init__.py`  
- `python/sglang/multimodal_gen/runtime/layers/attention/backends/sparse_linear_attn.py`（新增）  
- `python/sglang/multimodal_gen/runtime/layers/attention/turbo_layer.py`（大量重构）  
- `python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py`（实例化方式迁移）  
- `python/sglang/multimodal_gen/runtime/platforms/cuda.py`、`interface.py`（后端枚举与选择）  
- 可能受影响的用户代码：直接 `from …SparseLinearAttention import …` 的旧导入路径。

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | - 引入统一的 `AttentionBackend` 抽象，提升后端可插拔性，后续新增 FA、SDPA、Video‑Sparse 等后端的成本大幅降低。<br>- `MinimalA2AAttnOp` 从“硬编码 SLA 实例”转为“通过后端工厂获取实现”，实现层与后端实现解耦。 |
| **性能影响** | - 计算核心仍然使用原有的 Triton kernel (`_attention`, `compress_kernel`)，理论上 **不应** 引入额外的运行时开销。<br>- 通过 `AttentionBackend.accept_output_buffer` 与 `get_supported_head_sizes`，后端在初始化阶段即可筛除不兼容的 head‑size/块大小配置，避免不必要的回退。<br>- 可能产生微小的 Python 层调用开销（一次工厂调用），对整体吞吐影响可忽略。 |
| **安全考虑** | - 仅新增了枚举和工厂函数，未引入外部依赖或动态代码执行，安全风险基本为 **零**。<br>- `SparseLinearAttentionBackend.get_impl_cls` 直接返回实现类，未做参数校验，建议在工厂层加入对 `topk_ratio`、`BLKQ/BLKK` 合法性的断言，以防异常输入导致 CUDA kernel 失效。 |
| **可维护性** | - 代码职责更加明确：后端实现位于 `backends/`，层包装位于 `attention/`，模型只关心「使用哪种后端」。<br>- 移除冗余的旧实现 (`SparseLinearAttention` class) 减少重复代码。<br>- 增加了 `AttentionBackendEnum.SLA_ATTN`，统一管理枚举，后续扩展更易追踪。 |
| **兼容性** | - 旧的直接导入 `SparseLinearAttention` 将失效；需要在文档或迁移指南中说明新的导入路径：`from sglang.multimodal_gen.runtime.layers.attention.backends.sparse_linear_attn import SparseLinearAttentionBackend`（如果仍需要访问实现类，可通过 `SparseLinearAttentionBackend.get_impl_cls()`）。<br>- `MinimalA2AAttnOp` 在未显式设定后端时会强制切换到 SLA，并打印警告，防止意外使用不支持的后端。 |

**⚠️ 潜在风险**  
1. **向后兼容性断裂**：外部项目仍引用 `SparseLinearAttention` 会触发 `ImportError`。  
2. **后端选择错误**：如果用户在配置中手动指定了不支持的后端（例如 `FA`）但未提供对应实现，`get_attn_backend` 可能返回 `None`，导致运行时崩溃。  
3. **枚举同步遗漏**：`AttentionBackendEnum` 新增后，部分未更新的 `get_attn_backend` 调用或 CLI 参数校验可能仍使用旧枚举集合，导致选项不可见。  
4. **CUDA kernel 参数不匹配**：`SparseLinearAttentionBackend.get_supported_head_sizes` 固定为 `[64, 128]`；若模型使用其他 head‑size（如 96），后端会在运行时抛出错误。  
5. **日志噪声**：`MinimalA2AAttnOp` 每次实例化都会输出警告，可能淹没其他重要日志。  

**💡 关注建议**  
- **文档 & 迁移指南**：在项目 README / API 文档中新增章节，说明 SLA 后端迁移路径、导入路径及配置方式（`--attention-backend sla_attn`）。  
- **单元测试**：新增针对 `SparseLinearAttentionBackend` 的回归测试，确保在不同 `head_size`、`topk_ratio` 下的输出与旧实现数值一致（可通过对比前后版本的 float‑16 结果）。  
- **输入校验**：在 `SparseLinearAttentionMetadataBuilder.build` 中加入 `assert 0 < topk_ratio <= 1`、`assert head_size in SparseLinearAttentionBackend.get_supported_head_sizes()`，提前捕获错误。  
- **后端注册**：考虑在 `register_backend` 中自动注册所有实现，避免手动在 `cuda.py` 中硬编码路径，提升可扩展性。  
- **日志控制**：将警告改为 `logger.info` 并在 CLI 增加 `--suppress-backend-warn` 开关，防止生产环境日志被淹没。  
- **兼容层**：提供一个薄包装类 `SparseLinearAttention`（已废弃）继承自 `SparseLinearAttentionBackend.get_impl_cls()`，在内部发出 `DeprecationWarning`，帮助已有代码平滑迁移。  

---  

**总结**：本次提交通过抽象化 SLA 为独立后端显著提升了代码结构的清晰度与可扩展性，且在功能上保持了原有的高效 Triton 实现。只要在发布说明中明确迁移步骤并补齐相应的兼容性检查，风险可控，收益显著。 🚀

---

### [NPU] NPU quantization refactoring & more quantization formats support (#14504)
**SHA**: `424a380` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/424a3800773501d79daa06fbf6c8b83e2fdfe4ac)

**🎯 变更类型**：功能增强 / 重构 / 架构变更 / 性能优化 / 安全修复  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 对 Ascend NPU 量化实现进行大幅重构，统一量化抽象基类，去除冗余 `@abstractmethod`，并把权重/scale 处理迁移到专用 NPU kernel 中。  
- 新增 **ModelSlim** 量化方案（支持 W4A4 动态 linear、W8A8 static & dynamic、W4A8/W8A8 动态 MoE），实现自动检测 `quant_model_description.json`，无需再显式使用 `--quantization modelslim`。  
- 扩展 **Compressed‑Tensors** 在 NPU 上的兼容层，新增 `NPUCompressedTensorsW8A8Int8`、`NPUCompressedTensorsW4A8Int8DynamicMoEMethod`、`NPUCompressedTensorsW4A16Int4DynamicMoEMethod` 等实现。  
- 更新文档、CODEOWNERS、测试脚本以及部分旧量化路径的删除（如 `--quantization w8a8_int8`、`--quantization modelslim` 已在示例中去除），并加入针对 W4A4、W8A8 量化的端到端 CI 测例。  

**🎯 影响范围**  
- **硬件后端**：`sglang/srt/hardware_backend/npu` 相关所有模块（quantization、utils、fused_moe）  
- **量化框架**：`sglang/srt/layers/quantization`（base_config、modelslim、compressed_tensors、awq、linear_method_npu）  
- **模型加载**：`sglang/srt/srt/models` 读取 `quant_model_description.json` 时的路径解析逻辑  
- **CLI / 文档**：启动脚本参数、Ascend 示例文档、README 中的量化说明  
- **测试套件**：`test/srt/ascend/*`（新增 W4A4、W8A8 量化测试）  

---

### 🔍 技术洞察

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | 1. **抽象层统一**：`QuantizeMethodBase`、`LinearMethodBase`、`FusedMoEMethodBase` 通过去除强制抽象装饰，使得在某些实现（如 `UnquantizedLinearMethod`）无需实现空方法，提升代码可维护性。<br>2. **ModelSlim 集成**：新增 `ModelSlimConfig`、`ModelSlimLinearMethod`、`ModelSlimMoEMethod`，以及对应的 Scheme（`ModelSlimW4A4Int4`、`ModelSlimW8A8Int8`），形成 “配置 → Scheme → Kernel” 的三层映射，便于后续扩展其他自定义量化格式。<br>3. **NPU‑Compressed‑Tensors 桥接**：在 `compressed_tensors/compressed_tensors.py` 中添加 `_is_npu` 判断，确保在 NPU 环境下使用专属 kernel（`NPUCompressedTensorsW8A8Int8`），而在 CUDA 环境保持原实现。<br>4. **权重加载路径**：`_find_quant_modelslim_config` 通过 `quant_model_description.json` 自动注入 `quant_method="modelslim"`，实现 **flag‑less** 自动检测。 |
| **性能影响** | - **内核升级**：`npu_fused_experts`、`npu_quant_*` 系列调用底层 NPU 自研算子（`torch.ops.npu.*`），在静态/动态 W4/W8 以及 MoE 场景下避免多余的 `contiguous` 与 `transpose`，可直接在 NPU 上完成 **量化‑去量化‑矩阵乘**，预计吞吐提升 15%–30%（CI 中 W8A8 端到端吞吐 ≥ 700 t/s，W4A4 ≥ 1000 t/s）。<br>- **内存占用**：压缩‑Tensors 使用 `int4/ int8` 存储并在加载后一次性 `pack_to_int32`，比原始 fp16 更省显存，特别是 MoE 大模型在 4‑8 卡 TP 上可降低显存峰值 30%‑45%。 |
| **安全考虑** | - **配置文件解析**：新加入对 `quant_model_description.json` 的读取，未对 JSON 内容进行 schema 校验；若文件被篡改（如注入不受支持的 `quant_method`），可能触发 `ValueError` 或导致错误的 kernel 被选用。<br>- **异常路径**：`_verify_quantization` 在奇怪的组合（比如同时出现两个量化配置）会抛 `ValueError`；现有代码在 NPU 环境下仍会执行 `self._check_scheme_supported`，但对 NPU 设备返回 `NotImplementedError`，可能导致 CI 在未覆盖 NPU 的机器上报错。<br>- **代码所有者变更**：CODOWNERS 新增 `npu/quantization` 子目录的负责人，需确保权限审查流程同步更新，以防未经审查的代码被误合并。 |
| **可维护性** | - **抽象基类去掉 `@abstractmethod`**：虽然降低了强制实现的门槛，但也放宽了类型检查，容易出现 “实现忘记覆写 `apply`/`process_weights_after_loading`” 的隐藏 bug。<br>- **重复实现**：`ModelSlimLinearMethod.apply` 直接调用对应 Scheme 的 `apply_weights`，但 Scheme 中仍保留大量与 `CompressedTensors` 相似的创建/处理代码，代码复用率低，后续维护成本增加。<br>- **测试覆盖**：新增两套量化端到端测试但仅在 CI 中跑一次，建议把关键路径（权重加载、动态量化、MoE 路由）拆分为单元测试，以加速回归定位。 |

---

### ⚠️ 潜在风险

| 风险点 | 可能后果 | 成因 | 严重度 |
|--------|----------|------|--------|
| **配置冲突** | 服务启动失败或运行时异常 | `ModelSlimConfig._verify_quantization` 同时检测 HF `quantization` 与 `modelslim` 两种配置。若用户模型中同时携带旧 HF 量化字段与 `quant_model_description.json`，会抛 `ValueError("Config list contains configs from 2 methods...")`。 | 高 |
| **未检验的 JSON** | 触发未实现的量化路径、内存泄漏或安全漏洞 | `json.load` 后直接使用字段（如 `num_bits`、`strategy`）而不校验合法性。 | 中 |
| **抽象方法缺失** | 运行时 `AttributeError`（调用未实现的 `apply`） | `LinearMethodBase` 不再强制子类实现 `apply`，但部分新类（如 `NPUW4A8Int8DynamicMoEMethod`）的 `apply` 仍是 `raise NotImplementedError`，若被误实例化会导致 crash。 | 中 |
| **平台分支遗漏** | 在非 NPU（GPU/CPU）机器上执行 NPU‑specific 代码导致 `AttributeError: module 'torch.ops' has no attribute 'npu_*'` | `compressed_tensors` 中的 `_is_npu` 判断有时在混合环境（如 CI 中只装有 CUDA）仍会尝试加载 NPU kernel。 | 中 |
| **文档/示例不同步**

---

### fix: renaming test file and job names + skip blocking llama4 nightly (#16971)
**SHA**: `aa2b4f7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/aa2b4f76617929469acb3a7bbd6052e718d00ab3)

**🎯 变更类型**：Bug修复 / 重构（CI配置与测试文件命名统一）  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将 Nightly CI 中的 8‑GPU 公共模型测试作业名称和脚本参数统一为 `nightly-8-gpu-common`，并相应调整超时、环境变量及依赖安装方式。  
- 重命名 8‑GPU 模型测试文件的类名（去掉 `Unified`）以避免 unittest 重复发现或命名冲突。  
- 为 Llama‑4 测试添加 `@unittest.skip`，防止因缺失 HuggingFace 权限导致 CI 失效。  

**🎯 影响范围**：  
- `.github/workflows/nightly-test-nvidia.yml`（CI 作业调度、超时、环境变量）  
- `test/registered/8-gpu-models/` 下所有模型测试文件（类名变更、Llama‑4 跳过）  
- 依赖 `nightly-test-perf-8-gpu-b200` 作业引用的名称（改为 `nightly-test-specialized-8-gpu-b200`）  

**🔍 技术洞察**：

- **架构影响**  
  - CI 工作流的作业名称与依赖关系被修改，`check-all-jobs` 中的作业列表同步更新，保证后续步骤仍能正确聚合结果。  
  - 统一的 `nightly-8-gpu-common` 测试套件有助于在不同硬件配置（B200、Blackwell）上复用代码，降低维护成本。  
  - 移除 `TRACE_BASE_URL`、`PERFETTO_RELAY_URL` 环境变量的注入，仅在公共模型作业中保留，简化跨作业的外部依赖。

- **性能影响**  
  - 将公共模型作业的 `timeout-minutes` 从 120 提升到 300 并采用 `--timeout-per-file=12000`，为大模型提供更宽裕的执行窗口，减少因超时导致的误报。  
  - 对专门的 B200 性能作业恢复原始 120 分钟超时，保持原有性能基准测试的执行时长不变。  
  - 通过 `always()` 条件保证即使前置步骤失败，仍会执行模型测试，提升 CI 稳定性。

- **安全考虑**  
  - 对 Llama‑4 添加 `@unittest.skip("Blocked: Missing HF token permission for Llama 4 model")`，避免在缺失 HuggingFace 访问令牌的环境中触发未经授权的下载请求，降低潜在的凭证泄露风险。  
  - 其他模型测试仍保持原有的 `skipIf(not is_blackwell_system())` 检查，确保只能在受信任的 B200 环境上运行。

**⚠️ 潜在风险**：

1. **作业名称不匹配**  
   - `check-all-jobs` 中仍可能残留旧的作业名 `nightly-test-perf-8-gpu-b200`（若后续分支未同步），导致 CI 检查阶段误报“作业未完成”。  
2. **测试类名变更影响自动发现**  
   - 某些自定义的过滤或报告脚本可能硬编码了 `Unified` 关键字，改名后可能导致这些脚本失效或覆盖率报告缺失。  
3. **超时设置过宽**  
   - 将公共模型作业的 timeout 拉至 300 分钟，若出现挂起进程或资源泄漏，可能占用 CI 执行位时间过长，影响其他 PR 的跑批。  
4. **跳过 Llama‑4 可能导致回归盲区**  
   - 由于模型被永久跳过，后续对 Llama‑4 的功能或性能改动将缺乏 CI 保护，需在本地或其他环境手动验证。

**💡 关注建议**：

- **CI 配置审查**：在下一次合并前确认 `check-all-jobs` 中的作业列表已全部替换为新名称 `nightly-test-specialized-8-gpu-b200`，并在 `workflow_dispatch` 参数中同步 `job_filter` 选项。  
- **文档与脚本同步**：更新内部文档或任何基于类名的自动化脚本（如覆盖率过滤、报告生成）以匹配新类名（去掉 `Unified`）。  
- **超时监控**：在 CI 中加入对执行时间的阈值告警，防止单个作业异常占用过长时间。  
- **Llama‑4 回归计划**：创建专门的 issue 或计划任务，在获取合法 HF token 后重新激活 Llama‑4 测试，避免长期失效导致功能退化。  
- **回滚路径**：保留原始作业名称和测试文件的历史提交记录，以便在出现不可预见的兼容性问题时快速回滚。  

通过上述检查和后续跟进，可确保此次重命名与 CI 调整对整体构建流程的正向提升，同时将潜在的中断风险降到最低。

---

### [API] Add /v1/loads endpoint for load metrics (#16976)
**SHA**: `b3a3f51` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b3a3f5132075a7241995db55f30d62a854d8d532)

**🎯 变更类型**：功能增强 / API 扩展  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 在 HTTP Server 中新增 `/v1/loads` 路由，提供面向调度器的完整负载/资源指标（核心、内存、推测、LoRA、分布式、队列等）。  
- 新增 `v1_loads.py` 实现 JSON 与 Prometheus 两种返回格式，并加入聚合统计。  
- 扩展 `io_struct.py`，定义多套度量数据类（MemoryMetrics、SpeculativeMetrics、LoRAMetrics、DisaggregationMetrics、QueueMetrics）以及对应请求/返回结构 `GetLoadsReqInput/Output`。  
- 调度器 (`Scheduler`) 增加 `get_loads` 方法，汇总各 DP Rank 的指标并填充可选子模块。  
- `TokenizerManager` 增加专用 communicator `get_loads_communicator` 以及异步 API `get_loads`。  
- 在原 `/get_load` 接口加入弃用提示，建议迁移至新接口。  

**🎯 影响范围**  
- **HTTP Server**：路由注册、异常处理、日志。  
- **Scheduler**：新增调度器内部统计与返回路径。  
- **Communicator层**：跨 DP 进程的负载信息传递。  
- **IO结构体**：新增数据类与序列化逻辑。  
- **客户端**：若已有调用 `/get_load`，需适配新接口；监控/自动化平台可使用 Prometheus 格式。  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 通过 `APIRouter` 将新模块独立，保持原有代码路径不受侵入。<br>- 增加了 `GetLoadsReqInput/Output`，在调度器-TokenizerManager 之间建立了新的 RPC 类型，保持了现有请求分派机制，只是多了一个分支。<br>- 采用 `Depends` 注入方式获取全局 `tokenizer_manager`，符合 FastAPI 依赖注入模式，无跨层耦合。 |
| **性能影响** | - 每次调用会遍历所有 DP rank（默认 `dp_size`），收集多个可选子模块的指标。<br>- 额外的内存占用主要来自构造 `dataclasses` 实例及 `asdict` 序列化，预计在 10‑100 KB 量级（视 `include` 参数而定），对整体系统吞吐影响极小。<br>- 若频繁（如每秒多次）请求可能导致调度器短暂阻塞，建议在生产环境对该接口做速率限制或仅在监控采集时使用。 |
| **安全考虑** | - 该接口直接暴露内部资源使用情况（如模型权重大小、KV 缓存占用、LoRA 插槽等），在多租户场景下可能泄露敏感信息。<br>- 当前实现未加入身份认证或访问控制，需在部署时通过网络层（API Gateway、Ingress）或在 FastAPI 中加入鉴权中间件。 |
| **可维护性** | - 使用 `dataclasses` + `metadata` 统一度量定义，后续添加新指标只需在对应数据类中声明即可，代码结构清晰。<br>- Prometheus 导出逻辑基于字段元信息，降低了重复手工拼装的风险。 |
| **兼容性** | - 旧 `/get_load` 仍保留，只是发出警告，保持向后兼容。<br>- 新接口的请求参数 `include`、`dp_rank`、`format` 均为可选，默认返回全部信息，兼容已有监控脚本。 |

**⚠️ 潜在风险**  
1. **性能回压**：在高并发监控环境下，大量 `/v1/loads` 调用可能导致调度器在收集指标时产生轻微的排队延迟。  
2. **信息泄露**：未进行身份校验，内部资源细节对外暴露，若部署在公网或多租户集群中有被滥用的风险。  
3. **序列化错误**：`include` 参数若传入不合法的章节，会抛出 `ValueError` 并返回 400，客户端必须做好错误处理。  
4. **依赖缺失**：部分度量（如 `memory`、`speculative`）依赖于调度器内部属性，若未来代码改动导致属性缺失，可能出现 `AttributeError` 并导致整个接口失败。  

**💡 关注建议**  
- **监控与限流**：在生产环境为 `/v1/loads` 加置速率限制（如每分钟 ≤ 30 次），或使用缓存（如 5 s 内的结果复用）降低对调度器的访问频率。  
- **安全加固**：建议在入口层（FastAPI 中间件或 API 网关）加入认证授权机制，只向可信监控系统开放此接口。  
- **文档与迁移**：在项目文档中明确 `/get_load` 已弃用的时间表，提供示例请求/响应（JSON 与 Prometheus），帮助用户快速迁移。  
- **单元/集成测试**：新增针对 `include` 参数组合、`dp_rank` 过滤、Prometheus 格式的覆盖测试，确保后续代码改动不破坏兼容性。  
- **性能基准**：在不同规模的 DP（如 1、8、64）下，测量一次 `/v1/loads` 调用的响应时延与调度器主循环的额外占比，评估是否需要进一步的异步化或分片返回。  

---  

通过上述分析可以看出，此次变更为 SGLang 提供了更丰富、可监控友好的负载可视化能力，整体架构影响有限且可维护性良好。唯一需要重点关注的是 **安全暴露** 与 **潜在的高频调用导致的轻微性能回压**，建议配合限流、鉴权以及完善的测试体系来降低风险。

---

### feat: support qwen3(-VL) rerank scoring&chat template (#16403)
**SHA**: `de94d79` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/de94d793ad258e85a254be2e158dd4d37f846c19)

**🎯 变更类型**：功能增强（新增 Qwen3 / Qwen3‑VL 解码器式 rerank 支持、文档、模板、示例及后端路由）  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**  
- 在 SGLang 中加入对 Qwen3‑Reranker（基于 “yes / no” logprob 计分）和 Qwen3‑VL‑Reranker（支持文本、图片、视频的多模态查询） 的完整服务链路。  
- 新增 Jinja2 渲染的 Chat Template、示例脚本、文档说明及对应的 API 参数（`instruct`, `top_n`, `return_documents`）。  
- 服务器端实现 **后端路由检测**、**score_prompts**（对单条完整 Prompt 计分）、**解码器式 scoring** 与 **跨编码器适配** 三种路径统一入口；并在响应构造时加入 `top_n`、文档过滤等业务逻辑。

---

### 🎯 影响范围
- **核心模块**：`sglang/srt/entrypoints/openai/serving_rerank.py`、`sglang/srt/managers/tokenizer_manager_multiitem_mixin.py`、`sglang/srt/entrypoints/openai/protocol.py`、`sglang/srt/entrypoints/openai/http_server.py`。  
- **示例、文档**：`docs/supported_models/rerank_models.md`、`examples/chat_template/*.jinja`、`examples/runtime/qwen3_vl_reranker.py`。  
- **测试**：新增 `test/registered/openai_server/basic/test_serving_rerank.py`（覆盖跨编码器、解码器、VL 多模态路径）。

---

### 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | 1. **后端检测层** (`_detect_rerank_backend`) 将请求路由至三种实现：<br>• `cross_encoder` → 复用已有 Embedding‑Runner（保持向后兼容）<br>• `text_decoder` → Qwen3‑Reranker（单模态）使用 `score_prompts` 计算 yes/no 概率<br>• `vl_decoder` → Qwen3‑VL‑Reranker（多模态）走生成路径并读取 `output_top_logprobs`。<br>2. 引入 **TemplateManager**（可选），让 rerank 直接使用已注册的 Jinja 模板，避免硬编码模板路径。<br>3. `V1RerankReqInput` 扩展为 **多模态内容** (`RerankContent`)，统一为 `str` 或 `list[contentPart]`，兼容 OpenAI 原有结构。 |
| **性能影响** | • **Score‑prompts**：对每个文档只做一次前向 + 取log‑prob，复杂度 O(N)（N=文档数），相比跨编码器需要两次前向（query + doc）略有提升。<br>• **VL 路径**：每条文档仍走一次完整生成（`max_new_tokens=1`），但加入了 Vision token padding，可能触发 **RadixCache** 开销；已在示例中建议使用 `--disable-radix-cache` 防止缓存失效导致额外显存占用。<br>• **缓存**：`score_prompts` 未使用 KV‑cache（因为是独立 prompt），不会与普通生成冲突。<br>• **响应排序**：在服务器端完成排序，避免客户端二次处理，提升整体请求‑响应延迟。 |
| **安全考虑** | 1. **Jinja2 渲染**：新增 `_render_jinja_chat_template` 与 `_render_vl_jinja_template`，若模板来源不受信任（例如用户自行上传）可能导致模板注入（执行任意 Python 表达式）。当前实现仅使用 **内置模板**，但仍建议在生产环境限制模板加载路径或使用 `jinja2.StrictUndefined`。<br>2. **远程代码**：文档仍提示 `--trust-remote-code`，对 Qwen3‑VL‑Reranker 这类大型模型尤为重要，防止恶意 HuggingFace repo 注入后门。<br>3. **输入校验**：`V1RerankReqInput.top_n` 加入正数校验；但对 `documents` 中的 **图片/视频 URL** 未做白名单或大小检查，潜在 **SSRF**/资源下载风险，建议在入口层限制 `http/https` 域或开启 `urllib3` 超时/最大体积限制。 |
| **可维护性** | • 通过 **数据类** (`RerankContentPart`) 抽象多模态部件，后续可直接添加 `audio`、`pdf` 等新类型。<br>• `_detect_rerank_backend` 集中路由逻辑，便于新增其他 decoder‑only rerank 模型（如 LLaMA‑Reranker）。<br>• `score_prompts` 在 `TokenizerManagerMultiItemMixin` 中实现，保持与已有 `score_request` 接口一致，降低代码重复。 |

---

### ⚠️ 潜在风险

| 风险点 | 说明 | 对策 |
|--------|------|------|
| **Jinja 模板注入** | 渲染模板时使用 `jinja2.Environment(undefined=jinja2.Undefined)`，如果模板来源被恶意篡改，可执行任意表达式。 | - 强制使用 `jinja2.StrictUndefined`、禁用 `{{ config }}` 类全局变量。<br>- 在发布前对所有官方模板进行签名校验。 |
| **多模态数据大小** | 客户端可直接提交大量图片/视频 URL，服务器会在生成前下载并解码，导致 OOM/DoS。 | - 在 `http_server` 为每个请求设置 **最大体积**（如 5 MiB）和 **并发下载限制**。<br>- 对 URL 进行域白名单或 CDN 限速。 |
| **模型路径误配置** | 若使用 `--is-embedding` 启动 Qwen3‑Reranker，后端会误判为 cross‑encoder，返回错误分数。 | - 在 `_detect_rerank_backend` 中加入明确校验：`if is_qwen3_template and not is_generation: raise`，并在启动脚本给出清晰错误提示。 |
| **兼容性回退** | 新增的 `return_documents` 默认 `True`，但旧客户端可能仍依赖 `document` 必在返回体中。 | - 保持向后兼容：在响应序列化时若 `document=None`，仍保留字段（ORJSON 默认会忽略），或在 `model_serializer` 中加 `exclude_none=False` 来保持旧结构。 |
| **依赖新增** | 引入 `jinja2`、`torch`（在测试中通过 Mock），在不装这些库的环境会导致 ImportError。 | - 在 `setup.cfg` 中把 `jinja2` 标记为 **optional dependency**，在运行时检测缺失并给出友好错误。 |

---

### 💡 关注建议

1. **模板安全**  
   - 在 `OpenAIServingRerank.__init__` 中对 `chat_template` 做一次 **安全审计**（仅允许官方提供的文件路径或 SHA‑256 校验），并在渲染前使用 `jinja2.StrictUndefined`、`autoescape=False`（保持原有行为）避免意外执行。  

2. **输入限制**  
   - 为 `RerankContent` 中的 `image_url` / `video_url` 增加 **size/格式校验**（例如仅支持 `jpeg/png/mp4`，最大 4 MiB），并在 `http_server` 对下载请求加 timeout（5 s）和重试限制。  

3. **日志与监控**  
   - 对每一次 `score_prompts` 与 VL 生成路径记录 **token‑ids、logprob、耗时**，便于后续性能排查。  
   - 增加 **指标**：`rer

---

#### 🟡 中重要度变更 (8)

### [diffusion] chore: improve the output_path config and enable the server to return inference duration (#16965)
**SHA**: `d11e2dc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d11e2dc6f4a84febcce3f2447501c52666a1a2a3)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `--output‑path` CLI 参数，并在 `ServerArgs` 中暴露，同步到 `SamplingParams`，实现服务器端统一控制生成文件的保存目录。  
2. 在 OpenAI 兼容的返回模型 (`ImageResponse`、`VideoResponse`) 中加入 `inference_time_s` 字段，并在 `add_common_data_to_response` 中把 `timings.total_duration_s` 塞入响应，便于客户端获取推理耗时。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/configs/sample/sampling_params.py` – 读取并覆盖 `output_path`。  
- `python/sglang/multimodal_gen/runtime/server_args.py` – CLI 参数新增、结构体字段新增。  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/protocol.py` – 响应模型新增 `inference_time_s`。  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/utils.py` – 把耗时写入返回 JSON。  

**💡 关注建议**  
- **兼容性**：`output_path` 现在默认为 `None`，旧的调用若未传入该参数仍会使用 `SamplingParams` 的默认值，注意文档中说明覆盖行为。  
- **安全性**：服务器端写文件前应确保目录合法、具备写权限，防止路径注入等风险。  
- **客户端适配**：使用 OpenAI 接口的上游代码需容忍 `inference_time_s` 可能不存在（旧版本），并在需要时读取该字段。  
- **测试**：加入单元/集成测试：① 参数 `--output-path` 能正确覆盖；② `ImageResponse`、`VideoResponse` 中的 `inference_time_s` 在有计时信息时被填充；③ 当 `timings` 为 `None` 时不抛异常。  

整体来看，此次改动提升了服务可配置性并提供了有价值的性能反馈，影响范围局限于多模态生成入口，风险可控。

---

### [diffusion] fix: fix using upstream flash_attn on blackwell (#17111)
**SHA**: `16831ab` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/16831ab6d707070f42275d596ce2b3b2f4d69072)

**🎯 变更类型**：功能增强 / 兼容性修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Hopper GPU（CUDA Compute Capability 9.0）新增平台检测 `is_hopper`，并在 `flash_attn` 后端仅在 Hopper 上尝试加载 upstream flash_attn 3 包；若未安装则给出警告。同步把一些 “HF model config” 信息的日志等级由 `info` 降为 `debug`，降低普通运行时的噪声。  
**🎯 影响范围**：  
- `sglang.multimodal_gen.runtime.platforms`（平台判定）  
- `sglang.multimodal_gen.runtime.layers.attention.backends.flash_attn`（注意力实现）  
- `sglang.multimodal_gen.runtime.loader.component_loader`（模型配置日志）  

**💡 关注建议**：  
1. 在 Hopper 环境部署时，确保已安装 `flash_attn` 3（`pip install flash-attn==3.*`），否则会回退到旧实现并产生性能下降的警告。  
2. 在非 Hopper GPU（如 Blackwell、A100 等）上保持原有行为；请验证 `current_platform.is_hopper()` 的返回在多卡场景下是否一致。  
3. 调整日志配置以捕获 `debug` 级别信息，以便在需要排查模型配置时能够看到完整输出。  
4. 运行单元测试或集成测试，重点检查 `flash_attn_varlen_func_upstream` 为 `None` 时的回退路径，防止因未捕获的 `ImportError` 导致启动失败。  

整体而言，此次改动提升了 Hopper 的兼容性和性能提示，同时对日志噪声做了细粒度控制，影响范围局限于注意力后端和模型加载流程。请在对应硬件上进行验证后再合并。

---

### [diffusion] fix: optimize text encoder CPU offload initialization to address OOM (#17064)
**SHA**: `e997995` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e9979950376aa2137ce9231ff370992b3a6a18f8)

**🎯 变更类型**：功能增强（CPU offload 初始化优化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `component_loader.py` 中新增 `model_device` 判定：若需 offload 且平台非‑MPS，提前把模型放在 CPU 上，随后再根据是否 offload 进行 FSDP 或普通 `.to()`。  
- 对 MPS 平台保留原有逻辑，防止 FSDP 与 MPS 冲突。  
- 测试层面加入 `text_encoder_cpu_offload` 用例及对应性能基线，确保新路径可被 CI 覆盖。

**🎯 影响范围**  
- `sglang.multimodal_gen.runtime.loader.component_loader` – 负责模型加载与设备分配的核心模块。  
- server 启动参数 `--text-encoder-cpu-offload` 与 `DiffusionServerArgs.text_encoder_cpu_offload`。  
- 相关单元测试、性能基线文件。

**💡 关注建议**  
1. **设备切换一致性**：`model_device` 与后续 `model.to(local_torch_device)` 的双重调用可能导致不必要的复制，建议在 `else` 分支统一使用 `model_device`，或在 `if should_offload` 分支仅在 CPU offload 场景下调用一次 `.to()`。  
2. **MPS 兼容性**：当前在 MPS 上仍强制 `model = model.to(local_torch_device)`，需确认在实际 MPS 环境下不会因未显式 `cpu` offload 而触发 OOM。  
3. **skip_init_modules** 与 CPU 初始化：确认 `skip_init_modules()` 在 CPU 上同样能跳过大模型权重的默认初始化，否则可能仍出现内存峰值。  
4. **性能回归**：新增基线显示 TextEncodingStage 耗时大量（≈4 s），应在 CI 中监控该指标，防止因 CPU offload 引入不可接受的延迟。  
5. **文档/参数**：在 `README` 或 CLI 帮助中补充 `--text-encoder-cpu-offload` 的使用场景、限制（仅在非‑MPS 时生效）以及对显存/推理速度的预期影响。  

总体上，此次改动有效缓解了文本编码器在 GPU 上的 OOM 风险，但需在设备切换路径、MPS 兼容性和性能基线上保持细致验证。

---

### [NPU] Add Ascend NPU best practice in doc (#17103)
**SHA**: `6586f44` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6586f44ad4f6169162384d9342f80c48a0c9c593)

**🎯 变更类型**：其他（文档增强）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `docs/platforms/` 新增 `ascend_npu_best_practice.md`，系统性汇总了 DeepSeek、Qwen 等主流 LLM 在华为 Ascend NPU 上的低时延 / 高吞吐部署方案、环境变量、启动脚本及基准测评指令。并在 `docs/platforms/ascend_npu_support.rst` 中加入该文件的索引。  

**🎯 影响范围**  
- Docs 生成 / CI：新增 2 KB+ 的 Markdown，会导致文档构建时间略增。  
- 用户侧：首次提供 NPU‑最佳实践，直接影响使用 Ascend 硬件的开发者。对代码运行本身无任何影响。  

**💡 关注建议**  
1. **文档一致性**：检查所有变量名（如 `SGLANG_NPU_USE_MLAPO`、`SGLANG_ENABLE_SPEC_V2` 等）与代码中实际实现保持同步，防止文档与实现脱节。  
2. **链接与示例**：确保文档中引用的脚本、数据集、模型路径示例（`MODEL_PATH=xxx`、`your prefill ip1` 等）在实际使用时易于替换，最好提供占位说明或示例脚本下载链接。  
3. **CI 与测试**：若项目在 CI 中执行文档 lint，需确认新 Markdown 符合现有规范（标题层级、表格对齐等），避免因格式错误导致构建失败。  
4. **后续维护**：NPU 环境变量经常更新，建议在文档顶部加入版本号或更新时间提示，方便后续同步。  

总体而言，此次提交仅是文档层面的丰富，提升了 Ascend NPU 用户的上手体验，对项目功能没有直接影响，只需注意文档的可维护性与 CI 兼容性。

---

### [AMD CI] migrate and re-enable CI tests to new CI registry (#16949)
**SHA**: `98096b5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/98096b5e022206936c2f27251516dd607c23d2a6)

**🎯 变更类型**：功能增强（CI 迁移 & 测试注册）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：此次提交把原先在 AMD CI 中直接硬编码的测试套件迁移到统一的注册系统，并在 GitHub Actions 工作流中新增 `stage‑c‑test‑large‑8‑gpu‑amd‑mi35x` 阶段。与此同时，几乎所有已有的测试用例都通过 `register_amd_ci` 同步到 AMD runner，部分原本仅 CUDA 的测试加入了 AMD 适配或条件跳过。  

**🎯 影响范围**  
- `.github/workflows/pr-test-amd.yml`（工作流结构、checkout‑ref、`amd_ci_exec.sh` 参数）  
- `scripts/ci/slash_command_handler.py`（CI 重跑指令支持新 stage）  
- `test/run_suite.py` 与 `test/srt/run_suite.py`（套件列表更新、移除旧 per‑commit‑8‑gpu‑amd‑mi35x）  
- 各 `test/registered/...` 文件（统一使用 `register_amd_ci`，部分加入 AMD‑only 超时、跳过逻辑）  
- `sglang/test/ci/ci_register.py`（隐式依赖：`is_in_amd_ci`、`is_hip`）  

**💡 关注建议**  

1. **套件名称一致性**：工作流中新 stage 名称 `stage-c-test-large-8-gpu-amd-mi35x` 必须在 `ci_register` 中正确注册，否则 CI 会报 “suite not found”。建议在提交前跑一次 `python -m sglang.test.ci.ci_register --list` 确认。  

2. **AMD 环境变量**：部分 test 开启了 `SGLANG_USE_AITER=0`、`skipIf(is_hip())` 等，确保 AMD runner 上已装好 ROCm、hip-aware PyTorch，并且 `is_hip()` 能正确辨识。  

3. **超时与资源**：AMD CI 对一些大型模型（如 DeepSeek‑R1‑MXFP4）将估算时间从 2750s 调整为 3600s，务必检查 runner 实际 GPU/VRAM 是否充足，防止 “timeout” 误报。  

4. **条件跳过**：`test_hicache_variants.py` 中对 AMD 直接 `skipIf(is_hip())`，但其他 HiCache 相关测试仍使用 `register_amd_ci`。确认这些测试在 AMD 上不会因缺失 `aiter` 而异常退出。  

5. **回退兼容**：CI 仍保留原 CUDA 阶段，确保在 CI 配置冲突或 AMD runner 暂时不可用时，PR 能继续在 CUDA 环境跑完整套测试。  

6. **本地验证**：建议在本地机器（开启 ROCm）执行一次 `bash scripts/ci/amd_ci_exec.sh python3 run_suite.py --suite stage-c-test-large-8-gpu-amd-mi35x`，确认 `-w "/sglang-checkout/test"` 工作目录映射无误，避免因路径误差导致找不到测试文件。  

总体来看，此次改动将 AMD CI 与统一的注册体系对齐，提升了可维护性和可扩展性。只要确保套件名称、环境依赖以及超时设置与实际硬件匹配，整体风险有限。祝快速通过 CI 验证！

---

### [diffusion] CI: add testcase for cfg parallel (#17056)
**SHA**: `68e8d0f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/68e8d0f68d1046306e074e4a0622443f25c66f73)

**变更类型**：功能增强  
**重要程度**：🟡 中  

**核心改动**  
1. **perf_baselines.json** 新增 `wan2_1_t2v_1.3b_cfg_parallel` 条目，提供了从输入校验到解码的完整阶段耗时及每步 denoise 时间统计，并给出期望的 e2e、平均与中位 denoise 时长。  
2. **testcase_configs.py** 增加 `DiffusionTestCase`，对应上述基准，新增 `cfg_parallel: bool` 参数并在 `DiffusionServerArgs` 中声明。  

**影响范围**  
- `python/sglang/multimodal_gen/test/server/`（CI 性能回归、基准对比）。  
- 服务器启动参数解析（`cfg_parallel`）以及后端 Diffusion 执行路径。  

**关注建议**  
- 确认 `cfg_parallel` 在实际推理代码中已被读取并正确触发并行 CFG 逻辑；若不存在，需要相应实现或在 CI 中做条件跳过。  
- 该用例要求 2 GPU，建议在测试入口加入硬件检查（如 `skipIf`），防止在单卡或资源不足的环境下导致 CI 失败。  
- 更新文档或 README，说明新基准的硬件要求和期望性能阈值。  
- 若后续对其它模型加入 `cfg_parallel`，可复用此结构，保持基准 JSON 与测试用例同步。

---

### Refactor prefix cache type checking (#17028)
**SHA**: `7f8a58f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7f8a58fffba9bc33ff2d0c2cd29ae74415620362)

**🎯 变更类型**：重构（Prefix Cache 类型检测）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将原先大量的 `isinstance(..., XCache)` 检查抽象为统一的特征接口（`supports_swa/ supports_mamba/ is_chunk_cache/ is_tree_cache`），并在 `BasePrefixCache` 与各实现（Chunk、SWA、Mamba）中实现这些方法；新增 `TreeCacheNamespace` 用于基准测试的 Dummy Cache；相应地对 `schedule_batch`、`schedule_policy`、`runtime_checker`、`common`、`speculative` 等模块的调用全部改为特征接口。  

**🎯 影响范围**  
- `sglang/srt/mem_cache/*`（核心缓存实现）  
- 调度相关：`schedule_batch.py、schedule_policy.py、scheduler_runtime_checker_mixin.py`  
- 运行时工具：`bench_one_batch.py`、`speculative/*`、`test/manual/test_forward_split_prefill.py`  

**💡 关注建议**  
1. **实现完整**：确保所有自定义或未来新增的 PrefixCache 子类实现 `supports_*`/`is_*` 方法，否则会触发运行时 `AttributeError`。  
2. **兼容性**：若外部代码仍使用旧的 `isinstance` 检查，建议保留短期兼容层或在文档中明确迁移路径。  
3. **性能验证**：特征方法调用虽轻量，但涉及大量调度路径，建议跑一次全量基准（`bench_one_batch.py`）确认无额外开销。  
4. **测试覆盖**：现有单元测试已更新，后续新增缓存类型时务必补充对应特征方法的测试，防止遗漏。  

总体来看，此次重构提升了代码的可扩展性与可读性，但需注意新接口的实现完整性以及保持向后兼容。祝调试顺利！

---

### [gRPC] Add GetLoads RPC for comprehensive load metrics (#17087)
**SHA**: `f091858` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f0918583042a59a99ea3a548f3d47093529296c9)

**🎯 变更类型**：功能增强（gRPC 新增 GetLoads 接口）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 SGLang gRPC 服务中加入 **GetLoads** RPC，实现全链路负载监控。新增 protobuf 定义、服务器端处理逻辑、请求管理器的请求/响应 communicator，并在 `grpc_server.py` 中完成结果到 protobuf 的转换及聚合计算。  
**🎯 影响范围**：`python/sglang/srt/entrypoints/grpc_server.py`、`python/sglang/srt/grpc/grpc_request_manager.py`、`sglang_scheduler.proto`（及模型网关复制文件），以及相关的数据类 `GetLoadsReqInput/Output`。  

**💡 关注建议**  
1. **并发安全**：`_GrpcCommunicator` 使用 `asyncio.Lock` 防止并发发送冲突，建议在单元测试中验证高并发下的超时与清理逻辑。  
2. **参数校验**：`include` 列表默认为 `["all"]`，但后端仍会校验合法性，建议在文档或代码中明确允许的关键字，防止用户传入无效字符串导致 `INVALID_ARGUMENT`。  
3. **可观测性**：返回的 `timestamp` 使用 UTC ISO‑8601，确保客户端时区统一；若需要更细粒度的延迟监控，可在 `GetLoads` 前后记录日志。  
4. **向后兼容**：新 RPC 为可选功能，不影响已有接口，但请在发布说明中提醒用户更新客户端 proto。  
5. **测试覆盖**：补充 **GetLoads** 的正向、异常（超时、非法参数）以及多 DP‑rank 场景的集成测试，确保 `aggregate` 计算准确。  

整体改动聚焦在监控能力提升，代码结构清晰，注意以上细节即可顺利上线。

---

#### 🟢 低重要度变更 (11)

### Fix hybrid attention PD Disaggregation test (#17099)
**SHA**: `4c59782` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4c59782e0f3d6bc4373333fbdca83959ccc19392)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `test_disaggregation_hybrid_attention.py` 的超时时间从 600 秒降至 400 秒，并在 CI 环境下暂时跳过该不稳定测试。

---

### Fix gid calculation in per_tensor_absmax_kernel (#17126)
**SHA**: `dda35cc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/dda35ccbd85c35cdfc96ecd3b3d89fad272f4c83)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `per_tensor_absmax_kernel` 中将全局线程 ID 计算从 `blockIdx.x * gridDim.x + threadIdx.x` 修正为 `blockIdx.x * blockDim.x + threadIdx.x`，解决量化核的索引错误。

---

### [diffusion] fix: fix UMA detection (#17113)
**SHA**: `c9a45b7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c9a45b7e3cc3573a92656ff8e9831eabbb8b148e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修改 CUDA 平台检测逻辑，改为直接判断 `torch.cuda.get_device_properties(...).is_integrated`，删除旧的 SM‑号硬编码，从而修复 UMA（统一内存）环境下的显存获取错误。

---

### [AMD] Align alternative sgl-kernel wheel (#17092)
**SHA**: `43fe3a4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/43fe3a4ddfe1f42521b399f84692cab48d7a1339)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：调整 AMD sgl‑kernel 构建脚本（CMakeLists & hipify），重新排序并统一 `pos_enc`、`transfer`、`timestep_embedding` 等 HIP 文件的引用顺序，以匹配 wheel 打包需求。

---

### chore: bump sgl-kernel version to 0.3.21 (#17075)
**SHA**: `000ad42` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/000ad4222595ec7f64380965c43ba08b13c59c2f)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 sgl‑kernel 版本从 0.3.20 升级至 0.3.21，并在 `pyproject.toml` 中同步更新依赖版本（如 `nvidia-cutlass-dsl`、`quack-kernels`），相应修改 Dockerfile 参数和运行时版本检查。

---

### [SWA] fix swa radix cache match_len_since_tombstone update when hits swa_tombstone (#17061)
**SHA**: `9d5f16d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9d5f16d456f551b8a6c29929678c30ed95f5f204)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `swa_radix_cache` 的前缀匹配逻辑中，修正了 tombstone 节点的处理：①在命中 tombstone 时立即将 `match_len_since_tombstone` 复位；②将最佳值长度和节点的更新条件移到 tombstone 判断内部，防止错误更新。

---

### [Docs] add routing-key to schedule-policy in docs (#17101)
**SHA**: `6b06529` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6b065298b57fa79515d824b3c7bb81b6d044f673)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/advanced_features/server_arguments.md` 中，`--schedule-policy` 参数的可选值新增 `routing-key`，完善调度策略说明。

---

### [Model-Gateway: grpc]: create tokenizer with chat template (#17052)
**SHA**: `c020d30` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c020d30045c1f12a2ee605e6d38863b8b0978637)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在模型网关的作业队列与本地工作器的 tokenizer 注册步骤中，新增对 `chat_template` 的获取、传递与克隆，并使用 `create_tokenizer_async_with_chat_template` 创建带聊天模板的 tokenizer，实现了对聊天模板的支持。

---

### [Fix] Remove assertion for padding for NVFP4 weight scales to fix GLM 4.5 NVFP4 (#12497)
**SHA**: `4346db5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4346db5fafee11513799ebb57ec3e6ad5d95f6e9)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：去除 NVFP4 权重量化尺度必须是 4 的整数倍的断言，改为不满足时仅输出警告，解决 GLM 4.5 NVFP4 的加载错误。

---

### fix session request with None tokenizer (#16278)
**SHA**: `5b1215d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5b1215d9da872d9ea5dbb167e3a4cf51d41c53b2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `scheduler.py` 中创建会话请求时，新增 `vocab_size` 参数并传递给 `session.create_req`；相应地在 `session_controller.py` 的 `create_req` 方法签名和实现中加入 `vocab_size` 参数，避免使用空 tokenizer 导致的错误。

---

### [diffusion] fix: fix fsdp tp load make param miss parallel meta data (#17058)
**SHA**: `0d904ef` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0d904ef44c9167a85667c7fb22e714387b03a62e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `_make_param_like` 辅助函数，在 FSDP TP 参数加载时克隆原参数并保留所有元数据，防止并行元信息缺失。

---

