# 每日更新报告（2026-02-12）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-12 23:43:22 | Xiaoyu Zhang | speed up sgl-kernel build (#18586) |
| 2026-02-12 22:43:24 | Simo Lin | refactor: consolidate gRPC client into shared crate dependency (#18730) |
| 2026-02-12 22:06:23 | Kangyan-Zhou | Fix B200 installation issue (#18725) |
| 2026-02-12 21:33:32 | Yi Zhang | [BUGFIX] fix bug in handle mamba radix cache in server_args (#18723) |
| 2026-02-12 21:29:24 | Simo Lin | refactor: replace local proto compilation with smg-grpc-proto package (#18682) |
| 2026-02-12 21:09:21 | Scott Lee | Add `spec_accept_histogram` request statistic (#18332) |
| 2026-02-12 21:08:37 | Alison Shao | Fix flaky penalty tests by using higher temperature for effect comparison (#18380) |
| 2026-02-12 21:05:17 | Kangyan-Zhou | Make PR based docker and pypi workflow work for forked PR (#18720) |
| 2026-02-12 19:28:05 | Hudson Xing | add tool_choice=auto nightly test case (#18302) |
| 2026-02-12 17:13:16 | Thomas Wang | [AMD] Fix accuracy issue when running TP4 dsv3 model with mtp (#18607) |
| 2026-02-12 17:05:53 | YC Tseng | [AMD] reset AMD image release time and reduce CI queue time (#18707) |
| 2026-02-12 15:47:32 | Alison Shao | [CI] Fix torchaudio/torchvision CUDA version mismatch (#18211) |
| 2026-02-12 15:22:47 | chenxu214 | [Ascend]Support qwen3.5 (#18544)<br>This PR affects only the NPU. If any issues arise, please contact iforgetmyname. |
| 2026-02-12 15:16:54 | Alan Kao | [AMD] Enable release image build for ROCm 7.2.0 (#18698) |
| 2026-02-12 14:31:23 | Vinh H. Pham | [diffusion] fix: replace TextEncoderConfig with Qwen3TextConfig for Z-Image (#18560) |
| 2026-02-12 14:20:43 | JooYeon | fix: /metrics endpoint always reports engine_type="unified" in PD disaggregation mode (#18552) |
| 2026-02-12 14:15:26 | Li Jinliang | Update README commands to include model-path option (#18557) |
| 2026-02-12 13:38:26 | Zheng Li | [Qwen3_5] Refactor `Qwen3_5ForCausalLMMTP` class implementation (#18538) |
| 2026-02-12 13:31:27 | YAMY | [Flashinfer Autotune] Fix FlashInfer FP4 MoE autotuning crash by removing incorrect flatten on hidden_states_scale (#18500) |
| 2026-02-12 13:29:25 | YC Tseng | [AMD] rocm 7.2 image release, PR test, Nightly Test (#17799) |
| 2026-02-12 13:25:43 | Ke Bao | Update ci permission (#18693) |
| 2026-02-12 12:08:25 | Liangsheng Yin | Try fix the max-parallel for maunally triggered test again. (#18686) |
| 2026-02-12 11:27:47 | danielafrimi | [Mamba] Add float16 support for SSM cache dtype (#18444) |
| 2026-02-12 11:08:29 | Zhiyu | Update modelopt quantization config parsing (#13919) |
| 2026-02-12 11:06:51 | Liangsheng Yin | fix the max-parallel for `/rerun-stage` (#18658) |
| 2026-02-12 10:37:12 | fy | update glm5 readme on npu (#18657) |
| 2026-02-12 10:36:45 | Liangsheng Yin | List more CI runs for `pr-test` (#18650) |
| 2026-02-12 10:35:39 | R0CKSTAR | [diffusion] fix: webui cannot correctly display generated video using wan2.2 (#18473) |
| 2026-02-12 10:11:59 | liupeng374 | glm5 md (#18655) |
| 2026-02-12 09:33:35 | Jincong Chen | Add CI permission for Chen-0210 (#18494) |
| 2026-02-12 09:27:22 | Yi Zhong | Avoid kimi linear stream sync (#16186) |
| 2026-02-12 09:26:46 | Jiayi Yan | [Bugfix] fix config bug caused by PR #18273 (#18535) |
| 2026-02-12 09:23:55 | Yuwei An | [PCG] GPT OSS Triton Kernel Support (#18405) |
| 2026-02-12 08:55:07 | qianyue76 | [diffusion] docs: consolidate diffusion documentation into docs (#18095) |
| 2026-02-12 08:28:57 | Alison Shao | [CI] Install python3-dev for Triton JIT compilation on fresh runners (#18644) |
| 2026-02-12 08:12:57 | Lianmin Zheng | Clean up noisy startup log messages and refactor loader.py (#18531) |
| 2026-02-12 01:03:43 | Piotr Mazurek | Add LMF2 MoE model architecture (#17997) |
| 2026-02-12 01:00:30 | Ke Bao | Fix prefill stats for dllm (#18632) |
| 2026-02-12 00:05:29 | Alison Shao | [CI] Guard python3 call in install script for fresh runners (#18609) |

### 📊 统计摘要
> 本日共 39 个提交 | 🔴高 4 | 🟡中 9 | 🟢低 26
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [refactor: consolidate gRPC client into shared crate depen...](#2f283d8)
    - [refactor: replace local proto compilation with smg-grpc-p...](#92c5749)
    - [fix: /metrics endpoint always reports engine_type="unifie...](#c929766)
    - [[AMD] rocm 7.2 image release, PR test, Nightly Test (#17799)](#20554a0)
  - [🟡 中重要度变更 (9)](#-🟡-中重要度变更-9)
    - [speed up sgl-kernel build (#18586)](#9e9e949)
    - [Add `spec_accept_histogram` request statistic (#18332)](#c59b922)
    - [Fix flaky penalty tests by using higher temperature for e...](#0abe4a2)
    - [[Ascend]Support qwen3.5 (#18544)](#1edc69b)
    - [[Qwen3_5] Refactor `Qwen3_5ForCausalLMMTP` class implemen...](#4ed2548)
    - [[PCG] GPT OSS Triton Kernel Support (#18405)](#2bd8363)
    - [[diffusion] docs: consolidate diffusion documentation int...](#f06ab17)
    - [Clean up noisy startup log messages and refactor loader.p...](#5875ef0)
    - [Add LMF2 MoE model architecture (#17997)](#ded068a)
  - [🟢 低重要度变更 (26)](#-🟢-低重要度变更-26)
    - [Fix B200 installation issue (#18725)](#1b8f68a)
    - [[BUGFIX] fix bug in handle mamba radix cache in server_ar...](#b168723)
    - [Make PR based docker and pypi workflow work for forked PR...](#f116b3a)
    - [add tool_choice=auto nightly test case (#18302)](#f365643)
    - [[AMD] Fix accuracy issue when running TP4 dsv3 model with...](#e20e6c2)
    - [[AMD] reset AMD image release time and reduce CI queue ti...](#d6f0ef6)
    - [[CI] Fix torchaudio/torchvision CUDA version mismatch (#1...](#f20b170)
    - [[AMD] Enable release image build for ROCm 7.2.0 (#18698)](#0305d12)
    - [[diffusion] fix: replace TextEncoderConfig with Qwen3Text...](#feaa9e7)
    - [Update README commands to include model-path option (#18557)](#d91ce17)
    - [[Flashinfer Autotune] Fix FlashInfer FP4 MoE autotuning c...](#4546768)
    - [Update ci permission (#18693)](#93ede0d)
    - [Try fix the max-parallel for maunally triggered test agai...](#3404dda)
    - [[Mamba] Add float16 support for SSM cache dtype (#18444)](#e422bca)
    - [Update modelopt quantization config parsing (#13919)](#7e262b6)
    - [fix the max-parallel for `/rerun-stage` (#18658)](#b2d7fd5)
    - [update glm5 readme on npu (#18657)](#123f57b)
    - [List more CI runs for `pr-test` (#18650)](#10c6bee)
    - [[diffusion] fix: webui cannot correctly display generated...](#41e1fd0)
    - [glm5 md (#18655)](#c34832c)
    - [Add CI permission for Chen-0210 (#18494)](#165aff3)
    - [Avoid kimi linear stream sync (#16186)](#dc1309f)
    - [[Bugfix] fix config bug caused by PR #18273 (#18535)](#539bbf4)
    - [[CI] Install python3-dev for Triton JIT compilation on fr...](#7eaf866)
    - [Fix prefill stats for dllm (#18632)](#5d185ef)
    - [[CI] Guard python3 call in install script for fresh runne...](#dcc63dc)
#### 🔴 高重要度变更 (4)

### refactor: consolidate gRPC client into shared crate dependency (#18730)
**SHA**: `2f283d8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2f283d81525a19b1fd706f38c66a6f25924b0eb3)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 将 SGLang 模型网关内部实现的 gRPC 客户端（包括 SGLang Scheduler 与 vLLM Engine）迁移至统一的 `smg-grpc-client` 共享 crate，并在 `Cargo.toml` 中声明依赖。  
- 删除本地的 protobuf 编译过程、生成的 `proto` 文件以及原有的手写 client 实现，改为使用共享库提供的 `connect_with_trace_injector` 接口。  
- 在网关代码中添加 `OtelTraceInjector` 实现，以适配 `smg-grpc-client` 的 `TraceInjector` trait，实现 OpenTelemetry 追踪上下文的传递。  

**🎯 影响范围**  
- `sgl-model-gateway` 包内的所有 gRPC 调用入口（`grpc_client` 模块、`lib.rs` 的导出、构建脚本）。  
- 依赖 `smg-grpc-client` 的所有子模块（Scheduler、vLLM）以及与之交互的路由层。  
- CI/CD 构建流程（不再需要 `tonic-prost-build` 编译 protobuf）。  

---  

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **模块化提升**：gRPC 客户端抽离为独立 crate，实现代码复用，避免在多个项目中维护相同的 protobuf 与 client 实现。<br>- **依赖关系**：`sgl-model-gateway` 现在对 `smg-grpc-client` 产生硬依赖，版本升级会直接影响网关的编译和运行时行为。<br>- **接口统一**：所有后端（SGLang、vLLM）统一使用 `connect_with_trace_injector`，调用方只需要关心运行时类型，降低了分支逻辑的复杂度。 |
| **性能影响** | - **编译时间**：移除本地 `build.rs` 的 protobuf 编译显著缩减构建时间，CI 速度提升。<br>- **运行时开销**：`smg-grpc-client` 已经实现了与原实现相同的连接池、keep‑alive 参数，功能保持不变。<br>- **内存占用**：共享 crate 只加载一次 protobuf 描述，整体内存略有下降。 |
| **安全考虑** | - **攻击面收敛**：删除本地生成的 protobuf 代码后，潜在的代码注入或编译器漏洞点变少。<br>- **追踪注入**：新增的 `OtelTraceInjector` 通过 `MetadataMap` 注入 OpenTelemetry 头部，遵循 `smg-grpc-client` 的安全接口，未引入额外的可控风险。<br>- **依赖可信度**：需要审计 `smg-grpc-client` 的安全性（尤其是 `TraceInjector` 实现）以及其对 tonic 的使用是否存在未处理的异常情况。 |
| **可维护性** | - **代码量**：删除 1 500 行左右的手写 client 代码，仓库体积明显减小。<br>- **统一测试**：所有 gRPC 相关单元测试转移至 `smg-grpc-client`，网关自身只需保留对外 API 的集成测试。<br>- **升级路径**：后续若协议有变，只需在 `smg-grpc-client` 中更新，网关无需改动（只要 API 兼容）。 |
| **兼容性** | - 旧的内部 `grpc_client` 模块已被删除，外部依赖该模块的代码（如果有）需要改为使用 `smg_grpc_client` 提供的同名类型。<br>- `connect_with_trace_injector` 是新 API，原有 `connect` 调用已被替换，确保所有调用方都已迁移。 |

### ⚠️ 潜在风险  

1. **版本耦合风险**  
   - `sgl-model-gateway` 现在强依赖 `smg-grpc-client = "1.0.0"`。如果该 crate 在后续发布的次要/补丁版本中修改了 proto 定义或接口（尤其是 `TraceInjector`），可能导致编译或运行时错误。  
   - 建议在 `Cargo.lock` 中锁定精确版本，并在升级前进行完整的集成测试。

2. **接口兼容性**  
   - `connect_with_trace_injector` 需要传入 `Arc<dyn TraceInjector>`。如果以后改为接受其他类型，当前代码将需要同步修改。  
   - 确认 `smg-grpc-client` 对错误返回的包装（`Box<dyn Error + Send + Sync>`）与网关的错误处理保持一致。

3. **追踪上下文泄漏**  
   - `OtelTraceInjector` 直接调用已有的 `inject_trace_context_grpc`，若该函数在未来更改为异步或返回错误，需要相应地更新实现，否则可能导致追踪头部丢失。

4. **删除的 protobuf 文件**  
   - 虽然已在共享 crate 中提供相同的 protobuf 描述，但如果网关中仍有代码硬编码了旧的 proto 路径或字段（如单元测试），会导致编译错误。搜索残留引用并清理是必要的。

5. **构建脚本残留**  
   - `build.rs` 中的 `println!("cargo:rerun-if-changed=Cargo.toml");` 仍保留，但不再需要 `rerun-if-changed` 对 proto 文件的监控。虽然影响不大，但可进一步简化。

### 💡 关注建议  

- **依赖管理**：在根 `Cargo.toml` 中使用 `cargo update -p smg-grpc-client` 前，先在本地运行完整的单元/集成测试，确保向后兼容。可考虑在 CI 中加入对 `smg-grpc-client` 公开的 semver 规则检查。  
- **文档同步**：更新项目 README 与开发者文档，说明 gRPC 客户端已迁移至 `smg-grpc-client`，并提供对应的使用示例（尤其是 `connect_with_trace_injector`）。  
- **回滚方案**：保留当前提交的历史分支，以防共享 crate 出现不可兼容的破坏性改动可以快速回滚到本地实现。  
- **测试覆盖**：新增针对 `GrpcClient::new` 的集成测试，验证在不同 `runtime_type`（sglang / vllm）下能够成功建立连接并正确注入追踪元数据。  
- **安全审计**：对 `smg-grpc-client` 进行一次第三方安全审计，特别是其对 `tonic::transport::Channel` 的配置（如 keep‑alive 参数）是否符合生产环境的安全最佳实践。  
- **性能基准**：在关键路径（生成请求、嵌入请求）上跑一次微基准，确保抽离后没有额外的延迟或资源开销。  

---  

**结论**：此次重构通过将 gRPC 客户端抽象为共享库，在 **架构清晰度、构建效率和代码可维护性** 上获得显著提升。只要注意依赖版本的锁定、兼容性验证以及对 `TraceInjector` 实现的安全审查，风险可控，整体价值高，推荐在主分支合并后尽快在生产环境进行滚动部署。

---

### refactor: replace local proto compilation with smg-grpc-proto package (#18682)
**SHA**: `92c5749` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/92c5749f412d05720888fddc6de6e56fb6b928d6)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交用 `smg-grpc-proto` 发行版代替项目内部的 protobuf 编译流程。删除了本地 `.proto` 文件、编译脚本、`setup.py` 中的自定义构建逻辑，并将所有相关依赖升级为 `grpcio>=1.78.0`、`smg-grpc-proto>=0.3.3`。代码中改为直接从 `smg_grpc_proto` 包导入生成的 protobuf 模块。

**🎯 影响范围**：  
- Python 包构建与发行（`python/pyproject*.toml`、`setup.py`）  
- CI 检查（`.github/workflows/lint.yml`）  
- `.gitignore` 对生成文件的忽略规则  
- 运行时 gRPC 入口：`python/sglang/srt/entrypoints/grpc_server.py`  
- 所有依赖 protobuf 生成代码的模块（如 `sglang/srt/grpc/*`）  

**🔍 技术洞察**：

- **架构影响**  
  - 把 protobuf 定义抽离为独立的 PyPI 包 (`smg-grpc-proto`)，实现 **单一来源**，统一了各子模块（如 `sgl-model-gateway`）的协议定义。  
  - 消除了项目内部的 **编译‐时依赖**（`grpcio-tools`），简化了 `pyproject.toml` 中的 `build-system.requires`，构建过程仅依赖 `setuptools`。  
  - 删除了 `setup.py` 中的自定义 `build_py`、`develop`、`egg_info` Hook，回归到标准的 PEP‑517/518 构建模型，提升可维护性并兼容现代打包工具（如 `uv`、`pip wheel`）。  

- **性能影响**  
  - 编译阶段的 **proto 生成时间** 被完全省去，`pip install` 时只需下载预编译好的 `.whl`，显著加快 CI 与本地安装速度。  
  - 运行时没有额外的 **import 重写**（`_fix_imports`）步骤，启动时加载 `smg_grpc_proto` 的模块开销与普通依赖相同。  

- **安全考虑**  
  - 移除自定义构建脚本降低了 **供应链攻击面**（自定义 Python 代码在 build 环境中执行）。  
  - 同时引入了对外部第三方包 `smg-grpc-proto` 的 **依赖信任**，需要关注该包的签名、发布渠道以及更新策略，以防潜在的供应链风险。  

**⚠️ 潜在风险**：

1. **版本兼容性**  
   - 若 `smg-grpc-proto` 的 protobuf 定义与项目代码中使用的字段或服务签名出现不一致（例如新字段未向后兼容），将导致运行时 `AttributeError` 或 gRPC 调用失败。  
2. **供应链风险**  
   - 外部包的安全漏洞或恶意发布会直接影响所有使用该包的 SGLang 部署。  
3. **CI 失效**  
   - 删除的 “proto sync check” 可能隐藏 **跨仓库协议不一致** 的问题，若 `smg-grpc-proto` 与其他子仓库（如 `sgl-model-gateway`）的版本不匹配，仍会出现不兼容现象。  
4. **文档与示例**  
   - 项目文档中仍可能引用本地 `.proto` 路径，导致新手在本地调试时产生混淆。  

**💡 关注建议**：

- **锁定依赖版本**：在 `pyproject.toml` 中使用精确的 `smg-grpc-proto==0.3.3`（或采用 `~=0.3.3`）并在 CI 中做版本对齐检查，防止意外升级。  
- **兼容性回归测试**：新增针对 gRPC 接口的集成测试，确保所有服务端点 (`Generate`, `Embed`, `HealthCheck` 等) 在使用外部 proto 包时仍能正确序列化/反序列化。  
- **供应链监控**：在项目的 Dependabot / Renovate 配置中加入 `smg-grpc-proto` 的安全审计，及时捕获上游漏洞。  
- **文档同步**：更新 README、开发指南以及内部 wiki，说明 protobuf 已迁移至 `smg-grpc-proto`，并提供对应的导入示例。  
- **CI 兼容检查**：在 CI 中加入一个小步骤，`pip show smg-grpc-proto` 并校验其内部协议文件的哈希或版本号与预期一致，以取代原来的文件同步 diff。  

通过上述措施，可最大化利用外部 protobuf 包带来的构建简化与维护优势，同时将潜在的兼容性和供应链风险控制在可接受范围。

---

### fix: /metrics endpoint always reports engine_type="unified" in PD disaggregation mode (#18552)
**SHA**: `c929766` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c9297661b9185344d758c5be47653b8abc84c0cc)

**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**：  
- 修正了在 PD （Prefill‑Decode）拆分模式下，`/metrics` 接口错误地始终返回 `engine_type="unified"` 的问题。  
- 通过将对 `DisaggregationMode` 的比较统一改为使用其 `.value`（即枚举整数值）进行判断，确保在 Prefill、Decode 与 Unified 三种模式下能够正确映射到对应的 `engine_type`。  
- 影响的代码位于 gRPC 与 HTTP 服务入口以及调度器指标混入，实现统一的行为修正。  

**🎯 影响范围**：  
- `python/sglang/srt/entrypoints/grpc_server.py`（健康检查与 warmup 请求）  
- `python/sglang/srt/entrypoints/http_server.py`（HTTP 健康检查）  
- `python/sglang/srt/managers/scheduler_metrics_mixin.py`（指标初始化与 `engine_type` 生成）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 该改动仅涉及业务逻辑层面的枚举比较，不改变底层调度或网络协议，实现了**向后兼容**。  
  - 统一使用 `DisaggregationMode.*.value` 消除了 Python Enum 与原始整数在比较时的潜在歧义，提升代码可读性与一致性。  
- **性能影响**：  
  - 改动仅是一次枚举值比较，开销微乎其微，**不会对请求延迟或吞吐产生可感知影响**。  
- **安全考虑**：  
  - 修正后 `/metrics` 接口返回的 `engine_type` 更加准确，避免因错误的指标信息导致监控系统误判或触发错误的自动化响应（如错误的弹性伸缩策略）。  
  - 未涉及身份验证、数据泄露等安全敏感路径，**安全风险基本为零**。  

**⚠️ 潜在风险**：  
- **回归风险**：如果项目中其他地方仍然使用 `DisaggregationMode` 的枚举对象直接比较（而非 `.value`），可能出现不一致的行为，需要检查是否有遗漏的对应修改。  
- **兼容性**：外部插件或自定义脚本若依赖于旧的比较方式（例如 `== DisaggregationMode.PREFILL`），在升级后可能出现逻辑错误。  

**💡 关注建议**：  
1. **代码审计**：在全仓库搜索 `DisaggregationMode` 的使用，确保所有比较统一改为 `.value`，防止潜在的隐性错误。  
2. **测试覆盖**：新增或完善单元测试，覆盖 Prefill、Decode、Unified 三种拆分模式下 `/metrics`、健康检查与 warmup 接口返回的 `engine_type`。  
3. **监控验证**：在生产环境部署后，观察监控平台的 `engine_type` 指标是否立即呈现预期的三种值，确保改动生效。  
4. **文档更新**：在项目的部署或监控说明中注明 `engine_type` 的取值范围及对应的拆分模式，帮助运维人员快速定位问题。  

---

### [AMD] rocm 7.2 image release, PR test, Nightly Test (#17799)
**SHA**: `20554a0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/20554a0a4fb686dd00ffc83f7e45f12b14210fa6)

**🎯 变更类型**：功能增强 / 重构 / 性能优化  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 在仓库根目录新增两套 **AMD ROCm 7.2** CI 工作流（nightly 测试、PR 测试）以及对应的 **Dockerfile**（`docker/rocm720.Dockerfile`）和镜像发布流程（`release-docker-amd-rocm720-nightly-preview.yml`）。  
- 为不同 GPU 架构（MI30x、MI35x）以及 ROCm 7.2/7.0 统一了镜像查找、自动构建、缓存、并发控制逻辑。  
- 大幅改造 **CI 脚本**（`amd_ci_start_container.sh`、`amd_ci_install_dependency.sh`），加入可选的 **skip‑build** 参数、AITER 与 SGLang 构建分离、预热 AITER JIT、缓存持久化、模型缓存诊断等。  
- 在核心代码层面加入 **vLLM 可用性检测** 与 **fallback 实现**（layernorm、moe、quantization、unquant、server_args），保证在没有 vLLM 时仍能运行（尽管性能可能下降）。  
- 调整 **模型测试配置**，统一在 AMD 环境下默认打开 `SGLANG_USE_AITER=0`，并在特定情形（GPT‑OSS bf16）切换至 triton 后端；改进了分片调度、MoeRunner 后端选择逻辑以及超时/资源诊断。  
- 修复若干 **测试脚本**（diffusion、multimodal‑gen、RL、benchmark）在 AMD 环境下的路径、环境变量、超时设置。  
- 添加 **安全/性能环境变量**（如 `SGLANG_DISABLE_CUDNN_CHECK=1`、`HIP_FORCE_DEV_KERNARG=1`、`NCCL_MIN_NCHANNELS=112` 等）以及对 **MIOpen、AITER、MOONCAKE、TileLang** 的新依赖与构建步骤。  

---

### 🎯 影响范围  

| 受影响模块/组件 | 说明 |
|------------------|------|
| `.github/workflows/*` | 7 + 8 条新/改动的 CI 工作流（nightly、PR、docker release） |
| `docker/rocm720.Dockerfile` | 新的 ROCm 7.2 镜像构建脚本，支持 MI30x/MI35x、MORI、NIC 后端、Setuptools SCM 伪版本 |
| `scripts/ci/amd/*` | 容器启动、依赖安装、AITER 检测、预热、缓存处理逻辑全部重构 |
| `python/sglang/*` | 多处加入 **vLLM 可用性检测** 与 **native fallback**（层归一化、MoE、FP8 量化、权重打散），以及 `server_args` 中的后端切换 |
| `test/*` | 大量测试用例的跑批、超时、环境变量、模型列表、GPU‑arch‑specific 逻辑调整 |
| `python/sglang/srt/*` | MoE runner、TileLang、Moir、AITER 相关的 import 与分支路径更新 |
| `release-docker-amd-rocm720-nightly-preview.yml` | 镜像发布自动化（tag、pretend‑version、不同 GPU‑arch） |
| 全局变量/环境变量 | 新增/修改的运行时调优变量（`SGLANG_USE_AITER`、`SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN`、`SGLANG_ROCM_FUSED_DECODE_MLA` 等） |

---

### 🔍 技术洞察  

#### 1. 架构影响  
- **ROCm 7.2 支持**：首次在官方 CI 中使用 ROCm 7.2 基础镜像（`rocm/pytorch:rocm7.2_ubuntu22.04_py3.10_pytorch_release_2.9.1`），并在 Dockerfile 中通过 `ARG GPU_ARCH` 灵活切换 `gfx942`（MI30x） / `gfx950`（MI35x） 编译路径。  
- **模块化镜像选型**：通过 `find_latest_image` 在最近 7 天内自动挑选已推送的镜像，若不存在则使用 **硬编码的 preview 镜像**（`rocm/sgl-dev:v0.5.8.post1-rocm720-...-preview`），保证 CI 不会因镜像缺失卡住。  
- **AITER / Triton / VLLM 多后端抽象**：在 `sglang/srt/layers/*` 中引入 `_has_vllm_*` 标记，依据 `SGLANG_USE_AITER`、GPU‑arch、后端自动决定使用 AITER CK kernel、vLLM custom‑ops、或 **pure PyTorch** 实现。这样避免了因 vLLM 未随镜像一起打包而导致 `ImportError`，提升了 **容错性**。  
- **Mori NIC 后端**：Dockerfile 现在可选构建 `MORI`（基于 AMD AINIC），并通过 `--build-arg NIC_BACKEND=ainic` 控制。对非‑AINIC 环境仍保持兼容（`none`）。  
- **TileLang / TVM‑FFI**：在镜像里统一安装 TileLang（基于 ROCm‑llvm）以及 TVM‑FFI，以支撑 **JIT kernel**（如 QKV‑norm、MoeSum 的 triton fallback）。  
- **依赖分层**：`amd_ci_install_dependency.sh` 现在允许 **跳过 AITER、SGLang、测试时依赖**，方便在 CI 只跑轻量测试或调试镜像时节省时间。

#### 2. 性能影响  
| 影响点 | 正面效应 | 潜在负面 |
|--------|----------|----------|
| **预热 AITER JIT** (`amd_ci_warmup_aiter.py`) | 第一次调度时避免 kernel 编译耗时，显著降低 nightly/PR 测试时的 timeout 率。 | 若使用 **fallback**（pure PyTorch）则仍会受限于 CPU/GPU 计算，性能下降 ~30‑50%。 |
| **缓存持久化 (`/sgl-data`)** | 多 GPU 实例共享 HF、Miopen、AITER‑kernel 缓存，减少模型下载与编译时间。 | 需要确保 CI runner 磁盘配额足够，缓存膨胀可能导致磁盘 OOM。 |
| **后端自动切换** (`server_args.py`、MoE runner) | 对 GPT‑OSS bf16 自动切到 triton，规避 AITER CK kernel 在某维度上不支持的情况，保持 **吞吐量**。 | 在切到 triton 时会使用 **triton‑kernel**（不如 A

---

#### 🟡 中重要度变更 (9)

### speed up sgl-kernel build (#18586)
**SHA**: `9e9e949` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9e9e94926162d0b2bf189c9722e97dbdc59b7375)

**🎯 变更类型**：性能优化（加速 sgl‑kernel 构建）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 CI 与本地构建脚本中新增并默认开启并行编译参数（`BUILD_JOBS`、`NVCC_THREADS`），改为两步构建：先生成 “deps” 镜像并缓存，再在容器内部挂载宿主 ccache 目录进行实际 wheel 编译。默认 `BUILD_JOBS=64`、`NVCC_THREADS=8`，并对 ARM、默认情况的并行度做了更合理的上限控制。  

**🎯 影响范围**  
- `.github/workflows/release-whl-kernel.yml`：CI 构建并行度提升。  
- `sgl‑kernel/Dockerfile`：新增 `BUILD_JOBS`、`NVCC_THREADS` 参数以及 CMake 参数拼接，打印调试信息。  
- `sgl‑kernel/build.sh`：整体构建流程改为两阶段（deps + wheel），引入宿主 ccache 挂载、并传递新的 env/arg。  
- 可能受影响的用户：自行编译 sgl‑kernel、使用 CI 的项目、依赖本地 ccache 的开发者。  

**💡 关注建议**  
1. **兼容性检查**：新参数默认开启，若用户机器的 CPU/内存不足，`BUILD_JOBS=64` 可能导致 OOM，建议在文档或 CI 中提供 `BUILD_JOBS=auto` 的说明，或在脚本中 fallback 为 `$(nproc)`。  
2. **ccache 路径**：首次运行会在 `~/.cache/sgl-kernel/ccache` 创建目录，确认 CI 环境有足够磁盘空间；若不需要 ccache，需显式设置 `USE_CCACHE=0`。  
3. **Docker 镜像缓存**：两阶段构建引入了 `sgl‑kernel-deps` 镜像，确保 CI 中的 `docker pull` 或 `docker buildx prune` 不会误删此中间层。  
4. **ARM 与 x86 行为**：ARM 分支仍保持保守设置，未受 `BUILD_JOBS` 影响，保持原有稳定性。  
5. **日志与调试**：新增的 `echo` 输出有助定位并行度问题，建议在 CI 中保留这些日志以便后续性能回归。  

总体而言，此次改动在不破坏现有构建逻辑的前提下显著提升编译速度，唯一需要关注的是并行度对资源的需求以及 ccache 持久化策略。若项目对 CI 运行时长有严格限制，建议在 CI 参数中提供可调的 `BUILD_JOBS`/`NVCC_THREADS`。

---

### Add `spec_accept_histogram` request statistic (#18332)
**SHA**: `c59b922` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c59b9223e6839ddd971f872358ef4d02cad74e21)

**🎯 变更类型**：功能增强（为推理统计新增接受直方图）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 Speculative Decoding 过程中，新增 `spec_acceptance_histogram` 用以记录每一步接受的 Draft Token 数目分布，并在相关管理器、调度器、结果处理以及统计输出中完整传递、累计和导出该数据。  

**🎯 影响范围**  
- `srt/managers/io_struct.py`、`detokenizer_manager.py`、`multi_tokenizer_mixin.py` – 数据结构与字段转发。  
- `schedule_batch.py`、`scheduler_output_processor_mixin.py` – 请求对象新增直方图维护逻辑 (`update_spec_acceptance_histogram`)。  
- `speculative/eagle_info.py`、`speculative/ngram_info.py` – 统计接受 token 时同步更新直方图。  
- `tokenizer_manager.py` – 将直方图写入返回的 meta 信息。  

**💡 关注建议**  
1. **字段默认与兼容性**：`spec_acceptance_histogram` 在关闭 speculative decoding 时返回空列表，建议在所有对该字段的读取处加上 `or []` 防止 `None` 引发异常。  
2. **内存/性能**：直方图在极端高接受数场景会自动扩容，注意 `extend` 可能产生短暂的列表复制，建议限制最大接受长度或使用 `collections.Counter` 替代。  
3. **单元测试**：补充针对 `update_spec_acceptance_histogram` 的边界测试（0、首次出现的大数、连续调用），确保序列化/反序列化后保持一致。  
4. **监控仪表**：如果对外暴露统计接口，建议在 Prometheus/日志中新增对应指标，以便用户评估 speculative decoding 效率。  

整体改动清晰，未改变已有业务路径，风险主要在新字段的向后兼容和潜在的列表扩容开销，适度添加防御性检查即可。

---

### Fix flaky penalty tests by using higher temperature for effect comparison (#18380)
**SHA**: `0abe4a2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0abe4a22c6764ccf2be52f418fc2662e6eb4a2d0)

**🎯 变更类型**：其他（测试改进）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原先通过统计单词重复次数评估 `frequency/presence/repetition` penalty 的测试改为使用“词汇多样性”(unique words / total words) 统计。  
- 为避免在低温度下 penalty 对采样几乎无影响，统一在基准与惩罚参数中默认设定 `temperature=0.8`，并将 `max_tokens` 上限提升至 150。  
- 删除 `target_word` 参数，统一使用 `_get_vocab_diversity` 计算，并相应调整断言逻辑：正向 penalty → 多样性提升，负向 penalty → 多样性下降。  

**🎯 影响范围**  
- **test/registered/sampling/test_penalty.py**（全部）  
- 受影响的模块主要是 CI/单元测试层，对运行时代码无直接改动。  

**💡 关注建议**  
1. **可靠性**：虽然多次采样、提升温度可降低原有 flaky，但多样性指标仍受随机性影响，建议在 CI 中保留足够的迭代次数（≥5）或加入统计显著性检查，防止偶发误判。  
2. **语义一致性**：原测试聚焦特定词的重复，当前改用整体词汇分布，可能掩盖某些 penalties 对特定 token 的抑制效果。若产品文档仍声称“penalty reduces word repetition”，需同步更新说明或保留针对关键词的补充测试。  
3. **参数默认**：在 `baseline_params` 与 `penalty_params` 中硬写 `temperature=0.8`，若上层代码已显式设置温度会被覆盖。建议改为 `setdefault`（已使用）或在测试用例中显式声明，以免对自定义温度的其他测试产生意外影响。  
4. **性能**： `max_tokens` 从 50 提升至 150，生成文本更长会略增测试时长，CI 资源占用相应上升，建议监控整体耗时，必要时可通过 `pytest.mark.timeout` 限制上限。  
5. **回归防护**：新增私有辅助函数 `_get_vocab_diversity` 与 `run_generate_with_prompt`，若未来修改这两个函数的实现逻辑，需确保仍满足 penalty 测试的前提（如 token 生成的随机性）。可考虑在测试文件顶部加入注释，说明此测试的统计假设。  

总体来看，此次改动提升了 penalty 相关单元测试的稳定性，且对核心业务代码无侵入，只需关注上述潜在的随机性与语义差异，确保 CI 与文档保持一致。

---

### [Ascend]Support qwen3.5 (#18544)
**SHA**: `1edc69b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1edc69be085473f27314f885e43ab7e70ef09a23)

**🎯 变更类型**：功能增强 / 代码适配  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Ascend NPU 添加兼容路径，修正在非‑CPU（尤其是 NPU）环境下的导入与断言错误；在 `modelslim` 中对量化描述做一次性归一化，避免重复处理；在 Qwen‑3.5 MoE 实现中引入专家位置信息配置并把层前缀从 `".self_attn"` 改为 `".linear_attn"`，以配合新的后端实现。  

**🎯 影响范围**  
- `sglang/srt/layers/attention/hybrid_linear_attn_backend.py`（NPU 运行时路径）  
- `sglang/srt/layers/quantization/modelslim/modelslim.py`（量化配置）  
- `sglang/srt/models/qwen3_5.py`（Qwen‑3.5 MoE 模型及专家分布）  

**💡 关注建议**  
1. **NPU 兼容**：确认 `is_npu()` 在 Ascend 环境返回 true，避免误将 NPU 当作 CPU 进入旧路径；在 CI 中加入 NPU‑only 测试，检查 `cutedsl_fused_sigmoid_*` 等 CUDA‑only 实现不会被误引用。  
2. **量化描述归一化**：`_quant_description_normalized` 只在首次调用 `is_layer_skipped` 时设置，后续仍会使用已修改的 `self.quant_description`。若同一实例在多次 `is_layer_skipped` 前被外部读取，需要确认外部代码已经适配新键名。  
3. **前缀替换**：将 `".self_attn"` 改为 `".linear_attn"` 后，必须保证模型权重文件中的层名保持一致，否则加载会失败。建议在模型转换脚本中同步修改。  
4. **专家位置配置**：新增 `get_model_config_for_expert_location` 为后续专家调度提供统一入口，确认 `ModelConfigForExpertLocation` 在 `sglang/srt/eplb/expert_location.py` 中已实现并兼容旧模型配置。  

总体来看，此次提交在 NPU 支持和 MoE 专家分布上做了必要适配，风险集中在路径判断和层名前缀的改动，建议通过多平台（CPU、CUDA、Ascend）完整跑通模型推理后再合并。

---

### [Qwen3_5] Refactor `Qwen3_5ForCausalLMMTP` class implementation (#18538)
**SHA**: `4ed2548` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4ed2548427a0f01a969d6e518088bcb62a568f5d)

**🎯 变更类型**：重构、功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `qwen3_5.py` 中为不同模型类型（`qwen3_5_moe`, `qwen3_5_text`, `qwen3_5_moe_text`）新增 `is_layer_sparse / is_previous_layer_sparse / is_next_layer_sparse` 标记，并将这些标记传给 `LayerScatterModes.init_new`，以便在稀疏/非稀疏层之间正确切换。  
2. 将原有的 `Qwen3_5MultiTokenPredictor` 完全删除，`Qwen3_5ForCausalLMMTP` 现在复用 `Qwen3_5ForCausalLM` 实现，仅在 MTP 前后加入一次线性投射和两层 `GemmaRMSNorm` 预处理。  
3. 精简了权重加载逻辑：在 `load_fused_expert_weights` 中把 `mtp.` 前缀映射到新模型结构（`fc`、`norm`、`pre_fc`），去除不再使用的 `VocabParallelEmbedding`。  

**🎯 影响范围**  
- `python/sglang/srt/models/qwen3_5.py`（层稀疏标记、`LayerScatterModes` 初始化）  
- `python/sglang/srt/models/qwen3_5_mtp.py`（MTP 预测器重构、前向路径、新增 `fc` 与 RMSNorm）  
- 权重加载路径 `load_fused_expert_weights`（名称映射）  
- 可能影响 Pipeline‑Parallel 环境的 `is_last_rank` 判断与占位层逻辑  

**💡 关注建议**  
1. **稀疏标记一致性**：确认 `is_layer_sparse` 等标记在模型配置文件中与实际稀疏 MoE 层对应，防止误判导致调度错误。  
2. **权重兼容**：旧 checkpoint 在迁移到新结构前需执行一次模型转换脚本，确保 `fc`, `norm`, `pre_fc` 参数能够正确加载。  
3. **前向行为对比**：在多卡 / pipeline 环境下跑一次 `torch.autograd.profiler`，验证 `hidden_states` 拼接、预处理以及 `fc` 的数值与原实现保持一致。  
4. **接口保持**：`Qwen3_5ForCausalLMMTP` 仍接受 `input_embeds` 参数，但内部已强制使用 `forward_batch.mm_input_embeds`，建议在文档中说明此行为变化，以免上层调用出现未预期的 `None`。  
5. **回归测试**：加入对 `qwen3_5_moe`, `qwen3_5_text` 等配置的 MTP 推理路径的单元测试，覆盖 `idle`、`extend`、`draft_extend` 三种 `forward_mode` 场景，确保不再出现 `tensor size 0` 或残差未定义的错误。  

总体而言，此次重构提升了代码复用度和层稀疏调度的可配置性，但需要重点验证稀疏标记、权重映射以及在 Pipeline‑Parallel 环境下的行为一致性。

---

### [PCG] GPT OSS Triton Kernel Support (#18405)
**SHA**: `2bd8363` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2bd8363486e4c0cf1bc9f09e8e97073588a9684e)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `ForwardContext` 中新增 `moe_fusions`，并在 `set_forward_context` 传递该列表。  
2. `ModelRunner` 与 `PiecewiseCudaGraphRunner` 同步收集并使用 `moe_fusions`，以在分段 CUDA 图中执行完整的 MoE Fusion。  
3. `fused_moe_triton` 的 forward 逻辑放宽了对 `TopKOutput` 格式的断言，若非标准格式回退到普通实现。  
4. 在 `GptOss` 中注册了 `moe_impl` 自定义算子，利用 `forward_context.moe_fusions` 完成路由、Top‑K 与专家计算的统一调度。  
5. `ServerArgs` 对 `moe_runner_backend` 的默认选择做了细微调整，避免在开启分段图时错误覆写。

**🎯 影响范围**  
- `sglang/srt/compilation/piecewise_context_manager.py`  
- `sglang/srt/model_executor/model_runner.py`、`piecewise_cuda_graph_runner.py`  
- `sglang/srt/layers/moe/fused_moe_triton/layer.py`  
- `sglang/srt/models/gpt_oss.py`（自定义算子）  
- `sglang/srt/server_args.py`

**💡 关注建议**  
1. **前向上下文一致性**：`moe_fusions` 与 `moe_layers` 必须一一对应；建议在 `set_forward_context` 里加入长度校验或断言，以防模型结构变化导致下标错位。  
2. **兼容性**：默认值仍为 `None`，但部分代码直接访问 `forward_context.moe_fusions[layer_id]`。在未开启分段图或 MoE 模型时，应提前判空或提供回退路径，避免 `NoneType` 异常。  
3. **自定义算子注册**：`register_custom_op` 依赖 `out_shape="hidden_states"`，确认对应的 C++/triton 实现已同步更新，否则可能出现 shape 推断错误。  
4. **非标准 Top‑K 路径**：新加入的 `if not TopKOutputChecker.format_is_standard` 分支会走普通 forward_impl，需确保该实现已在所有后端（FlashInfer、Triton 等）注册，以免出现未实现的情况。  
5. **性能回归检测**：分段图现在会在非标准 Top‑K 时放弃图优化，建议在 CI 中加入对比基准，确保在标准路径下仍能获得预期的吞吐提升。  
6. **文档/示例更新**：解释 `moe_fusions` 的含义及在开启 `enable_piecewise_cuda_graph` 时的使用方式，帮助用户正确配置 `server_args`。  

总体而言，此次改动为 GPT‑OSS 在分段 CUDA 图下提供完整的 MoE Fusion 支持，提升了图执行的统一性和潜在性能。但需注意上下文同步、空值防护以及兼容性测试，以保证在多种模型配置下的稳健运行。

---

### [diffusion] docs: consolidate diffusion documentation into docs (#18095)
**SHA**: `f06ab17` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f06ab17a73eb03eec1b71569b9d94234ac6363b9)

**变更概览**  
本次 PR 以“文档合并”为核心，将散落在 `docs/` 各子目录的 Diffusion 相关内容统一搬迁、重命名并补齐缺失页面，形成 **`docs/diffusion/`**、**`docs/diffusion/performance/`** 等结构；同时更新了主索引 (`index.rst`) 与跨模块引用，使 Diffusion 文档成为项目文档体系的正式入口。

**影响范围**  
- 文档构建流程（Sphinx）——新增 `diffusion`、`performance`、`support_new_models`、`mindspore_models` 等文件；删除旧的 `image_generation/diffusion_models.md`。  
- 其他文档的超链接（如 `basic_usage/diffusion.md`、`supported_models/*`）被统一指向新路径。  
- CI 中的文档检查、链接校验、搜索索引等可能受影响。  

**关键风险 & 建议**  

| 风险点 | 说明 | 建议 |
|--------|------|------|
| **链接失效** | 大量 `../supported_models/.../diffusion_models.md`、`cache/*.md` 等旧路径被替换，若还有残余引用会导致构建报错或页面 404。 | 本地执行 `make html`，使用 `grep -r "diffusion_models.md" -n .` 检查是否仍有残留；同理检查 `cache/`、`performance/` 的旧链接。 |
| **TOC 同步** | `index.rst` 中新增的 toctree 项目顺序和层级必须与实际文件匹配；否则 Sphinx 会报 “document not found”。 | 运行 `sphinx-build -b html -W`，确保所有 toctree 条目均能解析。 |
| **搜索/跨文档引用** | 例如 `frontend_tutorial.ipynb`、`supported_models/extending/index.rst` 等文件中仍可能指向旧路径。 | 在 CI 中加入一步 “linkcheck” 检查，或手动在 notebook 中搜索 `diffusion_models.md`。 |
| **文档风格统一** | 部分标题从 `##` 改为 `**`、粗体变动，可能影响主题样式或可读性。 | 统一使用 Sphinx 推荐的标题层级（`#`、`##`），并在本地预览检查渲染效果。 |
| **新增页面未列入导航** | `mindspore_models.md` 已加入 `supported_models/extending`，但未在对应 `toctree` 中出现。 | 确认该文件在 `docs/supported_models/extending/index.rst` 中被引用，若遗漏请补充。 |
| **CI 文档体积** | 本次新增约 500 行文档，CI 构建时间将略增。 | 可在 CI 中缓存 `build/html`，或仅在文档改动时触发完整构建。 |

**总结**  
本次提交纯粹为文档结构重构与内容完善，代码逻辑未受影响。只要确保所有旧链接已彻底迁移、Sphinx TOC 与实际文件保持一致，并通过 `make html` 与 `make linkcheck` 验证，即可安全发布。建议在 PR 合并前在一个干净的虚拟环境中完整跑一次文档构建，以防意外遗漏的路径或渲染错误。

---

### Clean up noisy startup log messages and refactor loader.py (#18531)
**SHA**: `5875ef0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5875ef0a34749aaad40026c2ebb3599481214b8a)

**变更类型**：功能增强 / 代码清理  

**核心改动**  
1. **启动日志 & 运行时日志**：在 `launch_server.py` 中调用 `suppress_noisy_warnings()`，统一在 `utils/common.py` 抑制已知的第三方噪声警告；在 `OpenAIServingChat` 中添加 `_default_sampling_params_logged`，避免每次实例化都重复打印默认采样参数；删减 `moe/utils.py`、`model_runner.py`、`mamba_radix_cache.py`、`loader.py` 中的若干 `logger.info`，整体启动/加载日志更简洁。  
2. **模型加载逻辑**：`loader.py`  
   - 将 `_maybe_download_from_modelscope` 的返回类型统一为 `str`，不再返回 `None`。  
   - 新增正则常量 `_MTP_PATTERN` 与 `_filter_mtp_weights` 方法，抽离 MTP 权重筛选，代码更清晰。  
   - 引入 `enable_multithread_load` 配置开关，使用 `use_multithread` 统一控制多线程加载路径。  
3. **公共工具**：`utils/common.py` 新增 `suppress_noisy_warnings`，并在 `suppress_other_loggers` 中调用，集中管理噪声警告抑制。  

**影响范围**  
- 启动入口 `launch_server`、OpenAI chat 服务 `serving_chat.py`  
- MOE 相关实用、注意力后端初始化、Mamba 缓存日志  
- 模型加载与权重过滤 (`loader.py`)  
- 公共日志/警告抑制工具  

**关注建议**  
- **测试多线程加载**：确认 `enable_multithread_load` 在默认配置（False）和显式开启时均能正常工作，防止因线程数设置不当导致加载错误。  
- **警告抑制副作用**：`suppress_noisy_warnings` 在全局 import 时执行，可能隐藏用户实际需要的警告，建议在文档中注明可通过 `suppress_other_loggers(False)`（如后续提供）关闭。  
- **返回类型兼容**：虽然 `_maybe_download_from_modelscope` 现在必返回字符串，但仍保持对旧调用的兼容性；若外部自行检查 `None`，需同步更新。  
- **日志依赖**：若有监控或脚本依赖被删除的 `logger.info`（如权重加载时长），请相应迁移至显式计时或 `debug` 级别。  

总体而言，此次提交在不改变功能前提下，大幅降低启动/加载期间的噪声日志，并为多线程权重加载与 MTP 权重筛选提供更易维护的实现，建议全面回归测试后推送。

---

### Add LMF2 MoE model architecture (#17997)
**SHA**: `ded068a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ded068a76e00878881d52d5bfb791e0f60d7311b)

**变更类型**：功能增强（新增 LMF2‑MoE 模型）  
**重要程度**：🟡 中  

**核心改动**  
1. **配置层**：  
   - `sglang/srt/configs/__init__.py` 新增 `Lfm2MoeConfig` 并在 `__all__` 中导出。  
   - 新文件 `lfm2_moe.py` 完整实现了 MoE‑专用的 `PretrainedConfig`，包括 MoE 参数、layer‑type 校验、Mamba‑cache 参数注册等。  

2. **模型实现**：  
   - 新增 `sglang/srt/models/lfm2_moe.py`，实现  
     - `Lfm2MoeMLP`（前几层稠密 MLP），  
     - `Lfm2MoeSparseMoeBlock`（sigmoid‑routing MoE，使用 `FusedMoE`），  
     - `Lfm2MoeAttention`、`Lfm2MoeShortConv`、`Lfm2MoeDecoderLayer`、`Lfm2MoeModel`、`Lfm2MoeForCausalLM`。  
   - 重点处理了 Tensor‑Parallel 切分、Short‑Conv 缓存、专家权重加载兼容 HF 格式。  

3. **模型调度**：  
   - `model_executor/model_runner.py` 中的 `mamba2_config` 判定逻辑加入 `Lfm2MoeConfig`，保证 MoE 模型能够走 Mamba2 路径。  

4. **权重加载**：  
   - `Lfm2MoeForCausalLM.load_weights` 实现了对 HF MoE 权重的映射、专家参数的特殊处理（`FusedMoE.make_expert_params_mapping`），并兼容嵌入权重共享。  

5. **测试**：  
   - 在工具调用测试 `test_tool_choice.py` 中新增 `TestToolChoiceLfm2Moe`，验证 LFM2‑MoE 在 tool‑call 场景下的行为（两项已标记 skip，说明已知的 token‑stall 问题）。  

**影响范围**  
- **配置模块** (`sglang.srt.configs.*`)  
- **模型目录** (`sglang.srt.models.lfm2_moe`)  
- **模型调度/执行** (`sglang.srt.model_executor`)  
- **权重加载逻辑**（与 HF 的 MoE 权重映射）  
- **单元测试**（新增 LFM2‑MoE 专用用例）  

**关注建议**  
1. **兼容性**：确认 `CONFIG_MAPPING` 注册成功，防止在 `AutoConfig.from_pretrained` 时出现 `model_type` 未识别的错误。  
2. **权重加载**：在多机器/TP 环境下验证 `sharded_weight_loader` 对 conv 权重、expert 参数的切分是否与 HF checkpoint 完全一致；建议加入 CI 中的权重对比测试。  
3. **性能**：MoE 采用 sigmoid 路由与 `FusedMoE`，但 `routed_scaling_factor` 被手动乘回，需关注数值误差对推理结果的影响，尤其在 bf16 环境。  
4. **缓存管理**：`mamba2_config` 的返回依赖 `conv_L_cache` 与 `layer_types`，请确保 `layer_types` 长度与 `num_hidden_layers` 对齐，否则会在运行时抛异常。  
5. **文档/示例**：在 README / model‑list 中补充 LFM2‑MoE 说明，特别是 `--tool-call-parser lfm2` 参数的使用方式。  
6. **测试**：解除 `skip` 标记前，需要解决 “maxItems:1 bug causes whitespace stall” 导致的 token 卡顿，避免该模型在非流式调用时出现死等。  

总体来说，此次 PR 为 SGLang 引入了完整的 LFM2‑MoE 支持，涉及配置、模型实现、调度与权重加载多个层面，需重点验证多卡/TP 环境下的权重切分与数值一致性，并完善文档与测试。

---

#### 🟢 低重要度变更 (26)

### Fix B200 installation issue (#18725)
**SHA**: `1b8f68a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1b8f68af57c039055ec9b0b08b972989ee72a995)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 安装依赖脚本中新增 `apt-get update`，刷新 apt 索引，防止因旧缓存导致的 404 错误，从而解决 B200 安装失败的问题。

---

### [BUGFIX] fix bug in handle mamba radix cache in server_args (#18723)
**SHA**: `b168723` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b16872342485a365d0496f7a8901b1c90a58b5f0)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_args.py` 中为 `_handle_mamba_radix_cache` 调用新增 `support_mamba_cache=True` 参数，并将 `extra_buffer` 判定从 `elif` 改为 `if`，修正了 Mamba 缓存相关的逻辑错误，避免在非 CUDA 环境误用。

---

### Make PR based docker and pypi workflow work for forked PR (#18720)
**SHA**: `f116b3a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f116b3a51ba182be67843cdd8e307aa5a91db694)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 Docker 与 PyPI PR 工作流的输入改为仅接受 PR 编号，并使用 `refs/pull/${{ inputs.pr_number }}/head` 作为检出引用，以兼容内部及 fork 的 PR。

---

### add tool_choice=auto nightly test case (#18302)
**SHA**: `f365643` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f3656432c7a365c9e678f5f2dd02d9acb05437fb)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在工具调用测试中新增 `test_auto` 参数与对应测试，用于验证 `tool_choice=auto` 能正确返回 `tool_calls` 且无 `content`。

---

### [AMD] Fix accuracy issue when running TP4 dsv3 model with mtp (#18607)
**SHA**: `e20e6c2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e20e6c28b9bf56f632c64fdfeb5b8871237e8d88)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 Docker 镜像的 AITER 版本升级至 `v0.1.10.post3`；在 `aiter_backend.py` 中针对 `num_head==32` 强制开启 fast_mode，针对 `num_head==16` 的非‑fp8 KV‑cache 禁用持久化 MLA kernel，并将 `make_mla_meta_data` 的持久化标识改为 `False`，以修复 TP4‑dsv3 模型的准确性问题。

---

### [AMD] reset AMD image release time and reduce CI queue time (#18707)
**SHA**: `d6f0ef6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d6f0ef677b659ccc043f820a1ada53971cdcdf3f)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 AMD 镜像夜间发布的 cron 时间提前 1 小时；在 AMD CI 中将 multimodal‑gen 测试的触发条件简化为 `needs.check-changes.outputs.multimodal_gen == 'true'`；将 1‑GPU 扩散服务器测试超时由 60 分钟提升至 70 分钟，以降低 CI 队列等待时间。

---

### [CI] Fix torchaudio/torchvision CUDA version mismatch (#18211)
**SHA**: `f20b170` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f20b1703ce4e9fa7bac23910f71ee2a711cc36d3)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 安装脚本中新增检测 torch CUDA 版本的逻辑，若与目标 CUDA 版本不符，则重新从对应的 PyTorch 镜像源强制 reinstall torchaudio 与 torchvision，以解决 CUDA 版本不匹配导致的加载错误。

---

### [AMD] Enable release image build for ROCm 7.2.0 (#18698)
**SHA**: `0305d12` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0305d12df21233af9cb5d3e0717511fc7d3ea197)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在发布 CI 中加入 `rocm720` 支持，扩展矩阵至 `rocm700/rocm720`，根据版本选择对应 Dockerfile 与标签，统一镜像命名规则。

---

### [diffusion] fix: replace TextEncoderConfig with Qwen3TextConfig for Z-Image (#18560)
**SHA**: `feaa9e7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/feaa9e7e00e624eb25e375e81fd5d47c78080874)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 ZImage 流水线的文本编码器配置从 `TextEncoderConfig` 替换为 `Qwen3TextConfig`，并相应更新性能基准文件中的预期耗时指标，以匹配新编码器的运行时特性。

---

### Update README commands to include model-path option (#18557)
**SHA**: `d91ce17` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d91ce176bfbd47577effb13fe5258ab6a7c3a089)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 WebUI 使用说明中，将直接模型标识改为 `--model-path` 参数，更新相应的示例命令。

---

### [Flashinfer Autotune] Fix FlashInfer FP4 MoE autotuning crash by removing incorrect flatten on hidden_states_scale (#18500)
**SHA**: `4546768` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/454676811ec7fdf4f0dea43607a5fdcc02c38ddb)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 FlashInfer FP4 MoE 自动调优崩溃，去除对 `hidden_states_scale` 的错误 `flatten()` 操作，仅保留 `view(torch.float8_e4m3fn)`。

---

### Update ci permission (#18693)
**SHA**: `93ede0d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/93ede0db19e8045ffc9d9441544e8f20ff96b9ae)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中，将 `HandH1998` 的 `cooldown_interval_minutes` 从 60 调整为 0，允许该用户立即重新触发 CI。

---

### Try fix the max-parallel for maunally triggered test again. (#18686)
**SHA**: `3404dda` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3404dda5928aa2656692ccde697990acae95c6f4)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：完善手动触发测试的 max_parallel 判定，改用 `target_stage` 并在获取 PR 标签时先尝试 SHA，后回退至分支名称，以兼容 fork 与非‑fork PR。

---

### [Mamba] Add float16 support for SSM cache dtype (#18444)
**SHA**: `e422bca` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e422bcaed8879a8ac58ddd9ef10557c6725f2299)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在服务器参数 `--mamba-ssm-dtype` 及相应文档中新增 `float16` 支持，使 Mamba SSM 缓存可使用该数据类型。

---

### Update modelopt quantization config parsing (#13919)
**SHA**: `7e262b6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7e262b6496b7efb66c7b50f4f96da04881e6f8e8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在模型配置解析中新增对 ModelOpt 量化配置的识别与解析，统一把非 dict 的 `quantization_config`/`compression_config` 转为字典，并在已量化检查中加入相应判断。

---

### fix the max-parallel for `/rerun-stage` (#18658)
**SHA**: `b2d7fd5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b2d7fd5c87fa654837928a701a3e29b21e46bc4e)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/workflows/pr-test.yml` 中为 `/rerun-stage` 增加 `pull-requests` 权限，并通过 GitHub API 检查 PR 是否带有 “high priority” 标签，若有则将 `max_parallel` 设置为 14，否则为 3。

---

### update glm5 readme on npu (#18657)
**SHA**: `123f57b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/123f57b84bbaeb23e673d33598279fcf564f9607)

**变更类型**：文档更新  
**重要程度**：🟢低  
**摘要**：在 `docs/platforms/ascend_npu_glm5_examples.md` 中新增说明，使用提供的 Docker 镜像时需通过 `pip install git+https://github.com/huggingface/transformers.git` 重新安装 Transformers 主分支。

---

### List more CI runs for `pr-test` (#18650)
**SHA**: `10c6bee` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/10c6bee74f1c26b2a752304f814a1278f251fa0f)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 CI 工作流从 “List Active PR Runs” 改为 “List Active Runs”，去除仅 PR 的过滤，新增对非 PR（手动、定时等）运行的统计与展示，记录 event 与分支信息，并在最终汇总中加入非 PR 运行计数。整体为工作流脚本的功能扩展。

---

### [diffusion] fix: webui cannot correctly display generated video using wan2.2 (#18473)
**SHA**: `41e1fd0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/41e1fd0be735b88b923d55662b9f51ecf53edb6f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 WebUI 视频生成路径加入文件存在检查，若文件已生成则返回路径而非抛错；仅在文件缺失时抛出详细异常。

---

### glm5 md (#18655)
**SHA**: `c34832c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c34832c02c28360cca5657f26741001dd9d3fe7c)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：新增 `docs/platforms/ascend_npu_glm5_examples.md`，详细说明在 Ascend NPU 上部署与运行 GLM‑5 模型的环境准备、单/多节点部署、参数配置及评测方法。

---

### Add CI permission for Chen-0210 (#18494)
**SHA**: `165aff3` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/165aff38e12da18b3fce06bb7cfc62c9059a3525)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中新增 `Chen-0210` 条目，授予其标签触发 CI、重跑失败 CI、重跑阶段等权限，并设定 60 分钟冷却。

---

### Avoid kimi linear stream sync (#16186)
**SHA**: `dc1309f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/dc1309fc7e4a1f338a13bd3f91a549ac766d6cc0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `copy_` 替换为 `fill_`，在混合线性注意力后端的 metadata 重放中直接填充常量值，避免不必要的拷贝，提高效率并防止 Kimi 线性流同步问题。

---

### [Bugfix] fix config bug caused by PR #18273 (#18535)
**SHA**: `539bbf4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/539bbf485c5a05dd24f4c9bf08a71830251efbaa)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：简化 `may_get_weight_block_size`，移除对 `get_model_architecture` 的调用，直接调用 `_get_quantization_config`，删除不再使用的 `packed_modules_mapping` 参数。

---

### [CI] Install python3-dev for Triton JIT compilation on fresh runners (#18644)
**SHA**: `7eaf866` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7eaf866846e629259eb6c9a1ec3472afaa5660e9)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 脚本 `ci_install_dependency.sh` 中新增 `python3-dev` 包的安装，确保在全新 runner 上进行 Triton JIT 编译时拥有必要的 Python 开发头文件。

---

### Fix prefill stats for dllm (#18632)
**SHA**: `5d185ef` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5d185efb78c2794552c72b5cbf8d4f8dd9add3e7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `scheduler.py` 创建 DLLM 批次时，新增 `prefill_stats` 字段并记录预填充的输入/命中 token、生成比例、运行批大小和新序列数，以便后续日志统计。

---

### [CI] Guard python3 call in install script for fresh runners (#18609)
**SHA**: `dcc63dc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/dcc63dc5452c212d5a9e9944a37ade5ff4a3b315)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 安装脚本 `ci_install_dependency.sh` 中加入 python3、pip、venv 的安装与检测，改为 `python3 -m pip` 执行并在后续清理 Torch 编译缓存，提升新 runner 的兼容性。

---

