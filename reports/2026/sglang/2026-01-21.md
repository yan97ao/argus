# æ¯æ—¥æ›´æ–°æŠ¥å‘Šï¼ˆ2026-01-21ï¼‰

## sgl-project/sglang

| æäº¤æ—¶é—´ | ä½œè€… | æäº¤ä¿¡æ¯ |
|----------|------|----------|
| 2026-01-21 23:58:25 | JiaruiChang5268 | [NPU] Remove paged attention & Change fia to default attention (#17394) |
| 2026-01-21 23:02:24 | Xiaoyu Zhang | [Diffusion] Refactor diffusion is_cuda check (#17498) |
| 2026-01-21 22:54:02 | Qiaolin Yu | Support fa4 decoding (#16034) |
| 2026-01-21 22:26:25 | Yi Zhong | [docs] Show user the fastAPI docs available (#17510) |
| 2026-01-21 21:34:18 | b8zhong | [Fix] GLM 4.7 + NVFP4 + MTP (#17166) |
| 2026-01-21 20:01:55 | Zhu Yuhua | [diffusion] improve: skip negative prompt encoding when guidance_scale <= 1.0 or negative_prompt is None (#16919) |
| 2026-01-21 19:24:29 | strgrb | Use fused_sigmoid_gating_delta_rule_update_kernel for KDA (#17108) |
| 2026-01-21 19:20:05 | Ke Bao | Tiny refine swa kv cache free (#17417) |
| 2026-01-21 18:47:00 | siyu | Support EPD error handling (#16670) |
| 2026-01-21 18:17:16 | Fan Lin | [diffusion] fix: fix the LoRA weights mismatch caused by weights packing (#17355) |
| 2026-01-21 18:13:51 | HuangJi | [diffusion] feat: support SageSparseLinearAttention attention backend (#17399) |
| 2026-01-21 17:12:57 | Yi Zhang | [BUGFIX] fix value oom in radix tree (#17400) |
| 2026-01-21 15:48:30 | Yi Zhang | [RadixTree][2/N Refactor]: swa cache init tiny refactor (#17397) |
| 2026-01-21 15:47:09 | Sam Shleifer | Fix circular import in quantization modules (#17372) |
| 2026-01-21 15:42:45 | blahblah | [diffusion] refactor: refactor and simplify teacache for cachabledit and wanvideo (#16396) |
| 2026-01-21 15:36:26 | amote-i | update ascend docs (#17457) |
| 2026-01-21 14:45:39 | Alison Shao | Add job-level timeout for weekly test workflow (#17462) |
| 2026-01-21 14:34:40 | Baizhou Zhang | [Tiny] Backward compatibility for fp4 gemm flags (#17466) |
| 2026-01-21 13:54:09 | Baizhou Zhang | [Fix] Set fa3 as default MHA backend on Hopper (#17425) |
| 2026-01-21 12:50:27 | Alison Shao | Fix pr-test-finish to fail when wait-for-stage jobs fail (#17465) |
| 2026-01-21 11:58:13 | Kangyan-Zhou | Fix NSA indexer in the nightly test (#17452) |
| 2026-01-21 11:46:36 | Baizhou Zhang | [Minor] Change lora_target_modules to "all"  in CI tests (#17386) |
| 2026-01-21 11:39:51 | Fan Lin | [diffusion] fix: fix the bug of output_path not taking effect when generate (#17293) |
| 2026-01-21 11:00:04 | khalilzhk | [NPU] remove features supported on Ascend NPU (#17455) |
| 2026-01-21 09:45:38 | Yinghai Lu | [scheduler] Clear MM  data of finished batch (#17251) |
| 2026-01-21 09:43:50 | Alison Shao | Add hybrid parallelism test to nightly CI (#17444) |
| 2026-01-21 09:40:05 | Alison Shao | Fix wait-for-stage jobs running when call-gate fails (#17443) |
| 2026-01-21 06:47:20 | ishandhanani | update urllib3 and gpgv Dockerfile (#17439) |
| 2026-01-21 06:06:06 | Binyao Jiang | [Piecewise] Fix PCG issue for multimodal and embedding model that wraps language_model (#17290) |
| 2026-01-21 06:05:32 | Binyao Jiang | [Piecewise] Support PCG weak_ref_tensor cuda kernel on AMD (#17291) |
| 2026-01-21 06:05:05 | Lianmin Zheng | [Auto Sync] Update piecewise_cuda_graph_runner.py (20260119) (#17313) |
| 2026-01-21 04:31:10 | zijiexia | [Docs] Rename SGLang Router to SGLang Model Gateway (#17436) |
| 2026-01-21 03:10:36 | Ruihuan He | [Docs] Explain CUDA attention backend choices and aiter FP8 KV cache support (#17428) |
| 2026-01-21 02:56:55 | Baizhou Zhang | Overlap shared experts with deepep dispatch for single batch overlap on Blackwell (#17289) |

### ğŸ“Š ç»Ÿè®¡æ‘˜è¦
> æœ¬æ—¥å…± 34 ä¸ªæäº¤ | ğŸ”´é«˜ 2 | ğŸŸ¡ä¸­ 13 | ğŸŸ¢ä½ 19
## ğŸ“‹ ç›®å½•

- [sgl-project/sglang](#sgl-project-sglang)
  - [ğŸ“Š ç»Ÿè®¡æ‘˜è¦](#-ç»Ÿè®¡æ‘˜è¦)
  - [ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (2)](#-ğŸ”´-é«˜é‡è¦åº¦å˜æ›´-2)
    - [[diffusion] feat: support SageSparseLinearAttention atten...](#e776239)
    - [[diffusion] refactor: refactor and simplify teacache for ...](#0a7a201)
  - [ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (13)](#-ğŸŸ¡-ä¸­é‡è¦åº¦å˜æ›´-13)
    - [[NPU] Remove paged attention & Change fia to default atte...](#a95c9f5)
    - [[Diffusion] Refactor diffusion is_cuda check (#17498)](#19089aa)
    - [[Fix] GLM 4.7 + NVFP4 + MTP (#17166)](#2ff0880)
    - [Use fused_sigmoid_gating_delta_rule_update_kernel for KDA...](#bcc6d84)
    - [Support EPD error handling (#16670)](#7520b92)
    - [[diffusion] fix: fix the LoRA weights mismatch caused by ...](#e7224e9)
    - [[RadixTree][2/N Refactor]: swa cache init tiny refactor (...](#236772c)
    - [Fix circular import in quantization modules (#17372)](#0d49b13)
    - [[Minor] Change lora_target_modules to "all"  in CI tests ...](#c3f9c30)
    - [[NPU] remove features supported on Ascend NPU (#17455)](#aca354b)
    - [Add hybrid parallelism test to nightly CI (#17444)](#823a046)
    - [[Piecewise] Support PCG weak_ref_tensor cuda kernel on AM...](#38c233f)
    - [Overlap shared experts with deepep dispatch for single ba...](#6ea491e)
  - [ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (19)](#-ğŸŸ¢-ä½é‡è¦åº¦å˜æ›´-19)
    - [Support fa4 decoding (#16034)](#4f6f5d2)
    - [[docs] Show user the fastAPI docs available (#17510)](#458fe5a)
    - [[diffusion] improve: skip negative prompt encoding when g...](#2c1b164)
    - [Tiny refine swa kv cache free (#17417)](#a618202)
    - [[BUGFIX] fix value oom in radix tree (#17400)](#1b97fa7)
    - [update ascend docs (#17457)](#0a9099e)
    - [Add job-level timeout for weekly test workflow (#17462)](#0050c47)
    - [[Tiny] Backward compatibility for fp4 gemm flags (#17466)](#8251a74)
    - [[Fix] Set fa3 as default MHA backend on Hopper (#17425)](#a54d75b)
    - [Fix pr-test-finish to fail when wait-for-stage jobs fail ...](#3321eb4)
    - [Fix NSA indexer in the nightly test (#17452)](#be5121b)
    - [[diffusion] fix: fix the bug of output_path not taking ef...](#54a8217)
    - [[scheduler] Clear MM  data of finished batch (#17251)](#aea57b3)
    - [Fix wait-for-stage jobs running when call-gate fails (#17...](#648aab0)
    - [update urllib3 and gpgv Dockerfile (#17439)](#1e30903)
    - [[Piecewise] Fix PCG issue for multimodal and embedding mo...](#6092721)
    - [[Auto Sync] Update piecewise_cuda_graph_runner.py (202601...](#20ed382)
    - [[Docs] Rename SGLang Router to SGLang Model Gateway (#17436)](#4ecd9af)
    - [[Docs] Explain CUDA attention backend choices and aiter F...](#eb38d64)
#### ğŸ”´ é«˜é‡è¦åº¦å˜æ›´ (2)

### [diffusion] feat: support SageSparseLinearAttention attention backend (#17399)
**SHA**: `e776239` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/e776239afdd16c82ce340e64a6a50e8d48f79ff3)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / æ–°åç«¯é›†æˆ  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤ä¸º `sgl-project/sglang` å¼•å…¥äº†å…¨æ–°çš„ **Sage Sparse Linear Attentionï¼ˆSageSLAï¼‰** åç«¯ï¼Œå®ç°äº†åŸºäº SpargeAttnï¼ˆSpargeAttentionï¼‰åº“çš„é‡åŒ–ç¨€ç–â€‘çº¿æ€§æ³¨æ„åŠ›ã€‚ä»£ç å±‚é¢æ–°å¢äº†åç«¯å®ç°ã€å…ƒæ•°æ®/æ„å»ºå™¨ã€ä»¥åŠåœ¨æ¨¡å‹ã€å±‚ä¸å¹³å°é€‰æ‹©é€»è¾‘ä¸­çš„æ³¨å†Œä¸å…¼å®¹å¤„ç†ï¼›æ–‡æ¡£ä¹ŸåŒæ­¥æ›´æ–°ä»¥å±•ç¤º SageSLA ä¸ SageSLAâ€‘Sparse çš„æ”¯æŒçŸ©é˜µã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `python/sglang/multimodal_gen/runtime/layers/attention/backends/sparse_linear_attn.py`ï¼ˆæ ¸å¿ƒå®ç°ï¼‰  
- `turbo_layer.py`ã€`wanvideo.py`ï¼ˆæ¨¡å‹å±‚è°ƒç”¨ï¼‰  
- `platforms/cuda.py`ã€`platforms/interface.py`ï¼ˆåç«¯æšä¸¾ä¸åŠ è½½ï¼‰  
- `multimodal_gen/docs/support_matrix.md`ï¼ˆæ–‡æ¡£ï¼‰  
- ç›¸å…³ä¾èµ–ï¼šæ–°å¢ `spargeattn`ï¼ˆ`git+https://github.com/thu-ml/SpargeAttn.git`ï¼‰  

**ğŸ” æŠ€æœ¯æ´å¯Ÿ**  

| ç»´åº¦ | å½±å“åˆ†æ |
|------|----------|
| **æ¶æ„å½±å“** | - åœ¨æ³¨æ„åŠ›åç«¯æšä¸¾ `AttentionBackendEnum` ä¸­æ–°å¢ `SAGE_SLA_ATTN`ï¼Œå¹¶åœ¨å¹³å°é€‰æ‹©å‡½æ•°ä¸­æ˜ å°„åˆ° `SageSparseLinearAttentionBackend`ã€‚<br>- æ–°å¢ `SageSparseLinearAttentionBackend`ã€`SageSparseLinearAttentionMetadata` ä¸å¯¹åº” Builderï¼Œå®ç°äº†ç»Ÿä¸€çš„åç«¯æŠ½è±¡ï¼Œä¿æŒäº†ç°æœ‰ `SparseLinearAttentionBackend` çš„æ¥å£å…¼å®¹æ€§ã€‚<br>- é€šè¿‡ `TurboLayer` çš„å…¼å®¹æ€§æ£€æŸ¥ï¼Œä½¿å¾—åœ¨æœªæ˜¾å¼æŒ‡å®šæ—¶ä»å¯è‡ªåŠ¨å›é€€è‡³ SLA æˆ– SageSLAï¼Œä¿æŒå‘åå…¼å®¹ã€‚ |
| **æ€§èƒ½å½±å“** | - ä½¿ç”¨ SpargeAttn æä¾›çš„ **INT8 é‡åŒ– + Blockâ€‘Sparse** è®¡ç®—ï¼Œç†è®ºä¸Šåœ¨æ”¯æŒçš„ GPUï¼ˆsm90ã€sm80/86/87ï¼‰ä¸Šå¯æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶æå‡ååï¼ˆå°¤å…¶åœ¨é•¿åºåˆ—æˆ–å¤§æ¨¡å‹çš„ Vision/Video åœºæ™¯ï¼‰ã€‚<br>- å¯¹æ¯”åŸ SLAï¼šæ–°å¢äº† **çº¿æ€§æ³¨æ„åŠ›ï¼ˆFeatureâ€‘mapï¼‰** çš„æ®‹å·®è·¯å¾„ `proj_l`ï¼Œåœ¨ CUDA è®¡ç®—å®Œç¨€ç–éƒ¨åˆ†åå†åšä¸€æ¬¡è½»é‡åŒ–çº¿æ€§æ³¨æ„åŠ›ï¼Œæ•´ä½“è€—æ—¶çº¦ä¸º SLA çš„ **80%â€‘90%**ï¼ˆè§†ç¡¬ä»¶ä¸ topk_ratio è€Œå®šï¼‰ã€‚<br>- é‡‡ç”¨ `torch.amp.autocast`ã€BFloat16 é»˜è®¤ï¼Œè¿›ä¸€æ­¥åˆ©ç”¨ Tensor Coreã€‚ |
| **å®‰å…¨è€ƒè™‘** | - æ–°å¢çš„ç¬¬ä¸‰æ–¹ä¾èµ– `spargeattn` é€šè¿‡ Git+HTTPS å®‰è£…ï¼Œæœªç»è¿‡ PyPI å®¡è®¡ï¼Œå¯èƒ½å¼•å…¥ä¾›åº”é“¾é£é™©ã€‚<br>- æ‰€æœ‰æ–°å¢çš„ CUDA kernelï¼ˆtriton ä¸ fusedï¼‰åœ¨ `torch.cuda.get_device_capability` åˆ¤å®šåæ‰ä¼šè¢«è°ƒç”¨ï¼Œé¿å…åœ¨ä¸æ”¯æŒçš„è®¾å¤‡ä¸Šå´©æºƒã€‚<br>- ä»£ç ä¸­å¯¹ `ImportError` åšäº†é™çº§å¤„ç†ï¼ˆ`SAGESLA_ENABLED=False`ï¼‰ï¼Œä¿è¯åœ¨ç¼ºå¤±åº“æ—¶ä»å¯å›é€€è‡³åŸ SLAã€‚ |
| **å¯ç»´æŠ¤æ€§** | - ä»£ç éµå¾ªç°æœ‰ `AttentionBackend` æŠ½è±¡ï¼Œæ–°å¢ç±»ä¸æ„å»ºå™¨ä¿æŒäº†ç»Ÿä¸€çš„æ¥å£ï¼Œåç»­æ‰©å±•ï¼ˆå¦‚åŠ å…¥å…¶å®ƒé‡åŒ–ç­–ç•¥ï¼‰æˆæœ¬ä½ã€‚<br>- æ·»åŠ äº†å¤§é‡æ³¨é‡Šã€ç‰ˆæƒä¿¡æ¯ä»¥åŠå¼•ç”¨è®ºæ–‡ï¼Œä¾¿äºç¤¾åŒºç†è§£ã€‚<br>- æ–‡æ¡£åŒæ­¥æ›´æ–°ï¼Œç”¨æˆ·å¯ç›´æ¥åœ¨æ”¯æŒçŸ©é˜µé‡Œçœ‹åˆ° SageSLA çš„å…¼å®¹æƒ…å†µã€‚ |
| **å…¼å®¹æ€§** | - è€ç‰ˆæ¨¡å‹é»˜è®¤ä»ä½¿ç”¨ `SparseLinearAttentionBackend`ï¼ˆSLAï¼‰ï¼Œä¸å—å½±å“ã€‚<br>- æ–°æ¨¡å‹å¯é€šè¿‡ `--attention-backend sage_sla_attn`ï¼ˆæˆ– `attention_type=sagesla`ï¼‰æ˜¾å¼å¼€å¯ã€‚<br>- å½“ç”¨æˆ·åœ¨ä¸æ”¯æŒçš„ GPU ä¸Šè¿è¡Œæ—¶ï¼Œå°†è‡ªåŠ¨å›é€€åˆ°åŸ SLAï¼Œé¿å…å› ç¡¬ä»¶ä¸åŒ¹é…å¯¼è‡´çš„è¿è¡Œæ—¶é”™è¯¯ã€‚ |

**âš ï¸ æ½œåœ¨é£é™©**  
1. **ä¾èµ–å®‰è£…é£é™©**ï¼š`spargeattn` é‡‡ç”¨ `--no-build-isolation`ï¼Œåœ¨æŸäº›ç¯å¢ƒï¼ˆå¦‚ç¼ºä¹åŒ¹é…çš„ CUDAâ€¯toolkitã€GCC ç‰ˆæœ¬ï¼‰ä¸‹å¯èƒ½ç¼–è¯‘å¤±è´¥ï¼Œå¯¼è‡´éƒ¨ç½²è„šæœ¬ä¸­æ–­ã€‚  
2. **CUDA æ¶æ„åˆ†æ”¯**ï¼šä»£ç å¯¹ `sm90` ä¸ `sm80/86/87` åšäº†ä¸åŒçš„ kernel è°ƒç”¨è·¯å¾„ï¼Œè‹¥æ–° GPUï¼ˆå¦‚ Hopper ä¹‹åçš„æ¶æ„ï¼‰æœªåœ¨åˆ—è¡¨é‡Œï¼Œå¯èƒ½é»˜è®¤èµ°ä¸æœ€ä¼˜è·¯å¾„æˆ–å‡ºç°ä¸å¯é¢„æ–™çš„é”™è¯¯ã€‚  
3. **é‡åŒ–ç²¾åº¦**ï¼šINT8 é‡åŒ–åŠ ä¸Šè‡ªå®šä¹‰ clipping é˜ˆå€¼ (`pvthreshold=1e6`) åœ¨æç«¯æ•°å€¼åˆ†å¸ƒçš„è§†è§‰å¸§ç‰¹å¾ä¸Šå¯èƒ½å‡ºç°è½»å¾®ç²¾åº¦ä¸‹é™ï¼Œéœ€è¦åœ¨å®é™…ç”Ÿæˆä»»åŠ¡ä¸­éªŒè¯è´¨é‡ã€‚  
4. **å†…å­˜å ç”¨**ï¼š`block_map_lut_triton` ä¼šåœ¨ GPU ä¸Šç”Ÿæˆç¨€ç–æ˜ å°„ LUTï¼Œå¦‚æœåºåˆ—é•¿åº¦éå¸¸å¤§ï¼ˆ> 16kï¼‰æˆ– batch è¾ƒå¤§ï¼Œä»å¯èƒ½å‡ºç°æ˜¾å­˜å³°å€¼è¶…é™ã€‚  
5. **æ··åˆåç«¯å†²çª**ï¼šè‹¥åœ¨åŒä¸€è¿›ç¨‹ä¸­æ··ç”¨ `SparseLinearAttentionBackend` ä¸ `SageSparseLinearAttentionBackend`ï¼ˆä¾‹å¦‚åœ¨ä¸åŒæ¨¡å‹é—´åˆ‡æ¢ï¼‰ï¼Œéœ€è¦ç¡®ä¿ `AttentionBackendEnum` ä¸å…¨å±€çŠ¶æ€çš„åŒæ­¥ï¼Œå¦åˆ™å¯èƒ½å‡ºç°åç«¯å®ä¾‹è¢«é”™è¯¯å¤ç”¨çš„é—®é¢˜ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **CI / è‡ªåŠ¨åŒ–æµ‹è¯•**ï¼šåœ¨ CI ä¸­åŠ å…¥å¯¹ `spargeattn` ç¼–è¯‘çš„æ£€æŸ¥ï¼ˆè‡³å°‘ä¸€æ¬¡æˆåŠŸçš„ `pip install`ï¼‰ï¼Œå¹¶åœ¨å¤šç§ CUDAâ€¯ç‰ˆæœ¬ï¼ˆ11.8ã€12.1ï¼‰ä¸Šè·‘å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿å…¼å®¹æ€§ã€‚  
- **æ–‡æ¡£ä¸æç¤º**ï¼šåœ¨ README ä¸­æ˜ç¡®è¯´æ˜ â€œSageSLA éœ€è¦ GPUâ€¯sm80+ å¹¶ä¸”éœ€è¦è‡ªè¡Œå®‰è£… SpargeAttnâ€ã€‚åœ¨ `--attention-backend` å‚æ•°å¸®åŠ©é‡ŒåŠ å…¥ fallback æç¤ºã€‚  
- **ç›‘æ§é‡åŒ–è¯¯å·®**ï¼šæä¾›ä¸€ä¸ªå¯é€‰çš„ `--sage-sla-fp32-fallback` å¼€å…³ï¼Œå…è®¸åœ¨è´¨é‡æ•æ„Ÿçš„ä»»åŠ¡ä¸­ä¸´æ—¶å…³é—­é‡åŒ–è·¯å¾„ï¼Œä»¥ä¾¿å¿«é€Ÿå¯¹æ¯”ã€‚  
- **æ˜¾å­˜/æ€§èƒ½åŸºå‡†**ï¼šå»ºè®®åœ¨ä¸»æµå¡ï¼ˆA100ã€RTXâ€‘4090ã€H100ï¼‰ä¸Šæä¾›åŸºå‡†è¡¨ï¼Œæ ‡æ˜ `topk_ratio` å¯¹æ˜¾å­˜å’Œå»¶è¿Ÿçš„å…·ä½“å½±å“ï¼Œå¸®åŠ©ç”¨æˆ·è°ƒå‚ã€‚  
- **é”™è¯¯å¤„ç†**ï¼šåœ¨ `SageSparseLinearAttentionBackend.forward` ä¸­æ•è·å¯èƒ½çš„ `RuntimeError`ï¼ˆå¦‚ kernel launch å¤±è´¥ï¼‰ï¼Œåœ¨æ•è·åè‡ªåŠ¨å›é€€è‡³ `SparseLinearAttentionBackend`ï¼Œå¹¶è®°å½•è­¦å‘Šæ—¥å¿—ï¼Œæå‡é²æ£’æ€§ã€‚  
- **å®‰å…¨å®¡è®¡**ï¼šå»ºè®®åœ¨å‘å¸ƒå‰å¯¹ `spargeattn` æºç è¿›è¡Œä¸€æ¬¡ä¾›åº”é“¾å®‰å…¨å®¡è®¡ï¼Œæˆ–æä¾›é¢„ç¼–è¯‘çš„ wheel ä¾›ç”¨æˆ·ç›´æ¥å®‰è£…ã€‚  

---  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æäº¤ä¸º SGLang å¼•å…¥äº†ä¸€ä¸ª **é«˜æ•ˆä¸”å¯é‡åŒ–çš„ç¨€ç–â€‘çº¿æ€§æ³¨æ„åŠ›å®ç°**ï¼Œåœ¨ä¿æŒå‘åå…¼å®¹çš„åŒæ—¶ä¸º Video/Multimodal ç”Ÿæˆä»»åŠ¡æä¾›äº†æ˜¾è‘—çš„æ˜¾å­˜ä¸ç®—åŠ›ä¼˜åŠ¿ã€‚ä½†éœ€è¦åœ¨ä¾èµ–ç®¡ç†ã€è·¨æ¶æ„å…¼å®¹ä»¥åŠé‡åŒ–ç²¾åº¦æ–¹é¢åšå¥½è¿›ä¸€æ­¥çš„éªŒè¯ä¸é˜²æŠ¤ï¼Œä»¥ç¡®ä¿æ–°åç«¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç¨³å®šå¯ç”¨ã€‚

---

### [diffusion] refactor: refactor and simplify teacache for cachabledit and wanvideo (#16396)
**SHA**: `0a7a201` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/0a7a2017a02b88f6678804d6d024b100e809b02d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸ”´é«˜  

**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
æœ¬æ¬¡æäº¤ä¸º SGLang å¼•å…¥å¹¶ç»Ÿä¸€äº†ä¸¤å¥— Diffusion åŠ é€Ÿæ–¹æ¡ˆçš„å®ç°ï¼šâ‘  å°†åŸæœ‰çš„ Cacheâ€‘DiT é›†æˆè¿ç§»è‡³ç»Ÿä¸€çš„ `runtime.cache` åŒ…å¹¶å®ç°æ‡’åŠ è½½ï¼›â‘¡ æ–°å¢ TeaCacheï¼ˆåŸºäºæ—¶é—´æ­¥ L1 ç›¸ä¼¼åº¦çš„ç¼“å­˜ï¼‰å®ç°åŠæ–‡æ¡£ï¼Œå¹¶è®©æ‰€æœ‰æ”¯æŒçš„ DiT æ¨¡å‹é€šè¿‡ `TeaCacheMixin` è‡ªåŠ¨è·å¾—è¯¥åŠŸèƒ½ã€‚ä»£ç ç»“æ„å¾—åˆ°ç®€åŒ–ï¼Œæ–‡æ¡£ç»Ÿä¸€ï¼Œæµ‹è¯•ç”¨ä¾‹åŠ å…¥äº† TeaCache å¼€å…³éªŒè¯ã€‚

**ğŸ¯ å½±å“èŒƒå›´**ï¼š  
- `sglang/multimodal_gen/runtime/cache/*`ï¼ˆæ–°æ¨¡å—ã€`__init__`ã€`teacache.py`ã€`cache_dit_integration.py`ï¼‰  
- `sglang/multimodal_gen/runtime/models/dits/base.py`ï¼ˆ`CachableDiT` ç»§æ‰¿ `TeaCacheMixin`ï¼‰  
- `sglang/multimodal_gen/runtime/models/dits/wanvideo.py`ï¼ˆç¼“å­˜é€»è¾‘æ”¹å†™ï¼‰  
- `sglang/multimodal_gen/pipelines_core/stages/denoising.py`ï¼ˆå¯¼å…¥é¡ºåºè°ƒæ•´ï¼‰  
- æ–‡æ¡£ç›®å½• `docs/cache/*`ï¼ˆæ–°å¢ã€é‡å‘½åã€è¯´æ˜ï¼‰  
- æµ‹è¯•ä¸åŸºå‡† `test/server/*`ï¼ˆåŠ å…¥ `enable_teacache` å‚æ•°ã€extra_body ä¼ é€’ï¼‰  

---

### ğŸ” æŠ€æœ¯æ´å¯Ÿ

| ç»´åº¦ | å½±å“ |
|------|------|
| **æ¶æ„å½±å“** | - æ–°å¢ `runtime.cache` å­åŒ…ï¼Œå®ç°ç¼“å­˜ç›¸å…³çš„ç»Ÿä¸€å…¥å£ã€‚<br>- é€šè¿‡ `__getattr__` å®ç° **æ‡’åŠ è½½** çš„ Cache

---

#### ğŸŸ¡ ä¸­é‡è¦åº¦å˜æ›´ (13)

### [NPU] Remove paged attention & Change fia to default attention (#17394)
**SHA**: `a95c9f5` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/a95c9f5b815cc4fed373e1f3483868652a67a778)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ NPU æ³¨æ„åŠ›å®ç° `ascend_backend.py` ä¸­å½»åº•ç§»é™¤ **paged attention** è·¯å¾„ï¼Œç»Ÿä¸€æ”¹ç”¨ `torch_npu.npu_fused_infer_attention_score`ï¼ˆå³ FIAï¼‰è¿›è¡Œå‰å‘è§£ç ã€‚  
2. å¯¹ `eagle_draft_npu_graph_runner.py`ã€`npu_graph_runner.py` çš„å±æ€§è·å–é€»è¾‘è¿›è¡Œç®€åŒ–ï¼Œå»æ‰å¯¹ `get_attention_tp_size()` çš„æ¡ä»¶åˆ¤æ–­ï¼Œé»˜è®¤å§‹ç»ˆä½¿ç”¨ **MLAï¼ˆFIAï¼‰** çš„å±æ€§å/ç±»å‹ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `python/sglang/srt/hardware_backend/npu/attention/ascend_backend.py`ï¼ˆæ³¨æ„åŠ›å‰å‘è®¡ç®—ï¼‰  
- `python/sglang/srt/hardware_backend/npu/graph_runner/*`ï¼ˆNPU å›¾è¿è¡Œå™¨ï¼‰  
- ç›¸å…³çš„ `dp_attention` å…¬å…±å·¥å…·è¢«åˆ é™¤å¼•ç”¨ï¼Œå¯èƒ½æ³¢åŠåˆ†å¸ƒå¼å¹¶è¡Œçš„æ³¨æ„åŠ›åˆ‡åˆ†é€»è¾‘ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼špaged attention åœ¨ CANN ä½ç‰ˆæœ¬æˆ–æŸäº›ç¡¬ä»¶é…ç½®ä¸‹ä»æ˜¯å”¯ä¸€å¯ç”¨çš„å®ç°ï¼Œç§»é™¤åå¯èƒ½å¯¼è‡´åœ¨è¿™äº›ç¯å¢ƒä¸‹è¿è¡Œé”™è¯¯ã€‚å»ºè®®åœ¨ `AscendBackend` ä¸­åŠ å…¥ç‰ˆæœ¬æ£€æµ‹æˆ–å›é€€æœºåˆ¶ï¼Œå¹¶åœ¨æ–‡æ¡£ä¸­æ ‡æ˜æœ€ä½ CANN/é©±åŠ¨è¦æ±‚ã€‚  
2. **è¾¹ç•Œæ¡ä»¶æµ‹è¯•**ï¼šå°¤å…¶æ˜¯ batchâ€‘size å°äº TPâ€‘sizeï¼ˆæˆ– `get_attention_tp_size()`ï¼‰çš„æƒ…å†µã€ä»¥åŠ `forward_metadata.seq_lens_cpu_int` ä¸º `None` æ—¶çš„è·¯å¾„ï¼Œç¡®ä¿ `actual_seq_len_kv` çš„å¤„ç†ä¸å†å‡ºç°ç±»å‹ä¸åŒ¹é…ã€‚  
3. **å¼‚å¸¸æ•è·**ï¼š`torch_npu.npu_fused_infer_attention_score` è‹¥å› å†…å­˜æˆ–å¸ƒå±€é—®é¢˜æŠ›å¼‚å¸¸ï¼Œéœ€è¦åœ¨ `forward_decode_graph` ä¸­æ•è·å¹¶æä¾›å‹å¥½çš„é”™è¯¯æç¤ºï¼Œä»¥é˜²å›¾è¿è¡Œæ—¶ç›´æ¥å´©æºƒã€‚  
4. **æ–‡æ¡£åŒæ­¥**ï¼šæ›´æ–° README/ç¡¬ä»¶è¦æ±‚ç« èŠ‚ï¼Œæ˜ç¡®â€œé»˜è®¤ä½¿ç”¨ FIAï¼ˆFused Infer Attentionï¼‰ï¼Œä¸å†æ”¯æŒåˆ†é¡µæ³¨æ„åŠ›â€ã€‚  
5. **å•å…ƒ/é›†æˆæµ‹è¯•**ï¼šåŠ å…¥å¯¹ `use_mla=False` åˆ†æ”¯çš„å›å½’æµ‹è¯•ï¼Œä»¥åŠå¯¹ `EAGLEDraftCudaGraphRunner` åœ¨ NPU ç¯å¢ƒä¸‹çš„å›¾æ•è·/é‡æ”¾æµç¨‹çš„è¦†ç›–ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨ç®€åŒ–äº† NPU æ³¨æ„åŠ›å®ç°é€»è¾‘ï¼Œæé«˜äº†è¿è¡Œæ—¶ç»Ÿä¸€æ€§ï¼Œä½†åœ¨ä½ç‰ˆæœ¬ CANN æˆ–ç‰¹æ®Šå¹¶è¡Œé…ç½®ä¸‹å¯èƒ½å¼•å…¥å…¼å®¹æ€§é£é™©ï¼Œå»ºè®®åœ¨å‘å¸ƒå‰å®Œæˆä¸Šè¿°éªŒè¯ã€‚

---

### [Diffusion] Refactor diffusion is_cuda check (#17498)
**SHA**: `19089aa` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/19089aa431f8bd652366732d75908e270b20c8d1)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆDiffusion å­æ¨¡å—çš„ CUDA åˆ¤å®šï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šåœ¨å¤šä¸ª Diffusion ç›¸å…³çš„è¿è¡Œæ—¶æ–‡ä»¶ä¸­ï¼Œå°†åŸæœ¬å¯¹å¼ é‡ `tensor.is_cuda` çš„æ•£ç‚¹åˆ¤æ–­ç»Ÿä¸€æ”¹ä¸ºä¸€æ¬¡æ€§è¯»å– `current_platform.is_cuda()` å¹¶ç¼“å­˜ä¸º `_is_cuda`ï¼Œéšåä½¿ç”¨è¯¥å¸ƒå°”å€¼å†³å®šèµ° CUDA ä¸“ç”¨è·¯å¾„ã€‚ç›¸åº”åœ°å¢åŠ äº†å¯¹ `current_platform` çš„å¯¼å…¥ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `python/sglang/multimodal_gen/runtime/layers/custom_op.py`  
- `python/sglang/multimodal_gen/runtime/layers/layernorm.py`  
- `python/sglang/multimodal_gen/runtime/models/dits/flux*.py`  
- `python/sglang/multimodal_gen/runtime/models/dits/qwen_image.py`  
- `python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py`  
- `python/sglang/multimodal_gen/runtime/models/dits/zimage.py`  

ä¸Šè¿°æ–‡ä»¶å‡æ¶‰åŠ Qâ€‘K å½’ä¸€åŒ–ã€rotary embeddingã€torchâ€‘triton ä¼˜åŒ–ç­‰æ€§èƒ½å…³é”®è·¯å¾„ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **ç¼“å­˜çš„æ—¶æ•ˆæ€§**  
   - `current_platform.is_cuda()` åªåœ¨æ¨¡å—åŠ è½½æ—¶æ±‚å€¼ã€‚è‹¥è¿è¡Œæ—¶å‡ºç° **CPUâ€‘only** æ¨ç†ï¼ˆä¾‹å¦‚åœ¨åŒä¸€è¿›ç¨‹ä¸­åˆ‡æ¢ `torch.cuda.set_device` æˆ–ä½¿ç”¨ `torch.cuda.manual_seed` åå†åˆ‡æ¢åˆ° CPUï¼‰ï¼Œ`_is_cuda` å°†ä¿æŒæ—§å€¼ï¼Œå¯¼è‡´é”™è¯¯çš„åˆ†æ”¯é€‰æ‹©ã€‚å»ºè®®åœ¨å…³é”®è·¯å¾„ä»ä¿ç•™ `tensor.is_cuda` çš„äºŒæ¬¡æ£€æŸ¥ï¼Œæˆ–åœ¨ `current_platform` æ”¯æŒçš„ â€œdeviceâ€‘changeâ€ äº‹ä»¶æ—¶é‡æ–°åˆ·æ–° `_is_cuda`ã€‚  

2. **å¤šå¡ / å¤šè¿›ç¨‹åœºæ™¯**  
   - åœ¨ DistributedDataParallel æˆ–å¤šå¡æ¨ç†æ—¶ï¼Œä¸åŒ rank å¯èƒ½ä½¿ç”¨ä¸åŒè®¾å¤‡ã€‚æ¨¡å—çº§çš„å…¨å±€ `_is_cuda` å¯èƒ½ä¸åŒ¹é…å±€éƒ¨å¼ é‡çš„å®é™…è®¾å¤‡ã€‚è€ƒè™‘æ”¹ä¸º **å±€éƒ¨**ï¼ˆ`tensor.is_cuda`ï¼‰åˆ¤æ–­ï¼Œæˆ–åœ¨æ¯ä¸ª rank çš„å…¥å£å¤„é‡æ–°è®¾ç½®æ¨¡å—å˜é‡ã€‚  

3. **ä»£ç å¯è¯»æ€§ä¸ä¸€è‡´æ€§**  
   - éƒ¨åˆ†æ–‡ä»¶ä»ä¿ç•™ `tensor.is_cuda`ï¼ˆå¦‚ `custom_op.dispatch_forward`ï¼‰æœªæ”¹ä¸º `_is_cuda`ï¼Œå¯¼è‡´åˆ¤æ–­é€»è¾‘åˆ†æ•£ã€‚å»ºè®®ç»Ÿä¸€é£æ ¼ï¼šè‹¥å†³å®šä½¿ç”¨ç¼“å­˜ï¼Œåˆ™æ‰€æœ‰ç›¸å…³åˆ¤æ–­éƒ½æ”¹ä¸º `_is_cuda`ï¼›è‹¥ä¿ç•™å¼ é‡æ£€æŸ¥ï¼Œåˆ™ä»…åœ¨ä¸ä¾èµ–å…·ä½“å¼ é‡å±æ€§çš„è·¯å¾„ä½¿ç”¨å¹³å°æ ‡å¿—ã€‚  

4. **å•å…ƒæµ‹è¯•ä¸ CI**  
   - å¢åŠ  **CUDA ä¸ CPU åŒæ¨¡å¼** çš„å›å½’æµ‹è¯•ï¼ŒéªŒè¯åœ¨ CPU ç¯å¢ƒä¸‹ä¸ä¼šè¯¯èµ° CUDA åˆ†æ”¯ï¼›åœ¨ CUDA ç¯å¢ƒä¸‹åˆ‡æ¢ `torch.set_default_tensor_type` ä»èƒ½è§¦å‘æ­£ç¡®è·¯å¾„ã€‚  
   - åœ¨ CI ä¸­å¯åŠ¨ä¸¤ä¸ªå¹¶è¡Œä½œä¸šï¼šä¸€ä¸ªä»…è£…è½½ CPU ç‰ˆ PyTorchï¼ˆæˆ–ä½¿ç”¨ `torch.cuda.is_available=False`ï¼‰ï¼Œå¦ä¸€ä¸ªä½¿ç”¨çœŸå® CUDAã€‚ç¡®ä¿ä¸¤è€…å‡é€šè¿‡ã€‚  

5. **æ–‡æ¡£è¯´æ˜**  
   - åœ¨ `runtime/platforms.py` ä¸­æ³¨æ˜ `is_cuda` ä»£è¡¨ â€œå½“å‰å¹³å°æ˜¯å¦æ”¯æŒ CUDAâ€ï¼Œä¸ä»£è¡¨ **å¼ é‡å®é™…æ‰€åœ¨è®¾å¤‡**ï¼Œå¹¶åœ¨è¯¥ PR çš„è¯´æ˜ä¸­æé†’ä½¿ç”¨è€…åœ¨å¤šè®¾å¤‡æƒ…å¢ƒä¸‹è‡ªè¡Œåˆ¤æ–­ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡é‡æ„å¯é™ä½æ¯æ¬¡åˆ†æ”¯æ£€æŸ¥çš„å¼€é”€ï¼Œæå‡ Diffusion æ¨ç†çš„å¾®ç§’çº§æ€§èƒ½ã€‚ä½†éœ€æ³¨æ„ä¸Šè¿°ç¼“å­˜å¤±æ•ˆé£é™©ï¼Œç¡®ä¿åœ¨å¤šè®¾å¤‡æˆ–åŠ¨æ€åˆ‡æ¢åœºæ™¯ä¸‹ä»ä¿æŒæ­£ç¡®è¡Œä¸ºã€‚è‹¥ä¸Šè¿°é£é™©å¾—åˆ°è¡¥ä¸æˆ–æµ‹è¯•è¦†ç›–ï¼Œæ”¹åŠ¨ä»·å€¼æ˜¾è‘—ã€‚ç¥è°ƒè¯•é¡ºåˆ©ï¼

---

### [Fix] GLM 4.7 + NVFP4 + MTP (#17166)
**SHA**: `2ff0880` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/2ff0880a0ed1b81f0dc34e45fbccaa244cf80cf8)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º/å…¼å®¹æ€§ä¿®å¤  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼š  
1. æ”¹è¿› `ModelConfig._verify_quantization`ï¼Œåœ¨ CLIâ€‘æŒ‡å®šé‡åŒ–ä¸ HF é…ç½®çš„ `quant_method` å…¼å®¹æ—¶ä¸å†æŠ¥é”™ï¼Œå¹¶åœ¨è‰ç¨¿æ¨¡å‹é‡åŒ–ä¸åŒ¹é…æ—¶è‡ªåŠ¨ä½¿ç”¨æ£€æµ‹åˆ°çš„é‡åŒ–ã€‚  
2. åœ¨åŠ è½½ safetensors æƒé‡æ—¶æ–°å¢ `maybe_add_mtp_safetensors`ï¼Œè‡ªåŠ¨æŠŠç¼ºå¤±äº `model.safetensors.index.json` çš„ `mtp.safetensors` åŠ å…¥ï¼Œè§£å†³ GLMâ€‘4.7â€‘fp4 ç­‰ checkpoint æ‰“åŒ…é”™è¯¯ã€‚  
3. ä¸º GLM4â€‘Moe æ·»åŠ é»˜è®¤ `routing_method_type=DeepSeekV3`ï¼Œå¹¶åœ¨ SM100 GPU ä¸Šæ ¹æ® `flashinfer-python>=0.6.2` è‡ªåŠ¨å¯ç”¨ `flashinfer_trtllm` MoE åç«¯ã€‚  
4. æ–°å¢ `check_pkg_version_at_least` ç”¨äºå®‰å…¨çš„ç‰ˆæœ¬åˆ¤æ–­ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**ï¼š`model_config.py`ã€`loader.py`ã€`weight_utils.py`ã€`glm4_moe.py`ã€`server_args.py`ã€`common.py`ï¼ˆæ¨¡å‹é…ç½®ã€æƒé‡åŠ è½½ã€MoE è·¯ç”±ã€æœåŠ¡å™¨å¯åŠ¨ï¼‰ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**ï¼š  
- **å…¼å®¹æ€§æµ‹è¯•**ï¼šåœ¨ä¸åŒé‡åŒ–æ–¹å¼ï¼ˆmodeloptã€modelopt_fp4ã€NVFP4 ç­‰ï¼‰ä»¥åŠæœ‰/æ—  `mtp.safetensors` çš„ checkpoint ä¸Šè·‘å®Œæ•´æ¨ç†é“¾ï¼Œç¡®ä¿è‡ªåŠ¨å…¼å®¹å’Œè‡ªåŠ¨è¡¥æ–‡ä»¶ä¸ä¼šå¼•å…¥æœªé¢„æ–™çš„æƒé‡å†²çªã€‚  
- **æ—¥å¿—ä¸é”™è¯¯**ï¼š`maybe_add_mtp_safetensors` ä»…æ‰“å° warningï¼Œå»ºè®®åœ¨å…³é”®è·¯å¾„å¢åŠ ä¸€æ¬¡å®Œæ•´çš„æ–‡ä»¶åˆ—è¡¨æ—¥å¿—ï¼Œæ–¹ä¾¿ç”¨æˆ·å®šä½åŒ…è£…é”™è¯¯ã€‚  
- **æ€§èƒ½éªŒè¯**ï¼šå¼€å¯ `flashinfer_trtllm` ååŠ¡å¿…å¯¹æ¯” MoE ååä¸å»¶è¿Ÿï¼Œé˜²æ­¢åœ¨ä¸æ»¡è¶³ç‰ˆæœ¬æˆ–ç¡¬ä»¶æ¡ä»¶æ—¶è¯¯ç”¨ã€‚  
- **ç‰ˆæœ¬æ£€æŸ¥å®‰å…¨**ï¼š`check_pkg_version_at_least` ç›´æ¥ä½¿ç”¨ `importlib.metadata.version`ï¼Œè‹¥åŒ…æœªå®‰è£…è¿”å› False å·²è¶³å¤Ÿï¼Œä¿æŒç°æœ‰å®ç°å³å¯ã€‚  
- **æ–‡æ¡£æ›´æ–°**ï¼šè¯´æ˜â€œGLMâ€‘4.7â€‘fp4 åŒ…è£…ç¼ºé™·â€åŠå¯¹åº”è‡ªåŠ¨ä¿®å¤è¡Œä¸ºï¼Œæé†’ç”¨æˆ·è‹¥è‡ªè¡Œä¿®æ­£ index æ–‡ä»¶å¯å…³é—­è¯¥è­¦å‘Šã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨æå‡äº† GLMâ€‘4 ç³»åˆ—æ¨¡å‹çš„åŠ è½½é²æ£’æ€§å’Œ MoE åç«¯é€‚é…ï¼Œä½†éœ€é€šè¿‡å¤šç§ checkpoint ä¸ç¡¬ä»¶ç»„åˆçš„å›å½’æµ‹è¯•ï¼Œç¡®ä¿æ–°è‡ªåŠ¨åŒ–é€»è¾‘ä¸ä¼šåœ¨æç«¯åœºæ™¯ä¸‹å¯¼è‡´è¯¯åŠ è½½æˆ–æ€§èƒ½å›é€€ã€‚

---

### Use fused_sigmoid_gating_delta_rule_update_kernel for KDA (#17108)
**SHA**: `bcc6d84` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/bcc6d84f93fbfbbb64bf4c86356147acee042750)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `fused_sigmoid_gating_recurrent.py` ä¸­æ–°å¢ `IS_KDA` ç¼–è¯‘å¸¸é‡ï¼Œå®ç°å¯¹ Kâ€‘DAï¼ˆKernelâ€‘Denseâ€‘Attentionï¼‰è·¯å¾„çš„ä¸“å±ç´¢å¼•ä¸å¹¿æ’­ï¼›ç›¸åº”åœ°æŠŠé—¨æ§å¼ é‡ `a`ã€åç½® `dt_bias` æŒ‰ `(K,O)` ç»´åº¦å±•å¼€ï¼Œå¹¶åœ¨æ¿€æ´»ä¹˜æ³•ä¸ŠåŠ å…¥ `[:,None]` ä»¥æ”¯æŒå¤šâ€‘K è®¡ç®—ã€‚  
2. `hybrid_linear_attn_backend.py` å°†åŸå…ˆçš„ `fused_recurrent_kda` æ›¿æ¢ä¸ºç»Ÿä¸€çš„ `fused_sigmoid_gating_delta_rule_update`ï¼Œå¹¶å‘å‰å‘/è§£ç è·¯å¾„ç»Ÿä¸€ä¼ å…¥ `A_log`ã€`dt_bias` ä¸ `is_kda=True`ã€‚  
3. `kimi_linear.py` åœ¨éâ€‘decode åœºæ™¯ä»ä½¿ç”¨æ—§çš„ `fused_kda_gate`ï¼Œè€Œåœ¨ decode æ—¶ä¾èµ–åç«¯å·²èåˆçš„ KDA è®¡ç®—ï¼Œé¿å…é‡å¤è°ƒç”¨ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- Triton æ ¸å¿ƒå®ç°ï¼ˆ`fused_sigmoid_gating_delta_rule_update_kernel`ï¼‰  
- æ··åˆçº¿æ€§æ³¨æ„åŠ›åç«¯ (`HybridLinearAttnBackend`)  
- Kimiâ€‘Linear æ¨¡å‹å‰å‘å…¥å£  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å…¼å®¹æ€§**ï¼š`IS_KDA` ä¸ºç¼–è¯‘æ—¶å¸¸é‡ï¼Œéœ€ç¡®ä¿åœ¨æ‰€æœ‰æ”¯æŒçš„ GPU/é©±åŠ¨ä¸Šé‡æ–°ç¼–è¯‘ Triton kernelï¼Œå¦åˆ™å¯èƒ½å›é€€åˆ°æ—§è·¯å¾„æˆ–æŠ¥é”™ã€‚å»ºè®®åœ¨ CI ä¸­åŠ å…¥é’ˆå¯¹ä¸åŒæ˜¾å¡çš„ç¼–è¯‘æµ‹è¯•ã€‚  
- **æ•°å€¼æ­£ç¡®æ€§**ï¼šå› ä¸ºé—¨æ§ä¹˜æ³•æ”¹ä¸º `b_h *= exp(b_g[:,None])`ï¼Œéœ€ç¡®è®¤åœ¨å¤šâ€‘K åœºæ™¯ä¸‹æ¢¯åº¦ä¼ æ’­ä¸åŸå®ç°ä¿æŒä¸€è‡´ï¼ˆå°¤å…¶æ˜¯ `softplus_beta/threshold` å‚æ•°çš„å½±å“ï¼‰ã€‚å¯æ·»åŠ å•å…ƒæµ‹è¯•å¯¹æ¯” `fused_recurrent_kda` ä¸æ–°æ¥å£åœ¨éšæœºæ•°æ®ä¸Šçš„è¾“å‡ºè¯¯å·®ã€‚  
- **æ€§èƒ½éªŒè¯**ï¼šKâ€‘DA è·¯å¾„åœ¨ decode æ—¶ä¼šå¸¦æ¥é¢å¤–çš„å¹¿æ’­å¼€é”€ï¼Œå»ºè®®åœ¨é•¿åºåˆ—è§£ç åŸºå‡†ä¸Šæµ‹é‡æ ¸æ‰§è¡Œæ—¶é—´ã€æ˜¾å­˜å ç”¨ä»¥åŠååç‡ï¼Œç¡®ä¿æå‡æˆ–è‡³å°‘ä¸åŠ£äºåŸå®ç°ã€‚  
- **æ–‡æ¡£/å‚æ•°**ï¼š`forward_decode` ä¸­æ–°å¢ `A_log`ã€`dt_bias` å‚æ•°éœ€åœ¨æ¨¡å‹åºåˆ—åŒ–å’Œ checkpoint åŠ è½½é€»è¾‘ä¸­åŒæ­¥æ›´æ–°ï¼Œå¦åˆ™æ¢å¤æ¨¡å‹æ—¶å¯èƒ½ç¼ºå¤±è¿™äº›å¼ é‡ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨å°† Kâ€‘DA çš„é—¨æ§é€»è¾‘ç»Ÿä¸€è¿› `fused_sigmoid_gating_delta_rule_update`ï¼Œä»£ç è·¯å¾„æ›´ç®€æ´ï¼Œæ½œåœ¨æ€§èƒ½æå‡å¯è§‚ï¼›ä½†éœ€é‡ç‚¹éªŒè¯ç¼–è¯‘å…¼å®¹ã€æ•°å€¼ä¸€è‡´æ€§ä»¥åŠåºåˆ—åŒ–å®Œæ•´æ€§ã€‚

---

### Support EPD error handling (#16670)
**SHA**: `7520b92` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/7520b92927a503d83bae9c91eb6a17caf88a628f)

**å˜æ›´æ¦‚è§ˆ**  
æœ¬æ¬¡æäº¤ä¸º **EPDï¼ˆEncoderâ€‘Processingâ€‘Dispatchï¼‰é”™è¯¯å¤„ç†** æ·»åŠ äº†å…¨é“¾è·¯å¼‚å¸¸æ•è·ä¸ä¼ æ’­æœºåˆ¶ã€‚æ ¸å¿ƒæ”¹åŠ¨åŒ…æ‹¬ï¼š

1. **encode_receiver.py**  
   * å¼•å…¥ `WaitingImageRequestStatus` æšä¸¾ï¼Œä½¿ç”¨ `PENDING / SUCCESS / FAIL` è¡¨ç¤ºå›¾åƒè¯·æ±‚çŠ¶æ€ã€‚  
   * `EmbeddingData` æ‰©å±•å­—æ®µ `error_msg / error_code`ï¼Œåœ¨æ¥æ”¶åˆ°é”™è¯¯ä¿¡å·æ—¶ä¿å­˜å¹¶ç»ˆæ­¢ socketã€‚  
   * `WaitingImageRequest._try_recv_mm_data` æ£€æµ‹ `EmbeddingData.error_msg`ï¼Œå°†çŠ¶æ€åˆ‡æ¢ä¸º `FAIL`ã€‚  
   * `process_waiting_requests` ç°åœ¨è¿”å› `abort_reqs`ï¼Œåœ¨ `Scheduler.recv_requests` ä¸­è°ƒç”¨ `prepare_abort` å¹¶è¾“å‡ºé”™è¯¯æ—¥å¿—ã€‚  
   * `Scheduler` æ–°å¢ `create_req` è¾…åŠ©å‡½æ•°ï¼Œç”¨äºåœ¨å¼‚å¸¸åœºæ™¯ä¸‹ä»èƒ½ç”Ÿæˆ `Req` å¯¹è±¡ï¼ˆä¸»è¦ç”¨äºç»Ÿä¸€é”™è¯¯è¿”å›ï¼‰ã€‚

2. **encode_server.py**  
   * æ–°å¢ `MMError`ã€`BadRequestError`ã€`InternalError` ä¸‰ç±»å¼‚å¸¸ï¼Œç»Ÿä¸€é”™è¯¯ç ï¼ˆHTTPStatusï¼‰ã€‚  
   * `_encode` åŒ…è£¹æ•´ä¸ªå›¾åƒåŠ è½½ã€ç‰¹å¾æå–è¿‡ç¨‹ï¼Œæ•è·å¼‚å¸¸å¹¶æŠ›å‡ºå¯¹åº”å­ç±»ï¼›åœ¨å¼‚å¸¸è·¯å¾„ä¸­ç”Ÿæˆä»…å«é”™è¯¯ä¿¡æ¯çš„ `EmbeddingData` å¹¶æ”¾å…¥ `embedding_to_send`ã€‚  
   * `encode` æ¥å£è¿”å› `(bytes, len, dim, error_msg, error_code)`ï¼Œä½¿è°ƒç”¨æ–¹èƒ½å¤Ÿæ„ŸçŸ¥é”™è¯¯ã€‚  
   * `/encode` HTTP å…¥å£åœ¨å¼‚å¸¸æƒ…å†µä¸‹è¿”å›ç›¸åº”çš„ HTTP çŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯ï¼›å¯¹ `zmq_to_scheduler` åœºæ™¯åŠ å…¥åå°ä»»åŠ¡ `background_tasks`ï¼Œé˜²æ­¢åç¨‹æ³„æ¼ã€‚

3. **scheduler.py**  
   * `MMReceiver` æ„é€ å‡½æ•°æ–°å¢ `scheduler` å‚æ•°ï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆ `Req` æ—¶è·å–æ¨¡å‹é…ç½®ã€tokenizer ç­‰ä¿¡æ¯ã€‚  
   * `recv_requests` ç°åœ¨åŒæ—¶å¤„ç† `abort_reqs`ï¼Œè°ƒç”¨ `prepare_abort` å¹¶èµ° `stream_output` è¿”å›é”™è¯¯å“åº”ã€‚

4. **å…¶å®ƒ**  
   * å¯¹ `profile` æ¥å£çš„è¿”å›ç ç»Ÿä¸€ä¸º `http.HTTPStatus`ï¼›  
   * åˆ é™¤äº† `scheduler_output_processor_mixin` ä¸­æ— ç”¨çš„ç©ºè¡Œã€‚

**å½±å“èŒƒå›´**  
- å¤šæ¨¡æ€å›¾åƒç¼–ç è·¯å¾„ï¼š`MMReceiver`ã€`EncodeServer`ã€`EncodeReceiver`ã€‚  
- è°ƒåº¦å±‚ï¼š`Scheduler.recv_requests`ã€`Scheduler.create_req`ã€‚  
- ç½‘ç»œå±‚ï¼šHTTP `/encode`ã€`/send`ã€`/start_profile`ã€`/stop_profile`ã€‚  

**å»ºè®®**  
1. **å•å…ƒ/é›†æˆæµ‹è¯•**ï¼š  
   - æ„é€ å¼‚å¸¸è¾“å…¥ï¼ˆå¦‚éæ³•å›¾ç‰‡ URLã€æ–‡ä»¶è¯»å–é”™è¯¯ï¼‰éªŒè¯ `BadRequestError` è¢«æ•è·å¹¶è¿”å› 400ã€‚  
   - æ¨¡æ‹Ÿå†…éƒ¨å¼‚å¸¸ï¼ˆæ¨¡å‹æ¨ç†æŠ›å¼‚å¸¸ï¼‰ç¡®ä¿è¿”å› 500ã€‚  
   - å¯¹ `zmq_to_scheduler` åœºæ™¯ï¼Œæ£€æŸ¥ `abort_reqs` æ˜¯å¦è¢«æ­£ç¡®åŠ å…¥ `Scheduler.pending_requests`ï¼Œä¸”å®¢æˆ·ç«¯èƒ½å¤Ÿæ”¶åˆ°é”™è¯¯å“åº”ã€‚  

2. **èµ„æºæ¸…ç†**ï¼š`background_tasks` ä½¿ç”¨ `add_done_callback` å·²åšåŸºæœ¬å›æ”¶ï¼Œä»éœ€ç¡®è®¤åœ¨å¼‚å¸¸é€€å‡ºæˆ–æœåŠ¡å…³é—­æ—¶æ‰€æœ‰ä»»åŠ¡å·²è¢«å–æ¶ˆï¼Œä»¥é˜²æ³„æ¼ã€‚  

3. **é”™è¯¯ç æ˜ å°„**ï¼šç›®å‰ä»…åŒºåˆ† 400 ä¸ 500ï¼Œåç»­è‹¥å¼•å…¥æ›´ç»†ç²’åº¦é”™è¯¯ï¼ˆå¦‚ 429ã€503ï¼‰ï¼Œè¯·åœ¨ `MMError` å­ç±»ä¸­ç»Ÿä¸€å®šä¹‰å¹¶åœ¨ `Scheduler.recv_requests` ä¸­ç›¸åº”æ˜ å°„ã€‚  

4. **æ—¥å¿—å¯è§‚æµ‹æ€§**ï¼šåœ¨ `EmbeddingData.__repr__` ä¸­åŠ å…¥ `error_code`ï¼Œä¾¿äºæ’æŸ¥ï¼›å»ºè®®åœ¨ `Scheduler.process_waiting_requests` æ•è·å¼‚å¸¸æ—¶è®°å½•å®Œæ•´å †æ ˆã€‚  

5. **æ–‡æ¡£æ›´æ–°**ï¼šè¯´æ˜æ–°é”™è¯¯è¿”å›å­—æ®µ (`error_msg`, `error_code`) çš„è¯­ä¹‰åŠå®¢æˆ·ç«¯å¦‚ä½•å¤„ç†ï¼›æ›´æ–° `Encoder` ä¸ `Scheduler` çš„æ¥å£è¯´æ˜ã€‚  

æ€»ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸ºå¤šæ¨¡æ€ç¼–ç æµç¨‹åŠ å…¥äº†å®Œæ•´çš„é”™è¯¯ä¼ æ’­é“¾è·¯ï¼Œæå‡äº†ç³»ç»Ÿåœ¨å¼‚å¸¸åœºæ™¯ä¸‹çš„å¯ç”¨æ€§å’Œå¯è°ƒè¯•æ€§ã€‚åç»­å…³æ³¨å¼‚å¸¸è·¯å¾„çš„è¦†ç›–ç‡ä¸èµ„æºå›æ”¶å³å¯ã€‚

---

### [diffusion] fix: fix the LoRA weights mismatch caused by weights packing (#17355)
**SHA**: `e7224e9` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/e7224e96816b8a1a2e0e07a6a63d5fa6f30c2d1d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šBug ä¿®å¤ / åŠŸèƒ½å¢å¼º  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. ä¸º DiT æ¨¡å‹çš„ `feed_forward.w1`ã€`feed_forward.w3` å¢åŠ  LoRA å‚æ•° (`lora_A`ã€`lora_B`) çš„æ˜ å°„ï¼Œè§£å†³æƒé‡æ‰“åŒ…å LoRA ä¸æ¨¡å‹å±‚ä¸åŒ¹é…çš„é—®é¢˜ã€‚  
2. åœ¨ `runtime.layers.lora.linear` ä¸­å¯¹ LoRA äº§ç”Ÿçš„ delta åšç»´åº¦æ£€æŸ¥å¹¶å¼ºåˆ¶ reshape ä¸ºäºŒç»´ (â€‘1, out_dim)ï¼Œé˜²æ­¢ 3â€‘D/4â€‘D å¼ é‡å¯¼è‡´è®¡ç®—é”™è¯¯ã€‚  
3. åœ¨ LoRA adapter åŠ è½½æµæ°´çº¿ä¸­ï¼Œåˆå¹¶ LoRA å‚æ•°çš„æ–¹å¼ç”± `torch.cat(..., dim=1)` æ”¹ä¸º `torch.stack(..., dim=0)`ï¼Œå¹¶æ”¾å®½äº†åˆå¹¶åˆ¤å®šæ¡ä»¶ï¼Œä»¥å…¼å®¹ Tensorâ€‘Parallel åœºæ™¯ä¸‹çš„ (N, out_dim, r) å½¢çŠ¶ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `python/sglang/multimodal_gen/configs/models/dits/zimage.py`ï¼ˆæ¨¡å‹é…ç½®ï¼‰  
- `python/sglang/multimodal_gen/runtime/layers/lora/linear.py`ï¼ˆLoRA çº¿æ€§å±‚å®ç°ï¼‰  
- `python/sglang/multimodal_gen/runtime/pipelines_core/lora_pipeline.py`ï¼ˆLoRA é€‚é…å™¨åŠ è½½é€»è¾‘ï¼‰  
- ä»»ä½•ä½¿ç”¨ DiT `feed_forward` ä»¥åŠ LoRA èåˆçš„å¤šæ¨¡æ€ç”Ÿæˆè·¯å¾„ã€‚

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **å…¼å®¹æ€§æ£€æŸ¥**ï¼šæ–°å¢çš„æ­£åˆ™æ˜ å°„å¯èƒ½åŒ¹é…åˆ°æ„æ–™ä¹‹å¤–çš„å±‚åï¼Œå»ºè®®åœ¨ CI ä¸­åŠ å…¥æ¨¡å‹é…ç½®çš„å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿åªæœ‰ç›®æ ‡å±‚è¢«åŒ¹é…ã€‚  
- **reshape è¯­ä¹‰**ï¼š`delta.reshape(-1, delta.shape[-1])` å°†å‰ç½®ç»´åº¦å…¨éƒ¨åˆå¹¶ä¸º batch ç»´ï¼Œç¡®è®¤ä¸‹æ¸¸ä¸ä¼šä¾èµ–åŸå§‹ NÃ—R ç»“æ„ï¼›å¦‚æœ‰éœ€è¦å¯æ”¹ä¸º `delta.view(delta.shape[0], -1, delta.shape[-1])`ã€‚  
- **stack vs cat**ï¼š`torch.stack` ä¼šåœ¨æ–°ç»´åº¦ä¸Šå¤åˆ¶æ•°æ®ï¼Œç¡®ä¿åç»­ä»£ç ï¼ˆå¦‚ TP å‚æ•°åˆ‡åˆ†ï¼‰å·²æ”¹ä¸ºè¯»å– `weight[rank]` è€Œéæ²¿ç¬¬äºŒç»´æ‹¼æ¥ï¼›å¿…è¦æ—¶åœ¨æ³¨é‡Šä¸­æ˜ç¡®è¯¥çº¦å®šã€‚  
- **æ€§èƒ½éªŒè¯**ï¼šåœ¨ TPï¼ˆå¤šå¡ï¼‰å’Œå•å¡ä¸¤ç§ç¯å¢ƒä¸‹è·‘ä¸€æ¬¡å®Œæ•´æ¨ç†åŸºå‡†ï¼Œç¡®è®¤å†…å­˜å ç”¨ä¸ååæœªå‡ºç°å¼‚å¸¸å›é€€ã€‚  
- **å›å½’æµ‹è¯•**ï¼šå¢åŠ  LoRA åŒ…è£…ï¼ˆå« `lora_A/B`ï¼‰çš„æ¨¡å‹åŠ è½½ã€å¾®è°ƒã€æ¨ç†å…¨é“¾è·¯æµ‹è¯•ï¼Œä»¥é˜²æ­¢å› ç»´åº¦ reshape æˆ– stack æ–¹å¼å¼•å…¥çš„ç»†å¾®æ•°å€¼åå·®ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¿®å¤äº† LoRA æƒé‡æ‰“åŒ…åæ— æ³•æ­£ç¡®èåˆçš„å…³é”®ç¼ºé™·ï¼Œä¸”å¯¹ TP å‹å¥½ã€‚ä½†éœ€åœ¨å¤šå¡ç¯å¢ƒåŠä¸åŒæ¨¡å‹é…ç½®ä¸‹è¿›è¡Œå®Œæ•´å›å½’ï¼Œä»¥ç¡®ä¿ reshape ä¸ stack çš„è¡Œä¸ºåœ¨æ‰€æœ‰è·¯å¾„ä¸Šéƒ½ä¿æŒé¢„æœŸã€‚

---

### [RadixTree][2/N Refactor]: swa cache init tiny refactor (#17397)
**SHA**: `236772c` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/236772c0e1e21a4bd7b0cb38f067690c1be66e72)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆCache åˆå§‹åŒ–å‚æ•°ç»Ÿä¸€ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `CacheInitParams` ä¸­æ–°å¢ `sliding_window_size` å­—æ®µï¼Œç»Ÿä¸€ç”±ä¸Šå±‚è°ƒåº¦å™¨ä¼ é€’ã€‚  
2. `Scheduler.init_cache_with_memory_pool` ä¸å†æ˜¾å¼æŠŠ `sliding_window_size` ä½œä¸ºæ„é€ å‡½æ•°å‚æ•°ï¼Œè€Œæ˜¯ç›´æ¥åœ¨ `params` ä¸­æºå¸¦ã€‚  
3. `SWAChunkCache` ä¸ `SWARadixCache` æ„é€ ç­¾åä» `(params, sliding_window_size)` æ”¹ä¸ºä»… `(params)`ï¼Œå†…éƒ¨è‡ªè¡Œè¯»å– `params.sliding_window_size`ã€‚  
4. ç›¸åº”å•å…ƒæµ‹è¯•æ›´æ–°ï¼Œå»æ‰å¤šä½™çš„æ˜¾å¼å‚æ•°ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `python/sglang/srt/managers/scheduler.py`ï¼ˆè°ƒåº¦å™¨ç¼“å­˜åˆå§‹åŒ–ï¼‰  
- `python/sglang/srt/mem_cache/cache_init_params.py`ï¼ˆæ–°å¢å­—æ®µï¼‰  
- `python/sglang/srt/mem_cache/chunk_cache.py`ã€`swa_radix_cache.py`ï¼ˆæ„é€ å‡½æ•°æ”¹åŠ¨ï¼‰  
- `test/registered/radix_cache/test_swa_unittest.py`ï¼ˆæµ‹è¯•é€‚é…ï¼‰  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šç¡®ä¿æ—§ç‰ˆè°ƒç”¨ï¼ˆå¦‚å¤–éƒ¨æ’ä»¶æˆ–è„šæœ¬ï¼‰æœªå†ä¼ é€’ `sliding_window_size` å‚æ•°ï¼Œå¦åˆ™ä¼šå› ç­¾åä¸åŒ¹é…æŠ›å¼‚å¸¸ã€‚å¯ä»¥åœ¨æ–‡æ¡£æˆ– `__init__` ä¸­åŠ å…¥å…¼å®¹è­¦å‘Šã€‚  
2. **é»˜è®¤è¡Œä¸º**ï¼š`sliding_window_size` é»˜è®¤ `None`ï¼Œè°ƒç”¨æ–¹å¿…é¡»åœ¨å¯ç”¨ hybridâ€‘SWA å‰æ­£ç¡®èµ‹å€¼ï¼Œå¦åˆ™ `SWAChunkCache`/`SWARadixCache` åœ¨ `supports_swa` åˆ¤æ–­æ—¶å¯èƒ½å‡ºç°æ„å¤–ã€‚å»ºè®®åœ¨ `Scheduler.init_cache_with_memory_pool` ä¸­åŠ å…¥æ–­è¨€æˆ–æ—¥å¿—æé†’ã€‚  
3. **æµ‹è¯•è¦†ç›–**ï¼šç›®å‰åªæ›´æ–°äº†ä¸¤ä¸ªå•å…ƒæµ‹è¯•ï¼Œå»ºè®®è¡¥å……å¯¹ `CacheInitParams` æœªè®¾ç½® `sliding_window_size` æ—¶çš„è·¯å¾„ï¼ˆå¦‚æ™®é€š ChunkCacheï¼‰è¿›è¡ŒéªŒè¯ï¼Œé˜²æ­¢ None ä¼ å…¥å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ã€‚  
4. **æ–‡æ¡£åŒæ­¥**ï¼šæ›´æ–° READMEã€API è¯´æ˜ä»¥åŠ `CacheInitParams` çš„æ³¨é‡Šï¼Œæ˜ç¡®è¯¥å­—æ®µçš„å«ä¹‰ã€å–å€¼èŒƒå›´åŠä½•æ—¶å¿…é¡»æä¾›ã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡é‡æ„ç®€åŒ–äº†æ„é€ å‡½æ•°ç­¾åï¼Œä½¿ç¼“å­˜å±‚å¯¹æ»‘çª—å¤§å°çš„ä¾èµ–ç»Ÿä¸€åœ¨å‚æ•°å¯¹è±¡ä¸­ï¼Œä»£ç å¯è¯»æ€§æå‡ã€‚ä½†éœ€æ³¨æ„å‘åå…¼å®¹å’Œé»˜è®¤å€¼çš„åˆç†æ€§ï¼Œä»¥å…åœ¨å·²æœ‰éƒ¨ç½²ä¸­å‡ºç°éšè”½çš„åˆå§‹åŒ–é”™è¯¯ã€‚

---

### Fix circular import in quantization modules (#17372)
**SHA**: `0d49b13` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/0d49b13fdd7d96712efee7fbaf246c2e3a965304)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆè§£å†³å¾ªç¯ä¾èµ–ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤å°† `UnquantizedLinearMethod` çš„ç›´æ¥å¯¼å…¥ä»æ¨¡å—é¡¶éƒ¨è¿ç§»åˆ°éœ€è¦æ—¶çš„æ‡’åŠ è½½ä½ç½®ï¼Œåˆ†åˆ«åœ¨ `linear.py`ã€`auto_round.py`ã€`petit.py`ã€`quark.py`ã€`w4afp8.py` ä¸­åšäº†å±€éƒ¨ `import`ï¼Œå¹¶ç›¸åº”åˆ é™¤äº†åŸæ¥çš„é¡¶å±‚å¼•ç”¨ã€‚è¿™æ ·æ‰“ç ´äº† `sglang.srt.layers.linear` ä¸ `sglang.srt.layers.quantization` ä¹‹é—´çš„å¾ªç¯å¯¼å…¥ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  
- `sglang/srt/layers/linear.py`ï¼ˆåˆå§‹åŒ–æ—¶æ ¹æ® `quant_config` å†³å®šé‡åŒ–æ–¹å¼ï¼‰  
- æ‰€æœ‰é‡åŒ–å®ç°æ–‡ä»¶ (`auto_round.py`, `petit.py`, `quark/quark.py`, `w4afp8.py`)  
- å¯èƒ½é—´æ¥å—åˆ° `LinearBase`ã€`QuantizeMethodBase` ç­‰å…¬å…±åŸºç±»çš„å½±å“  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **è¿è¡Œæ—¶å¼€é”€**ï¼šå±€éƒ¨ `import` ä¼šåœ¨æ¯æ¬¡è°ƒç”¨å¯¹åº”å‡½æ•°æ—¶æ‰§è¡Œä¸€æ¬¡å¯¼å…¥ï¼Œè™½ç„¶ Python ä¼šç¼“å­˜æ¨¡å—ä½†ä»ä¼šäº§ç”Ÿä¸€æ¬¡æŸ¥æ‰¾ã€‚å»ºè®®åœ¨å‡½æ•°ä½“å¤–ä½¿ç”¨ `if TYPE_CHECKING:` è¿›è¡Œç±»å‹æç¤ºå¯¼å…¥ï¼Œä¿æŒè¿è¡Œæ—¶ä¸å˜çš„åŒæ—¶æå‡ IDE/ç±»å‹æ£€æŸ¥ä½“éªŒã€‚  
2. **çº¿ç¨‹å®‰å…¨**ï¼šåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹é¦–æ¬¡æ‡’åŠ è½½å¯èƒ½å¹¶å‘è§¦å‘ importï¼Œç†è®ºä¸Šå®‰å…¨ä½†å¯åœ¨æ–‡æ¡£ä¸­æ³¨æ˜å·²éªŒè¯ã€‚è‹¥æœ‰é«˜å¹¶å‘å¯åŠ¨è·¯å¾„ï¼Œè€ƒè™‘åœ¨æ¨¡å—åˆå§‹åŒ–æ—¶æå‰ä¸€æ¬¡æ€§å¯¼å…¥ã€‚  
3. **å•å…ƒæµ‹è¯•**ï¼šç¡®ä¿å·²æœ‰çš„é‡åŒ–è·¯å¾„ï¼ˆæœ‰/æ—  `quant_config`ï¼‰åœ¨ CI ä¸­å‡èƒ½é€šè¿‡ï¼Œç‰¹åˆ«æ˜¯ `LinearBase` çš„ `__init__` åœ¨ `quant_config=None` æƒ…å½¢ä¸‹ä»èƒ½æ­£ç¡®å®ä¾‹åŒ– `UnquantizedLinearMethod`ã€‚  
4. **æ–‡æ¡£ä¸æ³¨é‡Š**ï¼šåœ¨æ¯å¤„æ–°å¢çš„ `import` å‰åŠ å…¥ç®€è¦æ³¨é‡Šï¼Œè¯´æ˜ â€œæ‡’åŠ è½½ä»¥è§£é™¤å¾ªç¯ä¾èµ–â€ã€‚è¿™æœ‰åŠ©äºåç»­ç»´æŠ¤è€…å¿«é€Ÿå®šä½å¹¶é¿å…å†æ¬¡å¼•å…¥å¾ªç¯å¼•ç”¨ã€‚  
5. **å…¼å®¹æ€§æ£€æŸ¥**ï¼šå¦‚æœé¡¹ç›®åœ¨ Python <3.7 æˆ–ä½¿ç”¨ `pylint` ç­‰é™æ€åˆ†æå·¥å…·ï¼Œç¡®ä¿ `UnquantizedLinearMethod` ä»ç„¶åœ¨å…¬å¼€ API ä¸­å¯è¢« `from sglang.srt.layers.quantization.unquant import UnquantizedLinearMethod` æ­£å¸¸è®¿é—®ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡é‡æ„æˆåŠŸè§£å†³äº†å¾ªç¯å¯¼å…¥é—®é¢˜ï¼Œä»£ç å¯ç¼–è¯‘é€šè¿‡ã€‚åç»­åªéœ€å…³æ³¨æ‡’åŠ è½½çš„å¾®å°æ€§èƒ½ä¸å¯ç»´æŠ¤æ€§å³å¯ã€‚

---

### [Minor] Change lora_target_modules to "all"  in CI tests (#17386)
**SHA**: `c3f9c30` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/c3f9c30f99d09c60d49e48a517e305304377e0f3)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé‡æ„ï¼ˆæµ‹è¯•ç”¨ä¾‹è°ƒæ•´ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  
**ğŸ“‹ å˜æ›´æ‘˜è¦**ï¼šå°† CI ä¸­æ‰€æœ‰ LoRA ç›¸å…³æµ‹è¯•çš„ `lora_target_modules` å‚æ•°ç”±æ‰‹åŠ¨åˆ—ä¸¾çš„ä¸ƒä¸ªæ¨¡å—ï¼ˆ`q_projã€k_projã€v_projã€o_projã€gate_projã€up_projã€down_proj`ï¼‰ç»Ÿä¸€æ”¹ä¸º `["all"]`ï¼Œä»¥ç®€åŒ–é…ç½®å¹¶ç»Ÿä¸€æ¥å£ã€‚  
**ğŸ¯ å½±å“èŒƒå›´**ï¼š`test/registered/lora/`ã€`test/registered/rl/` ä¸‹çš„ LoRA åŠ è½½ã€æ›´æ–°ã€é©±é€ç­‰æµ‹è¯•æ–‡ä»¶ã€‚é—´æ¥æ¶‰åŠæ¨¡å‹åˆå§‹åŒ–æ—¶å¯¹ `lora_target_modules` ä¸º `"all"` çš„ä»£ç è·¯å¾„ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. ç¡®è®¤æ¨¡å‹å®ç°å¯¹ `"all"` çš„è§£æèƒ½å¤Ÿæ­£ç¡®è¿‡æ»¤æ‰æœªåˆ†é… LoRA æƒé‡çš„å­æ¨¡å—ï¼Œé˜²æ­¢å‡ºç°æ„å¤–çš„ `KeyError` æˆ–æ— æ•ˆå¼ é‡åŠ è½½ã€‚  
2. åœ¨ CI ä¸­è·‘é€šä¸åŒæ¨¡å‹æ—ï¼ˆLlamaã€Phi ç­‰ï¼‰æ—¶ç•™æ„æ—¥å¿—ï¼Œç¡®ä¿ `all` ä¸ä¼šå¯¼è‡´éšè—çš„æ€§èƒ½å›é€€æˆ–å†…å­˜æ³„æ¼ã€‚  
3. è‹¥é¡¹ç›®æ–‡æ¡£ä¸­å·²æœ‰ `lora_target_modules` çš„è¯´æ˜ï¼Œè¡¥å…… `"all"` ä»£è¡¨â€œæ‰€æœ‰å¯èƒ½çš„æŠ•å½±å±‚â€ï¼Œå¹¶æ³¨æ˜åœ¨ LoRA æƒé‡ç¼ºå¤±æ—¶çš„å®¹é”™è¡Œä¸ºã€‚  
4. è€ƒè™‘åœ¨åç»­çš„å•å…ƒæµ‹è¯•æˆ–åŸºå‡†æµ‹è¯•ä¸­åŠ å…¥é’ˆå¯¹ `"all"` å‚æ•°çš„ä¸“é—¨éªŒè¯ï¼Œä»¥é˜²ä»¥åå¯¹è¯¥ç‰¹æ€§çš„æ”¹åŠ¨å¯¼è‡´å›å½’ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä»…å½±å“æµ‹è¯•å±‚é¢ï¼Œæå‡äº†å¯ç»´æŠ¤æ€§ï¼›ä½†è¯·ç¡®ä¿åº•å±‚å¯¹ `"all"` çš„æ”¯æ’‘åœ¨æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ä¸Šä¿æŒç¨³å¥ã€‚

---

### [NPU] remove features supported on Ascend NPU (#17455)
**SHA**: `aca354b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/aca354bcb3f43ca4a4e727096b065262aa555bc8)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–ï¼ˆæ–‡æ¡£æ¸…ç†ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
æœ¬æ¬¡æäº¤åˆ é™¤äº† `docs/platforms/ascend_npu_support_features.md`ï¼ˆçº¦ 503 è¡Œï¼‰ï¼Œå¹¶åœ¨ `docs/platforms/ascend_npu_support.rst` ä¸­å»æ‰äº†å¯¹è¯¥æ–‡ä»¶çš„å¼•ç”¨ã€‚è¯¥æ–‡ä»¶åŸæœ¬åˆ—ä¸¾äº† Ascend NPU æ‰€æ”¯æŒçš„å‚æ•°çŸ©é˜µï¼ˆæ¨¡å‹ã€é‡åŒ–ã€è°ƒåº¦ã€æ—¥å¿—ç­‰ï¼‰ï¼Œç°åœ¨è®¤ä¸ºè¯¥ä¿¡æ¯å·²è¿‡æ—¶æˆ–å†—ä½™ï¼Œè½¬ç§»è‡³å…¶å®ƒæ–‡æ¡£æˆ–ä¸å†å…¬å¼€ã€‚

**ğŸ¯ å½±å“èŒƒå›´**  

| å—å½±å“æ¨¡å— | å½±å“æè¿° |
|-----------|----------|
| æ–‡æ¡£ç”Ÿæˆè„šæœ¬ / Sphinx é…ç½® | `toctree` ä¸­è‹¥ä»ç„¶å¼•ç”¨ `ascend_npu_support_features.rst` ä¼šå¯¼è‡´æ„å»ºå¤±è´¥ã€‚ |
| å…¬å¼€æ–‡æ¡£ç«™ç‚¹ï¼ˆsglang.ioï¼‰ | åˆ é™¤åè‹¥æœ‰å¤–éƒ¨é“¾æ¥æŒ‡å‘è¯¥é¡µä¼šå‡ºç° 404ã€‚ |
| å¼€å‘è€…/ç”¨æˆ·æ‰‹å†Œ | å¤±å»äº†ä¸€ä»½å…³äº NPU å‚æ•°æ”¯æŒçŠ¶å†µçš„å¿«é€Ÿå¯¹ç…§è¡¨ï¼Œå¯èƒ½éœ€è¦åœ¨å…¶ä»–ç« èŠ‚ï¼ˆå¦‚ `ascend_npu.md`ï¼‰è¡¥å……è¯´æ˜ã€‚ |

**ğŸ’¡ å…³æ³¨å»ºè®®**  

1. **ç¡®ä¿æ„å»ºé€šè¿‡**ï¼šåœ¨æœ¬åœ°æˆ– CI ä¸­é‡æ–°è¿è¡Œ `make html`ï¼Œç¡®è®¤ `ascend_npu_support.rst` çš„ `toctree` å·²åŒæ­¥æ›´æ–°ï¼Œæœªæ®‹ç•™å¯¹å·²åˆ é™¤æ–‡ä»¶çš„å¼•ç”¨ã€‚  
2. **æ£€æŸ¥å¤–éƒ¨é“¾æ¥**ï¼šæœç´¢ä»“åº“ï¼ˆåŒ…æ‹¬ READMEã€issue æ¨¡æ¿ã€å…¶ä»–æ–‡æ¡£ï¼‰æ˜¯å¦æœ‰æŒ‡å‘ `ascend_npu_support_features.md` çš„ URLï¼Œè‹¥æœ‰éœ€æ”¹ä¸ºæ–°ä½ç½®æˆ–åˆ é™¤ã€‚  
3. **è¡¥å…¨ä¿¡æ¯**ï¼šå¦‚æœè¯¥è¡¨æ ¼ä¸­æœ‰ä»ç„¶æœ‰æ•ˆçš„ç‰¹æ€§è¯´æ˜ï¼Œå»ºè®®åœ¨ `ascend_npu.md` æˆ– `ascend_npu_deepseek_example.md` ä¸­ç®€è¦æ¦‚è¿°ï¼Œé¿å…ç”¨æˆ·åœ¨æŸ¥æ‰¾æ”¯æŒçŸ©é˜µæ—¶äº§ç”Ÿå›°æƒ‘ã€‚  
4. **æ›´æ–°å‘å¸ƒè¯´æ˜**ï¼šåœ¨ç‰ˆæœ¬æ—¥å¿—ä¸­æ ‡è®° â€œç§»é™¤å·²è¿‡æ—¶çš„ Ascend NPU æ”¯æŒç‰¹æ€§è¡¨â€ï¼Œå¹¶æä¾›è¿ç§»æŒ‡å¼•ï¼ˆå¦‚â€œè¯·å‚è€ƒ `ascend_npu.md` ä¸­çš„æœ€æ–°æ”¯æŒåˆ—è¡¨â€ã€‚ï¼‰  
5. **å›é€€é£é™©**ï¼šè‹¥åç»­éœ€è¦è¯¥è¡¨æ ¼ï¼Œå»ºè®®ä¿ç•™å†å²åˆ†æ”¯æˆ–åœ¨ `docs/_archive/` ä¸­å­˜æ¡£ï¼Œä»¥ä¾¿å¿«é€Ÿæ¢å¤ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä»…æ¶‰åŠæ–‡æ¡£æ¸…ç†ï¼Œä¸å½±å“ä»£ç è¿è¡Œã€‚ä½†å¿…é¡»ç¡®ä¿æ–‡æ¡£æ„å»ºå’Œå¯¹å¤–é“¾æ¥çš„å®Œæ•´æ€§ï¼Œä»¥å…ç»™ä½¿ç”¨ Ascend NPU çš„ç”¨æˆ·å¸¦æ¥æ–‡æ¡£ç¼ºå¤±çš„è¯¯è§£ã€‚

---

### Add hybrid parallelism test to nightly CI (#17444)
**SHA**: `823a046` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/823a046e8f4242bf2eaaab5e2cb1589f7bfdb564)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼º / CI æ‰©å±•  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. æ–°å¢ GitHub Actions å·¥ä½œæµ **weekly-test-nvidia.yml**ï¼Œåœ¨æ¯å‘¨æ—¥è‡ªåŠ¨åœ¨ 8â€‘GPU H200 å®ä¾‹ä¸Šè¿è¡Œå®Œæ•´çš„ â€œweeklyâ€‘8â€‘gpuâ€‘h200â€ æµ‹è¯•å¥—ä»¶ã€‚  
2. åœ¨ `test/registered/test_hybrid_dp_ep_tp_mtp.py` ä¸­åŠ å…¥ `register_cuda_ci` è°ƒç”¨ï¼Œå°†åŸæœ¬ä»…æœ¬åœ°å¯æ‰‹åŠ¨è§¦å‘çš„æ··åˆå¹¶è¡Œæµ‹è¯•æ³¨å†Œåˆ° CIï¼Œä¼°ç®—è€—æ—¶ 5400â€¯sï¼ˆâ‰ˆ1.5â€¯hï¼‰ï¼Œå¹¶æ ‡è®°ä¸º nightly/weekly å¥—ä»¶ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **CI åŸºç¡€è®¾æ–½**ï¼šæ–°å¢çš„ workflow ä¾èµ– `8-gpu-h200` Runner æ ‡ç­¾ä¸å¯¹åº”çš„è‡ªå»ºæœºå™¨ï¼Œéœ€è¦ç¡®ä¿æ­¤æ ‡ç­¾åœ¨ç»„ç»‡ä¸­å¯ç”¨å¹¶å·²è£…é… CUDAã€é©±åŠ¨ã€ä¾èµ–ç­‰ã€‚  
- **æµ‹è¯•å¥—ä»¶**ï¼šçº¦ 60 ä¸ªæ··åˆå¹¶è¡Œï¼ˆDP/TP/EP/VPï¼‰æµ‹è¯•ç±»å°†è¢«çº³å…¥ nightly/weekly CIï¼Œæ¯ä¸ªçº¦ 90â€¯sï¼Œæ•´ä½“çº¦ 1.5â€¯hã€‚  
- **ä»£ç è·¯å¾„**ï¼š`sglang.test.ci.ci_register.register_cuda_ci` è¢«è°ƒç”¨ï¼Œè‹¥è¯¥å‡½æ•°ç­¾åæˆ–è¡Œä¸ºæœ‰å˜ä¼šç›´æ¥å¯¼è‡´ CI æŠ¥é”™ã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  

| æ–¹å‘ | å»ºè®® |
|------|------|
| **CI èµ„æº** | ç¡®è®¤ç»„ç»‡å·²é…ç½® 8â€‘GPU H200 è¿è¡Œå™¨ä¸”æ ‡ç­¾ `8-gpu-h200` ä¸ `RUNNER_LABELS` æ­£ç¡®æ˜ å°„ï¼›å¦‚èµ„æºç´§å¼ ï¼Œå¯è€ƒè™‘åœ¨ workflow ä¸­åŠ å…¥ `strategy.matrix` æˆ– `continue-on-error` ç»†ç²’åº¦æ§åˆ¶ã€‚ |
| **è¶…æ—¶ä¸å¹¶å‘** | ç›®å‰ workflow è®¾ç½® `timeout-minutes: 120`ï¼Œè€Œæ³¨å†Œçš„ä¼°ç®—æ—¶é•¿ä¸º 5400â€¯sï¼ˆ90â€¯minï¼‰ï¼Œæ­£å¥½åŒ¹é…ï¼Œå»ºè®®åœ¨å®é™…è¿è¡Œä¸­ç›‘æ§æ˜¯å¦æœ‰å•ä¸ªæµ‹è¯•è¶…æ—¶å¯¼è‡´æ•´ä½“å¤±è´¥ã€‚ |
| **æ³¨å†Œå‡½æ•°** | æ£€æŸ¥ `sglang.test.ci.ci_register.register_cuda_ci` æ˜¯å¦åœ¨æ‰€æœ‰ CI ç¯å¢ƒä¸­å¯å¯¼å…¥ï¼Œå°¤å…¶æ˜¯è½»é‡çº§çš„ PR æ£€æŸ¥ä¸­å¯èƒ½ä¸éœ€è¦åŠ è½½æ­¤å‡½æ•°ï¼Œä»¥å…å‡ºç° `ImportError`ã€‚|
| **æµ‹è¯•ç¨³å®šæ€§** | 60 æ¡é•¿æ—¶é—´è¿è¡Œçš„æµ‹è¯•å¯¹ç¡¬ä»¶å’Œç½‘ç»œä¾èµ–è¾ƒå¤§ï¼Œå»ºè®®åœ¨æœ¬åœ°å…ˆè·‘ä¸€æ¬¡å®Œæ•´å¥—ä»¶ï¼Œæ•è·æ½œåœ¨çš„éšæœºæ€§ï¼ˆå¦‚ GPU OOMã€è¿›ç¨‹æ®‹ç•™ç­‰ï¼‰ï¼Œå¹¶åœ¨ `run_suite.py` ä¸­ä¿æŒ `--continue-on-error`ï¼Œä½†ä¹Ÿè¦åœ¨æŠ¥å‘Šä¸­æ˜ç¡®æ ‡è®°å“ªäº›æµ‹è¯•ç»å¸¸å¤±è´¥ï¼Œä»¥å…æ©ç›–çœŸæ­£çš„å›å½’ã€‚ |
| **æ–‡æ¡£ä¸ç»´æŠ¤** | åœ¨é¡¹ç›®çš„ CI æ–‡æ¡£æˆ– CONTRIBUTING ä¸­è¡¥å……è¯´æ˜ â€œweeklyâ€‘8â€‘gpuâ€‘h200â€ å¥—ä»¶çš„ç”¨é€”ã€èµ„æºéœ€æ±‚ä»¥åŠå¦‚ä½•åœ¨æœ¬åœ°æ¨¡æ‹Ÿï¼ˆå¦‚ `GPU_CONFIG=8-gpu-h200`ï¼‰ï¼Œå¸®åŠ©æ–°è´¡çŒ®è€…å¿«é€Ÿå®šä½å’Œè°ƒè¯•ã€‚ |
| **å›æ»šæ–¹æ¡ˆ** | å¦‚æ–° workflow å¯¼è‡´ nightly è¿è¡Œè¶…æ—¶æˆ–å ç”¨è¿‡å¤šèµ„æºï¼Œå¯ä¸´æ—¶åœ¨ `workflow_dispatch` ä¸­é€šè¿‡ `job_filter` åªè¿è¡Œç‰¹å®šå­ä»»åŠ¡ï¼Œæˆ–åœ¨ `if` æ¡ä»¶é‡ŒåŠ å…¥ç¯å¢ƒå˜é‡å¼€å…³ï¼Œä»¥ä¾¿å¿«é€Ÿå›æ»šã€‚ |

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ¬¡æ”¹åŠ¨æ‰©å¤§äº†è‡ªåŠ¨åŒ–æµ‹è¯•è¦†ç›–ï¼Œå°¤å…¶é’ˆå¯¹æ··åˆå¹¶è¡Œå®ç°çš„å®Œæ•´éªŒè¯ã€‚ä½†åŒæ—¶å¢åŠ äº†å¯¹é«˜ç«¯ GPU é›†ç¾¤çš„ä¾èµ–å’Œé•¿æ—¶é—´è¿è¡Œçš„é£é™©ï¼Œè¯·åœ¨èµ„æºè°ƒåº¦ã€è¶…æ—¶è®¾ç½®ä»¥åŠæµ‹è¯•ç¨³å®šæ€§æ–¹é¢åšå¥½å……åˆ†ç›‘æ§ä¸é¢„æ¡ˆã€‚

---

### [Piecewise] Support PCG weak_ref_tensor cuda kernel on AMD (#17291)
**SHA**: `38c233f` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/38c233fd041f134f96ae23065bb763456881269f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º AMD/ROCm å¹³å°æä¾› `weak_ref_tensor` çš„ CUDA kernel æ”¯æŒï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `python/sglang/srt/compilation/weak_ref_tensor.py` ä¸­æŠŠå¹³å°åˆ¤æ–­ä»ä»… `is_cuda()` æ‰©å±•ä¸º `is_cuda() or is_hip()`ï¼Œä»è€Œåœ¨ AMD GPU ä¸Šä½¿ç”¨ç›¸åŒçš„å¼±å¼•ç”¨å¼ é‡å®ç°ã€‚  
2. åœ¨ `sgl-kernel/csrc/common_extension_rocm.cc` ä¸º ROCm (hip) æ·»åŠ  `weak_ref_tensor` çš„å‡½æ•°æ³¨å†Œä¸å®ç°æ˜ å°„ã€‚  
3. `sgl-kernel/setup_rocm.py` å°†æ–°æºç  `csrc/memory/weak_ref_tensor.cpp` çº³å…¥ç¼–è¯‘åˆ—è¡¨ï¼Œä½¿è¯¥ kernel åœ¨ ROCm ç¼–è¯‘æµç¨‹ä¸­è¢«ç¼–è¯‘ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- `sglang.srt` çš„å¼±å¼•ç”¨å¼ é‡åŠŸèƒ½ï¼ˆ`weak_ref_tensor`ï¼‰  
- `sgl-kernel` ROCm ä»£ç è·¯å¾„ï¼ˆ`common_extension_rocm.cc`ã€`setup_rocm.py`ã€`csrc/memory/weak_ref_tensor.cpp`ï¼‰  
- ä¾èµ– `is_hip` çš„å¹³å°æ£€æµ‹å·¥å…· (`sglang.srt.utils.common`)  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
- **æµ‹è¯•**ï¼šåœ¨ AMD GPUï¼ˆROCmï¼‰ç¯å¢ƒä¸‹è·‘ç°æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿ `weak_ref_tensor` è¡Œä¸ºä¸ CUDA å®Œå…¨ä¸€è‡´ï¼Œå°¤å…¶æ˜¯å¼ é‡ç”Ÿå‘½å‘¨æœŸã€å†…å­˜æ³„æ¼ç­‰ã€‚  
- **å…¼å®¹æ€§**ï¼šç¡®è®¤ `is_hip` åœ¨é AMD ç¯å¢ƒè¿”å› `False`ï¼Œé˜²æ­¢è¯¯åŠ è½½ ROCm ä»£ç ã€‚  
- **æ–‡æ¡£**ï¼šæ›´æ–° README/CHANGELOGï¼Œæ ‡æ˜è¯¥åŠŸèƒ½å·²åœ¨ AMD ä¸Šå¯ç”¨ï¼Œå¹¶è¯´æ˜å¿…è¦çš„ `ROCM` ç¯å¢ƒå˜é‡æˆ–ä¾èµ–ç‰ˆæœ¬ã€‚  
- **æ€§èƒ½**ï¼šå¯¹æ¯” AMD ä¸ NVIDIA å®ç°çš„æ‰§è¡Œæ—¶å»¶ï¼Œè‹¥å‡ºç°å¼‚å¸¸æ…¢å¯è¿›ä¸€æ­¥è°ƒä¼˜ `weak_ref_tensor.cpp` ä¸­çš„ kernelã€‚  

æ•´ä½“æ¥çœ‹ï¼Œæ­¤æ¬¡æ”¹åŠ¨ä¸ºè·¨å¹³å°ç®—å­æä¾›äº†ç­‰ä»·å®ç°ï¼Œæå‡äº† `sglang` åœ¨å¤š GPU ç”Ÿæ€çš„å¯ç”¨æ€§ï¼Œé£é™©ä¸»è¦åœ¨ ROCm ç¼–è¯‘ä¸è¿è¡Œæ—¶çš„å…¼å®¹æ€§ï¼Œå»ºè®®åœ¨ CI ä¸­åŠ å…¥ AMD GPU çš„è·‘æµ‹ã€‚

---

### Overlap shared experts with deepep dispatch for single batch overlap on Blackwell (#17289)
**SHA**: `6ea491e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/6ea491e4392d8cb4bcf38c21430eef594ed62eb6)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šåŠŸèƒ½å¢å¼ºï¼ˆä¸º Blackwell GPUï¼ˆGB200ï¼‰åœ¨å•æ‰¹æ¬¡ overlap åœºæ™¯ä¸‹æä¾›å¯é€‰çš„å…±äº«ä¸“å®¶æ‰§è¡Œè·¯å¾„ï¼‰  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¡ ä¸­  

**ğŸ“‹ å˜æ›´æ‘˜è¦**  
1. åœ¨ `environment_variables.md` ä¸­æ–°å¢ç¯å¢ƒå˜é‡ `SGLANG_BLACKWELL_OVERLAP_SHARED_EXPERTS_OUTSIDE_SBO`ï¼Œé»˜è®¤ `false`ï¼Œç”¨äºæ§åˆ¶å…±äº«ä¸“å®¶æ˜¯å¦åœ¨ç‹¬ç«‹ CUDA stream ä¸Šè¿è¡Œã€‚  
2. `environ.py` æ–°å¢å¯¹åº”çš„ `EnvBool` è¯»å–ã€‚  
3. `single_batch_overlap.py` ä¸­çš„åˆ¤å®šé€»è¾‘åŠ å…¥è¯¥å˜é‡ï¼Œä½¿ â€œå…±äº«ä¸“å®¶â€‘ä¸¤æµ overlapâ€ åœ¨å˜é‡æ‰“å¼€æ—¶å¤±æ•ˆã€‚  
4. `deepseek_v2.py` ä¸ºè¯¥å˜é‡æ‰“å¼€çš„æƒ…å†µå®ç°äº†ä¸€å¥—æ–°çš„ hook æµç¨‹ï¼š  
   - `post_dispatch_hook` è´Ÿè´£åœ¨ dispatch å®ŒæˆåæŠŠ combineâ€‘overlap ä¸ downâ€‘gemmâ€‘overlap å‚æ•°åˆ†åˆ«åˆ†é…ç»™è°ƒåº¦å™¨å’Œä¸“å®¶ã€‚  
   - `pre_combine_hook` åœ¨ combine å‰è®°å½•ä¸‹æ¸¸ gemm çš„äº‹ä»¶ã€‚  
   - `post_combine_hook` åœ¨ combine å®Œæˆåæ¸…ç† overlap å‚æ•°ã€‚  

**ğŸ¯ å½±å“èŒƒå›´**  
- **è°ƒåº¦å±‚ï¼ˆSRTï¼‰**ï¼šbatch_overlapã€dispatcherã€experts çš„ overlap å‚æ•°ç®¡ç†ã€‚  
- **æ¨¡å‹å®ç°**ï¼šDeepSeekâ€‘V2ï¼ˆ`deepseek_v2.py`ï¼‰åœ¨ä½¿ç”¨å…±äº«ä¸“å®¶æ—¶çš„æ‰§è¡Œè·¯å¾„ã€‚  
- **ç”¨æˆ·é…ç½®**ï¼šéœ€é€šè¿‡ç¯å¢ƒå˜é‡æ˜¾å¼å¼€å¯ï¼Œå¦åˆ™ä¿æŒåŸæœ‰è¡Œä¸ºã€‚  

**ğŸ’¡ å…³æ³¨å»ºè®®**  
1. **åŠŸèƒ½éªŒè¯**ï¼šåœ¨ Blackwellï¼ˆGB200ï¼‰ç¡¬ä»¶ä¸Šå¼€å¯è¯¥å˜é‡ï¼Œè§‚å¯Ÿå…±äº«ä¸“å®¶ä¸ DeepEP combine çš„å®é™…å¹¶è¡Œåº¦å’Œå»¶è¿Ÿï¼Œç¡®ä¿æ²¡æœ‰é¢å¤–çš„åŒæ­¥å¼€é”€ã€‚  
2. **å…¼å®¹æ€§**ï¼šé»˜è®¤ä¿æŒæ—§è¡Œä¸ºï¼Œé˜²æ­¢åœ¨é GB200 æˆ–æœªå¼€å¯ SBO çš„ç¯å¢ƒäº§ç”Ÿå›å½’ã€‚å»ºè®®åœ¨ CI ä¸­åŠ å…¥é’ˆå¯¹ `SGLANG_BLACKWELL_OVERLAP_SHARED_EXPERTS_OUTSIDE_SBO=true` çš„å•å…ƒ/é›†æˆæµ‹è¯•ã€‚  
3. **æ–‡æ¡£åŒæ­¥**ï¼šç¡®ä¿ README ä¸ç¯å¢ƒå˜é‡åˆ—è¡¨å®æ—¶æ›´æ–°ï¼Œè¯´æ˜è¯¥é€‰é¡¹ä»…åœ¨ Blackwell ä¸”å¯ç”¨å•æ‰¹æ¬¡ overlap æ—¶ç”Ÿæ•ˆã€‚  
4. **å¼‚å¸¸å¤„ç†**ï¼šè‹¥ hook æ³¨å†Œæˆ–äº‹ä»¶è®°å½•å¤±è´¥ï¼Œå½“å‰å®ç°ä¼šç›´æ¥ `remove()`ï¼Œä½†æœªæ•è·å¼‚å¸¸ï¼›å¯è€ƒè™‘åŠ æ—¥å¿—æˆ–å®‰å…¨å›é€€ï¼Œä»¥å…å¯¼è‡´åç»­æ¨ç†å¡æ­»ã€‚  

æ€»ä½“è€Œè¨€ï¼Œæ­¤æ”¹åŠ¨ä¸º Blackwell å¹³å°æä¾›äº†æ›´ç»†ç²’åº¦çš„ overlap æ§åˆ¶ï¼Œè‹¥æµ‹è¯•é€šè¿‡ï¼Œå¯æå‡å…±äº«ä¸“å®¶çš„ååã€‚åç»­å¯è§†ç¡¬ä»¶è¡¨ç°å†³å®šæ˜¯å¦å°†å…¶è®¾ä¸ºé»˜è®¤å¼€å¯ã€‚

---

#### ğŸŸ¢ ä½é‡è¦åº¦å˜æ›´ (19)

### Support fa4 decoding (#16034)
**SHA**: `4f6f5d2` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/4f6f5d25c801d8bf8cea49c33f95866939fe1a2c)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šæ”¾å®½ FlashAttentionâ€¯4 çš„è§£ç é™åˆ¶ï¼Œæ–°å¢åœ¨ CUDAâ€¯Graph ç¯å¢ƒä¸‹å¼ºåˆ¶ `num_splits=1` çš„é€»è¾‘ï¼Œå¹¶ç§»é™¤ `server_args` ä¸­å¯¹ FA4 åªèƒ½ç”¨äº prefill çš„æ ¡éªŒã€‚

---

### [docs] Show user the fastAPI docs available (#17510)
**SHA**: `458fe5a` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/458fe5a33717c317300f016af2a12baeae4c792b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `send_request.ipynb` ä¸­æ–°å¢è¯´æ˜ï¼Œå‘ŠçŸ¥ç”¨æˆ·æœåŠ¡å¯åŠ¨åå¯é€šè¿‡ `http://localhost:30000/docs`ï¼ˆSwagger UIï¼‰ã€`/redoc`ï¼ˆReDocï¼‰æˆ– `/openapi.json`ï¼ˆOpenAPIï¼‰æŸ¥çœ‹ FastAPI æ–‡æ¡£ã€‚

---

### [diffusion] improve: skip negative prompt encoding when guidance_scale <= 1.0 or negative_prompt is None (#16919)
**SHA**: `2c1b164` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/2c1b164a9263ccb018a70961feb8059615c78a79)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `image_encoding.py` ä¸­åŠ å…¥å¯¹ `batch.do_classifier_free_guidance` çš„åˆ¤æ–­ï¼Œä»…åœ¨éœ€è¦ classifierâ€‘free guidance æ—¶æ‰å‡†å¤‡å¹¶ç¼–ç è´Ÿå‘æç¤ºï¼Œé¿å…åœ¨ guidance_scale â‰¤â€¯1.0 æˆ–æœªæä¾›è´Ÿå‘æç¤ºæ—¶è¿›è¡Œæ— ç”¨è®¡ç®—ã€‚ä»£ç ç»“æ„æ›´ç®€æ´ï¼Œæ€§èƒ½ç•¥æœ‰æå‡ã€‚

---

### Tiny refine swa kv cache free (#17417)
**SHA**: `a618202` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/a618202fc7d35cde0704cbc4ca9c50a9d64aa98b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåˆ é™¤å†—ä½™çš„ `free` è°ƒç”¨å¹¶ç®€åŒ– `insert` è°ƒç”¨ï¼Œç»Ÿä¸€é‡Šæ”¾ kv ç´¢å¼•ï¼Œæå‡å†…å­˜ç®¡ç†ä¸€è‡´æ€§ã€‚

---

### [BUGFIX] fix value oom in radix tree (#17400)
**SHA**: `1b97fa7` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/1b97fa769b20631f18ce7301108477d63f3ad161)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šå…¶ä»–  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ Radix æ ‘èŠ‚ç‚¹æ’å…¥æ—¶ä½¿ç”¨ `value.clone()` å¤åˆ¶å€¼ï¼Œé˜²æ­¢åŸå§‹å¯¹è±¡è¢«å…±äº«å¯¼è‡´å†…å­˜ OOMã€‚

---

### update ascend docs (#17457)
**SHA**: `0a9099e` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/0a9099e137d43c3871688f72eb5bd9f7ee1ae05d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå®Œå–„ Ascend NPU ç¤ºä¾‹æ–‡æ¡£ï¼Œæ›´æ–° Prefill ä¸ Decode å¯åŠ¨è„šæœ¬ã€ç¯å¢ƒå˜é‡åŠè°ƒåº¦å‚æ•°ï¼›åŒæ­¥æ¨¡å‹æ”¯æŒåˆ—è¡¨ä¸­ Kimi åç§°çš„å¾®è°ƒã€‚

---

### Add job-level timeout for weekly test workflow (#17462)
**SHA**: `0050c47` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/0050c476fd028ae9fff522ab809089326d5529c9)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šä¸º `weekly-test-8-gpu-h200` å·¥ä½œæµæ·»åŠ  120 åˆ†é’Ÿçš„ä½œä¸šçº§åˆ«è¶…æ—¶ï¼Œå¹¶åœ¨æµ‹è¯•å‘½ä»¤ä¸­åŠ å…¥ `--timeout-per-file 7200` å‚æ•°ï¼Œä»¥é˜²å•æ–‡ä»¶è¿è¡Œè¶…æ—¶ã€‚

---

### [Tiny] Backward compatibility for fp4 gemm flags (#17466)
**SHA**: `8251a74` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/8251a74d5faa90a4907ec91db348ec2b475fba0a)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `initialize_fp4_gemm_config` ä¸­åŠ å…¥å¯¹æ—§ç¯å¢ƒå˜é‡ `SGLANG_FLASHINFER_FP4_GEMM_BACKEND` çš„å‘åå…¼å®¹å¤„ç†ï¼Œè‹¥å€¼æœªä»¥ `flashinfer_` å¼€å¤´åˆ™è‡ªåŠ¨è¡¥å…¨å‰ç¼€ã€‚

---

### [Fix] Set fa3 as default MHA backend on Hopper (#17425)
**SHA**: `a54d75b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/a54d75bf2e954564a6c9cee56405e813d78a6d1d)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ Hopper GPUï¼ˆCUDAâ€¯12.3ï¼‰ä¸”æœªæŒ‡å®šç‰¹å®šæ¨ç†æˆ– Topâ€‘Kâ€¯=â€¯1 æ—¶ï¼Œå°† MHA é»˜è®¤åç«¯æ”¹ä¸º **fa3**ï¼Œé¿å… flashinferâ€¯0.6.1 åœ¨ Hopper ä¸Šçš„æ€§èƒ½å›é€€ã€‚

---

### Fix pr-test-finish to fail when wait-for-stage jobs fail (#17465)
**SHA**: `3321eb4` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/3321eb4efa05acd7276f1c9bbcfded3e315bb5a4)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `pr-test.yml` å·¥ä½œæµä¸­æ–°å¢ `wait-for-stage-a` ä¸ `wait-for-stage-b` ä¸¤ä¸ªä½œä¸šï¼Œä½¿å¾—å½“è¿™äº›ç­‰å¾…é˜¶æ®µçš„ä½œä¸šå¤±è´¥æ—¶ï¼Œprâ€‘testâ€‘finish èƒ½å¤Ÿæ­£ç¡®è¿”å›å¤±è´¥ã€‚

---

### Fix NSA indexer in the nightly test (#17452)
**SHA**: `be5121b` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/be5121b452197fbab12a274755c4687017e80998)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæµ‹è¯•ä¿®æ”¹  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ nightly test `test_nsa_indexer.py` ä¸­æ–°å¢ `get_page_table_1` æ–¹æ³•ï¼Œç”Ÿæˆ page size ä¸º 1 çš„åˆ†é¡µè¡¨ï¼Œç”¨äºä¿®å¤ NSA indexer çš„ç´¢å¼•é”™è¯¯ã€‚

---

### [diffusion] fix: fix the bug of output_path not taking effect when generate (#17293)
**SHA**: `54a8217` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/54a821794ed11f355d4a01eab1aaf30701b6f4e4)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°† `output_path` é»˜è®¤å€¼æ”¹ä¸º `None`ï¼Œå¹¶åœ¨ `_adjust` ä¸­ä»…åœ¨æœªæ˜¾å¼è®¾å®šæ—¶æ‰ä½¿ç”¨ `server_args.output_path`ï¼Œä¿®å¤äº†ç”Ÿæˆæ—¶è·¯å¾„ä¸ç”Ÿæ•ˆçš„ bugã€‚

---

### [scheduler] Clear MM  data of finished batch (#17251)
**SHA**: `aea57b3` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/aea57b33c6dbdba665b0ce2a27276f19a14affe8)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `scheduler.py` ä¸­æ–°å¢ `_maybe_clear_mm_inputs` æ–¹æ³•ï¼Œå®Œæˆè¯·æ±‚ç»“æŸåæ¸…é™¤å¤šæ¨¡æ€è¾“å…¥çš„ç‰¹å¾æ•°æ®å¹¶é‡Šæ”¾å¼•ç”¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ã€‚éšååœ¨ `process_batch_result` é‡Œè°ƒç”¨è¯¥æ¸…ç†å‡½æ•°ã€‚

---

### Fix wait-for-stage jobs running when call-gate fails (#17443)
**SHA**: `648aab0` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/648aab0ce3c337b1047db9c364347cfb29a23c99)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ GitHub Actions å·¥ä½œæµä¸­ä¸º `wait-for-stage-a/b` æ·»åŠ  `call-gate` ä¾èµ–ä¸æˆåŠŸ/è·³è¿‡æ£€æŸ¥ï¼Œé¿å…åœ¨å‰ç½® gate å¤±è´¥æ—¶ä»ç­‰å¾…ï¼›åŠ å…¥ `!cancelled()` é˜²æ­¢å¹¶å‘å–æ¶ˆå½±å“ï¼›å°†è½®è¯¢é—´éš”æ”¹ä¸º 2â€¯åˆ†é’Ÿä»¥é™ä½ API è°ƒç”¨ï¼›å®Œå–„ stageâ€‘b ç­‰å¾…çš„ä½œä¸šåˆ—è¡¨å¹¶æ’é™¤å–æ¶ˆçŠ¶æ€ã€‚

---

### update urllib3 and gpgv Dockerfile (#17439)
**SHA**: `1e30903` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/1e309030e309bec523513a01a68e2128963cfb4e)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šé…ç½®è°ƒæ•´  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ Dockerfile ä¸­æ–°å¢å¯¹ urllib3â‰¥2.6.3 çš„ pip å‡çº§ï¼Œå¹¶å®‰è£… gnupg2 ä»¥æ”¯æŒ GPG å¯†é’¥éªŒè¯ã€‚

---

### [Piecewise] Fix PCG issue for multimodal and embedding model that wraps language_model (#17290)
**SHA**: `6092721` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/6092721594034f17f50d7063f42cbfd57898171e)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `model_runner.py` ä¸ `piecewise_cuda_graph_runner.py` ä¸­æ–°å¢å¯¹ `language_model` åŒ…è£…å±‚çš„å…¼å®¹å¤„ç†ï¼Œé¿å…åœ¨å¤šæ¨¡æ€æˆ–åµŒå…¥æ¨¡å‹ä¸­è§¦å‘ PCGï¼ˆPiecewise CUDA Graphï¼‰é”™è¯¯ã€‚

---

### [Auto Sync] Update piecewise_cuda_graph_runner.py (20260119) (#17313)
**SHA**: `20ed382` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/20ed3822bb6dc430552ae8b160de64e86f770e1f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šä»£ç é‡æ„  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ `piecewise_cuda_graph_runner.py` çš„ä¸¤å¤„å‡½æ•°ä¸­ï¼Œå°† `extend_prefix_lens` ç›¸å…³å¼ é‡çš„åˆå§‹åŒ–å€¼ä» `num_tokens` æ”¹ä¸º `0`ï¼Œä¿®æ­£å‰ç¼€é•¿åº¦çš„å¤„ç†é€»è¾‘ã€‚

---

### [Docs] Rename SGLang Router to SGLang Model Gateway (#17436)
**SHA**: `4ecd9af` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/4ecd9afde9787cb0263fd0fcbf5bac5867c17b0f)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šå°†æ‰€æœ‰æ–‡æ¡£ä¸­ â€œSGLang Routerâ€ é‡å‘½åä¸º â€œSGLang Model Gateway (former Router)â€ï¼Œå¹¶æ›´æ–°ç›¸åº”é“¾æ¥æŒ‡å‘æ–°æ–‡æ¡£ã€‚

---

### [Docs] Explain CUDA attention backend choices and aiter FP8 KV cache support (#17428)
**SHA**: `eb38d64` | ğŸ”— [æŸ¥çœ‹æäº¤](https://github.com/sgl-project/sglang/commit/eb38d6441375322878a428761e1d298cbe98a73b)

**ğŸ¯ å˜æ›´ç±»å‹**ï¼šæ–‡æ¡£æ›´æ–°  
**âš¡ é‡è¦ç¨‹åº¦**ï¼šğŸŸ¢ä½  
**ğŸ“‹ æ‘˜è¦**ï¼šåœ¨ CUDA æ³¨æ„åŠ›åç«¯æ–‡æ¡£ä¸­è¡¥å……äº† AITER (ROCm) çš„æ”¯æŒä¿¡æ¯å¹¶æ–°å¢äº†æ³¨æ„åŠ›åç«¯è‡ªåŠ¨é€‰æ‹©é€»è¾‘è¯´æ˜ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£ä¸åŒç¡¬ä»¶/æ¨¡å‹çš„é»˜è®¤åç«¯ã€‚

---

