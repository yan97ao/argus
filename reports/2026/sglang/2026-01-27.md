# 每日更新报告（2026-01-27）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-27 23:01:23 | fsygd | [diffusion] refactor: add arg to control the precision of dit (#17751) |
| 2026-01-27 22:10:57 | Baizhou Zhang | [DeepSeek] Update tests and document for DeepSeek V3.2 NVFP4 checkpoint (#17657) |
| 2026-01-27 19:03:53 | monkeyLoveding | [NPU] Adapt cann 8.5: use sfa and lightning indexer op from cann and CI update (#17615) |
| 2026-01-27 18:59:04 | Baizhou Zhang | [Doc] Tiny update description on torch compile (#17819) |
| 2026-01-27 18:43:17 | MikkoParkkola | fix(quantization): add sgl_kernel fallback for FP4 quantize on Blackwell GPUs (#17816) |
| 2026-01-27 17:41:17 | Xuchun Shang | [PP] fix wrong weight logic for tie_word_embeddings model (#15890) |
| 2026-01-27 14:24:00 | Yuxuan Zhang | [GLM-OCR] Support GLM-OCR Model (#17582) |
| 2026-01-27 14:08:24 | Taemin Jung | [Model] Add support for EXAONE-4.0 Model (#8205) |
| 2026-01-27 14:04:44 | laixin | [Bug Fix] Fix reasoning parser when continue_final_message=true (#17065) |
| 2026-01-27 13:10:32 | Hubert Lu | [AMD] Update dsv3.2 AMD GPU docs and unify ROCm TileLang build (#17783) |
| 2026-01-27 11:49:14 | shuwenn | [model-gateway] ignore error for embeddings/classify in PD router (#15931) |
| 2026-01-27 11:48:24 | shuwenn | fix: preserve disconnect events in api key middleware (#17253) |
| 2026-01-27 11:33:19 | shuwenn | [HiCache][HA 1/N] Support HiCache storage runtime attach/detach (#15892) |
| 2026-01-27 11:04:25 | Zhongdongming Dai | [chore]: improve time tracing of model loading process (#15426) |
| 2026-01-27 10:59:33 | Shangming Cai | [CI] Skip PD hybrid attention test with different TP temporarily (#17791) |
| 2026-01-27 10:57:00 | Yuhao Yang | model: support Kimi-K2.5 (#17789) |
| 2026-01-27 10:06:42 | WenhaoZhang | [diffusion] comfyui: support Qwen-Image, Multi-GPU Z-Image, and Enhanced ComfyUI Integration (#17678) |
| 2026-01-27 10:01:50 | FlyPanda | [bugfix] Internal processing of hf3fs crash # 16614 (#16938) |
| 2026-01-27 05:02:54 | Mahdi-CV | fix(processor): support InternS1 text_config in InternVL processor (#17040) |
| 2026-01-27 04:16:54 | zijiexia | [Docs] Add RL documentation (#17663) |
| 2026-01-27 02:26:40 | ybyang | Special logic for healthcheck (#17734) |
| 2026-01-27 02:26:20 | Liangsheng Yin | Introduce global `alloc_len_per_decode` & clean check decode memory (#15115) |
| 2026-01-27 01:46:33 | Douglas Yang | fix: remove truncation for test and job names in ci failure monitor (#17765) |
| 2026-01-27 01:45:44 | Makcum888e | [NPU] Split pyproject npu from pyproject other (#17641) |
| 2026-01-27 00:13:46 | Douglas Yang | fix: move nightly whl to cuda version folder (#17762) |

### 📊 统计摘要
> 本日共 25 个提交 | 🔴高 6 | 🟡中 13 | 🟢低 6
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (6)](#-🔴-高重要度变更-6)
    - [[GLM-OCR] Support GLM-OCR Model (#17582)](#7106f6c)
    - [fix: preserve disconnect events in api key middleware (#1...](#57e432d)
    - [[HiCache][HA 1/N] Support HiCache storage runtime attach/...](#fd3b179)
    - [model: support Kimi-K2.5 (#17789)](#479ab7a)
    - [[diffusion] comfyui: support Qwen-Image, Multi-GPU Z-Imag...](#0519b09)
    - [fix: move nightly whl to cuda version folder (#17762)](#51d139b)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [[diffusion] refactor: add arg to control the precision of...](#547e2d0)
    - [[DeepSeek] Update tests and document for DeepSeek V3.2 NV...](#1d942e4)
    - [[NPU] Adapt cann 8.5: use sfa and lightning indexer op fr...](#d578b41)
    - [fix(quantization): add sgl_kernel fallback for FP4 quanti...](#c56d19b)
    - [[PP] fix wrong weight logic for tie_word_embeddings model...](#dba264a)
    - [[Model] Add support for EXAONE-4.0 Model (#8205)](#81c0f5c)
    - [[Bug Fix] Fix reasoning parser when continue_final_messag...](#6c9b054)
    - [[bugfix] Internal processing of hf3fs crash # 16614 (#16938)](#2d8c22a)
    - [fix(processor): support InternS1 text_config in InternVL ...](#5399240)
    - [[Docs] Add RL documentation (#17663)](#dd97e1f)
    - [Introduce global `alloc_len_per_decode` & clean check dec...](#85d077f)
    - [fix: remove truncation for test and job names in ci failu...](#8643fb2)
    - [[NPU] Split pyproject npu from pyproject other (#17641)](#bba6e38)
  - [🟢 低重要度变更 (6)](#-🟢-低重要度变更-6)
    - [[Doc] Tiny update description on torch compile (#17819)](#832c756)
    - [[AMD] Update dsv3.2 AMD GPU docs and unify ROCm TileLang ...](#df42f4d)
    - [[model-gateway] ignore error for embeddings/classify in P...](#a723d1c)
    - [[chore]: improve time tracing of model loading process (#...](#1b56a88)
    - [[CI] Skip PD hybrid attention test with different TP temp...](#3ad3268)
    - [Special logic for healthcheck (#17734)](#5ab76ff)
#### 🔴 高重要度变更 (6)

### [GLM-OCR] Support GLM-OCR Model (#17582)
**SHA**: `7106f6c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7106f6c8e1509cd57abeafd5d50cb1beaffbc63c)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 新增 GLM‑OCR 推理模型实现，包括 Vision Attention、Patch Embedding、Patch Merger、完整的 `GlmOcrForConditionalGeneration` 与其 Next‑N（speculative decoding）变体。  
2. 在模型配置、权重加载、注意力实现中加入对 **qk_normalization_by_head_size** 的支持，以适配 GLM‑OCR 的视觉注意力规范化需求。  
3. 更新文档、模型列表、处理器注册以及通用工具，使 GLM‑OCR 能够在 SGLang 多模态流水线中被识别和使用。  

**🎯 影响范围**  
- `sglang/srt/models/`：新增 `glm_ocr.py`、`glm_ocr_nextn.py`；修改 `glm4.py`、`glm4v.py`、`vision.py`、`model_config.py`。  
- `sglang/srt/multimodal/processors/`：GLM‑OCR 加入图像处理器白名单。  
- `sglang/srt/utils/common.py`、文档 `supported_models`：模型识别与展示层面更新。  
- 依赖检查：`_verify_transformers_version` 中的最低版本从 `5.0.0` 调整为 `5.0.0dev0`。  

**🔍 技术洞察**  

- **架构影响**  
  - **模块化扩展**：GLM‑OCR 复用 GLM‑4V 的视觉子网结构（VisionAttention、PatchEmbed、PatchMerger），但通过子类化实现细节差异（如 `qk_normalization_by_head_size`）。  
  - **并行策略**：在 `model_config.py` 为 OCR Draft 模型添加 `GlmOcrForConditionalGenerationNextN`，保持与现有 PP/TP/DP 机制兼容。  
  - **权重加载**：`load_weights` 中专门处理 `visual` 前缀以及 VisionAttention 的 dummy‑head 填充，确保多卡部署时张量分片一致。  

- **性能影响**  
  - **额外规范化**：`qk_normalization_by_head_size` 在每个注意力头上执行 RMSNorm，增加一次 `reshape`、两次 `norm`，理论上会带来 ~5‑10% 的计算开销（视 head 数量而定），但对 OCR 任务的数值稳定性提升显著。  
  - **Dummy‑head 兼容**：通过 `vision_utils.pad_vit_attn_dummy_heads` 自动填充缺失的 head，避免显存碎片化，保持原有的高吞吐量。  
  - **Next‑N 解码**：新增 `GlmOcrForConditionalGenerationNextN` 复用了 GLM‑4V 的 speculative decoding 框架，若启动则可在 OCR 场景实现数倍加速。  

- **安全考虑**  
  - **模型加载**：新增模型依赖 HuggingFace 官方权重，未对 `trust-remote-code` 做显式限制，建议在生产环境中使用已审计的模型仓库或启用 `--trust-remote-code false`。  
  - **版本检查**：将 transformers 版本下限改为 `5.0.0dev0`，引入了预发行版依赖，可能导致未经过充分测试的 API 变化，引入潜在的兼容性风险。  

**⚠️ 潜在风险**  
1. **规范化实现错误**：`qk_normalization_by_head_size` 的 reshape/reshape 逻辑若与实际张量布局不匹配，可能导致显存泄漏或数值错误。  
2. **并行切分不一致**：权重填充逻辑 (`pad_vit_attn_dummy_heads`) 与新模型的 head 数量不匹配时，可能出现参数未加载或多余的 zero‑padding，导致推理精度下降。  
3. **版本依赖**：使用 `5.0.0dev0` 可能在 stable 环境中不可用，导致启动失败或意外行为。  
4. **文档遗漏**：文档中未标注对 `trust-remote-code` 的注意事项，使用未审计的第三方模型可能带来执行代码的风险。  

**💡 关注建议**  
- **单元/集成测试**：新增对 `GlmOcrVisionAttention` 的梯度检查和前向一致性测试，确保 `qk_normalization_by_head_size` 在不同 batch、seq 长度下保持数值稳定。  
- **并行验证**：在多 GPU（TP+PP）环境下跑权重加载基准，确认 dummy‑head 填充与实际张量切分一致。  
- **版本回退**：在 `requirements.txt` 中显式 pin 到 `transformers>=5.0.0.dev0,<6.0.0`，并在 CI 中加入 transformers 版本检测。  
- **安全加固**：在模型加载入口加入 `--trust-remote-code` 参数默认关闭，或在文档中明确提示用户审计模型来源。  
- **性能监控**：提供 `--log-vision-attn-norm-time` 开关，以便用户评估 RMSNorm 引入的额外开销，并根据实际需求决定是否开启。  

通过上述措施，可在保证功能可用性的同时，降低因新架构和依赖变动带来的风险，提升 GLM‑OCR 在 SGLang 多模态推理中的可靠性与性能。

---

### fix: preserve disconnect events in api key middleware (#17253)
**SHA**: `57e432d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/57e432d951d35d12514e1ab8cb3f72b17b83fe7b)

**🎯 变更类型**：Bug修复  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 将原先基于 FastAPI `@app.middleware("http")` 的 API‑Key 认证实现，替换为原生 ASGI 中间件 `_ApiKeyASGIMiddleware`，以在请求被拒绝或正常转发前保留客户端主动断开（disconnect）事件。  
- 相应地在测试层面加入了 `TestAbortWithApiKey`，通过并发请求并在子进程强制 `terminate`，验证在使用 API‑Key 时服务器能够检测到断连并触发 abort 逻辑，防止内存泄漏。  

**🎯 影响范围**  
- `python/sglang/srt/utils/auth.py`：核心认证中间件实现。  
- `python/sglang/test/test_utils.py`、`test/registered/scheduler/test_abort.py`：新增测试用例，涉及内存泄漏检查与 abort 场景。  
- 所有通过 `add_api_key_middleware` 注入认证的 FastAPI 路由（主要是 `/generate`、`/chat` 等对外 API）。  

**🔍 技术洞察**  

- **架构影响**  
  - **ASGI层面**：实现了真正的 ASGI 中间件，绕开了 FastAPI 对 `call_next` 的封装，确保 `receive` 中的 `http.disconnect` 事件能够被下游（如 `uvicorn`、`starlette`）捕获。  
  - **解耦**：中间件仅依赖 `starlette.requests.Request` 与 `fastapi_app` 实例，保持对业务代码的最小侵入，后续若迁移至其他 ASGI 框架（如 Quart）影响有限。  

- **性能影响**  
  - **微幅提升**：移除 `call_next` 的额外包装层，减少一次函数调用及相应的 `await`，对单请求的延迟几乎不可感知（预计 <0.1 ms）。  
  - **资源占用**：新增中间件类本身占用极小的内存，且在拒绝请求时直接返回 `ORJSONResponse`，避免了额外的路由解析和响应生成步骤。  

- **安全考虑**  
  - 仍然通过 `Authorization` Header 进行 API‑Key 鉴权，逻辑保持不变，未引入新的安全风险。  
  - 通过原生 ASGI 实现，能够更可靠地捕获异常和断连，防止因错误的连接保持导致的潜在拒绝服务（DoS）或资源泄漏。  

**⚠️ 潜在风险**  
1. **兼容性**：若项目中还有基于 FastAPI 的自定义 `@app.middleware` 方式依赖于 `call_next` 传递的 `Response` 对象进行后处理，直接切换为 ASGI 中间件可能导致这些后置逻辑失效。  
2. **异常处理**：现有实现只在 `decision.allowed` 为 `False` 时返回响应，未捕获 `decide_request_auth` 本身可能抛出的异常，异常将向上传递至 FastAPI 默认异常处理器，需确认已有捕获策略。  
3. **测试依赖**：新增测试使用多进程强制 terminate，可能在某些 CI 环境（尤其是 Windows）出现 flaky，需要确保 CI 配置支持 `multiprocessing`。  

**💡 关注建议**  
- **回归测试**：在完整的回归套件中加入对所有使用 `add_api_key_middleware` 的路由的断连场景验证，确保没有遗漏。  
- **文档更新**：在项目的 API‑Key 中间件使用说明里标注已改为 ASGI 原生实现，并解释为何这样可以 “preserve disconnect events”。  
- **异常防护**：考虑在 `_ApiKeyASGIMiddleware.__call__` 包裹 `decide_request_auth` 调用的 `try/except`，将意外异常转化为 500 错误并记录日志，避免异常泄漏到客户端。  
- **CI 兼容**：为跨平台 CI（如 GitHub Actions Windows runner）添加条件跳过或改写 `TestAbortWithApiKey`，防止因进程管理差异导致构建失败。  

以上即本次提交的技术分析。祝开发顺利 🚀

---

### [HiCache][HA 1/N] Support HiCache storage runtime attach/detach (#15892)
**SHA**: `fd3b179` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fd3b179ffd2748df4c477c839043f7b1efc18ce1)

**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
本次提交为 HiCache 的 **L3 存储后端** 引入了 **运行时动态挂载 / 卸载** 能力。通过新增的 HTTP Admin API（`PUT/DELETE /hicache/storage-backend`）以及对应的内部请求结构，用户可以在服务已经启动且正在提供推理的情况下，**无需重启** 即启用或关闭文件、Mooncake、HF3FS 等持久化缓存后端。实现涉及 **Scheduler、TokenizerManager、CacheController、HiRadixCache** 等核心组件的改造，并加入了严格的 **空闲判定**、线程安全的 **存储线程启停**、以及进程退出时的自动清理逻辑。

---

**🎯 影响范围**  
- `python/sglang/srt/entrypoints/http_server.py` – 新增 Admin API 路由。  
- `python/sglang/srt/managers/scheduler.py` – 空闲检查、调度层对 `attach/detach` 的调度分发。  
- `python/sglang/srt/managers/tokenizer_communicator_mixin.py` – 新增 Communicator 与请求/响应结构。  
- `python/sglang/srt/managers/cache_controller.py` – 完全重写存储后端的生命周期管理（线程启动/停止、异常回滚、DP 同步）。  
- `python/sglang/srt/mem_cache/hiradix_cache.py` – 增加运行时配置、metrics 开关、`atexit` 自动清理、对 `attach/detach` 的统一入口。  
- `python/sglang/srt/managers/io_struct.py` – 新增 `Attach/DetachHiCacheStorageReq*` 数据类。  
- 文档和测试 (`docs/advanced_features/*.md`、`test/registered/hicache/test_hicache_storage_runtime_attach_detach.py`)。  

---

**🔍 技术洞察**  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | <ul><li>在 **Scheduler → CacheController → HiRadixCache** 三层链路中加入了 **控制路径**（Admin API → TokenizerManager → Scheduler → CacheController → HiRadixCache）。</li><li>通过 `tp_group` 统一创建的自定义 **prefetch TP group**，确保在多卡（TP）环境下的存储操作同步。</li><li>新增 **`storage_stop_event`** 与原有 `stop_event` 分离，避免在运行时挂载/卸载时影响核心的 KV 传输缓冲。</li><li>引入 `atexit` 自动调用 `HiRadixCache.shutdown`，提升进程退出时的资源回收完整性。</li></ul> |
| **性能影响** | <ul><li>**空闲检查** 确保在无请求、无排队情况下才进行 attach/detach，几乎不产生运行时开销。</li><li>首次 attach 时会启动 **prefetch / backup 线程**（两个 daemon 线程，各自携带独立队列），会占用额外 CPU 与内存（`prefetch_queue`、`backup_queue`、`prefetch_buffer` 等），但仅在后端启用后才生效。</li><li>detach 时需要 **drain** 所有控制队列并强制释放 host page 资源，若 queue 较大可能出现短暂的阻塞（`join(timeout=10)`），对服务的响应时间产生可控制的上限影响。</li><li>新实现避免了在每次请求期间检查存储状态，保持了原有的高吞吐路径不受额外分支影响。</li></ul> |
| **安全考虑** | <ul><li>所有新 API 均受 **admin API key** 保护，缺失时返回 400/401 错误并给出明确提示。</li><li>`AttachHiCacheStorageReqInput` 对 `hicache_storage_prefetch_policy` 与 `hicache_write_policy` 进行白名单校验，防止非法值导致异常行为。</li><li>对 **storage_backend_extra_config_json** 仅做 JSON 解析，未做业务层面的白名单校验（如文件路径、网络地址），如果后端实现本身缺少安全校验，可能导致**路径遍历**、**未授权远程访问** 或 **恶意代码执行**（取决于后端实现）。</li><li>在多 TP 场景下，attach 需要所有 rank 成功，否则返回错误，但 **未实现自动 rollback**，可能导致部分节点已挂载、部分未挂载的不一致状态。</li></ul> |
| **可维护性** | <ul><li>请求结构统一使用 `io_struct`，便于后续扩展。</li><li>大量新增的错误处理与日志（`logger.exception`）提升了排障可观测性。</li><li>代码对 **partial failure** 的处理仍有缺口（如 DP 中部分成功后未回滚），后续可考虑实现事务化的 attach 流程。</li></ul> |

---

**⚠️ 潜在风险**  

1. **状态不一致**：在多 GPU / 多 TP 场景下，`attach` 可能在部分 rank 成功后其他 rank 失败，导致集群状态不一致，随后 `detach` 可能无法彻底清理。  
2. **资源泄漏**：如果 `attach` 期间抛异常或 `detach` 过程中线程未能正常退出，可能留下未释放的 host page、后台线程、或未关闭的存储后台连接。  
3. **安全暴露**：未对 `hicache_storage_backend_extra_config_json` 的字段进行细粒度校验，攻击者可利用后端实现（如 Mooncake、HF3FS）的漏洞进行文件读写或网络攻击。  
4. **服务可用性**：`_is_idle_for_hicache_storage_op` 只能保证调度层无请求，但 **外部并发请求** 仍可能在检查后立即进入运行状态，导致 attach 成功后立即出现冲突错误或数据不一致。  
5. **性能抖动**：首次 attach 会触发线程创建与大量内存分配，若在高峰期执行可能出现短暂的 CPU/内存峰值，影响整体吞吐。  

---

**💡 关注建议**  

1. **强化 DP 同步**  
   - 在 `attach_hicache_storage_wrapped` 中加入 **事务式** 机制：所有 rank 成功后才更新全局状态，若任意 rank 失败，则在已成功

---

### model: support Kimi-K2.5 (#17789)
**SHA**: `479ab7a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/479ab7a4e7e40d4227d892f9d3da871424a316bb)

**🎯 变更类型**：功能增强 / 架构变更 / 安全修复  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 新增 Kimi‑K2.5（K2‑VL）模型的完整实现，包括配置 (`kimi_k25.py`)、视觉塔 (`MoonViT3dPretrainedModel` 等子模块)、多模态投影、权重加载与推理路径。  
- 在 SGLang 配置、模型注册、推理入口、参数解析、ReasoningParser、ServerArgs 等全局模块中加入对该模型的支持。  
- 新增对应的多模态处理器 `kimi_k25.py`，并在基础处理器中声明 `grid_thws` 字段以兼容 K2‑K25 的时空网格信息。  

**🎯 影响范围**  
- **模型层**：`sglang.srt.models.kimi_k25`（新模型类与视觉塔实现）  
- **配置层**：`sglang.srt.configs.kimi_k25`、`__init__.py`、`model_config.py`、`hf_transformers_utils.py`  
- **推理层**：`entrypoints/openai/serving_chat.py`（reasoning 检测）、`parser/reasoning_parser.py`（新增 KimiK2Detector）  
- **服务器层**：`server_args.py`（模型特定参数调整、DP attention 兼容）  
- **多模态处理层**：`multimodal/processors/kimi_k25.py`、`base_processor.py`（新增 `grid_thws`）  

**🔍 技术洞察**  

- **架构影响**  
  - **模块化扩展**：通过新增 `KimiK25ForConditionalGeneration` 实现，将视觉塔、MM 投影与文本模型解耦，保持了 SGLang 原有的 “语言模型 + 多模态投影” 设计哲学。  
  - **统一视图**：在 `model_config._derive_model_shapes` 中加入对 KimiK25 架构的判定，使其走同一套 head‑dim 与注意力架构路径（MLA）。  
  - **分布式注意力**：`VisionAttention` 现在在 TP 场景下显式执行 `all_reduce`（`get_attention_tp_group().all_reduce`），确保跨卡的输出一致性，提升多卡推理的可用性。  
  - **配置注册**：在 `hf_transformers_utils._CONFIG_REGISTRY` 与 `configs.__init__` 中注册，使 `AutoConfig.from_pretrained` 能直接识别 Kimi‑K2.5。  

- **性能影响**  
  - **Vision Tower**：采用 3D Patch‑Embedding + 时空池化 (`tpool_patch_merger`) 并行化处理，可在每帧 14×14 的 Patch 上实现 2×2 合并，显著降低 token 数量，降低显存占用。  
  - **自适应批次划分**：`vision_tower_forward_auto` 根据 `KIMIV_VT_INFER_MAX_PATCH_NUM` 动态切分 batch，避免一次性加载过多 patch 导致 OOM，同时保持高吞吐。  
  - **TP 归约**：在多卡情况下只在投影后做一次 `all_reduce`，对性能影响可接受（一次同步开销），但比起之前的 “不做归约” 在多卡推理时更具正确性。  
  - **编译优化**：`get_rope_shape` 使用 `torch.compile(dynamic=True)`，在首次调用时生成固定 64×64 的插值缓存，提升后续 RoPE 计算速度。  

- **安全考虑**  
  - **权重加载**：新增 `load_weights` 对视觉塔和语言模型分别执行，使用 `default_weight_loader` 且在缺失权重时抛异常，避免 silently 忽略关键参数。  
  - **输入校验**：`BaseMultimodalProcessor` 中增加 `grid_thws` 字段，确保视觉塔接收的时空维度合法；`KimiK2Detector` 只在 `force_reasoning` 为 True 时解析 `<think>`，防止误触发未知标签。  
  - **未发现新的 CVE 或代码注入风险**，所有新增代码均为内部调用、无外部输入直接执行的路径。  

**⚠️ 潜在风险**  

| 风险点 | 说明 | 可能影响 | 缓解措施 |
|--------|------|----------|----------|
| **TP 归约导致额外同步** | 在 `VisionAttention` 中对 `output` 使用 `all_reduce`，若网络带宽或卡间同步不佳，推理延迟可能上升。 | 多卡部署时吞吐下降 | 可通过 `envs.SGLANG_ENABLE_TP_REDUCE`（假设新增）或在 `server_args` 中关闭 TP（`enable_tp_attention=False`）进行回退。 |
| **RoPE 缓存尺寸固定** | `get_rope_shape` 第一次调用使用硬编码 `(64,64)`，若实际输入分辨率超过 64，插值会产生信息损失。 | 视觉模型在高分辨率视频上精度下降 | 可在配置中暴露 `rope_max_size` 参数或在首次调用时根据实际 `height/width` 动态决定。 |
| **权重映射路径变化** | 对视觉塔权重的名称做了多次 replace（`wqkv.` → `attn.qkv_proj.` 等），若未来 upstream 改名，加载可能失败。 | 部署升级后模型加载报错 | 保持 `load_weights` 中的映射规则同步更新，并在 CI 中加入权重兼容性测试。 |
| **多模态处理器兼容性** | 新增 `kimi_k25.py` 只实现图像处理，未覆盖音频/视频。若用户同时提交音频/视频，会进入旧的处理路径，可能导致 `grid_thws` 与 `vision_tower` 不匹配。 | 运行时异常或错误的 token 计数 | 在 `process_mm_data_async` 中加入对 `audios`/`videos` 的显式检查或抛出明确错误。 |
| **推理占用显存** | 视觉塔输出在 patch merging 前仍保留完整帧的 token，若 batch 中有大量高分辨率视频，显存峰值可能超过预期。 | OOM 并导致服务不可用 | 在 `vision_tower_forward_auto` 中加入显存预估并动态调节 `max_infer_batch`，或在 `server_args` 中提供 `max_video_patches` 上限。 |

**💡 关注建议**  

1. **监控 TP 同步开销**  
   - 在多卡部署时打开 `torch.distributed` 的 profiling，关注 `all_reduce` 时延。如果出现瓶颈，可考虑在 `VisionAttention` 中添加 `if self.tp_size > 1 and self.training:` 之类的条件，仅在推理阶段或特定配置下执行归约。  

2. **RoPE 可配置化**  
   - 建议在 `KimiK25VisionConfig` 中加入 `rope_max_resolution: int = 64`，并在 `get_rope_shape` 中读取，避免硬编码。  

3. **权重兼容性测试**  
   - 在 CI 中加入基于官方 `kimi/k2.5` checkpoint 的加载测试，验证 `load_weights` 能成功映射所有关键层（vision tower、mm projector、language model）。  

4. **显存预警**  
   - 在 `ServerArgs._handle_model_specific_adjustments` 对 `KimiK25ForConditionalGeneration` 设置默认 `max_batch_total_seq_len` 限制，防止一次性提交超大视频导致 OOM。  

5. **文档与示例**  
   - 补全 README/文档，展示 Kimi‑K2.5 支持的多模态输入格式（特别是 `grid_thws` 结构），并给出示例代码片段。  

6. **回退路径**  
   - 为旧版本的 `KimiVL` 保持兼容：在 `KimiK2_5VLImageProcessor` 中若检测到模型不是 `KimiK25ForConditionalGeneration`，仍可使用原有 `KimiVL` 处理器，避免用户因模型名称错误而无法启动。  

**总体结论**  
此次提交为 SGLang 引入了全新 Kimi‑K2.5 多模态模型，完成了从配置、权重加载、推理算子到服务器层面的全链路支持。实现方式遵循 SGLang 现有的插件化架构，风险主要集中在分布式同步、RoPE 插值以及显存管理上。

---

### [diffusion] comfyui: support Qwen-Image, Multi-GPU Z-Image, and Enhanced ComfyUI Integration (#17678)
**SHA**: `0519b09` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0519b0935f211e312d8c002ec64df26a0be628c6)

**🎯 变更类型**：功能增强 / 多模型支持 / 性能优化 / 架构变更 / 测试补强  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 为 ComfyUI 插件加入 **Qwen‑Image / Qwen‑Image‑Edit**（多模态图像）支持，兼容 **Multi‑GPU Z‑Image**（序列并行）以及 **Flux**。  
- 新增 **SGLDiffusionExecutor** 子类（`QwenImageExecutor`、`QwenImageEditExecutor`）和对应 **LoRA** 加载节点，实现对 ComfyUI 前端 **latents / prompts / image_latents** 的直接注入。  
- 引入 **`model_type`** 选项、`suppress_logs` 开关、ComfyUI‑专属 **`ComfyUILatentPreparationStage`**，以及对 `ScheduleBatch`、`GPUWorker`、`BaseStage` 的日志/形状处理改动。  
- 大幅扩充 **pipeline**（`ComfyUIQwenImagePipeline` / `ComfyUIQwenImageEditPipeline`），实现 **safetensors** 直接加载、权重映射、FSDP 并行、torch.compile 选项等。  
- 新增 **完整单元测试**、示例工作流 JSON、README 更新以及少量容错/兼容代码（ComfyUI 依赖缺失时安全降级）。

---

## 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | • 通过 `executors/__init__.py` 将 Qwen 系列 executor 纳入统一调度框架。<br>• `generator.py` 中 `pipeline_class_dict` 与 `executor_class_dict` 增添 `qwen_image`、`qwen_image_edit` 两条映射，使模型/执行器的选择在同一入口完成。<br>• `nodes.py` 新增 `model_type` 输入，允许在 ComfyUI UI 层面显式指定模型种类，避免自动探测错误。<br>• `SGLDLoraLoader` 通过 `set_lora` 直接调用 SGLang Diffusion API，实现 LoRA 动态注入，保持与已有 `LoRA` 节点的统一接口。<br>• `ComfyUILatentPreparationStage` 被加入 `ComposedPipelineBase`，在 **前置** 处理阶段保留原始 `raw_latent_shape`，解决多 GPU SP（Sequence Parallel）下 **4D → 3D** 的形状丢失问题。<br>• `sampling_params.py` 增加 `suppress_logs`，在 `Req`/`OutputBatch` 传递后可在高频调度路径（GPUWorker、日志）压制冗余输出，提升调度层的吞吐与可读性。 |
| **性能影响** | • **Multi‑GPU Z‑Image**：`ZImageExecutor` 继续使用 `self.should_suppress_logs` 判断是否打印日志，避免在每个 timestep 产生海量 `info`，降低 CPU‑GPU 同步开销。<br>• **Flux / Qwen**：在 `comfyui_flux_pipeline.py` 中对 **双块/单块** QKV 权重拆分逻辑优化，直接产生 `to_q/k/v` 而不是后置切片，减少一次 `torch.cat`/`view` 操作。<br>• **torch.compile** 与 **tp/sp** 选项仍可在 `SGLDOptions` 中配置，未改变原有加速路径。<br>• `maybe_unpad_latents` 兼容 3‑dim 序列与 4‑dim 空间两种 `raw_latent_shape`，防止在 SP 场景中因错误切片导致额外的 `tensor.clone()`。<br>• `ComfyUIZImagePipeline` 与 `ComfyUIQwenImagePipeline` 在 `create_pipeline_stages` 中显式插入 `ComfyUILatentPreparationStage`，可在 **SP** 环境下通过 `contiguous()` 保证内存布局，提升 `all_gather` 与 `shard` 之间的拷贝效率。 |
| **安全考虑** | • **文件路径**：`SGLDLoraLoader` 直接使用 `folder_paths.get_full_path("loras", lora_name)`，若 ComfyUI 的 `loras` 目录被恶意写入，可能导致任意文件读取/写入。建议在插件层验证文件后缀或使用白名单。<br>• **safetensors 加载**：对 `.safetensors` 文件的直接读取已受 `safetensors` 本身的安全检查（结构化、不可执行），风险相对低。<br>• **异常降级**：`__init__.py` 包装 `ImportError` 防止在没有 ComfyUI 环境时抛异常，提升 CI/测试安全性。<br>• **日志抑制**：`suppress_logs` 只影响信息级日志，不影响错误/警告，安全性不受影响。 |
| **可维护性** | • 新增大量 **测试文件**（Flux、Z‑Image、Qwen、Qwen‑Edit），覆盖从 `DiffGenerator` → `Req` → `Executor` → `OutputBatch` 的完整路径，提升回归安全。<br>• `README.md` 与工作流 JSON 同步更新，降低使用门槛。<br>• `base.py` 与 `zimage.py` 中的 `maybe_unpad_latents`、`gather_latents_for_sp` 小幅改动，保持向后兼容但引入了 `contiguous()`，需在后续文档注明。<br>• 代码中大量 **try/except**、`if not req.suppress_logs` 逻辑分散，建议抽象为统一的 **LoggingHelper**，避免未来忘记在新路径加入抑制判断。 |

---

## ⚠️ 潜在风险

1. **向后兼容性**  
   - `nodes.py` 移除了 `cache_strategy` 输入；外部节点如果仍引用会报错。  
   - `SGLDOptions` 增加 `model_type`，但旧的工作流仍使用 `"auto-detect"`，在未能正确推断时可能加载错误的 executor。  
   - `ScheduleBatch` 新增 `vae_image_sizes`、`suppress_logs`，老版调用代码若手动构造 `Req`（单元测试或脚本）需要同步更新，否则会触发 `TypeError`。

2. **日志抑制误用**  
   - `suppress_logs` 默认 `False`，但在高频调用（如每 step 调度）未显式开启时仍会产生大量日志，可能导致磁盘 I/O 瓶颈。  
   - 逆向场景（调试）若误将其设为 `True`，将错失重要的时间戳/内存占用信息，排障困难。

3. **LoRA 路径安全**  
   - LoRA 名称直接拼接成 `nickname`（若为空则使用 UUID），但未对 `lora_name` 做完整的路径遍历检查，潜在 **路径穿越** 风险。  
   - 需要在 UI 层或插件层限制仅加载 `.safetensors`/`.bin` 并确保在 `loras` 子目录内部。

4. **多 GPU / SP 兼容**  
   - `maybe_unpad_latents` 对 3‑dim `raw_latent_shape` 作了简化处理，若出现 **2‑dim**（极端压缩）或 **5‑dim**（时序）形状，

---

### fix: move nightly whl to cuda version folder (#17762)
**SHA**: `51d139b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/51d139b867e7e4cb560cfa861d6040b89abd3dde)

**🎯 变更类型**：Bug修复 / 功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
1. 在 nightly 发布工作流中新增 `cuda_version` 输入，并默认使用 `129`。  
2. `scripts/update_nightly_whl_index.py` 改为在 `cu{version}` 目录下生成索引文件，支持将不同 CUDA 版本的 wheel 分离存放。  
3. 索引根页面自动保留已有 package（如 `sglang`、`sg-kernel`），并在根目录列出全部子目录。  

**🎯 影响范围**：  
- GitHub Actions 工作流 `.github/workflows/release-pypi-nightly.yml`  
- Nightly wheel 索引生成脚本 `scripts/update_nightly_whl_index.py`  
- 访问 nightly wheel 的 URL（从 `.../whl/nightly/` 迁移至 `.../whl/cu{version}/`）  
- 相关 CI/CD 文档与用户安装指令  

**🔍 技术洞察**  

- **架构影响**  
  - 将 wheel 按 CUDA 版本划分子目录，使得同一 repository 能同时托管多种 CUDA 构建，提升可扩展性。  
  - 索引根页面改为动态读取已有子目录，避免硬编码，符合 PEP 503 规范。  
  - 新增 `--cuda-version` 参数，使脚本可复用，后续可能支持更多 CUDA 版本而无需改动代码。  

- **性能影响**  
  - 索引生成仅在 CI 环境执行，额外的文件系统操作（创建子目录、读取根索引）开销极低，对整体构建时间影响可忽略不计。  
  - 对用户侧的 `pip install` 没有运行时性能变化，只是索引 URL 改变。  

- **安全考虑**  
  - 变更不涉及代码执行逻辑或依赖更新，未引入新的安全风险。  
  - 仍然通过 GitHub Releases 的公开 URL 提供 wheel，保持原有的访问控制模型。  

**⚠️ 潜在风险**  

1. **向后兼容性**：旧的安装指令仍指向 `.../whl/nightly/`，如果未同步更新文档或用户脚本，将导致 404 错误。  
2. **默认 CUDA 版本硬编码**：工作流默认 `129`，若在未来默认版本切换未同步到脚本默认值，可能出现索引路径不匹配。  
3. **索引根页面合并逻辑**：在读取已有根索引时仅匹配 `<a href="([^"]+)/">`，若根页面格式有细微变化（如加入属性），可能导致失效。  
4. **并发写入**：如果多个 CUDA 版本的 nightly 同时生成（如在同一 PR 中并行），可能产生竞争写入根 `index.html`，导致包列表不完整。  

**💡 关注建议**  

- **文档同步**：立即在 README、安装指南以及 CI 文档中更正 `pip install` 示例，指向 `https://sgl-project.github.io/whl/<cu_version>/`。  
- **兼容层**：考虑在 `nightly/` 目录下保留一个软链接或重定向页面，指向最新的 CUDA 版本目录，以兼容已有脚本。  
- **默认值统一**：将工作流默认值 `129` 与 Python 脚本默认值统一（可通过环境变量或统一配置文件），防止不一致。  
- **并发安全**：若未来会并行构建多 CUDA 版本，建议在写根 `index.html` 前加锁或采用 “读取‑‑‑写入” 原子操作（如使用 `tempfile` 再 `mv`）。  
- **监控验证**：在 CI 中加入一步检查：确认生成的索引 URL 能被 `pip install` 正常解析（例如使用 `pip install --dry-run`）。  

通过上述措施，可确保新目录结构平稳过渡并提升 nightly wheel 的可维护性与可扩展性。

---

#### 🟡 中重要度变更 (13)

### [diffusion] refactor: add arg to control the precision of dit (#17751)
**SHA**: `547e2d0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/547e2d037e560529b677e7f462f72a4875932b94)

**🎯 变更类型**：功能增强/重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 Diffusion CLI 文档中新增 `--dit-precision {DTYPE}` 参数，允许用户在 fp32、fp16、bf16 之间显式指定 DiT 模型的计算精度。  
2. 将原有的 `set_default_dtype` 上下文管理器迁移至 `sglang.multimodal_gen.runtime.loader.utils`，实现为 `set_default_torch_dtype`（使用 try/finally 确保恢复），并在 FSDP 与 ComfyUI pipeline 中统一使用。  
3. 相应删除了 `contextlib` 的导入以及冗余代码。

**🎯 影响范围**  
- CLI 文档 (`python/sglang/multimodal_gen/docs/cli.md`)  
- 运行时加载模块 (`fsdp_load.py`)  
- 工具函数库 (`utils.py`)  
- ComfyUI‑ZImage pipeline (`comfyui_zimage_pipeline.py`)  

**💡 关注建议**  
- **开发者**：确认 `--dit-precision` 参数在入口层（parser）已正确转为 `torch.dtype`，并在调用 `set_default_torch_dtype` 前进行合法性检查；新增单元测试覆盖不同 dtype 的加载路径，尤其是异常恢复场景。  
- **用户**：使用新参数时请确保硬件及后端（如 flash attention、xformers）支持所选精度，否则可能触发运行时错误。若不指定，仍采用原有默认 dtype。  
- **兼容性**：旧代码已移除 `set_default_dtype`，若外部项目仍直接 import，请改为 `set_default_torch_dtype`。  

整体改动提升了精度控制的可配置性，同时通过更加稳健的上下文管理器避免了异常情况下的 dtype 泄漏。

---

### [DeepSeek] Update tests and document for DeepSeek V3.2 NVFP4 checkpoint (#17657)
**SHA**: `1d942e4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1d942e4eef5e4ee5ba18f1b8fd134780c5465227)

**🎯 变更类型**：功能增强 / 测试覆盖  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在文档 `docs/basic_usage/deepseek_v32.md` 中新增 *NVFP4 checkpoint* 的启动说明，明确量化方式 `modelopt_fp4` 与 MoE runner 后端的要求。  
2. `test/srt/run_suite.py` 新增 `test_deepseek_v32_fp4_4gpu.py`，并在测试集合中加入相应条目，使 CI 能在 4‑GPU 环境下验证 DeepSeek‑V3.2‑NVFP4（FP4）模型的功能与 GSM8K few‑shot 评估。

**🎯 影响范围**  
- 文档子系统（用户使用指引）  
- SRT 测试框架（新增 4‑GPU FP4 测试）  
- 启动脚本/参数解析（`--quantization modelopt_fp4`、`--moe-runner-backend`、`--kv-cache-dtype fp8_e4m3` 等）  

**💡 关注建议**  
- **依赖检查**：确保 `flashinfer_trtllm`、`flashinfer_cutlass`、`flashinfer_cutedsl` 等后端在目标环境中可用，必要时在 `requirements.txt` 或 CI 环境说明中标注。  
- **CI 资源**：该测试需要 4 张 GPU 与较长的启动超时（1200 s），请在 CI 配置中使用 `skip-if` 或 `is_in_ci()` 判断，避免在无 GPU 环境下报错。  
- **代码路径一致性**：确认服务器启动代码已实现 `modelopt_fp4` 量化分支及 `kv-cache-dtype fp8_e4m3` 的兼容逻辑，防止文档与实际实现不符。  
- **测试稳健性**：当前 `self.assertGreater(metrics["accuracy"], 0.935)` 较为严格，建议在非 NVIDIA GPU 或不同驱动版本上容忍轻微波动，或将阈值写入可配置常量。  

整体来看，此次提交完善了 DeepSeek‑V3.2‑NVFP4 的使用文档并加入端到端的功能验证，对用户和开发者都是有价值的补充。确保依赖与 CI 环境匹配后即可安全合并。

---

### [NPU] Adapt cann 8.5: use sfa and lightning indexer op from cann and CI update (#17615)
**SHA**: `d578b41` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d578b41badc8f7ed48299841662b86f1f3168744)

**🎯 变更类型**：功能增强 / 依赖升级  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 将 NPU CI 与 Docker 环境的 CANN 版本从 8.3 升级至 8.5，并同步更新镜像标签、依赖 URL。  
- 通过 `torch_npu` 替换原先的 `custom.ops`（`npu_sparse_flash_attention`、`npu_lightning_indexer`），并在调用时加入 `attention_mode`、`return_softmax_lse` 等新参数。  
- 精简 CI 脚本：去除 BiSheng、CustomOps 的单独安装，改为直接 pip 安装 `triton-ascend`；统一使用 `unzip` 解压预编译好的 `sgl‑kernel‑npu` 包。  
- Dockerfile、CI 工作流、依赖脚本同步更新，版本号、TAG 也同步至 2026.01.21，MemFabric 版本升级至 1.0.5。

**🎯 影响范围**  
- **NPU 运行时**：`python/sglang/srt/hardware_backend/npu/*`（attention、utils）以及 `nsa_indexer.py`。  
- **CI/CD**：`.github/workflows/*-npu.yml`、`scripts/ci/npu/npu_ci_install_dependency.sh`。  
- **Docker 镜像**：`docker/npu.Dockerfile`、`release-docker-npu*.yml`。  
- **依赖版本**：torch‑npu、triton‑ascend、memfabric、sgl‑kernel‑npu。

**💡 关注建议**  
1. **接口兼容性**：`torch_npu.npu_sparse_flash_attention` 与旧 `custom.npu_sparse_flash_attention` 参数略有差异，确保所有调用都加上 `attention_mode=2`、`return_softmax_lse=False`，防止运行时错误。  
2. **模型推理正确性**：注意 `torch_npu.npu_sparse_flash_attention` 返回 3 元组（output, softmax_lse, something），当前代码仅取第一个，建议在单元测试中加入软max LSE 的验证，以防精度回退。  
3. **CI 稳定性**：新版镜像已去除自定义 compiled ops，确认内部 `triton-ascend` wheel 与 CANN 8.5 完全匹配；如出现 “op not found” 报错，需检查 `torch_npu` 与 `triton-ascend` 的兼容矩阵。  
4. **文档与示例**：更新 README/CI 指南，说明 NPU 环境现在直接使用 `torch_npu`，不再需要手动安装 CustomOps、BiSheng。  
5. **回退路径**：在 CI 中保留对旧镜像（8.3）的一键切换方式，防止新镜像在部分硬件（如旧版 910）上不兼容导致测试全线失效。  

总体来看，此次升级提升了对最新 Ascend CANN 的适配度，简化了依赖链，但需在测试与文档层面确认新算子签名与行为的一致性，防止因参数变化导致的功能回退。

---

### fix(quantization): add sgl_kernel fallback for FP4 quantize on Blackwell GPUs (#17816)
**SHA**: `c56d19b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c56d19b9779cc5e4b613e9fee5cb1f13fe5f1a0b)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `modelopt_quant.py` 中为 FP4 量化增加了后备实现。当在 Blackwell GPU（SM‑12x）上 `flashinfer` 的 `fp4_quantize` 不可用时，改为从 `sgl_kernel.scaled_fp4_quant` 导入并使用该实现，避免 ImportError 导致模型加载或推理失败。  

**🎯 影响范围**：  
- `python/sglang/srt/layers/quantization/modelopt_quant.py`（量化路径）  
- 依赖该模块的模型压缩、推理入口（如 `sglang.srt`）  
- 相关的测试脚本和 CI（GPU 环境）  

**💡 关注建议**：  
1. **功能验证**：在没有 `flashinfer`（或仅在 Blackwell GPU 上缺失）且已装 `sgl_kernel` 的环境下跑一次 FP4 量化的完整路径，确认 `fp4_quantize` 被正确指向 `scaled_fp4_quant`，并检查输出数值与原实现一致。  
2. **兼容性检查**：确保 `sgl_kernel` 版本与项目要求匹配，若未来 `flashinfer` 恢复兼容，也不会因仍使用旧实现而产生性能回退。可以在 `__main__` 或单元测试里打印实际使用的实现以便调试。  
3. **异常处理**：当前在 `is_sm120_supported()` 为真且 `flashinfer` 导入失败时会直接切走 fallback，这里已用 `try/except` 包裹，建议在日志中加入提示信息（如 `logger.warning("flashinfer fp4_quantize unavailable, fallback to sgl_kernel")`），帮助使用者定位问题。  
4. **性能评估**：`sgl_kernel.scaled_fp4_quant` 与 `flashinfer.fp4_quantize` 的实现细节可能不同，建议在关键模型（如 LLaMA‑2‑70B）上对比量化时间和后处理精度，确认不会出现显著性能退化。  

总体来说，此改动提升了在新一代 Blackwell GPU 上的可用性，风险主要在于后备实现的兼容性和性能差异，建议通过上述测试进一步确认。

---

### [PP] fix wrong weight logic for tie_word_embeddings model (#15890)
**SHA**: `dba264a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/dba264ac73381c2c7a6a60a0e52d967c308b7aae)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：修正了 PP（pipeline parallel）场景下 `tie_word_embeddings` 模型的权重绑定逻辑。原实现先在各 rank 之间显式 `send/recv` 共享 `embed_tokens.weight`，随后在加载权重时再次尝试替换 `lm_head.weight`，导致权重未正确同步或被二次覆盖。此次改为在 `load_weights` 时直接把 `model.embed_tokens.weight` 写入 `lm_head.weight`（仅在最后一个 rank 且 `config.tie_word_embeddings=True` 时），并移除冗余的 PP 通信代码。

**🎯 影响范围**  
- `python/sglang/srt/models/qwen2.py`  
- `python/sglang/srt/models/qwen3.py`  
- 依赖这些模型的分布式推理路径（pipeline parallel）以及开启词向量‑输出层权重共享的配置。

**💡 关注建议**  
1. **功能验证**：在单机、单卡以及多卡（world_size>1）下分别跑一次全模型权重加载与一次前向推理，确保 `lm_head.weight` 与 `embed_tokens.weight` 完全相同。  
2. **兼容性检查**：`self.lm_head` 在非 PP 场景仍为占位层，确认此时 `load_weights` 不会误触 `if self.pp_group.is_last_rank` 条件。  
3. **异常处理**：如果 `lm_head.weight` 未在 `params_dict` 中出现（如模型结构变动），当前代码会直接跳过，可考虑加入日志提醒。  
4. **性能测评**：移除跨 rank 的 `send/recv` 后，启动时间和显存使用应有所下降，建议对比基准。  
5. **单元测试**：新增针对 `tie_word_embeddings=True`、PP 多卡以及非 PP 环境的权重同步测试，防止回归。  

总体来说，此次改动恢复了权重共享的正确行为，风险主要集中在未覆盖的特殊模型变体上，建议通过上述测试确认后再上线。

---

### [Model] Add support for EXAONE-4.0 Model (#8205)
**SHA**: `81c0f5c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/81c0f5c5adf0a66e85a908162f770c515a97ade6)

**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增 `Exaone4ForCausalLM` 完整实现，包括 Encoder‑Decoder、注意力、Gated‑MLP、RMSNorm、词表嵌入、LM‑Head、Logits 处理等，支持量化、并行（TP/PP/DP）以及滑动窗口注意力。  
- `server_args.py` 中对 `Exaone4ForCausalLM` 增加模型特化处理：若配置 `sliding_window_pattern`，强制使用 `fa3`、`triton` 或 `trtllm_mha` 注意力后端，并关闭混合 SWA 内存。  

**🎯 影响范围**  
- **模型注册**：`python/sglang/srt/models/exaone4.py` → 影响模型加载、权重映射、推理路径。  
- **分布式/并行层**：`distributed.py`、`layers/*`（注意力、线性、norm、embedding）被新模型调用。  
- **服务器启动参数**：`server_args.py` 的模型特化逻辑会影响启动时对 Exaone‑4 的默认后端选择和内存策略。  
- **权重加载**：新增对 QKV、gate_up 参数的分片映射及 FP8 scale 重映射，影响检查点兼容性。  

**💡 关注建议**  
1. **后端兼容性**：在部署前确认已编译并加载 `fa3`/`triton`/`trtllm_mha` 任一后端，否则启动会因断言失败。  
2. **滑动窗口配置**：`sliding_window_pattern` 必须与模型层数匹配；若未提供或配置错误，模型会回退到普通注意力，可能导致性能下降。  
3. **量化/并行**：核查 `QuantizationConfig` 与 `bitsandbytes` 参数映射是否与实际 checkpoint 对齐，尤其是 QKV、gate_up 的分片。  
4. **权重加载日志**：权重未匹配时会产生 warning，建议在 CI 中添加对 `Exaone4` checkpoint 的完整加载测试。  
5. **混合 SWA**：已强制关闭，如后续支持需同步更新 `server_args` 中的检查逻辑。  

整体来看，新增模型实现较为完整，但对依赖后端和 sliding‑window 参数的要求更严格，建议在多节点/多 GPU 环境下进行一次端到端的推理回归验证。

---

### [Bug Fix] Fix reasoning parser when continue_final_message=true (#17065)
**SHA**: `6c9b054` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6c9b054ab70faad77a5e0f014e46dbf3dca6953d)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**核心改动**  
- 为 `ReasoningParser` 新增 `request` 参数，并在构造时解析 `continue_final_message` 标记及其前置内容。  
- 各推理检测器（DeepSeek、Qwen3、Kimi、GPT‑OSS、MiniMax、NanoV3）改造为接受 `continue_final_message`、`previous_content`，并在 `detect_and_parse` 中兼容跨块的 `</think>`。  
- 调整 OpenAI/HTTP 接口层，在调用解析器时传入 `request`，确保在流式或非流式返回时均能使用该特性。  

**影响范围**  
- `python/sglang/srt/parser/reasoning_parser.py` 与所有检测器实现。  
- `sglang/srt/entrypoints/openai/*` 与 `http_server.py` 的调用路径。  

**关注建议**  
1. **向后兼容**：默认值保持原行为，确认未传 `request` 时仍能正常工作。  
2. **类型安全**：`request` 采用 `ChatCompletionRequest`，建议在模块顶部统一导入，避免循环依赖。  
3. **测试覆盖**：补充含 `continue_final_message=True`、assistant‑role 最后一条消息以及跨块 `</think>` 的单元/集成测试。  
4. **文档更新**：说明该 flag 的使用场景及对 `messages[-1]` 内容的要求。  
5. **性能评估**：新增字符串拼接与计数开销极小，建议在高并发路径做一次基准验证。  

总体改动方向明确，解决了推理结束标记被截断导致的解析错误；只要做好兼容验证和测试，即可安全上线。

---

### [bugfix] Internal processing of hf3fs crash # 16614 (#16938)
**SHA**: `2d8c22a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2d8c22a15ee84272516a7d6b798a7d698757cafa)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `hf3fs_usrbio_client` 中的 `batch_read` / `batch_write` 添加了异常捕获与日志记录，避免因 I/O 准备或提交错误导致进程崩溃，并在出错时返回全 0 的结果。  
- 在 `storage_hf3fs` 的 `_batch_get` 与 `_batch_set` 中加入对返回页索引长度不匹配的检查，记录错误并在写入失败时回滚已预分配的页。  

**🎯 影响范围**  
- `python/sglang/srt/mem_cache/storage/hf3fs/hf3fs_usrbio_client.py`（批量读写路径）  
- `python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py`（缓存的批量获取/写入逻辑）  

**💡 关注建议**  

1. **异常处理策略**  
   - 当前在出错时返回全 0 结果，调用方需要自行判断并可能重新尝试。建议在文档中明确该行为或考虑抛出自定义异常，以防上层误把全 0 当作成功返回。  

2. **资源回收**  
   - `_batch_set` 在索引长度不匹配时已调用 `confirm_write` 回收已分配页，确保不会产生泄漏。请检查其它路径（如异常发生前的 `reserve_and_allocate_page_indices`）是否也需要相同的回滚。  

3. **日志可观测性**  
   - 日志使用 `logger.error`，已包含关键上下文（rank、长度信息），但建议加入 `exc_info=True`（已在 `read_shm` 处使用），统一在所有异常捕获处记录堆栈，便于调试。  

4. **线程安全**  
   - 读写方法分别使用 `@rsynchronized` / `@wsynchronized` 装饰器，保持了原有同步语义。新增的异常返回路径不会破坏锁的使用。  

5. **单元测试**  
   - 增加针对异常路径的测试：准备阶段异常、提交阶段超时、`page_indices` 长度不匹配等，确保返回值与日志符合预期。  

6. **性能考虑**  
   - 额外的异常捕获对正常路径影响微乎其微，可接受。若在高并发场景下频繁触发异常，返回全 0 可能导致上层大量重试，建议监控异常率并在运行时适当降级或报警。  

**总结**：本次修改通过防御性编程显著提升了 `hf3fs` 存储层的稳健性，避免了因底层 I/O 错误导致的进程崩溃。后续关注异常回滚的一致性、日志完整性以及相应的异常测试即可完成迁移。祝开发顺利！

---

### fix(processor): support InternS1 text_config in InternVL processor (#17040)
**SHA**: `5399240` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/539924037fbc6b64a75c554ed338cb8c5abe5343)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `InternVL` 处理器加入对 `InternS1` 配置的兼容。  
- 在读取文本模型配置时，先尝试 `hf_config.llm_config`，若为空则回退到 `hf_config.text_config`，再不行则直接使用根对象。  
- 相应地统一 `architectures` 与 `max_position_embeddings` 的获取逻辑，避免原先仅访问 `llm_config` 时出现 `AttributeError`。  

**🎯 影响范围**  
- `python/sglang/srt/multimodal/processors/internvl.py`（处理器初始化）。  
- 任何依赖该处理器的多模态模型加载路径，尤其是使用 InternS1 或者架构信息放在顶层的模型。  

**💡 关注建议**  
- **测试**：在本地或 CI 中加入对 InternS1（`text_config`）模型的加载验证，确认 token、上下文长度等仍保持预期。  
- **兼容性**：`llm_arch` 现在可能为 `None`，调用方若依据该值做分支判断，需要增加 `None` 检查。  
- **文档**：更新 README/注释，说明该处理器已支持两种配置字段，以免用户仍按旧方式提供配置导致困惑。  
- **回退**：`max_position_embeddings` 的获取已经统一为 `text_cfg`，请确认在没有该字段时仍能正确回落到 `CONTEXT_FALLBACK`。  

总体来看，此修复提升了多模态模型的适配性，对现有功能无显著影响，只需关注新增的 `None` 场景和相应的单元测试即可。

---

### [Docs] Add RL documentation (#17663)
**SHA**: `dd97e1f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/dd97e1fe3866d77918d62ea3f31aca7add11d1cf)

**📝 变更概述**  
本次 PR 主要为 SGLang 添加 RL（Reinforcement Learning）相关的文档，包括在 `README.md`、`docs/index.rst` 中加入 “RL & Post‑Training Backbone” 概述，以及全新的 `docs/advanced_features/sglang_for_rl.md`，详细说明内存释放/恢复、权重更新（磁盘、tensor、分布式）、暂停/继续生成、确定性推理和负载均衡路由等 API 与使用场景。

**🔧 影响范围**  
- 文档体系（README、Sphinx 索引、单独的 RL 手册）  
- 对外宣传信息（在首页和索引中加入 RL 背景）  
- 与代码层面无直接修改，但文档中涉及的 endpoint（如 `POST /release_memory_occupation`、`/update_weights_from_disk` 等）需要在已有实现中保持一致。

**⚠️ 潜在风险**  
1. **链接/引用失效**：文档中引用的内部路径（`python/sglang/srt/managers/io_struct.py#L1381`）若文件结构调整会导致死链。  
2. **Sphinx 构建错误**：新增的 rst 条目缩进或标题层级不符合 Sphinx 规范，可能导致文档构建失败。  
3. **内容与实现不匹配**：若实际服务器未实现某些 API（如 `pause_generation`、`init_weights_update_group`），使用者会遇到 “endpoint not found” 的错误。  
4. **术语统一性**：RL 文档使用的标记（`abort`, `retract`, `in_place`）需要在代码注释或 OpenAPI 文档中保持一致，避免混淆。

**💡 建议**  
- **CI 检查**：在 CI 中加入 `make html`（或等价的 Sphinx 构建）步骤，确保所有新增 rst 能成功渲染。  
- **链接校验**：使用工具（如 `repo-health`、`mkdocs‑link‑check`）自动检测文档内部链接的有效性。  
- **API 对齐**：对照代码库的 `engine` 实现，确认文档列出的所有 REST endpoint 均已实现，并在正文中加入对应的源码链接或示例。  
- **版本化说明**：在 Release Notes 中标记 “新增 RL 文档”，方便使用者快速定位。  
- **语言统一**：保持中英文表述一致，尤其是关键术语（如 “deterministic inference”、 “load‑balancing router”）在其他文档和示例代码中保持相同翻译。  

总体来说，此次改动提升了 SGLang 在 RL 场景的可见度和使用指南，对代码功能无直接影响，只需确保文档与实现保持同步、构建通过即可。

---

### Introduce global `alloc_len_per_decode` & clean check decode memory (#15115)
**SHA**: `85d077f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/85d077f44dfe93c82fbaa49318503296a2190a55)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 引入全局函数 `get_alloc_len_per_decode`，统一各处对 **decode‑阶段预分配长度** 的计算，去除原来在 `EagleDraftInput` 中的类常量 `ALLOC_LEN_PER_DECODE`。  
2. 精简并统一内存检查逻辑：  
   - `ScheduleBatch.new_page_count_next_decode` 拆分为 `new_tokens_required_next_decode`，只返回实际需要的 token 数；  
   - `check_decode_mem` 直接使用 `token_to_kv_pool_allocator.available_size()`，不再区分 hybrid‑SWA 与普通 allocator。  
3. 删除 `self.decode_mem_cache_buf_multiplier`（在 `Scheduler.init_cache_with_memory_pool` 中），所有内存裕度的计算改为基于 `get_alloc_len_per_decode`。  
4. 相关调用处同步改写（`eagle_worker`, `eagle_worker_v2`, `eagle_info` 等），并对 `SWATokenToKVPoolAllocator.available_size` 实现返回两者最小值。  

**🎯 影响范围**  
- `python/sglang/srt/managers/*`（`schedule_batch.py`, `scheduler.py`, `utils.py`）  
- `python/sglang/srt/speculative/*`（`eagle_info.py`, `eagle_info_v2.py`, `eagle_worker.py`, `eagle_worker_v2.py`）  
- 内存池实现 `srt/mem_cache/swa_memory_pool.py` 与通用 `mem_cache/common.py`  
- `decode.py` 中的可分配 token 计算  

**💡 关注建议**  
1. **兼容性检查**：新函数默认 `page_size==1` 时仍可用，若后续支持 `page_size>1` 需要在 `get_alloc_len_per_decode` 中实现对应的对齐逻辑，否则会抛 `NotImplementedError`。  
2. **Hybrid‑SWA 迁移**：虽然 `available_size` 已兼容混合 allocator，但旧的 `full_available_size`/`swa_available_size` 接口仍保留，确保其它未同步代码不会因属性缺失崩溃。  
3. **测试覆盖**：重点验证 speculative‑v1 与 v2 两种路径在高并发 decode 场景下的内存回收/预分配是否仍保持原有的安全边界，尤其是 `retract_decode` 循环的终止条件。  
4. **文档与配置**：在 README/参数说明中补充 `alloc_len_per_decode` 的计算公式，以及对 `page_size>1` 暂不支持的声明，防止用户误解。  

总体而言，此次改动统一了 decode 预分配的计算方式，简化了内存检查路径，提升了代码可维护性。只要通过上述兼容性与回归测试，即可安全合并。

---

### fix: remove truncation for test and job names in ci failure monitor (#17765)
**SHA**: `8643fb2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8643fb2f525877de973db6188ddaff18fa57fc25)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `scripts/ci_monitor/ci_failures_analysis.py` 中，删除了对 CI 失效报告中 **test 名称** 与 **job 名称** 的字符截断逻辑，改为直接使用完整字符串。此前的实现会把超过 30/25 个字符的名称强制截断为 `…`，导致在实际排查问题时看不到完整路径或 job 标识。  

**🎯 影响范围**  
- `scripts/ci_monitor/ci_failures_analysis.py`（Markdown 报告生成）  
- 可能间接影响 CI 监控平台的 UI/邮件展示（行宽、表格对齐）  

**💡 关注建议**  

1. **报告可读性**  
   - 虽然展示完整名称有助于定位，但极长的路径或 job 名称仍可能导致 Markdown 表格列宽失衡、换行或导致邮件客户端渲染异常。建议在上层（如 HTML 邮件模板或 CI Dashboard）再做一次视觉截断或加入 CSS/Markdown 换行符，以保持布局。  

2. **安全/转义**  
   - 完整名称可能包含 Markdown 保留字符（`|`, `` ` ``, `*` 等），目前没有统一转义处理。建议在插入 `test_display`、`job_display` 前使用 `markdown_escape()`（或手动替换）防止表格错位或注入。  

3. **向后兼容与测试**  
   - 变更仅影响展示层，业务逻辑未改动。仍建议在项目的 CI 中添加或更新单元测试，验证生成的 Markdown 在包含极长名称的情况下仍然符合预期（表头对齐、链接正确）。  

4. **文档与注释**  
   - 代码中之前有 “Truncate names” 注释已被删除，建议在新代码块添加注释说明 **为何保留完整名称**（为了解决调试信息缺失），并提醒后续维护者注意潜在的布局问题。  

5. **性能影响**  
   - 去掉字符串切片几乎没有性能影响，且提升了可读性，确认无额外依赖或资源开销。  

**总结**：此次修改解决了因名称被截断导致的排查困难，是一次合理的功能修正。但请在后续的报告渲染层面考虑对超长字符串的视觉处理与 Markdown 转义，以防止展示异常。保持相应的测试覆盖，确保改动不影响 CI 监控系统的整体可用性。

---

### [NPU] Split pyproject npu from pyproject other (#17641)
**SHA**: `bba6e38` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bba6e38ff87cb13b16a94bb5b2a0e56a198b536f)

**🎯 变更类型**：功能增强（NPU 平台专属依赖拆分）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `python/pyproject_npu.toml`，将原 `pyproject_other.toml` 中的 NPU 相关依赖抽离为独立配置，并提供 `all_npu、dev_npu` 可选项。  
2. CI 与 Docker 镜像改为在构建时使用 `pyproject_npu.toml`，对应的 pip 安装标记从 `srt_npu` → `all_npu` / `dev_npu`。  
3. 工作流 `pr-test-npu.yml` 增加 `multimodal_gen` 过滤，确保多模态代码变更也触发 NPU 测试。  
4. 文档、release 脚本同步更新，说明如何在源码或 Docker 中切换 NPU 环境。

**🎯 影响范围**  
- `docker/npu.Dockerfile`、`scripts/ci/npu/*`（构建/依赖安装）。  
- CI 工作流 `pr-test-npu.yml`、`release-docker-npu-nightly.yml`。  
- `python/pyproject_*` 系列文件及对应的 `pip install -e` 入口。  
- 文档 `docs/platforms/ascend_npu.md` 与 release 版本脚本。

**💡 关注建议**  
1. **兼容性检查**：确保老用户在未更换 `pyproject.toml` 前仍能通过 `pyproject_other.toml` 正常安装；新增文件不应破坏现有 CPU/XPU CI。  
2. **依赖一致性**：`pyproject_npu.toml` 中的 `all_npu` 仅引用 `sglang[diffusion]`，如后续需要补充 NPU‑only 包（如 runtime_common）请同步更新。  
3. **CI 触发规则**：`pr-test-npu.yml` 现在会在 `multimodal_gen` 相关路径改动时也运行 NPU 测试，确认相关测试覆盖率足够，防止误报或超时。  
4. **文档与脚本同步**：发布新版本时，`bump_sglang_version.py` 已加入 `pyproject_npu.toml`，请在发布流程中验证该文件被正确打包进源码发行包。  
5. **本地调试**：开发者在本机使用 NPU 时，需要手动 `mv python/pyproject_npu.toml python/pyproject.toml` 或使用 `pip install -e python[dev_npu]`，建议在 README 中给出一步到位的脚本。  

总体来看，此次拆分提升了平台依赖的可维护性，避免了在非 NPU 环境中拉取不必要的包；但需确保 CI、文档和发布脚本的同步更新，以防出现版本不一致或安装错误。

---

#### 🟢 低重要度变更 (6)

### [Doc] Tiny update description on torch compile (#17819)
**SHA**: `832c756` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/832c7565494b72731fd8f91e5f1d7ff5e2c389a6)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_arguments.md` 中更新了 `torch.compile` 的说明，新增“不再维护”提示；调整了 fp8 kv‑cache 的 dtype 选项描述。

---

### [AMD] Update dsv3.2 AMD GPU docs and unify ROCm TileLang build (#17783)
**SHA**: `df42f4d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/df42f4d386d3958191b87930db0f7c6ca5ff1581)

**🎯 变更类型**：文档更新/配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：更新了 ROCm Dockerfile 与 DeepSeek‑V3.2 使用文档，统一 TileLang 构建流程并同步镜像标签至 v0.5.8，兼容 MI30x/MI35x GPU。

---

### [model-gateway] ignore error for embeddings/classify in PD router (#15931)
**SHA**: `a723d1c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a723d1c5ef1a551ac62d2d53566b10c994ad5653)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 PD 路由器中新增 `route_embeddings` 与 `route_classify` 实现，统一返回不支持的错误并记录警告，避免因缺少实现而产生未捕获异常。

---

### [chore]: improve time tracing of model loading process (#15426)
**SHA**: `1b56a88` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1b56a886bb92f3d6a753c6b252d5e4f5872a7e3a)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在模型加载流程中新增时间统计，记录 torch distributed、模型权重加载、注意力后端初始化等关键步骤的耗时，并在日志中输出，便于性能分析。

---

### [CI] Skip PD hybrid attention test with different TP temporarily (#17791)
**SHA**: `3ad3268` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3ad3268e0634e85ead82a2d0dd5c52f66ed85e08)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 `test_disaggregation_hybrid_attention.py` 中的混合注意力测试添加 `@unittest.skipIf(is_in_ci(), ...)`，在 CI 环境下暂时跳过该不稳定 flaky 测试。

---

### Special logic for healthcheck (#17734)
**SHA**: `5ab76ff` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5ab76ff2202b0c4740511069126cff707692e09f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `detokenizer_manager.py` 中新增 `is_health_check_request` 方法，实现对健康检查请求的专门处理：不记录解码状态、直接构造 `DecodeStatus`、并在完成后安全删除对应状态，避免因健康检查导致的状态异常。  

---

