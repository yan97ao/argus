# 每日更新报告（2026-01-20）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-20 23:59:05 | GuoweiWangU | [FIX] fix mambaish model pp kv cache compute (#17334) |
| 2026-01-20 23:56:21 | Ke Bao | Use attn_tp_group for all reduce in token embedding (#17403) |
| 2026-01-20 23:55:00 | Ke Bao | Disable swa memory for trtllm-mha backend (#17429) |
| 2026-01-20 23:44:16 | Qiaolin Yu | [New Model] GLM4.7-Flash (#17247) |
| 2026-01-20 23:09:44 | BingjiaWang | [OPT] DeepSeekV3.2: optimize indexer weight_proj-mma performance (#17205) |
| 2026-01-20 23:06:55 | Baizhou Zhang | Add flag that enables NCCL mlp sync batch for overlap scheduler (#17288) |
| 2026-01-20 21:22:27 | Ke Bao | Update default attn backend to fa3 for mimo model (#17419) |
| 2026-01-20 20:10:56 | Shangming Cai | [Doc] Update pipeline parallelism documentation (#17414) |
| 2026-01-20 20:08:28 | Julian Huang | [Fix]: correctly fetch ds32 config  in tuning_fused_moe_triton (#17409) |
| 2026-01-20 19:31:23 | amote-i | update docs of Ascend plateform (#17358) |
| 2026-01-20 18:47:08 | ympcMark | [3/N] Achieve fault tolerance at the DP level (#11657) |
| 2026-01-20 15:00:54 | Douglas Yang | fix: Release pr pypi fix (#17382) |
| 2026-01-20 14:29:24 | Yuan Luo | [Kimi-Linear] Refactor kimi-linear gate calculation to avoid duplicated code (#17160) |
| 2026-01-20 12:31:56 | Michael | [AMD] Add DeepSeek-V3.2 and VLMs model in nightly tests (#17179) |
| 2026-01-20 12:29:54 | Thomas Wang | Disable mla persistent kernel when not using fp8 kv_cache (#17327) |
| 2026-01-20 12:28:06 | Hexq0210 | [NPU] Update NPU doc for model and features supported (#17385) |
| 2026-01-20 12:03:33 | Ratish P | [diffusion] fix: enforce 16-pixel alignment for flux2 to prevent shape mismatch crash (#17302) |
| 2026-01-20 11:27:13 | Shangming Cai | [PD] Fix PP dynamic chunking with DP attention (#17339) |
| 2026-01-20 10:36:56 | shuwenn | feat: support bitsandbytes quantization algorithm (#15325) |
| 2026-01-20 10:33:17 | Raghav Ravishankar | fix function calling for Trinity (#17364) |
| 2026-01-20 10:31:46 | Alison Shao | Enable parallel stage execution for scheduled CI runs (#16880) |
| 2026-01-20 10:31:33 | Alison Shao | Fix runner utilization workflow to use 24h default (#17378) |
| 2026-01-20 10:22:30 | zijiexia | [Docs] Fix formatting in Evaluating New Models with SGLang (#17376) |
| 2026-01-20 09:52:30 | b8zhong | [Docker] Fix CUDA 13 installing wrong `nvidia-nccl-cu13` due to `nixl-cu13` not breaking system package (#17370) |
| 2026-01-20 08:35:03 | zijiexia | [Docs] Add new model evaluation docs (#17043) |
| 2026-01-20 07:48:49 | Aurick Qiao | Pipe customized_info through CudaGraphRunner output (#17088) |
| 2026-01-20 06:36:19 | Alison Shao | Move test_autoround.py to stage-b-test-large-1-gpu suite (#17336) |
| 2026-01-20 06:35:58 | Alison Shao | Disable unit-test-backend-4-gpu-gb200 job (#17367) |
| 2026-01-20 05:23:11 | Douglas Yang | fix: updating pypi workflow with new base version formatting in pyproject.toml (#17366) |
| 2026-01-20 05:04:15 | YC Tseng | [AMD] fix perf ci errors (#17363) |
| 2026-01-20 02:00:29 | Hudson Xing | fix(ci): apply MMMU retry logic to all affected test files (#17329) |
| 2026-01-20 02:00:00 | shuwenn | [CI] fix test_vlm_models.py (#17049) |
| 2026-01-20 00:16:07 | YC Tseng | [AMD] CI - add partitions for stage-b-test-small-1-gpu-amd (#17345) |
| 2026-01-20 00:07:39 | Bingxu Chen | [AMD CI] Migrate and Add More Testcases (#17116) |

### 📊 统计摘要
> 本日共 34 个提交 | 🔴高 4 | 🟡中 12 | 🟢低 18
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (4)](#-🔴-高重要度变更-4)
    - [[New Model] GLM4.7-Flash (#17247)](#76b06be)
    - [[AMD] Add DeepSeek-V3.2 and VLMs model in nightly tests (...](#a3addd6)
    - [feat: support bitsandbytes quantization algorithm (#15325)](#8fb4552)
    - [fix(ci): apply MMMU retry logic to all affected test file...](#c1282da)
  - [🟡 中重要度变更 (12)](#-🟡-中重要度变更-12)
    - [Use attn_tp_group for all reduce in token embedding (#17403)](#d97066d)
    - [Add flag that enables NCCL mlp sync batch for overlap sch...](#55c6164)
    - [update docs of Ascend plateform (#17358)](#603f386)
    - [[3/N] Achieve fault tolerance at the DP level (#11657)](#f7a5e42)
    - [fix: Release pr pypi fix (#17382)](#d50dcd9)
    - [[Kimi-Linear] Refactor kimi-linear gate calculation to av...](#e6b7c04)
    - [[NPU] Update NPU doc for model and features supported (#1...](#1b192cf)
    - [Enable parallel stage execution for scheduled CI runs (#1...](#17c04b1)
    - [Disable unit-test-backend-4-gpu-gb200 job (#17367)](#057b07f)
    - [fix: updating pypi workflow with new base version formatt...](#91d8c52)
    - [[AMD] CI - add partitions for stage-b-test-small-1-gpu-am...](#1a053a8)
    - [[AMD CI] Migrate and Add More Testcases (#17116)](#2ea02f0)
  - [🟢 低重要度变更 (18)](#-🟢-低重要度变更-18)
    - [[FIX] fix mambaish model pp kv cache compute (#17334)](#16802fb)
    - [Disable swa memory for trtllm-mha backend (#17429)](#ce2d686)
    - [[OPT] DeepSeekV3.2: optimize indexer weight_proj-mma perf...](#612026a)
    - [Update default attn backend to fa3 for mimo model (#17419)](#91a4cd8)
    - [[Doc] Update pipeline parallelism documentation (#17414)](#23d765d)
    - [[Fix]: correctly fetch ds32 config  in tuning_fused_moe_t...](#db2425a)
    - [Disable mla persistent kernel when not using fp8 kv_cache...](#6988a0f)
    - [[diffusion] fix: enforce 16-pixel alignment for flux2 to ...](#c560e14)
    - [[PD] Fix PP dynamic chunking with DP attention (#17339)](#71cb9d0)
    - [fix function calling for Trinity (#17364)](#84aef37)
    - [Fix runner utilization workflow to use 24h default (#17378)](#55c4288)
    - [[Docs] Fix formatting in Evaluating New Models with SGLan...](#9f8b79f)
    - [[Docker] Fix CUDA 13 installing wrong `nvidia-nccl-cu13` ...](#7dc3cbe)
    - [[Docs] Add new model evaluation docs (#17043)](#79ddc34)
    - [Pipe customized_info through CudaGraphRunner output (#17088)](#09a9d21)
    - [Move test_autoround.py to stage-b-test-large-1-gpu suite ...](#7e40d52)
    - [[AMD] fix perf ci errors (#17363)](#e9a44ea)
    - [[CI] fix test_vlm_models.py (#17049)](#71279e3)
#### 🔴 高重要度变更 (4)

### [New Model] GLM4.7-Flash (#17247)
**SHA**: `76b06be` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/76b06bee03e8d5e5fbd57dfbdbc80688705988ac)

**🎯 变更类型**：功能增强（新增 GLM‑4.7‑Flash 模型及其轻量化实现）  

**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 在 `model_config.py` 中为草稿模型加入对 `Glm4MoeLiteForCausalLM` 的兼容处理，统一了模型架构列表与形状推导逻辑。  
- 新增完整的 **GLM‑4 Lite** 推理实现（`glm4_moe_lite.py`），包括轻量化 MLP、稀疏 MoE Block、DecoderLayer、Model、ForCausalLM，全部基于 SGLang 已有的 DeepSeek‑V2 组件复用。  
- 调整了 OpenAI API 入口、注意力前向逻辑以及服务器参数，使新的模型能够参与 Speculative Decoding、DP‑Attention、共享专家融合等特性。  

**🎯 影响范围**  
- **核心模型配置**：`sglang/srt/configs/model_config.py`  
- **模型实现**：`sglang/srt/models/glm4_moe_lite.py`（新增 800 行）以及关联的注意力/层通信模块。  
- **服务端启动与推理路径**：`sglang/srt/server_args.py`、`sglang/srt/entrypoints/openai/serving_chat.py`、`sglang/srt/models/deepseek_common/attention_forward_methods/forward_mha.py`。  
- **公共工具**：`sglang/srt/utils.py`（新增 `next_power_of_2`）以及原有的 `BumpAllocator` 使用。  

**🔍 技术洞察**  

- **架构影响**  
  - **模块化复用**：GLM‑4 Lite 继承了 DeepSeek‑V2 的 DecoderLayer、AttentionMLA、MoE 基类，保持了 SGLang 框架的统一抽象，降低了维护成本。  
  - **模型注册**：在 `ServerArgs._handle_model_specific_adjustments`、`_handle_speculative_decoding`、`auto_choose_speculative_params` 中加入新架构标识，实现了与现有特性（如 Flash‑Infer、Speculative Decoding、共享专家融合）的无缝对接。  
  - **稀疏‑MoE 统一**：通过 `Glm4MoeLiteSparseMoeBlock` 与 `Glm4MoeLiteMLP` 的组合，保持了稀疏专家路由逻辑，同时提供了 “lite” 版的全模型并行/专家并行兼容路径。  

- **性能影响**  
  - **Rope Scaling 关闭**：针对 `Glm4MoeLite` 直接把 `self.scaling` 设为 1，避免了 Yarn‑Rope 计算开销；对该模型的推理 latency 有明显下降（尤其在长上下文场景）。  
  - **Power‑of‑2 检查**：在 MHA 复制路径加入 `next_power_of_2` 判断，仅在 head/维度为 2 的幂时走 FA3 后端的 fp8 路径，提升了 FP8/FP16 混合精度下的吞吐。  
  - **共享专家融合**：`determine_num_fused_shared_experts` 中的条件判断确保在满足 CUDA SM≥90、显存足够时启用共享专家 fused 计算，显著降低跨卡 all‑reduce 开销。  
  - **内存分配**：对 small‑batch、uint8 权重的 `gemm_output_zero_allocator` 进行提前分配，降低了短序列推理时的 CUDA malloc 开销。  

- **安全考虑**  
  - 代码路径没有引入外部依赖或网络交互，仅在本地模型加载和推理阶段执行。  
  - 新增的 `return_dict=False` 参数在 Jinja 模板渲染中防止意外返回大对象，略微降低了潜在的 DoS 向量。  
  - 由于模型权重加载逻辑变得更复杂，仍需确保 **权重验证**（文件哈希、签名）在上层 CLI 中完成，防止恶意修改的 checkpoint 被误装载。  

**⚠️ 潜在风险**  
1. **权重映射错误**：`load_weights` 中对 fused shared experts 的重命名与 expert 参数映射逻辑相当复杂，稍有命名不匹配就会导致权重 silently‑skip，进而出现推理质量下降。  
2. **TP/EP 不匹配**：在专家并行 (EP) 与张量并行 (TP) 同时开启时，`Glm4MoeLiteSparseMoeBlock` 对 `tp_size > n_routed_experts` 抛异常；若用户误配参数会导致服务启动失败。  
3. **Rope‑Scaling 失效**：对 Lite 模型强制关闭 rope_scaling，若未来出现需要 RoPE 扩展的需求，需要手动恢复对应代码。  
4. **CUDA SM 检查**：`next_power_of_2` 判断仅在 head/维度为 2 的幂时使用 FA3，若硬件升级但维度不满足幂次，仍会回退到老实现，可能出现性能回退。  
5. **新增依赖**：`sglang/srt/utils.py` 中的 `next_power_of_2` 若在旧版本 SGLang 中缺失，外部插件或自定义编译可能出现 import 错误。  

**💡 关注建议**  
- **测试覆盖**：在 CI 中加入 GLM‑4 Lite 的权重加载、模型并行、专家并行、共享专家融合三维度的矩阵对比测试，确保权重不被遗漏。  
- **参数校验**：在 `ServerArgs` 启动阶段添加对 `tensor_model_parallel_size` 与 `n_routed_experts` 的兼容性检查，给出友好错误提示。  
- **文档更新**：在模型列表说明中标明 “Lite” 版不支持 RoPE‑Scaling，且仅在 SM≥90 且显存 ≥ XX GB 时才能开启共享专家融合。  
- **监控报警**：对模型加载阶段的 `weight_names` 长度与预期进行校验，若缺失关键权重（如 `gate_up_proj.weight`），记录警告并可回退到标准 GLM‑4 Moe 实现。  
- **回滚策略**：提供 `--disable-glm4lite` 启动开关，以便在发现兼容性或性能问题时快速回退至旧模型实现。  

---  
此次合并在功能层面显著扩展了 SGLang 对最新 GLM‑4 系列的支持，同时在架构复用与性能优化上做了多项精细调度。若配合充分的测试与配置校验，可安全地在生产环境投入使用。

---

### [AMD] Add DeepSeek-V3.2 and VLMs model in nightly tests (#17179)
**SHA**: `a3addd6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a3addd6203cab13d1b61e7df555722104d03591c)

**🎯 变更类型**：功能增强 / 性能优化  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 为 AMD nightly CI 引入全新模型（DeepSeek‑V3.2、VLM 系列）及对应的 **准确性** 与 **性能** 测试，新增 2‑GPU 文本、VLM 性能套件，并对 MI35x 8‑GPU 场景加入 DeepSeek‑V3.2 基础版与 MTP 变体的基准。  
- 扩展 CI workflow，添加 4 条新 job（文本/视觉‑2GPU 性能、DeepSeek‑V3.2 准确性/性能），并在 `amd_ci_exec.sh` 中加入针对 HuggingFace 下载超时的智能重试（仅在非测试错误时触发离线模式）。  
- 精简报告生成逻辑：自动剔除 warm‑up（重复 batch‑size）结果，统一 markdown 样式，减少噪声。  

**🎯 影响范围**  
- `.github/workflows/nightly-test-amd.yml`（CI job 增加、依赖矩阵更新）  
- `scripts/ci/amd_ci_exec.sh`（HF 下载重试逻辑）  
- `test/registered/amd/*`（accuracy、perf、VLM、text‑model 测试套件）  
- 相关的模型配置文件、环境变量（`SGLANG_USE_AITER`、`HF_HUB_OFFLINE`、`GPU_CONFIG`）  
- CI 资源占用（GPU 2/8、容器 start‑up 与模型缓存）  

**🔍 技术洞察**  
- **架构影响**：在原有的 AMD nightly 流水线上分层新增 **accuracy** 与 **performance** 两大子套件，保持对 MI30x 与 MI35x 两类硬件的统一调度。新增的 `nightly‑perf‑2‑gpu‑text`、`nightly‑perf‑2‑gpu‑vlm` 等 job 通过 `if: (github.repository == ...)` 条件复用同一 workflow，实现“一次提交触发多硬件/多模型”。  
- **性能影响**：  
  - 新增 **DeepSeek‑V3.2** 基准（basic + MTP）以及 **Qwen‑3‑VL‑30B**、**Kimi‑VL** 等大模型，预计把 nightly CI 总运行时从 2.5 h 提升至约 4 h，特别是 8‑GPU MI35x 的 MTP 试验会产生较高显存占用（`--mem-fraction-static 0.7`）。  
  - 报告函数 `generate_simple_markdown_report` 通过跳过 warm‑up 结果避免误导的吞吐量峰值，使后续趋势分析更可靠。  
- **安全考虑**：仅在网络异常时切换 `HF_HUB_OFFLINE=1`，不会泄露凭证；新增的模型均来源于公开 HuggingFace，未引入额外的二进制或本地文件。  
- **可维护性**：所有新测试均复用现有 `register_amd_ci`、`NightlyBenchmarkRunner`，统一的 `ModelLaunchSettings` 与 env‑var 机制降低了重复代码，未来添加新模型只需在对应列表中声明。  

**⚠️ 潜在风险**  
1. **CI 资源耗时激增**：8‑GPU MI35x 的 DeepSeek‑V3.2 MTP 基准及多模型 VLM 测试可能导致 GPU 队列排队，影响其他 PR 的反馈速度。  
2. **显存/OOM**：`--mem-fraction-static 0.85`（basic）和 `0.7`（MTP）在 MI35x 仍可能触发 OOM，尤其在 batch‑size = 64、输入 4096 tokens 时。  
3. **模型下载依赖**：首次运行仍需完整下载 DeepSeek‑V3.2（≈30 GB），若缓存未持久化，离线重试会直接失败。  
4. **报告跳过逻辑误判**：若 batch‑size 列表中出现 genuine duplicate（不同配置但相同 size），当前 `if results[0].batch_size == results[1].batch_size` 会误删有效数据。  
5. **退出码过滤**：`amd_ci_exec.sh` 将 1/137/255 视为非网络错误直接终止，若未来出现新的非网络错误码（如 124 超时）可能错失重试机会。  

**💡 关注建议**  
- **监控 CI 时长**：在每次 nightly 结束后记录总体耗时，若超过阈值（如 5 h）考虑拆分或调低部分 batch‑size。  
- **显存预估**：在 `nightly‑perf‑8‑gpu‑mi35x‑deepseek‑v32‑mtp` 前加入显存检查脚本，若检测到 OOM 前置条件则自动降级 batch‑size。  
- **缓存持久化**：在容器启动脚本中显式挂载 HF cache 目录 (`/data2/models/huggingface`) 到宿主机，以避免每次重新下载。  
- **报告改进**：将 warm‑up检测改为 “若 batch‑size 相同且 `input_len`、`output_len` 亦相同” 再跳过，降低误删风险。  
- **退出码策略**：将网络相关错误码（如 124、255）统一归类为可重试，避免因新错误码导致误判。  
- **文档同步**：在项目 README/CONTRIBUTING 中更新 “AMD nightly 新增模型”和 “性能基准运行要求”章节，提醒贡献者相应的环境变量与资源需求。  

---  
整体来看，此次提交显著丰富了 AMD 平台的模型覆盖与基准能力，提升了对新模型（DeepSeek‑V3.2、最新 VLM 系列）的验证深度，但也带来了资源占用与 OOM 风险，需要在 CI 调度与显存管理上保持谨慎监控。

---

### feat: support bitsandbytes quantization algorithm (#15325)
**SHA**: `8fb4552` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8fb45523f339f2eb3ea6815934d12afd43778b8b)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交在 SGLang 的量化子系统中引入了对 **BitsAndBytes**（BNB）量化算法的原生支持。新增 `BitsAndBytesConfig` 配置类并将其注册进量化映射表，同时实现了对应的线性层 (`BitsAndBytesLinearMethod`) 与 Mixture‑of‑Experts 层 (`BitsAndBytesMoEMethod`) 的权重量化、反量化与前向计算逻辑，支持 4bit 与 8bit 两种模式。  
**🎯 影响范围**：  
- `python/sglang/srt/layers/quantization/__init__.py`（量化工厂注册）  
- 新增 `python/sglang/srt/layers/quantization/bitsandbytes.py`（所有 BNB 相关实现）  
- 受影响的核心模块包括 **LinearBase、FusedMoE、权重注册/属性系统、custom op 注册**，以及任何使用 `override_quantization_method` 的模型加载路径。  

---

### 🔍 技术洞察  

- **架构影响**  
  - **模块化扩展**：通过在量化映射表中加入 `"bitsandbytes": BitsAndBytesConfig`，实现了即插即用的扩展，保持了现有量化框架的统一接口 (`QuantizationConfig.get_quant_method`)。  
  - **抽象层次**：实现了 `BitsAndBytesLinearMethod` 继承自 `LinearMethodBase`、`BitsAndBytesMoEMethod` 继承自 `FusedMoE`，符合 SGLang 现有的 “量化‑方法‑层” 三层抽象，未破坏已有层级结构。  
  - **自定义 Op**：使用 `direct_register_custom_op` 将 `apply_bnb_4bit` 注入 Torch 自定义算子，避免了 Python 层循环，提高了 4bit 前向的执行效率，同时保留了 “fake_impl” 供 JIT/Profiler 使用。  
  - **可选依赖**：在 `BitsAndBytesLinearMethod.__init__`、`BitsAndBytesMoEMethod.__init__` 中做了版本检查并抛出明确的 `ImportError`，实现了对 `bitsandbytes` 的可选依赖管理。  

- **性能影响**  
  - **显存**：4bit 量化将权重压缩至 1/8（或 1/4，视 `quant_ratio` 而定），大幅降低显存占用；8bit 量化提供较小的压缩但仍能节省约 50% 显存。  
  - **计算吞吐**：通过 `bitsandbytes` 的 `matmul` / `matmul_4bit` 高效实现，预期在支持的 GPU（Ampere+）上可获得 1.5‑2.0× 的推理加速，尤其在大模型、长上下文场景下收益明显。  
  - **额外开销**：首次前向会创建 `MatmulLtState` 与量化状态对象，可能带来一次性的 GPU 内存分配和 CPU‑GPU 同步开销；`_apply_4bit_fake` 为占位实现，实际运行时仍需加载 `bitsandbytes` 库。  
  - **8bit 路径**：当前仅实现了 8bit 权重创建，`_apply_8bit_dequant` 与 `BitsAndBytesMoEMethod._create_weights_8bit` 均抛出 `NotImplementedError`，若用户误配置 `load_in_8bit=True` 会在运行时直接报错。  

- **安全考虑**  
  - **外部库引入**：`bitsandbytes` 是第三方 C++/CUDA 扩展，潜在的安全风险主要来源于库本身的二进制安全性及其对 CUDA 驱动的依赖。项目已通过版本校验 (`>=0.46.1`) 限制，降低了因 API 变更导致的安全/稳定性问题。  
  - **自定义 Op 注入**：`direct_register_custom_op` 直接向 Torch 注册原生函数，若实现出现内存越界或未捕获异常，可能导致进程崩溃。当前实现已提供 `fake_impl`，但在生产环境仍需确保底层实现的健壮性。  
  - **配置解析**：`BitsAndBytesConfig.from_config` 对缺失字段提供默认值，避免因恶意或错误配置导致异常泄露。  

---

### ⚠️ 潜在风险  

1. **依赖兼容性**  
   - `bitsandbytes` 对 CUDA、torch 版本高度敏感；在不同硬件/驱动组合下可能出现加载失败或性能退化。  
2. **未实现的 8bit MoE 路径**  
   - `BitsAndBytesMoEMethod._create_weights_8bit` 与 `_apply_8bit_dequant` 暂未实现，若用户开启 `load_in_8bit=True` 将直接抛错，影响使用体验。  
3. **跳过层逻辑 (`is_layer_skipped_bnb`)**  
   - 采用了两种匹配方式（子串与前缀交集），在复杂模块层级中可能产生误匹配，导致本应量化的层被跳过或相反。  
4. **自定义 Op 与 JIT**  
   - 注册的 `apply_bnb_4bit` 只在运行时可用，若模型被 TorchScript 导出或在不支持自定义 Op 的环境下使用，可能触发 `AttributeError`。  
5. **内存泄漏/状态增长**  
   - `MatmulLtState` 中会在每次前向后更新 `CB`、`CxB`，若生成计数 (`generation`) 失控或未在 `profile_run` 场景正确清理，可能导致显存泄漏。  

---

### 💡 关注建议  

- **文档与示例**：在官方文档中明确标注 **BitsAndBytes** 为可选依赖，提供安装指令、支持的 CUDA/torch 版本表，以及 4bit 与 8bit 的使用限制（尤其是 8bit MoE 尚未实现）。  
- **单元测试**：新增针对 `BitsAndBytesLinearMethod` 的前向、权重创建、skip‑module 匹配路径的测试；对 `apply_bnb_4bit` 的自定义 Op 在不同 Torch 版本下的兼容性做回归。  
- **错误容错**：在 `BitsAndBytesConfig` 或量化工厂里加入对 `load_in_8bit=True && MoE` 场景的提前校验，给出友好的提示而非 `NotImplementedError`。  
- **依赖检查**：在项目的 `setup.py`/`pyproject.toml` 中将 `bitsandbytes>=0.46.1` 标记为 **optional**，并在运行时打印实际导入的库版本，便于用户排障。  
- **性能基准**：提供官方 benchmark（显存、吞吐）对比（FP16 vs 4bit/8bit），帮助用户判断在不同模型规模下的性价比。  
- **安全审计**：建议在 CI 中加入 `bitsandbytes` 的二进制校验（如 hash 校验）以及针对自定义 Op 的内存安全检测（ASAN/Valgrind）以防止潜在的底层崩溃。  

--- 

**总结**：此 PR 将 BitsAndBytes 量化完整集成进 SGLang，为用户提供了显存压缩和推理加速的强大选项，整体架构保持一致且易于扩展。但鉴于对外部 CUDA‑C++ 库的依赖、未完成的

---

### fix(ci): apply MMMU retry logic to all affected test files (#17329)
**SHA**: `c1282da` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c1282da236062b774a27de269914ab6351e2c222)

**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
在 `mmmu_vlm_kit.py` 中实现了统一的 **MMMU 评估重试逻辑**，能够检测 lmms_eval 输出中的 parquet 损坏错误，自动清理本地缓存并强制重新下载数据集后再次执行。随后在所有受影响的测试文件中把原有的 `subprocess.run` 调用替换为该统一函数，以提升 CI 稳定性，防止因数据集缓存损坏导致的误报失败。  

**🎯 影响范围**：  
- `python/sglang/test/kits/mmmu_vlm_kit.py`（核心实现）  
- CI/手动 nightly 测试脚本：`test/manual/nightly/test_vlms_piecewise_cuda_graph.py`、`test/manual/nightly/test_vlms_vit_cuda_graph.py`  
- 注册 CI 测试：`test/registered/vlm/test_encoder_dp.py`  
- SRT 分布式测试：`test/srt/test_epd_disaggregation.py`  

**🔍 技术洞察**：  
- **架构影响**：仅涉及测试层工具库的改动，不会影响运行时业务代码或模型服务的架构。统一的 `_run_lmms_eval_with_retry` 为后续新增的 MMMU 相关测试提供了可复用的容错入口，提升了测试代码的可维护性和一致性。  
- **性能影响**：  
  - 正常情况（无缓存损坏）仍保持一次 `subprocess.run`，性能几乎不变。  
  - 出现 parquet 损坏时会执行一次额外的清理、强制重新下载以及第二次 `subprocess.run`，导致单次测试耗时约翻倍（最坏情况约 2 ×），但这类错误本身已导致测试失败，重试带来的额外耗时可接受。  
  - 引入的 `temp_set_env` 只在局部作用域生效，对全局环境影响可控。  
- **安全考虑**：  
  - 使用 `subprocess.run(..., check=True, capture_output=True, text=True)`，并未对外部输入做拼接，风险极低。  
  - 环境变量 `HF_HUB_OFFLINE` 与 `HF_DATASETS_DOWNLOAD_MODE` 的强制设置仅在子进程上下文内生效，不会泄露或持久化到宿主环境。  
  - 若后续出现路径注入等新需求，需要审计 `_cleanup_mmmu_dataset_cache()` 实现细节，确保仅删除预期缓存目录。  

**⚠️ 潜在风险**：  
1. **误判缓存损坏**  
   - `_is_mmmu_parquet_corruption` 的匹配规则若过宽，可能将正常的警告信息误判为损坏，触发不必要的缓存清理和二次下载，导致 CI 时间膨胀。  
2. **并发冲突**  
   - 在并行 CI 任务中多实例同时执行清理/强制重新下载，可能出现竞争条件（同一缓存目录被并发删除），导致后续任务再次报错。  
3. **资源占用**  
   - 强制重新下载大型 MMMU 数据集会消耗额外的网络带宽和磁盘 I/O，尤其在资源受限的 CI 环境中可能触发配额限制。  
4. **异常传播**  
   - 若 `_cleanup_mmmu_dataset_cache()` 返回 `False`，当前实现会抛出 `RuntimeError`，这会中断整个测试套件，需要上层捕获或 CI 设定重试策略。  

**💡 关注建议**：  
- **强化检测函数**：为 `_is_mmmu_parquet_corruption` 添加更精准的正则/关键字匹配，并加入单元测试，防止误判。  
- **并发安全**：在清理缓存前使用文件锁（如 `fasteners.InterProcessLock`）或 CI 级别的排他机制，确保同一机器上仅有一个实例执行 “force_redownload”。  
- **日志与可观测性**：在缓存清理与重下载步骤加入更详细的日志（包括时间戳、下载的文件大小），帮助定位 CI 环境中频繁出现的损坏根因。  
- **超时与重试上限**：考虑为重试过程设定最大重试次数（如 2 次），避免因持续的网络或磁盘异常导致无限循环。  
- **文档更新**：在项目的 CI/测试指南中记录此重试机制的行为、触发条件以及可能的副作用，提醒贡献者关注缓存目录容量。  
- **监控资源使用**：在 CI 配置中监控数据集下载的网络流量与磁盘占用，避免因频繁强制下载导致配额耗尽。  

通过以上措施，可在提升 CI 稳定性的同时，最小化因重试逻辑引入的潜在副作用。

---

#### 🟡 中重要度变更 (12)

### Use attn_tp_group for all reduce in token embedding (#17403)
**SHA**: `d97066d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d97066d209b97f7a9808cf9c1dfa4bee4163c23b)

**🎯 变更类型**：功能增强（将词表嵌入的跨 GPU 规约统一使用 `attn_tp_group`）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `dp_attention.py` 中新增 `attn_tp_all_reduce`，统一通过 `get_attention_tp_group()` 完成跨 GPU 全规约。  
2. `VocabParallelEmbedding` 新增 `use_attn_tp_group` 参数并在构造时记录，前向时根据该标识决定是使用新实现的 `attn_tp_all_reduce` 还是原来的 `tensor_model_parallel_all_reduce`。  
3. 在 DeepSeek 系列模型（`deepseek_nextn.py`、`deepseek_v2.py`）的嵌入层初始化处，将原来的 `enable_tp=not is_dp_attention_enabled()` 改为 `use_attn_tp_group=is_dp_attention_enabled()`，实现 “DP‑Attention 开启时使用注意力 TP 组进行规约”。  

**🎯 影响范围**  
- `python/sglang/srt/layers/dp_attention.py`（全局通信方法）  
- `python/sglang/srt/layers/vocab_parallel_embedding.py`（词表嵌入层）  
- 两个 DeepSeek 模型文件的嵌入层初始化逻辑  

**💡 关注建议**  
1. **兼容性验证**：确保在未开启 DP‑Attention 时仍保持原有 `tensor_model_parallel_all_reduce` 行为，防止模型在单机/单卡场景出现多余的跨组通信。  
2. **性能基准**：对比使用 `attn_tp_all_reduce` 与原 `tensor_model_parallel_all_reduce` 在多节点、不同 GPU 拓扑下的延迟和吞吐，确认新实现带来的收益。  
3. **错误检查**：`use_attn_tp_group` 默认值未在构造函数中显式给出，调用方必须显式传参；建议在 `VocabParallelEmbedding.__init__` 添加默认 `False` 防止因遗漏导致属性未定义。  
4. **文档更新**：在 API 文档和模型配置说明里补充 `use_attn_tp_group` 参数的意义、取值条件以及与 `enable_tp` 的关系。  
5. **回滚路径**：若在特定硬件上出现同步错误，可临时将 `use_attn_tp_group` 设为 `False`，回退至原有规约实现，保证服务不中断。

---

### Add flag that enables NCCL mlp sync batch for overlap scheduler (#17288)
**SHA**: `55c6164` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/55c616427d12f9c7151b08a68e37c0352c03c69b)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增环境变量 `SGLANG_NCCL_ALL_GATHER_IN_OVERLAP_SCHEDULER_SYNC_BATCH`（默认 `false`），用于在 overlap scheduler 下准备 MLP 同步批次时切换为 NCCL All‑Gather，而不是原来的 Gloo。  
- 文档、环境变量定义 (`environ.py`) 以及调度器实现 (`scheduler_dp_attn_mixin.py`) 相应更新。  

**🎯 影响范围**  
- `python/sglang/srt/environ.py` – 增加 `EnvBool` 定义。  
- `python/sglang/srt/managers/scheduler_dp_attn_mixin.py` – `prepare_mlp_sync_batch_raw` 的分支条件新增对该 flag 的检查，决定使用 NCCL 还是 Gloo。  
- `docs/references/environment_variables.md` – 对外说明。  

**💡 关注建议**  
1. **兼容性**：默认关闭，保持现有行为。若开启，请确保运行环境装有 NCCL 并且 GPUs、进程组配置匹配，否则可能出现 dead‑lock 或显存错误。  
2. **回退机制**：建议在代码中加入 NCCL 可用性检测（如 `torch.distributed.is_nccl_available()`），在不可用时自动回退到 Gloo 并给出警告。  
3. **测试**：补充单元/集成测试，覆盖 flag 为 true/false 两种路径，验证在多卡、不同 offload‑tags 场景下的正确性和性能提升。  
4. **文档**：在环境变量章节明确说明使用前置条件（NCCL 安装、进程组配置），并给出典型的开启示例。  
5. **监控**：上线后关注 `prepare_mlp_sync_batch_raw` 的时延与资源占用，确认 NCCL 真的带来网络同步的加速。  

整体而言，此改动为可选的性能优化，风险可控，只要在生产环境开启前做好依赖检查和回退处理即可。

---

### update docs of Ascend plateform (#17358)
**SHA**: `603f386` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/603f386c6bc92d6dae5af257279daf8e64a65e5b)

**变更类型**：文档更新（Ascend 平台）  
**重要程度**：🟡 中  

**变更摘要**  
- 新增 `docs/platforms/ascend_contribution_guide.md`，详细说明贡献流程、环境搭建、CI 触发方式等。  
- 对 `ascend_npu.md`、`ascend_npu_deepseek_example.md`、`ascend_npu_support*` 系列文档进行大量补充：更新 HDK、MemFabric、torch‑npu、triton 的版本号与安装命令；加入 docker 离线构建提示、开启/关闭 CI 的 slash‑command 说明；对部分服务器参数表格重新标记支持情况（A2/A3），并新增多层 Eagle、Diffusion‑LLM 等实验特性。  
- 在模型支持表中加入 Llama‑2‑7B、Janus‑Pro‑1B、MiniCPM‑o‑2.6 等模型；对部分模型（如 openai/gpt‑oss‑120b）标记为不支持。  

**影响范围**  
- **文档使用者**：从事 Ascend NPU 部署、调试、贡献的开发者与运维人员。  
- **CI/CI‑Permission**：依赖 “run-ci” 标签的 PR 需要按新说明触发。  
- **示例脚本**：部分命令行参数（如 `--enable-metrics`、`--soft-watchdog-timeout`、`--disable-tokenizer-batch-encode`）已改为不支持或默认关闭，旧脚本需相应调整。  

**关注建议**  
1. **环境同步**：在本地或 Docker 中使用最新的依赖版本（HDK 25.3、MemFabric 1.0.3、torch‑npu 2.8.0、triton‑ascend < 3.2.0rc），防止因旧镜像导致安装失败。  
2. **参数检查**：对照新的参数表格审视启动脚本，移除已废弃或默认关闭的 flag，避免启动时报错。  
3. **模型列表**：若计划使用新增模型（如 Llama‑2‑7B、Janus‑Pro），确认对应的权重文件已放在可访问的仓库或对象存储。  
4. **CI 触发**：对非核心贡献者，务必在 PR 中添加 “run-ci” 标签或使用 slash‑command，防止 CI 被限流或卡住。  
5. **贡献流程**：阅读新建的贡献指南，使用 pre‑commit、单独分支、PR 模板，可显著提升合并通过率。  

总体而言，此次提交不影响代码运行，只是对 Ascend 相关文档的系统性梳理与更新，使用者需根据最新说明校准环境与启动参数，以免出现兼容性问题。

---

### [3/N] Achieve fault tolerance at the DP level (#11657)
**SHA**: `f7a5e42` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f7a5e425c3a2b677f97e869a166420f13f142554)

**🎯 变更类型**：功能增强（DP 级容错）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `parallel_state.py` 为每个进程组新增 `active_ranks` / `active_ranks_cpu` 张量，用于记录当前 rank 是否存活，并在 Mooncake 后端创建对应的 `pg_options`。  
- 引入 `ActiveRanksOutput` 消息，在调度器、数据并行控制器、Tokenizer 管理器之间传播 DP 级存活状态，实现对失效 rank 的屏蔽与自动 rebalance。  
- `scheduler_dp_attn_mixin` 在全局信息 gather 时为失效 TP/DP‑rank 填充默认（空）信息，防止 All‑Gather 卡死。  
- `model_runner` 在检测到活跃 rank 与上一次不一致时触发 EPLB（Elastic Parallel‑Load‑Balancing）重新分配。  
- 单元测试加入故障注入（`pkill`）验证 DP‑TP‑EP 多级容错路径。

**🎯 影响范围**  
- **分布式通信层**：`sglang/srt/distributed/parallel_state.py`、`torch.distributed` 组创建逻辑。  
- **调度与控制**：`data_parallel_controller.py`、`scheduler.py`、`scheduler_dp_attn_mixin.py`。  
- **消息结构**：`io_struct.py` 新增 `ActiveRanksOutput`。  
- **Tokenizer 与模型执行**：`tokenizer_manager.py`、`model_runner.py`。  
- **测试套件**：`test/srt/ep/test_mooncake_ep_small.py`。

**💡 关注建议**  
1. **活跃状态同步**：确认 `active_ranks` 在所有进程间及时同步（`sync_active_to_cpu`），避免误判导致请求被错误过滤。  
2. **性能成本**：全局 All‑Gather 现在会额外复制活跃掩码并在失效 rank 上写回 fallback 张量，建议在大规模集群做基准测试，评估额外通信与内存开销。  
3. **后端兼容性**：当前容错仅在 Mooncake EP 后端激活，若用户切换到 NCCL/torch backend，需要保持 `use_pynccl` 判断一致，注意 `backend == "mooncake"` 的新限制。  
4. **异常恢复**：EPLB 触发后会在 `model_runner` 中重新执行一次前向，确认上层业务（如流式输出）能够安全重入，必要时在 `ElasticEPStateManager` 添加幂等检查。  
5. **测试覆盖**：CI 中已跳过部分故障注入测试，建议在稳定分支开启，防止回归；同时补充对 `active_ranks_cpu` 与 `active_ranks` 不一致情况的单元测试。  

总体而言，此次改动为 DP 级故障恢复奠定了框架，核心逻辑相对集中，若保持活跃状态的高可靠同步，系统在单节点失效情况下的可用性将显著提升。

---

### fix: Release pr pypi fix (#17382)
**SHA**: `d50dcd9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d50dcd9b616f5a55e265cd3b862c52a900f65b59)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `release-pypi-pr.yml` 中改用 `FULL_VERSION` 读取完整版本号，再通过 `sed` 去除 `.dev*` 与 `+*` 后得到干净的 `BASE_VERSION`，防止因后缀导致的版本冲突。  
2. 将 PR 包安装指令从硬编码 `${{ needs.build-pr-wheel.outputs.wheel_version }}` 调整为正确的变量引用，同时新增 `uv` 安装示例，提升对 `uv` 包管理器的支持。  

**🎯 影响范围**  
- CI/CD：GitHub Actions 的发布流程（`release-pypi-pr.yml`）  
- 文档/使用指南：增加 `uv` 安装方式，影响使用者的安装指令  

**💡 关注建议**  
- **CI 侧**：在多个分支上触发工作流，验证 `sed` 正则在不同版本格式（如 `1.2.3.dev0+gabc`、`1.2.3+abc`）下均能得到期望的 `BASE_VERSION`。  
- **安全性**：`sed` 依赖 POSIX 环境，若在 Windows‑runner 上运行可能失效，建议加上跨平台备选（如 Python 脚本）或限制 runner 为 Linux。  
- **文档**：同步更新 README/CHANGELOG，说明 `uv` 安装方式的必要额外参数（`--extra-index-url`、`--index-strategy unsafe-best-match`）。  
- **回滚**：保留旧的 `BASE_VERSION` 计算方式，以防新正则误删有效的预发布标签。  

总体来看，此次修改仅影响 CI 生成的 wheel 版本号和安装示例，对项目运行时代码无直接影响，风险较低。建议在 PR 合并前完成多平台 CI 验证后再发布。

---

### [Kimi-Linear] Refactor kimi-linear gate calculation to avoid duplicated code (#17160)
**SHA**: `e6b7c04` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e6b7c04947eebc2239b7937ed7f509386d208f11)

**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 Kimi‑Linear 中对 **beta**（sigmoid‑scaled b_proj）和 **forget‑gate**（fused_kda_gate）的计算从 `HybridLinearAttnBackend` 中抽出来，放到 `KimiLinear.forward` 里统一完成，避免在 `forward_decode` / `forward_extend` 两个路径上出现重复代码。  
- 同时删减了 `hybrid_linear_attn_backend.py` 中未再使用的 `A_log、dt_bias、b_proj、f_a_proj、f_b_proj、hidden_states` 参数以及相关 import，改为只接受 `beta` 与 `gate` 两个已预处理的张量。  
- 在 `kimi_linear.py` 中新增对 `beta`、`forget_gate`（即 `gate`）的计算并通过 `kwargs` 传递，同时将原来的 `g` 变量改名为 `norm_gate`，保持与 `FusedRMSNormGated` 接口一致。

**🎯 影响范围**  
- `python/sglang/srt/layers/attention/hybrid_linear_attn_backend.py`（解码/扩展路径）  
- `python/sglang/srt/models/kimi_linear.py`（模型前向）  
- 相关的 `fla.kda` 导入和 `FusedRMSNormGated` 调用。

**💡 关注建议**  
1. **兼容性检查**：确认项目中其他模型或后端是否仍依赖被删除的 `A_log、dt_bias、b_proj…` 参数，若有需相应适配。  
2. **张量维度**：`beta` 与 `gate` 在 `forward` 中被 `unsqueeze(0)`，确保在解码/扩展时仍保持 `[1, B, H, D]` 形式；可加入断言或单元测试防止维度错误。  
3. **性能验证**：虽然代码去除了重复计算，但仍应对比 decode/extend 的吞吐和延迟，确保未引入额外拷贝或同步开销。  
4. **测试覆盖**：补充对 `KimiLinear` 的 decode 与 extend 两条路径的回归测试，覆盖不同 batch/seq 长度，验证 `FusedRMSNormGated` 使用的 `norm_gate` 正确。  
5. **文档/注释**：在 `HybridLinearAttnBackend` 的 `forward_*` 参数说明中标记已删除的字段，避免未来误用。  

总体来看，此次重构提升了代码维护性，风险主要在参数迁移和维度保持上，建议通过上述检查确保平滑发布。

---

### [NPU] Update NPU doc for model and features supported (#17385)
**SHA**: `1b192cf` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1b192cf198751f04a96462998c582dbd7747cad6)

**🎯 变更类型**：其他（文档更新）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次 PR 主要对 Ascend NPU 相关文档进行大量修订：更新 DeepSeek‑V3.2 数据集规模、将 `TASK_QUEUE_ENABLE` 默认改为 1、补全与 `--revision`、`--fastapi-root‑path`、`--grpc-mode` 等参数的支持状态；同步服务器/路由启动脚本中的 `max‑concurrency`、`request‑rate`、`dp‑size`、`tp‑size` 等配置；在特性表中把此前标记为 ✖ 的选项（如 `--speculative‑draft‑load‑format`、`--abort‑on‑priority‑when‑disabled`、`--enable‑nsa‑prefill‑context‑parallel`、`--forward‑hooks`、`--config` 等）统一标记为 ✔，并对模型列表、特性兼容性等做了纠正。

**🎯 影响范围**  
- 文档目录 `docs/platforms/ascend_npu_*`（最佳实践、特性支持、模型列表）  
- 相关脚本示例中的环境变量与启动参数  
- 使用 NPU 的用户在阅读文档、编写配置文件时的参考依据  

**💡 关注建议**  
1. **代码‑文档同步**：请在本地或 CI 中检查对应实现（`sglang` 参数解析、NPU 后端）是否已真正支持新标记为 ✔ 的功能，避免文档误导。  
2. **回归测试**：针对 `TASK_QUEUE_ENABLE=1`、`dp‑size`/`tp‑size` 调整等修改，跑通一遍 NPU 多卡、PD‑Separation 场景的 benchmark，确保性能与预期一致。  
3. **示例校验**：文档中的 benchmark 命令已修改 `--request-rate`、`--num-prompts` 等，建议在最新镜像上执行一次，确保参数不会因默认值变化导致运行错误。  
4. **持续维护**：后续若有新特性加入或已有特性被废弃，及时更新对应表格的 ✅/✖ 标记，保持文档的可查阅性与可信度。  

以上修改对代码本身无直接影响，但对使用者的部署与调参流程至关重要，请务必在发布前确认文档与实际实现保持一致。

---

### Enable parallel stage execution for scheduled CI runs (#16880)
**SHA**: `17c04b1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/17c04b109d0724cf602c3a77d310899f040cdb0f)

**🎯 变更类型**：功能增强（为定时 CI 引入并行阶段执行）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 为 `pr-test.yml` 增加 `test_parallel_dispatch` 输入，模拟定时运行的并行调度行为。  
- 重构并发分组规则，使其同时包含分支、PR SHA 与目标 stage，避免 PR 与 main‑branch 之间的互相取消。  
- 新增 `wait‑for‑stage-a/b` 两个轮询作业，仅在 PR 场景下执行，用来强制顺序执行；在定时或并行调度时直接跳过，从而实现真正的并行。  
- 大量 `if:` 条件加入 `github.event_name != 'schedule'`、`inputs.test_parallel_dispatch` 与 `!inputs.target_stage` 的判断，以在不同触发方式下灵活选择执行路径。  
- 为工作流授予 `actions: write` 权限，支持 `actions/github-script` 轮询 GitHub API。

**🎯 影响范围**  
- CI 配置（`.github/workflows/pr-test.yml`、`.github/workflows/pr-gate.yml`）  
- 所有依赖此工作流的测试、构建、基准等 stage（stage‑a、stage‑b、stage‑c、sgl‑kernel、jit‑kernel、multimodal‑gen、unit‑test‑backend 等）  
- 触发方式：pull‑request、schedule、workflow_dispatch（用于手动并行调度）

**💡 关注建议**  

1. **并发分组冲突**  
   - 新的 `group: pr-test-${{ github.head_ref || github.ref_name || 'default' }}-${{ inputs.pr_head_sha || 'current' }}-${{ inputs.target_stage || inputs.ref || 'all' }}` 已经把 stage 纳入组名，确保同一 PR 不会因并行 stage 被误取消。但请在实际运行中观察是否出现 “group name too long” 或因分支名/SHA 中出现非法字符导致创建失败。

2. **API 速率与权限**  
   - `wait‑for‑stage-*` 使用 `actions/github-script` 轮询 `listJobsForWorkflowRun`，在大量 PR 同时触发时可能触及 GitHub API rate‑limit。建议在脚本里加入 `try/catch` 并在超限时使用 `core.setFailed` 提示，或在文档中提醒 CI 资源配额。  
   - 已添加 `permissions: actions: write`，确认仓库的 `GITHUB_TOKEN` 仍具备该权限（默认是 `write`），避免因 token scope 限制导致脚本失败。

3. **条件表达式的可读性**  
   - `if:` 中多层嵌套的布尔表达式相当冗长，建议抽取为 reusable‑expression 或在注释中阐明各子条件的意义（如 “schedule or parallel‑dispatch → 跳过等待”， “!target_stage → 正常全链路”），便于后期维护。

4. **文档与 UI**  
   - 新增的 `test_parallel_dispatch` 输入在 UI 中未提示用途，建议在工作流 README 或 CONTRIBUTING 中补充说明，用法示例以及何时需要手动开启。  
   - 同时更新 CI 触发说明，说明定时运行已不再受 `wait‑for‑stage` 限制，所有 stage 将并行启动。

5. **回归测试**  
   - 由于大量 `needs` 链路被 `wait‑for‑stage-a/b` 替换，建议在 CI 环境跑一次完整的 schedule（或手动 `workflow_dispatch`）并验证：  
     - 所有 stage 均被触发且不被错误取消；  
     - PR 场景仍保持原有的顺序执行；  
     - 在并行调度模式下 `test_parallel_dispatch=true` 能模拟 schedule 行为。

总体来看，此次改动为定时 CI 引入了显著的并行能力，对现有 PR 流水线几乎不产生负面影响，只要在实际运行中关注 API 额度、并发分组的唯一性以及条件可维护性，即可平稳过渡。 🚀

---

### Disable unit-test-backend-4-gpu-gb200 job (#17367)
**SHA**: `057b07f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/057b07fc50011f4106e191cfda157169e971bf2b)

**🎯 变更类型**：其他（CI流程调整）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交将 GitHub Actions 工作流 `.github/workflows/pr-test.yml` 中的 `unit-test-backend-4-gpu-gb200` 任务整体注释掉，改为 “Disabled: GB200 runner needs repair”。同时在 `pr-test-finish` 的 job 列表中将该任务标记为已禁用。核心目的是避免因 GB200 GPU 实例不可用而导致 CI 失败。

**🎯 影响范围**  
- **CI/CD 流程**：`unit-test-backend-4-gpu-gb200` 负责在 4‑GPU GB200 机器上运行后端单元测试，关闭后该平台的测试不再执行。  
- **测试覆盖**：涉及 `per-commit-4-gpu-gb200` 套件的代码路径将失去在实际 GB200 硬件上的验证，可能影响对特定 CUDA 12.9、AArch64 环境的兼容性检测。  
- **后续工作流**：`pr-test-finish` 中的 `needs` 列表仍然引用了该 job，已改为注释，避免因缺失依赖导致整个 PR 检查卡住。  

**💡 关注建议**  
1. **监控 Runner 修复进度**：在 GB200 实例恢复前，定期检查 GitHub 项目 Issue/PR（如 #17367）并在 runner 可用后恢复该 job。  
2. **临时替代方案**：若该平台的兼容性对发布至关重要，可考虑在其他可用 GPU 实例上临时跑相同套件（如 4‑GPU‑b200）或在本地自行验证。  
3. **文档与状态展示**：更新 CI 状态页或 CONTRIBUTING 文档，注明该测试已被暂时禁用，以免贡献者误以为全部测试均已通过。  
4. **CI 依赖检查**：确保没有其他工作流或自动发布脚本仍然显式依赖 `unit-test-backend-4-gpu-gb200` 的成功结果，防止隐藏的构建阻断。  

整体来看，此次改动仅影响 CI 稳定性，不会改变代码功能。但需留意因测试缺失导致的潜在回归风险，并在 runner 修复后及时恢复对应 job。

---

### fix: updating pypi workflow with new base version formatting in pyproject.toml (#17366)
**SHA**: `91d8c52` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/91d8c52d6d03a8838e21c706e999bdd305652ef1)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 `release-pypi-pr` 工作流中获取基础版本的方式从直接读取 `python/sglang/version.py` 改为使用 `setuptools_scm` 动态解析。  
- 在同一工作流里把 `pyproject.toml` 的 `dynamic = ["version"]` 改为静态写入，直接写入 PR 生成的 `WHEEL_VERSION`。  

**🎯 影响范围**  
- `.github/workflows/release-pypi-pr.yml`（CI/CD 流程）  
- `python/sglang/version.py`（已不再作为唯一版本来源）  
- `python/pyproject.toml`（版本字段写法）  

**💡 关注建议**  
1. **版本一致性**：确保 `setuptools_scm` 能在工作流根目录正确读取 Git tag/commit，否则 `BASE_VERSION` 可能为空或不匹配 `version.py`。  
2. **依赖管理**：工作流每次 `pip install setuptools-scm` 可能增大运行时长，建议使用缓存或在项目的 `requirements.txt` 中锁定版本。  
3. **pyproject.toml 兼容性**：删除 `dynamic` 会影响其他依赖（如 `poetry`、`build`）对动态版本的期待，需确认本项目不再使用这些特性。  
4. **回退路径**：若后续仍需 `version.py`，保持其生成逻辑或在 CI 中同步更新，以免出现“源码版本 vs 打包版本不符”。  
5. **本地测试**：在本地运行 `python -c "from setuptools_scm import get_version; print(get_version(root='..'))"` 检查输出是否与期望的基准版本一致，再推送。  

总体上，此次修改提升了版本获取的可靠性，但需关注工作流环境、依赖缓存以及 `pyproject.toml` 静态化后对其他工具的影响。

---

### [AMD] CI - add partitions for stage-b-test-small-1-gpu-amd (#17345)
**SHA**: `1a053a8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1a053a810c1957f2d32ac1e78d524a263d9186ac)

**🎯 变更类型**：其他（CI 与测试配置）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
1. 在 AMD CI 工作流 `pr-test-amd.yml` 中，将分区数量从 12 扩增至 13（`part: [0…12]`），并相应把 `--auto-partition-size` 调整为 13。  
2. 移除 DeepSeek‑V32 系列测试中 `model-loader-extra-config` 的 `num_threads` 参数，仅保留 `enable_multithread_load:true`。  
3. 上调部分 AMD 测试的估算执行时间 (`est_time`) 以匹配更长的运行时（从 ~100/149 提升至 200）。  

**🎯 影响范围**  
- `.github/workflows/pr-test-amd.yml`（CI 任务调度）  
- `test/registered/amd/*`（DeepSeek‑V32 基础和多线程并行加载测试）  
- `test/registered/attention/test_triton_sliding_window.py`、`test/registered/openai_server/basic/test_openai_server.py`（AMD CI 注册的估算时间）  

**💡 关注建议**  
1. **分区一致性**：确认 `run_suite.py` 能正确解析 `--auto-partition-size 13`，并且所有 13 任务都有对应的 `part` 值；若有硬编码 12 的场景需同步修改。  
2. **线程配置**：去掉 `num_threads` 可能导致加载速度回落，建议在 AMD 环境验证是否仍满足性能要求，或在文档中注明此变更的原因。  
3. **估算时间**：`est_time` 仅用于 CI 调度，建议在实际运行后观察是否仍偏低/高，必要时进一步微调，以避免 CI 超时或资源浪费。  
4. **回归测试**：在 AMD GPU（mi325、mi35x 等）上完整跑一次 `stage-b-test-small-1-gpu-amd`，确保新增分区不会导致遗漏或重复测试。  
5. **文档更新**：若 CI 文档或贡献指南中提及分区数量或线程配置，记得同步修改。  

总体而言，此次改动主要是为 AMD CI 增加并行度并简化线程参数，风险集中在 CI 调度和可能的加载性能波动，建议按上述要点做好验证。

---

### [AMD CI] Migrate and Add More Testcases (#17116)
**SHA**: `2ea02f0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2ea02f06420d427dbddc66d3f1af21bf493ea8bf)

**🎯 变更类型**：功能增强（CI/测试覆盖扩充）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**：  
- 将 AMD CI 工作流迁移至统一 `stage‑*‑amd` 套件，并新增多项 GPU‑AMD 专用单元、性能、准确率测试（DeepSeek‑V3、Moe‑TopK、Deterministic、HiCache、Quant、RoPE 等）。  
- 为大量已有测试补充 `register_amd_ci`，并在部分 determinism、flashinfer 测试中通过 `skipIf(is_in_amd_ci())` 避免不兼容的路径。  
- 调整 `run_suite.py` 中 AMD 套件名称、删除已迁移至 `registered/amd/nightly` 的 8‑GPU 测试，使 CI 只跑对应的 AMD‑CPU/GPU 阶段。  

**🎯 影响范围**：  
- `.github/workflows/pr-test-amd.yml`（CI 任务调度、容器启动、超时、依赖安装）  
- `test/registered/**` 中大量测试文件以及注册入口 `ci_register.py`  
- `test/run_suite.py` 与 `test/srt/run_suite.py` 的套件映射列表  
- 可能的资源占用（新增 8‑GPU 大模型测试、性能基准）  

**💡 关注建议**：  
1. **CI 稳定性**：新增的 8‑GPU 大模型测试（DeepSeek‑V3）对显存与节点要求较高，建议确认对应 runner（MI300X 等）已在 `pr-test-amd.yml` 中配置，并适当调高 `timeout-per-file`。  
2. **注册一致性**：部分旧 `suite` 名称仍在 `run_suite.py` 中出现（如 `stage-a-test-1-amd`），确保所有引用已统一，否则可能出现 “suite not found” 错误。  
3. **跳过逻辑**：`skipIf(is_in_amd_ci())` 已在 determinism 测试中使用，若未来在 AMD 上加入对应实现，需要同步移除该跳过或加入 AMD‑专属实现。  
4. **依赖检查**：新增的 `wave_attention`、`speculative eagle` 等特性依赖外部库（如 `wave_lang`），请在 AMD CI 镜像中提前安装或在 CI 脚本中加入检测/报错。  
5. **性能基准阈值**：benchmarks 中对 token/s、accuracy 的阈值写入硬编码（如 `speed > 12`），建议在 CI 中使用环境变量或配置文件，以便不同硬件平台灵活调节。  

总体而言，此次 PR 大幅提升了 AMD 平台的测试覆盖度和性能基准，但也增加了 CI 运行时长和资源需求，务必在 CI 环境充分验证后再合并。

---

#### 🟢 低重要度变更 (18)

### [FIX] fix mambaish model pp kv cache compute (#17334)
**SHA**: `16802fb` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/16802fb6b2166e2d1042b355322b934f9978e475)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 mambaish 模型 KV‑Cache 计算中层数的统计方式，仅计入在 `start_layer` 与 `end_layer` 区间内的 `full_attention_layer_ids`，防止因跨区间层导致的缓存大小误估。

---

### Disable swa memory for trtllm-mha backend (#17429)
**SHA**: `ce2d686` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ce2d686e948e518b0d228047ca839c34fe411b21)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `disable_hybrid_swa_memory` 参数并在模型配置和服务器参数中使用，默认关闭 trtllm‑mha 后端的混合 SWA 内存，以避免不兼容的 KV 索引转换。

---

### [OPT] DeepSeekV3.2: optimize indexer weight_proj-mma performance (#17205)
**SHA**: `612026a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/612026ad2c82424f8fd6907c432efc213f69ff15)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `weights_proj` 参数类型改为 `bfloat16`，在调用时先使用原 dtype 再转换为 `float32`，提升索引器的 MMA 性能。

---

### Update default attn backend to fa3 for mimo model (#17419)
**SHA**: `91a4cd8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/91a4cd86487b4cfc9718e5dcb1654b5bf3859a74)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `is_fa3_default_architecture` 中新增 `"MiMoV2FlashForCausalLM"`，使 MiMo 模型使用默认的 fa3 注意力后端。

---

### [Doc] Update pipeline parallelism documentation (#17414)
**SHA**: `23d765d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/23d765d1f2f5afad6783ac6fdcb2a1713b390535)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 pipeline_parallelism 文档中加入了博客链接，完善了动态分块预填的算法说明、流式执行细节及调优指南，并补充了实用的层分区优化提示。

---

### [Fix]: correctly fetch ds32 config  in tuning_fused_moe_triton (#17409)
**SHA**: `db2425a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/db2425a00b03eae56535328820352bf0e90dd4ed)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `benchmark/kernels/fused_moe_triton/common_utils.py` 中的模型配置获取方式从 `transformers.AutoConfig` 替换为内部 `get_config`，修正了 ds32 配置获取错误。

---

### Disable mla persistent kernel when not using fp8 kv_cache (#17327)
**SHA**: `6988a0f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6988a0f5706c05ebc40ba9ef5c09243bcbf9886c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `aiter_backend.py` 中，仅在使用 fp8 KV 缓存时才启用 `mla_ps_kernel`，否则改为非持久化内核，以避免非 fp8 场景的性能下降。

---

### [diffusion] fix: enforce 16-pixel alignment for flux2 to prevent shape mismatch crash (#17302)
**SHA**: `c560e14` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c560e142169ea816f1441e18772a2cbcde9761b4)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `flux.py` 中加入 VAE 缩放因子，将图像宽高强制对齐至 16 像素倍数，避免 Flux2 因尺寸不匹配导致崩溃；同时删除多余的 `task_type` 定义。

---

### [PD] Fix PP dynamic chunking with DP attention (#17339)
**SHA**: `71cb9d0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/71cb9d0302e90bbadee274d908b0aca4e21f814e)

**🎯 变更类型**：代码重构  

**⚡ 重要程度**：🟢低  

**📋 摘要**：在 `scheduler_pp_mixin.py` 中加入 DP Attention 判定、统一使用 `model_config`，并在 Profiling 时设置 `global_num_tokens`，简化张量创建和 forward 调用。  

---

### fix function calling for Trinity (#17364)
**SHA**: `84aef37` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/84aef37808040b05b86aa61b8308ebf7506e7199)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在函数调用解析器中加入 `TrinityDetector`，实现对 `<think>` 标签的过滤并复用 Qwen25Detector 的检测与解析逻辑；新增 `trinity_detector.py` 文件并在 `function_call_parser.py` 中注册该检测器。

---

### Fix runner utilization workflow to use 24h default (#17378)
**SHA**: `55c4288` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/55c4288b3e243c40131a690355ead461b94dba2d)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 GitHub Actions 中 `runner-utilization.yml` 工作流的默认 `--hours` 参数从 6 小时改为 24 小时，以提升默认统计时长。

---

### [Docs] Fix formatting in Evaluating New Models with SGLang (#17376)
**SHA**: `9f8b79f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9f8b79f16feae0f2642364dc7a2affe2c9e4b65e)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `docs/developer_guide/evaluating_new_models.md` 中，将原生 `[!TIP]` 提示语法改为 MyST `{tip}` 代码块，统一格式并提升可读性。

---

### [Docker] Fix CUDA 13 installing wrong `nvidia-nccl-cu13` due to `nixl-cu13` not breaking system package (#17370)
**SHA**: `7dc3cbe` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7dc3cbe7ca37dbf6b12eb09b93a129044ad3ecc6)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Dockerfile 中，为 CUDA 13 环境安装 `nixl-cu13` 时添加 `--no-deps` 参数，避免错误安装系统包 `nvidia-nccl-cu13`。

---

### [Docs] Add new model evaluation docs (#17043)
**SHA**: `79ddc34` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/79ddc34c1ca4bb9cbc74be313e75028dadee525f)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增《Evaluating New Models with SGLang》文档，详细列出模型准确性与性能评估的命令、步骤及报告要求，并在文档目录中加入对应入口。

---

### Pipe customized_info through CudaGraphRunner output (#17088)
**SHA**: `09a9d21` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/09a9d214f7f0387b92deba5b10ecd19adfd90c18)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `CudaGraphRunner.replay` 中新增 `customized_info=output.customized_info` 参数，确保自定义信息在图执行回放时正确传递。

---

### Move test_autoround.py to stage-b-test-large-1-gpu suite (#17336)
**SHA**: `7e40d52` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7e40d5263520e647f8fc94bcb6820712a1477a38)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `test_autoround.py` 的 CUDA CI 注册从 `stage-b-test-small-1-gpu` 改为 `stage-b-test-large-1-gpu`，仅更新套件名称，功能未变。

---

### [AMD] fix perf ci errors (#17363)
**SHA**: `e9a44ea` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e9a44ea6075122a0d475a9d819ef05558cd581c7)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/workflows/pr-test-amd.yml` 中，将原先直接运行的性能基准测试改为使用 `-w /sglang-checkout/test/registered/perf` 工作目录，并切换到拆分后的 `test_bench_serving_1gpu_part*` 测试类，解决 AMD CI 中的 perf 错误。

---

### [CI] fix test_vlm_models.py (#17049)
**SHA**: `71279e3` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/71279e31f7234a70ac3ced890deb2bfa8aa29f6d)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_vlm_models.py` 中使用 `tempfile.TemporaryDirectory` 为每个模型创建独立的临时目录，以避免缓存冲突和日志相互覆盖。

---

