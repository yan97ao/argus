# 每日更新报告（2026-01-22）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-22 23:36:38 | Baizhou Zhang | [hotfix] Reenable all reduce fusion on sm100 (#17591) |
| 2026-01-22 23:15:08 | Yuhao Yang | model: step3-vl-10b (#17513) |
| 2026-01-22 23:08:38 | Jincong Chen | [BUGFIX] Skip Mamba Cache Slot 0 to Avoid Using Dummy Cache (#17404) |
| 2026-01-22 22:55:23 | Jacob Gordon | ci(pre-commit): avoids extraneous codespell exclusions (#17590) |
| 2026-01-22 22:32:15 | Xiaoyu Zhang | [Diffusion] Make the apply_qknorm function easier to use (#17537) |
| 2026-01-22 22:05:47 | triple-mu | [diffusion] model: optimize torch.compile (#17472) |
| 2026-01-22 22:02:05 | chenxu214 | [NPU] bugfix with Kimi-k2 and bge-reranker-v2 model (#17478) |
| 2026-01-22 19:25:06 | chenxu214 | Change naming for graph mode on multiplatform (#17469) |
| 2026-01-22 17:42:06 | Minglei Zhu | refactor Qwen3-Next with a new RadixLinearAttention (#17373) |
| 2026-01-22 17:36:31 | zhangheng | [RadixTree][3/N Refactor]：Support unified insert/evict params (#17401) |
| 2026-01-22 17:24:06 | Shangming Cai | Add ZhengWG to CI_Permission (#17572) |
| 2026-01-22 15:27:25 | Baizhou Zhang | Add xyjixyjixyji to CI_Permission (#17559) |
| 2026-01-22 15:26:35 | Chandrakant Khandelwal | [NPU] [Bug Fix] Fix typo in npu device check in gpt_oss.py (#17553) |
| 2026-01-22 15:03:10 | Zaili Wang | [Fix] fix device orientation for image processor (#15859) |
| 2026-01-22 14:04:50 | Zaili Wang | [CPU][Fix CI] Solidate torch version for sgl-kernel-cpu and fix device orientation error (#17460) |
| 2026-01-22 13:31:57 | Baizhou Zhang | [Kernel] Little refactor of flashinfer allreduce norm fusion (#17474) |
| 2026-01-22 13:13:34 | Chi McIsaac | [diffusion] feat: enable passing Cache‑DiT config for diffusers backend (#16662) |
| 2026-01-22 12:34:24 | YC Tseng | [AMD] fix amd ci dpskv32 (#17432) |
| 2026-01-22 12:29:55 | Baizhou Zhang | [hotfix] Fixes on cuda 13 docker image (#17541) |
| 2026-01-22 12:22:20 | Yingchun Lai | fix: prefer to use max_completion_tokens rather than max_tokens (#17516) |
| 2026-01-22 12:21:11 | Serge Panev | [NVIDIA] Fix CUDA arch requirement in nvfp4 cast (#12581) |
| 2026-01-22 12:17:02 | Baizhou Zhang | [HotFix]Fix dtype mismatch in nsa indexer on AMD device (#17518) |
| 2026-01-22 11:11:20 | Piotr Mazurek | Add Liquid Foundation Model (LFM2) (#16890) |
| 2026-01-22 10:03:27 | Kangyan-Zhou | Update release-branch-cut.yml for  actions: write (#17539) |
| 2026-01-22 09:10:08 | cen121212 | Optimize Qwen3-VL video memory usage (#16366) |
| 2026-01-22 08:23:53 | Xiaoyu Zhang | [Diffusion] Support select fa2 backend in hopper (#17514) |
| 2026-01-22 08:10:52 | Alison Shao | Increase wait-for-stage timeouts to handle long queue times (#17536) |
| 2026-01-22 07:34:02 | Alison Shao | Fix import path for UnquantizedLinearMethod in test (#17529) |
| 2026-01-22 07:31:03 | Alison Shao | Remove test_gpt_oss_4gpu.py from __not_in_ci__ (keep in per-commit-4-gpu) (#17534) |
| 2026-01-22 06:48:32 | Lianmin Zheng | [Auto Sync] Update detokenizer_manager.py, io_struct.py, mu... (20260120) (#17442) |
| 2026-01-22 06:48:02 | DarkSharpness | [Chore] include all jit files in building packages (#17493) |
| 2026-01-22 06:01:04 | Alison Shao | Temporarily disable flaky test_gpt_oss_4gpu.py on B200 (#17528) |
| 2026-01-22 04:29:14 | Jacob Gordon | ci(codespell): centralizes list of ignorable words (#17524) |
| 2026-01-22 04:28:33 | Lingjun Wen | [new-model] Add support for `Cohere2ForCausalLM` behind Command-A and Command-R Models (#16927) |
| 2026-01-22 04:04:09 | Lianmin Zheng | [Auto Sync] Update environ.py, fp8.py (20260121) (#17486) |
| 2026-01-22 04:02:37 | Jacob Gordon | ci: avoids duplication of codespell config (#17519) |
| 2026-01-22 01:25:32 | Yunmeng | [Misc] Fix argument help string formatting (#17416) |
| 2026-01-22 01:23:28 | YC Tseng | [AMD] CI - Fix sgl-kernel unittest (#17490) |
| 2026-01-22 01:04:19 | Ke Bao | Disable swa memory for gpt-oss with spec (#17517) |

### 📊 统计摘要
> 本日共 39 个提交 | 🔴高 3 | 🟡中 7 | 🟢低 29
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [model: step3-vl-10b (#17513)](#f7a0bcd)
    - [[RadixTree][3/N Refactor]：Support unified insert/evict pa...](#f33022d)
    - [Add Liquid Foundation Model (LFM2) (#16890)](#d6e2b88)
  - [🟡 中重要度变更 (7)](#-🟡-中重要度变更-7)
    - [[Diffusion] Make the apply_qknorm function easier to use ...](#5324027)
    - [[NPU] bugfix with Kimi-k2 and bge-reranker-v2 model (#17478)](#5d299c2)
    - [refactor Qwen3-Next with a new RadixLinearAttention (#17373)](#419bbce)
    - [[diffusion] feat: enable passing Cache‑DiT config for dif...](#71482dd)
    - [[AMD] fix amd ci dpskv32 (#17432)](#17807ca)
    - [fix: prefer to use max_completion_tokens rather than max_...](#a5bbcda)
    - [[Auto Sync] Update detokenizer_manager.py, io_struct.py, ...](#b74a57a)
  - [🟢 低重要度变更 (29)](#-🟢-低重要度变更-29)
    - [[hotfix] Reenable all reduce fusion on sm100 (#17591)](#283a2da)
    - [[BUGFIX] Skip Mamba Cache Slot 0 to Avoid Using Dummy Cac...](#72f790b)
    - [ci(pre-commit): avoids extraneous codespell exclusions (#...](#42523d0)
    - [[diffusion] model: optimize torch.compile (#17472)](#3705f90)
    - [Change naming for graph mode on multiplatform (#17469)](#a4dc432)
    - [Add ZhengWG to CI_Permission (#17572)](#2262c5c)
    - [Add xyjixyjixyji to CI_Permission (#17559)](#8dae6ec)
    - [[NPU] [Bug Fix] Fix typo in npu device check in gpt_oss.p...](#61abff6)
    - [[Fix] fix device orientation for image processor (#15859)](#6a8f68b)
    - [[CPU][Fix CI] Solidate torch version for sgl-kernel-cpu a...](#672eb37)
    - [[Kernel] Little refactor of flashinfer allreduce norm fus...](#e2d3353)
    - [[hotfix] Fixes on cuda 13 docker image (#17541)](#fafa171)
    - [[NVIDIA] Fix CUDA arch requirement in nvfp4 cast (#12581)](#e95668a)
    - [[HotFix]Fix dtype mismatch in nsa indexer on AMD device (...](#3373545)
    - [Update release-branch-cut.yml for  actions: write (#17539)](#f2ae066)
    - [Optimize Qwen3-VL video memory usage (#16366)](#0c2993e)
    - [[Diffusion] Support select fa2 backend in hopper (#17514)](#590969e)
    - [Increase wait-for-stage timeouts to handle long queue tim...](#e6ccb29)
    - [Fix import path for UnquantizedLinearMethod in test (#17529)](#d6ea2c5)
    - [Remove test_gpt_oss_4gpu.py from __not_in_ci__ (keep in p...](#9be2a3a)
    - [[Chore] include all jit files in building packages (#17493)](#95f59c1)
    - [Temporarily disable flaky test_gpt_oss_4gpu.py on B200 (#...](#85d9af5)
    - [ci(codespell): centralizes list of ignorable words (#17524)](#858f317)
    - [[new-model] Add support for `Cohere2ForCausalLM` behind C...](#cf89351)
    - [[Auto Sync] Update environ.py, fp8.py (20260121) (#17486)](#1fdf5ca)
    - [ci: avoids duplication of codespell config (#17519)](#cda43ff)
    - [[Misc] Fix argument help string formatting (#17416)](#3908985)
    - [[AMD] CI - Fix sgl-kernel unittest (#17490)](#b827e9d)
    - [Disable swa memory for gpt-oss with spec (#17517)](#d725487)
#### 🔴 高重要度变更 (3)

### model: step3-vl-10b (#17513)
**SHA**: `f7a0bcd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f7a0bcda1e089d4631f302665b4c3872d2789fbd)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
本次提交为 SGLang 引入了全新的 10B 参数多模态模型 **StepVLForConditionalGeneration**（StepVL‑10B），并在模型检测与调用链路中新增 **HermesDetector** 用于解析 `<tool_call>` 格式的工具调用。为兼容新模型，更新了模型配置列表、模型权重加载逻辑、图像 Token 配置以及多模态处理器，使得 SGLang 既能支持原有 Step3VL，也能直接使用 StepVL‑10B。

**🎯 影响范围**：  
- `python/sglang/srt/models/step3_vl_10b.py`（全新模型实现）  
- `python/sglang/srt/function_call/hermes_detector.py`（新增函数调用解析器）  
- `python/sglang/srt/function_call/function_call_parser.py`（注册 HermesDetector）  
- `python/sglang/srt/configs/model_config.py`（模型列表新增 StepVL）  
- `python/sglang/srt/multimodal/processors/step3_vl.py`（图像 Token 与模型映射扩展）  
- `python/sglang/srt/utils/common.py`（默认架构判断加入 StepVL）  

---

### 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - **模型拆分**：将视觉塔（PerceptionEncoder）与语言塔（Qwen‑3）解耦，视觉特征通过 `vit_large_projector` 投影至语言模型隐藏维度。<br>- **权重加载**：在 `StepVLForConditionalGeneration.load_weights` 中显式区分视觉与语言权重，并对名称做兼容性映射（e.g., `attn.in_proj_weight → attn.qkv_proj.weight`），确保能够直接从原始 checkpoints 加载。<br>- **函数调用**：新增 `HermesDetector`，通过正则捕获 `<tool_call>` 包裹的 JSON，统一到已有 `FunctionCallParser` 框架。 |
| **性能影响** | - **显存需求**：10B 参数模型（约 20 GB FP16）+ Vision Encoder（约 2 GB） → 单卡显存需求提升至 ~24 GB，需在多卡或高显存机器上部署。<br>- **推理吞吐**：VisionEncoder 与语言模型串联，额外的 `vit_large_projector` 线性层会带来轻微的额外算子开销；总体推理速度预计下降 10‑20%。<br>- **并行**：使用 `ColumnParallelLinear`/`RowParallelLinear`，已保留张量并行能力，仍可在多卡环境中保持伸缩性。 |
| **安全考虑** | - **HermesDetector** 使用 `json.loads` 直接解析用户生成的文本，若工具调用内容未受信任，可能触发 **JSON 解码攻击**（如异常嵌套导致服务异常）。建议在捕获异常后返回安全的错误信息，避免泄露内部堆栈。<br>- **模型权重映射**：对权重名称的硬编码替换若出现未覆盖的层，会抛出 `ValueError`，可能导致服务启动失败。应在 CI 中加入权重检查脚本。 |
| **可维护性** | - **代码复用**：VisionEncoder 代码从 `perception_models` 迁移而来，保持了模块化，但文件体积大（>500 行），建议抽离为独立子模块 `sglang.srt.models.perception_encoder`，便于单元测试。<br>- **检测器统一**：`HermesDetector` 与其他 detector 实现保持同一基类，易于后续新增。 |

---

### ⚠️ 潜在风险

1. **显存超限**：在标准 16 GB GPU 环境下无法加载，可能导致 OOM。<br>2. **权重兼容性**：如果官方 checkpoint 命名规则变化（如新增层或改名），当前替换规则会失效。<br>3. **工具调用解析错误**：不完整或恶意的 `<tool_call>` 内容可能导致 `json.loads` 抛异常，若未妥善处理会导致服务崩溃。<br>4. **Token ID 冲突**：`<im_patch>` 的 token id 固定为 128001，若底层 tokenizer 更新或自定义 tokenizer 中已使用该 ID，可能产生冲突。<br>5. **多模型共存**：`Step3VLForConditionalGeneration` 与新 `StepVLForConditionalGeneration` 共用同一 processor 列表，若上下文中混用模型，可能出现 token‑id 不匹配或图像特征维度误用。

---

### 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | - 在 CI 中加入显存检查脚本，确保在低显存机器上提示“需要 ≥24 GB”。<br>- 为 `HermesDetector` 添加 **输入长度上限** 与 **异常安全包装**（捕获 `JSONDecodeError`，返回空调用列表）。<br>- 编写单元测试覆盖：<br>  1. 权重名称映射的完整性检查；<br>  2. `get_image_feature` 对不同 `num_patches`、`patch_pixel_values` 场景的行为；<br>  3. `HermesDetector` 对合法/非法 JSON 的解析路径。 |
| **运维/部署** | - 推荐使用 8 A100 (40 GB) 或同等显存的机器；如只能使用 24 GB，需开启 **模型并行** 或 **FP8/INT8** 量化。<br>- 在模型启动脚本中加入 `--model StepVLForConditionalGeneration` 参数检查，防止误加载旧模型配置。 |
| **安全审计** | - 对外暴露的工具调用 API 加入 **输入白名单**（仅允许特定工具名），并在 `HermesDetector` 解析后对 `arguments` 做 schema 验证。<br>- 确认 `json.loads` 只用于可信来源的内容；若需要对未知用户输入做解析，可考虑使用安全的 JSON 库（如 `orjson`）或手写解析器。 |
| **文档** | - 更新模型列表文档，注明 **StepVL‑10B** 需要的显存、支持的多模态 token、以及兼容的 `hf_config`。<br>- 在函数调用文档中加入 **Hermes** 格式示例，说明 `<tool_call>` 包裹的 JSON 结构。 |

--- 

**结论**：此次提交在功能上显著扩展了 SGLang 的多模态能力，引入了大规模视觉‑语言模型并完善了工具调用解析路径。若在显存、权重兼容及安全解析方面做好前置保障，能够为用户提供更强的视觉理解与工具协同能力。建议在正式发布前完成上述风险缓解措施及测试覆盖。

---

### [RadixTree][3/N Refactor]：Support unified insert/evict params (#17401)
**SHA**: `f33022d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f33022d039c61ec139499b6651a43017ea4dd131)

**🎯 变更类型**：重构 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：在 `sglang/srt/mem_cache` 体系中引入 **统一的插入/驱逐参数结构**（`InsertParams`、`EvictParams`）以及对应的结果结构（`InsertResult`、`EvictResult`），并统一所有缓存实现（Radix、Mamba、SWA、Chunk、HiRadix、RadixCacheCpp、LMCache、Storage 等）以及调度策略的签名。所有调用方（包括单元测试）已改为使用新结构，旧的 `insert(key, value, …)` 与 `evict(num_tokens, …)` 接口被废弃。

---

### 🔍 技术洞察  

| 维度 | 影响 |
|------|------|
| **架构影响** | 1. **统一 API**：所有前缀缓存、树形缓存、块缓存等实现现在共享同一套参数/返回对象，降低代码重复度，便于后续在 `InsertParams` 中加入新字段（如新的 cache 类型特有的元数据）而无需逐一改动实现。<br>2. **接口层次清晰**：`BasePrefixCache` 现在明确 `insert → InsertResult`、`evict → EvictResult`，调用者能够直接获取插入得到的前缀长度以及驱逐的真实 token 数量/类型（full、SWA、Mamba）。<br>3. **向后兼容风险**：项目内部已全部迁移，但外部使用 `sglang` 的第三方代码仍可能调用旧签名，导致运行时 `TypeError`。需要提供兼容层或文档提醒。 |
| **性能影响** | 1. **微小开销**：使用 `dataclasses` 创建轻量对象，CPU/内存开销在纳秒级别，可忽略。<br>2. **代码路径未变**：核心的插入/驱逐算法（例如 Radix 树的遍历、优先级堆）保持不变，性能基准应保持原状。<br>3. **返回值使用**：原先部分实现忽略 `evict` 返回值；现在返回 `EvictResult`，如果调用方不再使用返回值，几乎没有额外成本。 |
| **安全考虑** | - 不涉及网络或文件 I/O，安全风险基本不变。<br>- 统一参数结构有助于 **参数校验**：可以在 `InsertParams.__post_init__` 中统一检查 `key`、`value` 的合法性，降低因各实现自行验证导致的遗漏。当前提交未加入此类校验，建议后续补充。 |
| **可维护性** | - **代码可读性提升**：调用处更直观（`InsertParams(key=…, value=…, chunked=True, priority=5)`），不必记忆每个实现的位置参数顺序。<br>- **测试覆盖**：所有相关单元测试已同步改写，新增对 `EvictResult` 的断言，提升变更的安全网。<br>- **文档/类型提示**：`BasePrefixCache` 方法签名已更新，IDE 自动补全将更友好。 |

---

### ⚠️ 潜在风险  

1. **外部破坏兼容**  
   - 第三方项目或旧的脚本仍使用 `tree.insert(key, value, ...)`、`tree.evict(num_tokens)` 形式，会抛 `TypeError`。  
   - **建议**：在 `BasePrefixCache` 或各子类中保留 *deprecated* 包装方法（如 `def insert_legacy(self, key, value, …): return self.insert(InsertParams(...))`），并在文档中标记已废弃。

2. **默认字段遗漏**  
   - `InsertParams` 包含多个可选字段（`mamba_value`, `prev_prefix_len`, `swa_evicted_seqlen` 等），某些实现（ChunkCache）仅使用 `key/value`。如果调用方未显式提供这些字段，默认值必须足够安全。当前实现已给出默认 `0` / `None`，但未来若加入必须非零的字段，需同步更新所有实现的默认处理逻辑。

3. **返回值未被检查**  
   - `evict` 现在返回实际被驱逐的 token 数量。已有代码往往忽略返回值，若内部驱逐策略因粒度问题驱逐超过请求量，外部仍认为只驱逐了指定数量，可能导致后续分配判断错误。  
   - **建议**：在关键路径（如 `alloc_paged_token_slots_extend`、`alloc_req_slots`）使用返回的 `EvictResult` 来更新内部计数或做断言。

4. **并发/锁定行为**  
   - `evict` 与 `insert` 在多线程环境下仍通过 `inc_lock_ref/dec_lock_ref` 控制。统一参数不改变锁定逻辑，但若未来在 `InsertParams` 中加入 **priority**（已存在）并实现基于 priority 的抢占，需确保锁定顺序一致，以防死锁。

5. **测试依赖变化**  
   - 单元测试已全部改为新 API，若某些 CI 环境仍缓存旧的 `.pyc`，可能出现 “duplicate definition” 错误。确保 **清理构建缓存**（`git clean -fdx`）或在 CI 中重新安装依赖。

---

### 💡 关注建议  

| 目标 | 建议 |
|------|------|
| **兼容性** | - 添加 **deprecated** shim functions或 `@deprecated` 装饰器，保持旧签名 1 年以上的向后兼容窗口。<br>- 在发布日志和 README 中明确说明 `insert`/`evict` 已改为对象参数。 |
| **文档** | - 更新 `sglang/srt/mem_cache/base_prefix_cache.py` 中的类注释，列出 `InsertParams` 与 `EvictParams` 各字段的意义与适用缓存类型。<br>- 为每个实现（RadixCache、MambaRadixCache、SWA…）补充 **示例代码**，帮助使用者快速上手。 |
| **类型安全** | - 为 `InsertParams.__post_init__` 加入 **断言**：`assert isinstance(self.key, RadixKey)`、`assert self.value is None or isinstance(self.value, torch.Tensor)`。<br>- 在 `evict` 前检查 `params.num_tokens >= 0`、`params.swa_num_tokens >= 0` 等。 |
| **监控/日志** | - 在 `evict` 的实现里加入 **日志**（DEBUG 级别）打印各类驱逐量（full、SWA、Mamba），便于后续调优。 |
| **性能回归** | - 在 CI 中保留 **基准测试**（如 `bench_mem_cache.py`），对比新旧实现的吞吐量与延迟，确保统一参数没有意外引入性能下降。 |
| **测试覆盖** | - 为 `InsertResult`、`EvictResult` 增加 **属性校验**（如 `result.prefix_len >= 0`），并在关键路径（调度器、cache controller）加入 **断言**，防止返回非法负值。 |
| **后续扩展** | - 统一结构为后续 **跨缓存类型的统一调度**（例如同时考虑 KV、Mamba、SWA）提供便利，未来可以在 `InsertParams` 中加入 `metadata: dict`，实现更灵活的插件化。 |

---

**结论**：此次提交通过统一插入/驱逐参数与返回对象，显著提升了 `sglang` 内存缓存子系统的**可维护性**与**可扩展性**，对业务功能（前缀匹配、KV 复用）没有实质性性能回退。唯一需要重点关注的是 **向后兼容** 与 **返回

---

### Add Liquid Foundation Model (LFM2) (#16890)
**SHA**: `d6e2b88` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d6e2b88288ffd21c67342ebbeda0a9152e843ec1)

**🎯 变更类型**：功能增强 / 新模型集成  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
本次提交为 SGLang 项目新增对 **Liquid Foundation Model 2 (LFM2)** 的完整支持，涉及：  
1. 在 `configs` 中加入 `Lfm2Config`，并在全局 `__init__` 中注册。  
2. 实现 LFM2 的模型主体 `Lfm2ForCausalLM`，包括混合注意力‑卷积（ShortConv）层、Mamba2 缓存、KV‑cache 兼容等。  
3. 为 LFM2 增加专属 **function‑call detector**（Pythonic 与 JSON 双格式），并在 `FunctionCallParser` 中注册。  
4. 调整 `mamba_utils` 支持 `float16`/`bfloat16` 双精度，兼容 LFM2 的卷积缓存 dtype。  
5. 在 `model_executor`、`server_args` 中加入 LFM2 的特殊调度、缓存、后端限制逻辑。  
6. 完备单元测试及 OpenAI‑compatible server 测试，确保工具调用、流式解析、模型生成在 LFM2 上可用。  

**🎯 影响范围**  
- **配置层**：`sglang.srt.configs.lfm2`、`__init__.py`  
- **模型层**：`sglang.srt.models.lfm2`、`model_executor/model_runner`、`mamba_utils`  
- **功能调用层**：`sglang.srt.function_call.lfm2_detector`、`function_call_parser`  
- **服务器启动层**：`sglang.srt.server_args`（对 radix cache、attention backend 的限制）  
- **测试层**：新增模型、解析、工具调用的单元测试与 OpenAI‑Server 端到端测试。  

---

## 🔍 技术洞察

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 引入 **Hybrid Decoder**（attention + ShortConv）需要在 `ForwardBatch` 与 `HybridReqToTokenPool` 中新增对 **conv 缓存** 的管理。<br>- `Lfm2Config` 通过 `full_attention_layer_ids` / `linear_layer_ids` 动态区分 KV‑cache 与卷积缓存，实现统一的请求‑到‑token 调度。<br>- `model_runner.mamba2_config` 扩展至 `Lfm2Config`，保证调度器可获取正确的层配置信息。<br>- `server_args` 对 LFM2 明确禁止 `triton`、`mamba extra buffer`，并在 SM100 GPU 上默认切换到 `flashinfer`，防止第一层非注意力导致的兼容性问题。 |
| **性能影响** | - **ShortConv** 使用 Mamba‑optimized `causal_conv1d_fn / _update`，在 **prefill** 阶段能够一次性完成卷积，**decode** 时仅更新卷积缓存（`kernel‑1`）对应的几条 token，理论上 **线性** 内存与 **常数** 时间复杂度。<br>- 通过 `mamba2_state_dtype` 现在可自行选择 `float16`，在显存受限的机器上可把卷积状态降到 `float16`，显著降低显存占用。<br>- 由于 LFM2 前几层可能是卷积，`disable_radix_cache` 必须开启，否则会触发不兼容的页面大小（`page_size=1`）错误；相应的 `disable_overlap_schedule` 也会被迫打开，导致 **解码吞吐** 可能略低于纯注意力模型。<br>- 通过 `attention_backend=flashinfer`（SM100）可以恢复高效的 Flash‑Attention，抵消卷积层带来的额外开销。 |
| **安全考虑** | - 新增的 `lfm2_detector` 解析 **Pythonic** 表达式时使用 `ast`，仅接受 **字面量**（`Constant`、`Dict`、`List`、`Tuple`、`UnaryOp`），并显式禁止任意代码执行，安全性与已有 `PythonicDetector` 保持一致。<br>- `parse_streaming_increment` 对 **partial token** 进行缓冲，避免因不完整输入产生错误解析或异常泄漏。<br>- 环境变量 `SGLANG_MAMBA_CONV_DTYPE` 与 `SGLANG_MAMBA_SSM_DTYPE` 受控在模型内部使用，不会直接暴露给外部请求。 |
| **可维护性** | - 代码结构遵循现有 **config‑model‑detector** 三层模式，新增文件数量适中（`lfm2.py`、`lfm2_config.py`、`lfm2_detector.py`）。<br>- `CONFIG_MAPPING._extra_content["lfm2"] = Lfm2Config` 直接覆盖 HF 注册，避免重复注册冲突。<br>- 单元测试覆盖 **parser、streaming、模型加载、生成**，提供回归保障。<br>- 通过 `mamba_utils` 中的 dtype 映射统一处理，可在其他模型（如未来的 LFM3）复用。 |

---

## ⚠️ 潜在风险

| 风险类别 | 具体描述 | 可能后果 | 缓解措施 |
|----------|----------|----------|----------|
| **运行时兼容性** | LFM2 首层可能为 **ShortConv**，而 `triton` 后端仅支持注意力层；若用户强制 `--attention-backend triton`，会在启动时报错或崩溃。 | 服务器启动失败，无法对外提供服务。 | `server_args` 已加入断言，启动时直接报错；建议在文档中明确“不支持 triton”。 |
| **显存泄漏** | `conv_weight` 在 `load_weights` 中通过 `squeeze(1)` 转换，若模型权重文件未遵循 `(hidden,1,kernel)` 形状，可能导致维度不匹配。 | 初始加载阶段抛异常，模型不可用。 | 添加检查：`assert loaded_weight.dim()==3`，并在异常信息中提醒用户使用正确的权重文件。 |
| **缓存不一致** | `HybridReqToTokenPool` 为每个 LFM2 的 conv 层分配 `conv` 缓存，但 `forward_batch.forward_mode.is_idle()` 时直接返回 `hidden_states`，可能导致 **缓存未初始化** 的情况下仍调用 `self.conv`，触发 `None` 错误。 | 解码阶段崩溃。 | 在 `Lfm2ShortConv.forward` 前加入 `if conv_state is None: raise RuntimeError(...)` 或在 `HybridReqToTokenPool` 初始化时确保所有 conv 缓存创建。 |
| **流式解析卡顿** | `parse_streaming_increment` 依赖完整 `<|tool_call_end|>`，若模型输出 **缺失结束标记**（例如因截断或错误），检测器会一直缓冲，导致后续 normal text 永久不输出。 | 客户端长时间无响应。 | 在 `StreamingParseResult` 返回时加入超时/最大缓冲长度检查，超过阈值时强制 flush 并记录 warning。 |
| **dtype 不匹配** | `mamba2_state_dtype` 默认 `bfloat16`，但部分 GPU（如 RTX 30 系列）不支持 `bfloat16`，若未显式设置 `SGLANG_MAMBA_CONV_DTYPE=float16`，可能导致运行时报错。 | 启动时失败或出现 NaN。 | 在 `server_args` 检测当前 CUDA capability，若不支持 `bfloat16` 自动强制 `float16`，并给出提示。 |
| **工具调用安全** | 虽然 `Lfm2Detector` 使用 `ast` 限制，但仍会接受 **负数**、**布尔**、**None** 等字面量，如果业务侧对参数做精细校验会被绕过。 | 潜在业务逻辑错误（如负数 ID）。 | 在业务层（Tool 注册）加入 **schema validation**，不依赖模型侧的解析安全。 |

---

## 💡 关注建议

1. **文档与使用指南**  
   - 在官方 README / Model Zoo 中加入 LFM2 的 **部署限制**（不支持 `triton`、必须开启 `--disable-radix-cache`、推荐 `flashinfer`），并说明 `SGLANG_MAMBA_CONV_DTYPE` 默认值与显存/性能关系。  
   - 示例脚本展示 **环境变量** 配置

---

#### 🟡 中重要度变更 (7)

### [Diffusion] Make the apply_qknorm function easier to use (#17537)
**SHA**: `5324027` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/53240270075c2f0651a9909980c174da342e4a5f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**核心改动**  
1. `layernorm.apply_qk_norm` 现在在内部自行判断是否可以使用 fused‑inplace 版本，若不支持会回退到普通 RMSNorm，并移除了对 `can_use_fused_inplace_qknorm` 与 `current_platform.is_cuda()` 的外部判定。  
2. 所有 Flux 系列模型（`flux.py、flux_2.py、zimage.py`）的 Q/K 归一化逻辑统一改为直接调用 `apply_qk_norm`，去掉了原来的条件分支和警告。  
3. 相应的导入 `can_use_fused_inplace_qknorm` 以及 `_is_cuda` 变量被删除，导致代码更简洁、易维护。  

**影响范围**  
- `sglang/multimodal_gen/runtime/layers/layernorm.py`（RMSNorm 回退路径）。  
- `sglang/multimodal_gen/runtime/models/dits/flux*.py`、`zimage.py`（注意力层 Q/K 归一化）。  

**关注建议**  
- **功能兼容**：确认 `apply_qk_norm` 在 AMD/ROCm 环境下仍能正确执行 RMSNorm 回退，否则可能出现运行时错误。  
- **性能评估**：在 CUDA 环境下确保 fused‑inplace 仍被触发，并与旧实现进行基准对比，避免因统一调用导致的微小性能回退。  
- **In‑place 安全**：`allow_inplace=True` 仍然使用，确保传入的张量在非 fused 场景下不会被意外修改，必要时在 `apply_qk_norm` 中加入拷贝保护。  
- **测试覆盖**：新增跨平台（CUDA、ROCm、CPU）单元测试，验证 `apply_qk_norm` 的分支路径以及警告信息（若需要）是否如预期。  
- **文档更新**：在 API 文档中说明 `apply_qk_norm` 现在是统一入口，外部不再需要自行检测 fused 能力。  

整体来看，此次改动提升了代码的可读性和可维护性，只要确认回退路径与性能保持一致，即可安全合并。

---

### [NPU] bugfix with Kimi-k2 and bge-reranker-v2 model (#17478)
**SHA**: `5d299c2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5d299c25c04a9e28170659b7588afcc0b6f84599)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 Ascend NPU 注意力实现 (`ascend_backend.py`) 中，新增 `causal` 判定并在交叉注意力或 Encoder‑Only 场景下关闭因果掩码，防止原本的强制因果导致结果错误。  
2. `compressed_tensors_moe.py` 将权重属性名从 `intermediate_size_full` 改为 `moe_intermediate_size`，兼容最新模型配置。  
3. `rotary_embedding.py` 放宽设备判断条件，加入对 NPU 的显式支持，避免在 NPU 环境下误走 CUDA 路径。  
4. `bert.py` 根据全局 server 参数 `is_embedding` 自动切换 pooling 类型（CLS ↔ LAST），提升 BERT 用作向量检索时的表现。  
5. 新增 NPU 上的向量模型（bge‑large‑en‑v1.5）预填日志对齐测试，用 CI 验证 embedding 输出是否在容差内。

**🎯 影响范围**  
- `sglang/srt/hardware_backend/npu/attention/ascend_backend.py`（注意力前向）  
- `sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py`（MoE 权重加载）  
- `sglang/srt/layers/rotary_embedding.py`（旋转位置编码）  
- `sglang/srt/models/bert.py`（BERT pooling 逻辑）  
- CI 测试目录 `test/registered/ascend/embedding_models/`（新 NPU embedding 用例）

**💡 关注建议**  
- **功能验证**：在 CPU、CUDA、NPU 三种后端分别跑一次完整的推理对比，确保 `causal` 分支在交叉注意力/Encoder‑Only 场景下不再出现 mask 错误。  
- **向后兼容**：`moe_intermediate_size` 变更可能影响使用旧模型配置的用户，建议在 `weight_utils` 或配置解析层提供兼容映射或报错提示。  
- **文档更新**：在模型部署文档中注明 `is_embedding` 参数对 BERT pooling 的影响，以及 NPU 环境下需要的 `torch_dtype`（float16）和 `attention_backend="ascend"`。  
- **CI/测试**：新加入的 embedding 测试已标记为 nightly，建议在 PR 合并前在本地开启 NPU 模拟或使用容器跑一次，以防止 “embeddings are not all close” 的 flaky。  
- **性能监控**：`causal` 条件加入后，部分非因果场景会少做一次 mask，留意是否出现意外的吞吐提升或数值漂移。  

总体来看，此次提交修复了 NPU 注意力的关键逻辑错误并提升了模型在向量检索任务中的易用性，改动范围适中，建议在完整的多后端回归后合并。

---

### refactor Qwen3-Next with a new RadixLinearAttention (#17373)
**SHA**: `419bbce` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/419bbcee10a5a2f76b8b2612c7c2fbde3d0be645)

**🎯 变更类型**：重构 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `sglang/srt/layers/attention` 中新增 `RadixLinearAttention`，实现 GDN‑style 线性注意力，并把原有的 `RadixAttention` 相关逻辑迁移到新的后端 `HybridLinearAttnBackend`。  
- `HybridLinearAttnBackend` 的 `forward_decode / forward_extend` 参数签名从直接传递 `q/k/v` 改为统一使用 `layer: RadixLinearAttention` 并通过 `layer` 本身提供卷积权重、bias、激活等配置。  
- `Qwen3-Next` 模型在初始化时实例化 `RadixLinearAttention`（`self.linear_attn`），并在 `_forward` 中直接调用该层，去掉了旧的 `kwargs` 组装逻辑。  

**🎯 影响范围**  
- **attention/backend**：`hybrid_linear_attn_backend.py` 大幅改写，兼容全注意力和线性注意力两套路径。  
- **新模块**：`radix_linear_attention.py`（线性注意力层的包装）。  
- **模型层**：`models/qwen3_next.py` 增加 `self.linear_attn`，修改前向路径以使用新层。  
- **可能间接影响**：调度器、CUDA Graph、MemoryPool 等使用 `layer.layer_id`、`conv_weights` 等属性的代码，需要确认仍能通过 `RadixLinearAttention` 访问到相同字段。

**💡 关注建议**  
1. **兼容性验证**：确保老模型仍能在 `full_attn_layers` 中走原有 `RadixAttention` 路径，防止意外切换到线性注意力导致精度回退。  
2. **单元/集成测试**：覆盖 `forward_decode`、`forward_extend` 两种模式，特别是 `is_target_verify` 分支和 CUDA‑graph 场景。  
3. **参数一致性**：`RadixLinearAttention` 复制了 `RadixAttention` 的属性（`conv_weights、bias、A_log、dt_bias`），检查模型序列化/加载时这些张量是否被正确保存与恢复。  
4. **性能基准**：线性注意力的 GPU 内核与原始实现不同，建议对比 decode/extend 的吞吐与显存占用，确保改动没有引入意外的回退。  
5. **文档/注释**：由于新 API 引入了可选的 `q/k/v` 与 GDN 参数 `mixed_qkv、a、b`，建议在 `HybridLinearAttnBackend`、`RadixLinearAttention` 添加使用说明，帮助后续维护者快速定位参数来源。  

总体来看，此次改动为 Qwen3‑Next 引入了更高效的线性注意力实现，代码结构更清晰，但涉及多处后端签名修改，务必通过完整回归测试确认功能与性能均保持预期。

---

### [diffusion] feat: enable passing Cache‑DiT config for diffusers backend (#16662)
**SHA**: `71482dd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/71482dd17152ee864a75c2f215d65139c026a6f5)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 Diffusers 后端新增对 Cache‑DiT 加速配置的支持。通过 `--cache-dit-config` 传入 YAML/JSON 文件，可在 Diffusers pipeline 中自动加载并启用 Cache‑DiT；同时对注意力后端参数做了统一处理，完善了文档和 CLI 参数。  

**🎯 影响范围**  
- `sglang.multimodal_gen.runtime.pipelines.diffusers_pipeline`：新增 `_apply_cache_dit`，在加载管道后调用外部 `cache_dit`；注意力后端参数检查逻辑同步更新。  
- `sglang.multimodal_gen.runtime.server_args`：新增 `cache_dit_config` 字段并在 CLI 中注册 `--cache-dit-config`，对 `--attention-backend` 的帮助信息做了细化。  
- `sglang.multimodal_gen.configs.sample.sampling_params`：在解析用户参数时剔除 `diffusers_kwargs`，避免冲突。  
- 文档 (`attention_backends.md`, `cache_dit.md`, `cli.md`) 与 `pyproject.toml` 中依赖版本同步升级至 `cache-dit==1.2.0`。  

**💡 关注建议**  
1. **兼容性**：保留旧的 `--diffusers-attention-backend` 参数（已映射到 `--attention-backend`），但在发布说明中标记为即将废弃，防止用户突发失效。  
2. **依赖管理**：确保 CI 环境安装 `cache-dit>=1.2.0`，并在 `requirements.txt` 或 `pyproject.toml` 中锁定上限，以免未来的重大版本破坏 API。  
3. **错误处理**：当前在导入或启用 Cache‑DiT 时会抛出异常，建议捕获后给出更明确的提示（如“请检查 config 路径或升级 cache‑dit”），提升用户体验。  
4. **测试覆盖**：新增单机与多卡（`world_size>1`）两种路径的单元/集成测试，验证 `--cache-dit-config` 在不同后端、不同注意力设置下的行为。  
5. **文档同步**：更新示例脚本和快速上手章节，确保所有新参数在帮助信息 (`sglang generate -h`) 中可见。  

总体而言，此次改动为 Diffusers 流程引入了强大的缓存加速能力，影响主要集中在 pipeline 加载与 CLI 参数解析，建议做好兼容性、依赖和测试保障后再正式发布。

---

### [AMD] fix amd ci dpskv32 (#17432)
**SHA**: `17807ca` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/17807caf82c8a1c186741e23c171c3c9dc7cac70)

**变更类型**：其他（CI 与 AMD 适配）  
**重要程度**：🟡 中  

**变更摘要**  
1. 将 AMD CI 的分区数从 3 降为 2，配合单机 8 GPU 运行，以缩短 CI 时长。  
2. 在 `nsa_indexer.py` 中为 HIP（AMD）平台加入显式 dtype 转换，防止 `torch.compile` 在混合精度下出现类型不匹配。  
3. 调整测试超时、服务器启动超时以及部分 DeepSeek‑V3.2 的 AMD CI 跳测策略，使 DP（数据并行）测试仅在 nightly 曝光，TP（Tensor‑Parallel）仍保留。  

**影响范围**  
- `.github/workflows/pr-test-amd.yml`（CI 任务调度）  
- `python/sglang/srt/layers/attention/nsa/nsa_indexer.py`（注意力投射层）  
- `python/sglang/test/test_utils.py`（默认超时常量）  
- `test/registered/amd/*`（DeepSeek‑V3.2 相关单元测试）  

**关注建议**  

1. **CI 分区改动**：改为 `part: [0, 1]` 并把 `--auto-partition-size` 设为 2，确保测试脚本仍能覆盖全部子集。建议在本地或自建 AMD runner 上跑一次完整的矩阵，验证不存在遗漏的分区。  
2. **dtype 转换**：在 ` _project_and_scale_head_gates` 与 `_get_logits_head_gate` 中加入 `x = x.to(self.weights_proj.weight.dtype)`，这只在 HIP 环境生效。请确认 `self.weights_proj.weight.dtype` 与模型权重保持一致（通常是 `torch.float16`），防止隐式提升导致显存浪费或性能下降。  
3. **`torch.compile` 条件**：目前在 HIP 环境使用 `lambda f: f` 绕过编译，若未来希望开启编译，需要补全对应的编译路径并确保上述 dtype 转换仍有效。  
4. **测试超时**：`DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH` 从 3000 s 改为 3600 s，以匹配 H200 大模型启动时间。请检查 CI 机器的资源限制，防止因超时阈值过大导致卡住。  
5. **跳测策略**：DP 测试已在 AMD CI 中被 `skipIf(is_in_amd_ci())` 屏蔽，只保留 TP 测试。若后续想在 AMD CI 中重新打开 DP，需重新评估运行时长与资源消耗。  
6. **性能阈值微调**：在 `test_deepseek_v32_mtp.py` 中把对 AMD CI 的速度阈值从 60 调至 55，避免因硬件波动导致误报。建议在多次 CI 运行后观察实际达标率，再决定是否收紧或放宽。  

整体来看，此次改动针对 AMD 平台的 CI 稳定性与资源利用率进行了一系列裁剪和兼容性修补，风险相对可控。后续请关注 AMD runner 上的完整矩阵执行情况以及 `torch.compile` 在 HIP 环境的潜在升级路径。

---

### fix: prefer to use max_completion_tokens rather than max_tokens (#17516)
**SHA**: `a5bbcda` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a5bbcda968b69c89e8943d6dfdb6de8a3dd6a8d9)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 `sglang/srt/entrypoints/openai/protocol.py` 中，生成请求的采样参数时，将 `max_new_tokens` 的取值顺序从 **`self.max_tokens or self.max_completion_tokens`** 改为 **`self.max_completion_tokens or self.max_tokens`**，确保在同时提供两者时优先使用 `max_completion_tokens`（即 OpenAI API 中的 `max_completion_tokens`），而不是旧的 `max_tokens`。

**🎯 影响范围**  
- `python/sglang/srt/entrypoints/openai`：OpenAI 兼容层的请求构造逻辑。  
- 与 `max_completion_tokens` 参数交互的上层业务（如 ChatCompletion、Completion 接口）会得到更符合官方语义的 token 限制。  

**💡 关注建议**  
1. **回归测试**：重点检查含 `max_completion_tokens` 的调用路径，确认返回的 `max_new_tokens` 与期望一致，尤其是对 `max_tokens` 仍在使用的旧客户端保持兼容。  
2. **文档同步**：在使用说明中明确 `max_completion_tokens` 的优先级，以免用户对两者含义产生混淆。  
3. **边界情况**：验证 `max_completion_tokens=0` 或 `None` 时行为是否仍回退到 `max_tokens`，防止意外截断。  
4. **性能影响**：改动仅涉及参数选择，不会影响运行时性能。  

总体来说，此改动提升了与 OpenAI 官方 API 的一致性，风险有限，只需针对相关接口做一次完整的参数回归即可。

---

### [Auto Sync] Update detokenizer_manager.py, io_struct.py, mu... (20260120) (#17442)
**SHA**: `b74a57a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b74a57a8d96ed47f9fa074536c9a3cb56be13884)

**🎯 变更类型**：功能增强 & 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交对 SGLang 进行了一系列内部结构和接口的优化，包括：① 文档中对 CUDA 环境的安装说明进行细化；② 从所有 `pyproject*.toml` 中移除 `torchao==0.9.0` 依赖；③ **HTTP API** 大幅调整：废除 `/get_weight_version` 与 `/weight_version`，统一使用返回的 `/model_info`；删除 `/generate_from_file` 接口；④ 统一 **routed_experts** 字段命名、结构及其在 `io_struct`、`detokenizer_manager`、`multi_tokenizer_mixin`、`scheduler_output_processor_mixin`、`tokenizer_manager` 中的传递逻辑；⑤ 修正 `UnquantizedLinearMethod` 的导入路径，并在部分单元测试中加入断言细节。

**🎯 影响范围**  
- **核心模块**：`sglang/srt/managers/*`（detokenizer、tokenizer、io_struct、scheduler_output_processor、multi_tokenizer_mixin）  
- **HTTP Server**：`sglang/srt/entrypoints/http_server.py`（端点变更、依赖 `UploadFile` 被移除）  
- **依赖管理**：`python/pyproject*.toml`（去除 `torchao`）  
- **测试**：`test/registered/openai_server`、`test/registered/quant`  
- **文档**：`docs/get_started/install.md`

**💡 关注建议**  
1. **兼容性**：项目已有使用 `/get_weight_version`、`/weight_version` 或 `/generate_from_file` 的用户将收到 404，建议在升级指南中明确迁移路径，或在后向兼容层保留旧接口的重定向。  
2. **字段迁移**：所有内部或外部代码若直接访问 `output_routed_experts`（如日志、监控、模型分析工具），需改为 `routed_experts`，并注意该字段现在同时包含 **输入+输出** token，新增的 `routed_experts_start_len` 用于截断起始位置。  
3. **依赖变动**：`torchao` 被移除后，若业务依赖其量化或稀疏特性，需要自行引入或等待后续社区适配；CI/CD 中的构建脚本应同步更新。  
4. **测试更新**：确保新字段在 `BatchStrOutput`、`BatchTokenIDOutput` 的序列化/反序列化路径已覆盖；现有单元测试已加入对 `logprobs.tokens` 类型的检查，可进一步加入对 `routed_experts` 的断言。  
5. **文档同步**：`install.md` 已加入强制 reinstall torch 的说明，但对应的 CUDA 版本号应保持与官方 PyTorch wheel 对齐，避免误导用户。  

总体来看，此次改动提升了内部数据一致性并简化了 HTTP 接口，但会对已有集成产生破坏性影响，务必在发布说明中提供迁移指南并考虑短期的兼容层。

---

#### 🟢 低重要度变更 (29)

### [hotfix] Reenable all reduce fusion on sm100 (#17591)
**SHA**: `283a2da` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/283a2daeaa88532299fc5a9c6db5bcab41f6ef97)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `communicator.py` 中恢复 sm100 上的 all‑reduce 融合，改为在支持 sm90 或 sm100 时启用，并添加说明注释。

---

### [BUGFIX] Skip Mamba Cache Slot 0 to Avoid Using Dummy Cache (#17404)
**SHA**: `72f790b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/72f790bf6f1fee2b31bd80a6652976fda9d9c3ce)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `MemoryPool` 中将空闲槽位初始化及清空逻辑从 `0..size-1` 改为 `1..size`，跳过占位的 slot 0，防止写入 dummy cache。

---

### ci(pre-commit): avoids extraneous codespell exclusions (#17590)
**SHA**: `42523d0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/42523d0364f1177b280f2ec3bdfba3cabccc9bad)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `.pre-commit-config.yaml` 中对 `codespell` 的冗余 `exclude` 列表，改为在 `chat_template.rs` 中使用 `codespell:ignore` 注释屏蔽特定导入。对功能无影响，仅简化 CI 配置。

---

### [diffusion] model: optimize torch.compile (#17472)
**SHA**: `3705f90` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3705f9062954ff809585c4b85176c3e32b48d120)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将原先的 `compile_module_with_torch_compile` 重命名为 `_maybe_enable_torch_compile`，改为仅在 `nn.Module` 上调用 `module.compile`，并统一使用 `nn.Module` 类型注解，简化 torch.compile 调用路径。

---

### Change naming for graph mode on multiplatform (#17469)
**SHA**: `a4dc432` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a4dc4325876d37841ff51226538ba768e48f5334)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在日志和设备图初始化中引入 `graph_backend` 映射，将 `cuda graph`、`cpu graph`、`npu graph` 的命名统一抽象，提升跨平台可读性，代码量微增。

---

### Add ZhengWG to CI_Permission (#17572)
**SHA**: `2262c5c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2262c5c9b556270ebf42175f16d6d666b41fd4dd)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中新增 `ZhengWG` 条目，赋予其标签触发 CI、失败 CI 重新运行、阶段重新运行等权限，并设置 60 分钟冷却。

---

### Add xyjixyjixyji to CI_Permission (#17559)
**SHA**: `8dae6ec` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8dae6ec03cb38aedf8a558a6b6f91c0a2bf54c87)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中新增用户 `xyjixyjixyji` 的 CI 权限，允许打标签运行 CI、重跑失败的 CI 并可重新运行阶段。

---

### [NPU] [Bug Fix] Fix typo in npu device check in gpt_oss.py (#17553)
**SHA**: `61abff6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/61abff66c150b559517f25a769f4740e23bbdefa)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `gpt_oss.py` 中将设备检查的变量名 `is_npu` 修正为正确的 `_is_npu`，保证在 NPU 环境下使用 `npu_swiglu_oai` 激活函数。

---

### [Fix] fix device orientation for image processor (#15859)
**SHA**: `6a8f68b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6a8f68b6d25f4519619472388654e3625f39dd99)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `base_processor.py` 中新增对 CPU 与 XPU 环境的检测与 device 选择逻辑，修正图像处理器在不同硬件上的设备分配错误。

---

### [CPU][Fix CI] Solidate torch version for sgl-kernel-cpu and fix device orientation error (#17460)
**SHA**: `672eb37` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/672eb37534bfc960f881dbcc07e975004907c958)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：统一 CPU 环境的 Torch 版本，新增 torchaudio、torchao 依赖，修正 `parallel_state` 中设备初始化逻辑，改名 CPU 包 (`sglang`→`sglang‑cpu`、`sgl‑kernel`→`sgl‑kernel‑cpu`)，并相应更新 Dockerfile、文档和配置文件。

---

### [Kernel] Little refactor of flashinfer allreduce norm fusion (#17474)
**SHA**: `e2d3353` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e2d33531f396018ee2dc1a361c67b85223870590)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `communicator.py` 中抽取 FlashInfer all‑reduce 融合条件为 `apply_flashinfer_allreduce_fusion`，简化并统一判断逻辑，同时加入对 SM100 性能回退的注释，删除冗余的批次大小检查。

---

### [hotfix] Fixes on cuda 13 docker image (#17541)
**SHA**: `fafa171` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fafa1715292aaad00163483bffb9947df36fe07e)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CUDA 13 Docker 镜像构建中新增 `INSTALL_FLASHINFER_JIT_CACHE=1` 参数；并在所有 pyproject 配置文件中统一加入 `torchao==0.9.0` 依赖。

---

### [NVIDIA] Fix CUDA arch requirement in nvfp4 cast (#12581)
**SHA**: `e95668a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e95668abc7811c53e427ae7f986d0e9bc8af6bfd)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `nvfp4_quant.cuh` 中对 CUDA 架构的判定从 `> 1000` 改为 `>= 1000`，修正对 SM100 系列及以上的 PTX 指令兼容性检查。

---

### [HotFix]Fix dtype mismatch in nsa indexer on AMD device (#17518)
**SHA**: `3373545` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3373545b9fba8f578efa5eb68c3212538ecb1e2b)

**🎯 变更类型**：代码重构（修复）  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `nsa_indexer.py` 中，根据设备选择参数数据类型，CUDA 使用 `bfloat16`，非 CUDA（如 AMD）使用 `float32`，解决了 AMD 上的 dtype 不匹配问题。

---

### Update release-branch-cut.yml for  actions: write (#17539)
**SHA**: `f2ae066` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f2ae066a6b77afa82800174e92efa7067db57830)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/workflows/release-branch-cut.yml` 中新增 `actions: write` 权限，确保工作流能够对 GitHub Actions 进行写操作。

---

### Optimize Qwen3-VL video memory usage (#16366)
**SHA**: `0c2993e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0c2993eed03a39f8b927a9c40b440b848408fd3c)

**变更类型**：其他  
**重要程度**：🟢低  
**摘要**：在 `qwen3_vl.py` 中，对 `get_video_feature` 加入在使用前将 `item.feature` 移至模型设备，使用完后再转回 CPU，以降低并发时的显存占用，防止 OOM。

---

### [Diffusion] Support select fa2 backend in hopper (#17514)
**SHA**: `590969e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/590969ee9c6af96ea484623241ca8530577986f7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `cuda.py` 中新增对 `FA2`（FlashAttention2）注意力后端的选择支持，加入日志提示并返回对应实现类路径。

---

### Increase wait-for-stage timeouts to handle long queue times (#17536)
**SHA**: `e6ccb29` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e6ccb2949b443111d20dba31ae5d97d850101cf3)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 GitHub Actions 工作流中阶段等待超时从 60/90 分钟提升至 240/480 分钟，并将轮询间隔固定为 2 分钟，以应对长队列等待。

---

### Fix import path for UnquantizedLinearMethod in test (#17529)
**SHA**: `d6ea2c5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d6ea2c529c772ae30dee257af517eaf48db25aae)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在测试文件中修正了 `UnquantizedLinearMethod` 的导入路径，确保测试能够正确引用该类并顺利运行。

---

### Remove test_gpt_oss_4gpu.py from __not_in_ci__ (keep in per-commit-4-gpu) (#17534)
**SHA**: `9be2a3a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9be2a3a9a332978641b6af29004d14c21d271d7f)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test/srt/run_suite.py` 中移除 `test_gpt_oss_4gpu.py`（700 行） 的禁用记录，使其仅保留在 `per-commit-4-gpu` 目录，避免 CI 中的间歇性失败。

---

### [Chore] include all jit files in building packages (#17493)
**SHA**: `95f59c1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/95f59c13fd0802042d588a6e302a04b3d4adabed)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `pyproject*.toml` 中的 `package-data` 从列举的单文件模式改为递归通配 `srt/**/*` 与 `jit_kernel/**/*`，一次性包含所有 JIT 相关文件。

---

### Temporarily disable flaky test_gpt_oss_4gpu.py on B200 (#17528)
**SHA**: `85d9af5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/85d9af51da7e898748743f3d105e299b9e6d16ec)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 B200 环境的测试套件中，将 `test_gpt_oss_4gpu.py` 从默认执行列表中移除，并在 `per-commit-4-gpu-b200` 分支中标记为已禁用，以规避该测试的间歇性失败（#17527）。

---

### ci(codespell): centralizes list of ignorable words (#17524)
**SHA**: `858f317` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/858f317f1339c04ee714e8b83c0058745f122041)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 codespell 的可忽略词统一写入 `.codespellrc`，并在 pre‑commit 配置中去掉额外的 `-L` 参数，保持列表同步。

---

### [new-model] Add support for `Cohere2ForCausalLM` behind Command-A and Command-R Models (#16927)
**SHA**: `cf89351` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cf893516913f02fa733cda4dfeb085e9d48f4696)

**🎯 变更类型**：代码重构 / 功能扩展  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `commandr.py` 中加入对 Cohere2（Command‑A / Command‑R）模型的兼容，新增 `Cohere2Config`，实现滑动窗口注意力和可选 logit scaling；同步更新文档中的模型列表。

---

### [Auto Sync] Update environ.py, fp8.py (20260121) (#17486)
**SHA**: `1fdf5ca` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1fdf5cac396579e78c028e6511c90972aed246ba)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增环境变量 `SGLANG_USE_AITER_FP8_PER_TOKEN`，在 fp8 量化路径中根据该标志及 AIter 支持，切换为 per‑token 量化并在动态情况下使用 `shuffle_weight`，提升对 AIter 的兼容性。

---

### ci: avoids duplication of codespell config (#17519)
**SHA**: `cda43ff` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cda43ffa4d4f04e099e3919c4954aedc0899fc82)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：新增 `.codespellrc` 统一代码拼写检查配置，删除各 `pyproject*.toml` 中重复的 `codespell` 配置，并在 pre‑commit 中使用该文件，避免配置重复。

---

### [Misc] Fix argument help string formatting (#17416)
**SHA**: `3908985` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/390898545eb61ea76a4e7f7c39ca811b7151724d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_args.py` 中，将帮助字符串中的 `%` 转义为 `%%`，防止 argparse 在输出帮助信息时出现格式化错误。

---

### [AMD] CI - Fix sgl-kernel unittest (#17490)
**SHA**: `b827e9d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b827e9d3819659383ccab31405d7a9dd699180f7)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 AMD CI 工作流中为 `sgl-kernel` 添加 `.github/workflows/pr-test-amd.yml` 触发路径，同时移除两个 AMD 相关的单元测试命令。

---

### Disable swa memory for gpt-oss with spec (#17517)
**SHA**: `d725487` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d725487dc8eaa56eaef39d248d46b2cc1c678b27)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_args.py` 中新增对 `speculative_algorithm` 的检测，若不为空则强制关闭混合 SWA 内存并给出警告，以规避 GPT-OSS 模型在推测解码下的 SWA 内存问题。

---

