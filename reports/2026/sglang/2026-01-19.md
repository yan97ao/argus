# 每日更新报告（2026-01-19）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-19 22:39:40 | zhangheng | [RadixTree][1/N Refactor]: Support unified match_prefix params (#17142) |
| 2026-01-19 22:36:52 | Ke Bao | Evict swa kv cache during decoding (#17220) |
| 2026-01-19 21:45:39 | ybyang | Fix v32 continue_final_message not work (#16567) |
| 2026-01-19 21:25:33 | Xiaoyu Zhang | [Diffusion] Apply qknorm to flux2 and apply lightx2v rms_norm_one_pass kernel(without residual) (#17305) |
| 2026-01-19 20:09:07 | b8zhong | [Refactor] Set `fp4-gemm-backend=auto` on SM100 and rename `fp4-gemm-backend` with `flashinfer_` prefix (#17309) |
| 2026-01-19 17:45:27 | Shu Wang | Inclusion of nvfp4 blockscale in EPLB Rebalance (#17158) |
| 2026-01-19 15:25:24 | Alison Shao | Migrate performance, accuracy, and quantization tests to CI registry (#17177) |
| 2026-01-19 15:24:11 | Xiaoyu Zhang | Revert "[Perf] fuse q, k norm for Flux2Attention (#17241)" (#17332) |
| 2026-01-19 15:20:22 | Alison Shao | fix(ci): rate limit and permission errors in trace publishing (#17238) |
| 2026-01-19 15:19:39 | Alison Shao | [CI] Add partition to stage-b-test-large-1-gpu (11->12) (#17245) |
| 2026-01-19 14:33:34 | Minglei Zhu | [Perf] fuse q, k norm for Flux2Attention (#17241) |
| 2026-01-19 14:13:48 | Kartik Ramesh | KV Cache Events with Attention DP bug fix (#16030) (#16412) |
| 2026-01-19 14:06:11 | yudian0504 | Fix kernel selection in biased_grouped_topk_gpu (#17325) |
| 2026-01-19 13:16:27 | Gaoji Liu | support new qwen3_coder_detector (#16744) |
| 2026-01-19 12:32:19 | Lianmin Zheng | [Auto Sync] Update tokenizer_manager.py (20260119) (#17317) |
| 2026-01-19 11:57:28 | Lianmin Zheng | Update code sync scripts (#17319) |
| 2026-01-19 11:54:11 | Yongfei Xu | [DeepSeek v3.2] Opt MTP decode cuda batch sizes and nsa implementation (#16961) |
| 2026-01-19 11:50:16 | Lee Nau | Use dsv3 optimized routing `fused_topk_deepseek` instead of `moe_fused_gate` (#15347) |
| 2026-01-19 11:36:16 | Baizhou Zhang | [Minor] Correct sglang version when installing from source (#17315) |
| 2026-01-19 11:23:15 | Shangming Cai | [PD] Optimize MHA models pp util calculation logic (#17306) |
| 2026-01-19 10:43:17 | Glen Liu | [Feature] overlap LoRA weight loading with compute (#15512) |
| 2026-01-19 08:39:20 | Mick | Update CODEOWNERS for multimodal_gen (#17308) |
| 2026-01-19 05:33:19 | Koushik Dutta | [GLM 4.7] Add RTX 6000 Pro aka sm120 (#17235) |
| 2026-01-19 04:54:36 | Jinyan Chen | [jit-kernel] Add CuTe DSL GDN Decode Kernel (#15631) |
| 2026-01-19 04:13:41 | Todobe | [NPU]Support GPT-OSS for NPU (#14197) |
| 2026-01-19 00:41:59 | Jerry Ji | [Refactor] Split out deepseek v2 weight loader function into mixin (#16649) |
| 2026-01-19 00:28:36 | Xiaoyu Zhang | [Diffusion] Apply jit qk_norm to flux1 (#17296) |

### 📊 统计摘要
> 本日共 27 个提交 | 🔴高 2 | 🟡中 13 | 🟢低 12
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [Migrate performance, accuracy, and quantization tests to ...](#8916b9d)
    - [fix(ci): rate limit and permission errors in trace publis...](#fb88fb6)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [[RadixTree][1/N Refactor]: Support unified match_prefix p...](#20b0523)
    - [Evict swa kv cache during decoding (#17220)](#ce8a6ac)
    - [Fix v32 continue_final_message not work (#16567)](#ebca587)
    - [[Diffusion] Apply qknorm to flux2 and apply lightx2v rms_...](#cc410a1)
    - [[Refactor] Set `fp4-gemm-backend=auto` on SM100 and renam...](#f374623)
    - [support new qwen3_coder_detector (#16744)](#858a4d6)
    - [Update code sync scripts (#17319)](#fc4b932)
    - [Use dsv3 optimized routing `fused_topk_deepseek` instead ...](#84c8390)
    - [[Feature] overlap LoRA weight loading with compute (#15512)](#ad1b4e4)
    - [[GLM 4.7] Add RTX 6000 Pro aka sm120 (#17235)](#d3eafc7)
    - [[jit-kernel] Add CuTe DSL GDN Decode Kernel (#15631)](#e00b434)
    - [[NPU]Support GPT-OSS for NPU (#14197)](#733de6b)
    - [[Refactor] Split out deepseek v2 weight loader function i...](#9343372)
  - [🟢 低重要度变更 (12)](#-🟢-低重要度变更-12)
    - [Inclusion of nvfp4 blockscale in EPLB Rebalance (#17158)](#5c02217)
    - [Revert "[Perf] fuse q, k norm for Flux2Attention (#17241)...](#a3d9a21)
    - [[CI] Add partition to stage-b-test-large-1-gpu (11->12) (...](#2d72e16)
    - [[Perf] fuse q, k norm for Flux2Attention (#17241)](#6494667)
    - [KV Cache Events with Attention DP bug fix (#16030) (#16412)](#5836324)
    - [Fix kernel selection in biased_grouped_topk_gpu (#17325)](#9fe56cd)
    - [[Auto Sync] Update tokenizer_manager.py (20260119) (#17317)](#e619f53)
    - [[DeepSeek v3.2] Opt MTP decode cuda batch sizes and nsa i...](#d2105d4)
    - [[Minor] Correct sglang version when installing from sourc...](#ea879c7)
    - [[PD] Optimize MHA models pp util calculation logic (#17306)](#0227db8)
    - [Update CODEOWNERS for multimodal_gen (#17308)](#51f147a)
    - [[Diffusion] Apply jit qk_norm to flux1 (#17296)](#330605c)
#### 🔴 高重要度变更 (2)

### Migrate performance, accuracy, and quantization tests to CI registry (#17177)
**SHA**: `8916b9d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8916b9d080acec83c23f1d3c8f5d74d1738ebc8f)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 将性能、准确度、量化相关的单元/端到端测试迁移到 CI 注册表目录 `test/registered/*`，并在每个测试文件中通过 `register_cuda_ci` 声明所属 CI 套件与预估执行时长。  
2. 更新 GitHub Actions 工作流（`pr-test.yml`、`pr-test-amd.yml`）的执行脚本，使其在 CI 环境中使用 `-w /sglang-checkout/test/registered/<suite>` 工作目录来运行对应的注册套件。  
3. 为多个 GPU、不同负载的性能基准（单卡、双卡、FP8、EAGLE、LoRA、VLM、Score/Embedding API、MoE、PP）新增或重构 test 文件，细化了每个场景的基准指标阈值。  
4. 为量化测试（AWQ、BNB、GGUF、GPTQ、Marlin‑MoE）统一挂载到 `registered/quant` 并标记套件 `stage-b-test-large-1-gpu`。  
5. 调整 CI 超时设置（多数作业 `timeout-minutes: 60`），统一使用 `actions/download-artifact@v4`（部分仍用 v6），并删除老的 `test_srt/test_bench_serving.py`（搬迁至 `registered/perf`）。  

**🎯 影响范围**  
- CI 工作流与脚本：`.github/workflows/pr-test.yml`、`.github/workflows/pr-test-amd.yml`。  
- 测试套件目录：`test/registered/perf/*`、`test/registered/eval/*`、`test/registered/quant/*`。  
- 相关 Python 模块：`sglang.test.ci.ci_register`（新增注册 API），以及若干 `test_*.py` 的 import 路径调整。  
- CI 运行时依赖：`scripts/ci/amd_ci_exec.sh`、`ci_install_dependency.sh`、`ci_install_deepep.sh` 等脚本的工作目录参数。  

**🔍 技术洞察**  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | - 通过 `ci_register.register_cuda_ci` 将测试与 CI 套件解耦，CI 只需依据套件名称自动挑选对应的测试文件，提升可维护性与可扩展性。<br>- 测试目录结构从平铺迁移为分层（`registered/perf`、`registered/eval`、`registered/quant`），代码层面无运行时依赖，仅影响 CI 入口脚本。<br>- CI 作业的 `-w /sglang-checkout/test/registered/<suite>` 参数将工作目录定位到对应套件，避免不同套件之间的交叉污染。 |
| **性能影响** | - 新增的基准测试覆盖了更多硬件配置（1‑GPU、2‑GPU、FP8、EAGLE、LoRA、VLM、Score/Embedding），可以在 CI 中直接捕获回归，对模型吞吐/延迟阈值提供更严格的监控。<br>- 由于每个基准均带有 `est_time` 估算，CI 调度器能够更合理地分配机器资源，长期看有助于降低整体流水线的 “超时 & 重跑” 成本。<br>- 增加的基准数量（约 30+ 额外的测试文件）会拉长单次 PR 的 CI 时长，尤其是 large‑gpu 套件；已通过统一 `timeout-minutes: 60` 限制单作业上限，防止卡死。 |
| **安全考虑** | - 测试代码本身仍然在受限的 sandbox 中执行，未引入新的网络访问或特权操作。<br>- 部分准确度测试会 `git clone https://github.com/merrymercy/human-eval.git`，这在 CI 环境中属于外部依赖，需确保网络访问受限且只读取公开仓库。<br>- 量化测试会下载模型权重（通过 `huggingface_hub`），同样依赖外部网络，建议在 CI 中使用缓存或预先拉取，以免因网络波动导致 CI 失败。 |
| **可维护性** | - 通过注册 API 把每个测试的期望执行时间显式声明，便于后续 CI 调度与资源规划。<br>- 所有旧的 `test_bench_*` 文件已集中搬迁，避免重复代码；但删除文件的同时需要确认所有 CI 作业已切换到新路径（已在工作流中完成）。<br>- `run_suite.py` 中的 suite 列表同步更新，确保 `stage-b-test‑*‑performance` 与 `stage-b-test‑*‑accuracy` 能被 CI 正确发现。 |
| **兼容性** | - 仅影响 CI 流程，对普通用户 `pip install sglang`、运行 `sglang` 命令行不产生任何变化。<br>- 对于本地开发者，若仍使用旧的 `test/*.py` 路径，需要手动切换到 `registered` 目录或通过 `register_cuda_ci` 调用。 |

**⚠️ 潜在风险**  
1. **路径不一致**：如果某些 CI 作业未同步更新 `-w` 参数，仍会在根 `test/` 目录查找旧文件，导致 “Test file not found” 错误。  
2. **超时/资源争夺**：新增的大量性能基准（尤其是 2‑GPU FP8、MoE、PP）在资源紧张的公共 runner 上可能触发 `timeout-minutes: 60`，导致部分作业提前终止，需要监控实际耗时并适时调整 `est_time`。  
3. **外部依赖不稳定**：`human-eval`、`huggingface` 权重下载若因网络或 HF 访问限制失败，会使准确度或量化套件报错，建议在 CI 中使用缓存（`actions/cache`) 或镜像。  
4. **并行度变化**：部分基准使用 `--auto-partition-id` 与 `--auto-partition-size` 分片执行，若 CI runner 上的 GPU 数量与预期不符（例如内部测试使用 2‑GPU 但 CI 实际提供 1‑GPU）会导致参数不匹配错误。  
5. **CI 版本回退**：工作流中仍保留 `actions/download-artifact@v6`（仅在 `stage-b-test-4-gpu-b200`），但多数已切换到 v4，若 runner 尚未支持 v6 可能产生兼容性问题。  

**💡 关注建议**  
- **CI 验证**：在主分支上新增一次 **dry‑run** 工作流，仅运行 `-w /sglang-checkout/test/registered/perf`，确保所有新路径与注册套件能够被自动发现。  
- **资源监控**：在 CI 中加入步骤记录每个基准的实际运行时长（如 `time -p python3 ...`），将结果回填到 `register_cuda_ci` 的 `est_time`，实现动态调优。  
- **缓存外部依赖**：利用 `actions/cache` 缓存 `human-eval` 与 HuggingFace 模型权重，降低网络波动导致的 flaky。  
- **文档同步**：在项目的 `CONTRIBUTING.md` 与 `README.md` 中增加 “CI 注册表目录结构” 与 “如何本地运行新基准” 的说明，防止贡献者仍使用旧路径。  
- **回滚策略**：若在某次合并后出现大量 CI 失败，可快速回滚 `pr-test.yml` 中的 `-w` 参数到原始 `test/` 目录，或暂时

---

### fix(ci): rate limit and permission errors in trace publishing (#17238)
**SHA**: `fb88fb6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fb88fb672eff72c0e619266b1c7b1752d2385f77)

**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
- 将原先把 GitHub 403 错误当作“配额/权限错误”统一处理的逻辑拆分为 **速率限制** 与 **权限错误** 两类判定。  
- 新增 `is_permission_error` 与 `is_rate_limit_error` 两个检测函数，对 403 错误根据返回体文字进行区分。  
- 在 CI trace‑publish 流程中，遇到权限错误时直接打印明确提示并 **退出 CI**，而速率限制错误仍保持原有的“跳过/重试”策略。  

**🎯 影响范围**：  
- `scripts/ci/publish_traces.py`（CI 自动化脚本）  
- 受 CI 环境（nightly CI）以及使用该脚本的任何仓库的 trace 上传流程影响。  

**🔍 技术洞察**  

- **架构影响**：  
  - 仅在 CI 辅助脚本层面进行改动，不触及项目核心库、模型运行时或服务端代码，属于 **横向工具的可插拔改进**。  
  - 引入了更细粒度的错误分类，使 CI 工作流对外部 GitHub API 的异常处理更具可预测性。  

- **性能影响**：  
  - 新增的字符串匹配（`any(phrase in error_body.lower() for phrase in …)`）在异常路径上执行，耗时极低，对整体 CI 时长几乎无影响。  
  - 通过提前 `sys.exit(1)` 终止 CI，避免了在权限异常情况下的无效重试，间接节省了 CI 资源。  

- **安全考虑**：  
  - 脚本对 **权限错误** 给出了明确的错误信息并要求更新 `GH_PAT_FOR_NIGHTLY_CI_DATA`，增强了 **凭证正确性的可追溯性**。  
  - 未引入新的依赖或泄露敏感信息，安全风险保持不变。  

**⚠️ 潜在风险**  

1. **误判风险**：  
   - 依赖 `error_body` 文本进行关键词匹配，若 GitHub 返回的错误信息格式变更或出现未覆盖的短语，可能把真实的 **权限错误** 当作 **速率限制**（导致 CI 静默跳过），或相反导致 CI 过早退出。  

2. **CI 中断风险**：  
   - `sys.exit(1)` 会立即终止整个 CI 任务，若 token 权限误配置（例如只读 token）导致频繁失败，可能影响其他 CI 步骤的执行，需要运维及时修正。  

3. **兼容性**：  
   - 脚本假设 `HTTPError` 实例携带 `error_body` 属性；在某些异常包装层（如 urllib3、requests）中该属性可能不存在，导致 `is_rate_limit_error` / `is_permission_error` 始终返回 `False`。  

**💡 关注建议**  

- **完善检测逻辑**：  
  - 考虑同步检查 HTTP 响应头（如 `X-RateLimit-Remaining`、`Retry-After`）来辅助判定速率限制，降低对 `error_body` 文本的依赖。  
  - 为 `is_permission_error` 与 `is_rate_limit_error` 增加单元测试，覆盖常见的 GitHub 错误响应样例（包括不同语言或区域的错误信息）。  

- **CI 失败策略**：  
  - 在 CI 配置中为该脚本添加 **可选的“容错”开关**（例如 `ALLOW_PERMISSION_FAILURE=false`），方便在 token 暂时失效时不阻塞后续非关键步骤。  
  - 在 CI 报告中添加 **错误码/日志归档**，便于在 CI 平台（GitHub Actions、Jenkins 等）中快速定位 token 权限问题。  

- **文档与运维**：  
  - 更新项目的 CI 文档，明确说明 `GH_PAT_FOR_NIGHTLY_CI_DATA` 必须具备 `contents: write` 权限，及如何在 GitHub 仓库的 **Settings → Secrets** 中更新。  
  - 考虑在 CI 初始化阶段检查 token 权限（使用 GitHub API `GET /repos/{owner}/{repo}`）并提前给出警告，避免在上传 trace 时才触发中断。  

- **回滚准备**：  
  - 若在生产 CI 中出现误判导致频繁中断，保持对原始 `publish_traces.py`（未区分错误类型）的回滚脚本，以快速恢复 CI 正常运行。  

---  

通过以上改进，CI 在面对 GitHub API 的速率限制与权限错误时将拥有 **更明确的行为分支**，提升了错误可诊断性与资源利用率，同时也需要做好 **误判防护** 与 **运维文档同步**，以保证 CI 的平稳运行。

---

#### 🟡 中重要度变更 (13)

### [RadixTree][1/N Refactor]: Support unified match_prefix params (#17142)
**SHA**: `20b0523` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/20b0523eca688febb98873f4ab50e44bbabbb9f6)

**🎯 变更类型**：重构（统一 `match_prefix` 参数）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增 `MatchPrefixParams` dataclass，用于封装 `match_prefix` 所需的 **key**、`cow_mamba`、`req` 等可选信息。  
- `BasePrefixCache.match_prefix` 以及所有具体缓存实现（RadixCache、MambaRadixCache、SwaRadixCache、ChunkCache、HiRadixCache、RadixCacheCpp、LMC RadixCache 等）统一改为接受 `MatchPrefixParams` 实例。  
- 调度层 (`schedule_batch.py`, `schedule_policy.py`) 以及单元测试全部迁移到新调用方式。  

**🎯 影响范围**  
- **核心缓存抽象**：`sglang/srt/mem_cache/base_prefix_cache.py`、`sglang/srt/mem_cache/*_cache.py`。  
- **调度管理**：`sglang/srt/managers/schedule_batch.py`、`schedule_policy.py`。  
- **测试套件**：所有 Radix‑Cache、Mamba、SWA 相关的单元测试。  
- **外部插件或用户代码**：若直接调用 `cache.match_prefix(RadixKey(...), ...)`，将因签名不匹配而编译错误。  

**💡 关注建议**  
1. **兼容性**：建议在 `BasePrefixCache` 中保留旧的 `match_prefix(key, **kw)` 兼容包装（内部转化为 `MatchPrefixParams`），并在文档中标注即将废弃，以免外部项目突发崩溃。  
2. **默认参数**：`MatchPrefixParams` 已给 `cow_mamba` 与 `req` 提供默认值，确保现有内部调用无需改动太多逻辑。检查所有调用点是否已显式传递 `req`/`cow_mamba`，避免出现 `None` 引发的运行时错误。  
3. **文档与示例**：更新 `mem_cache` 模块的使用说明，展示 `MatchPrefixParams(key=…, req=req, cow_mamba=True)` 的典型写法，尤其在 Mamba 场景下的说明。  
4. **性能评估**：包装层相对轻量（dataclass），但在高频路径（每个 token 的前缀匹配）仍会产生一次对象创建，建议在性能关键路径（如 `RadixCache.match_prefix`）使用 `MatchPrefixParams.__slots__` 或在 C++ 实现中直接接受结构体，以降低 GC 开销。  
5. **测试完整性**：所有改动已同步至单元测试，确保覆盖 `extra_key` 隔离、分页对齐、节点分裂等边界情况。提交后请运行全量 CI，关注是否有遗漏的第三方插件调用。  

> **结论**：本次重构提升了接口一致性，为后续扩展（如加入更多上下文信息）奠定基础。只要在发布前提供向后兼容包装并完善文档，即可安全推广到生产环境。

---

### Evict swa kv cache during decoding (#17220)
**SHA**: `ce8a6ac` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ce8a6ac69086df099c1331e0ed0f059f99940318)

**变更类型**：功能增强（SWA KV‑Cache 销毁）  
**重要程度**：🟡 中  

**变更概要**  
1. 为 `Req` 增加 `swa_evicted_seqlen`，统一记录在不同缓存类型（radix、chunk）中已被淘汰的序列长度。  
2. 在 `ScheduleBatch` 中实现 `maybe_evict_swa` 与 `_evict_swa`，把原本散落在 `alloc_for_extend/alloc_for_decode`、Eagle 代码里的手动 `evict_swa` 调用迁移到统一调度点。  
3. `CacheInitParams` 去除对 `sliding_window_size/attention_chunk_size` 的冗余字段，改为在创建 `SWAChunkCache` 时显式传参。  
4. `SWAChunkCache` 只保留 `sliding_window_size`，并在构造时强制校验。  
5. `SWARadixCache` 接口扩展为接受 `swa_evicted_seqlen`，在插入节点时根据已淘汰长度决定是直接写入、拆分节点或忽略，避免对已回收的 KV 再次写入。  
6. 相关调用处（`common.alloc_for_extend`、`alloc_for_decode`、Eagle 预处理）改为统一调用 `batch.maybe_evict_swa()`，并在 decode 流程里统一递增 `req.decode_batch_idx`。

**影响范围**  
- `python/sglang/srt/managers/schedule_batch.py`、`scheduler.py`、`common.py`  
- KV‑Cache 实现：`mem_cache/chunk_cache.py`、`mem_cache/swa_radix_cache.py`、`mem_cache/cache_init_params.py`  
- 预测/草稿流：`speculative/eagle_info_v2.py`、`eagle_worker.py`  

**关注建议**  

1. **向后兼容**：`CacheInitParams` 结构变更可能影响外部脚本或自定义插件的实例化；确保在项目入口或文档中说明 `sliding_window_size` 必须通过 `SWAChunkCache` 构造参数提供。  
2. **页面对齐**：大量断言检查 `page_size` 对齐和 `swa_evicted_seqlen` 对齐，运行时若缓存配置非 1‑倍页面会直接抛异常。建议在 CI 中加入针对非 1‑page_size 场景的单元测试。  
3. **调度正确性**：`maybe_evict_swa` 通过 `req.decode_batch_idx % sliding_window_size == 1` 触发，若 `sliding_window_size` 与实际模型窗口不匹配可能导致提前或延迟回收，进而出现 KV‑冲突。建议在模型加载阶段验证 `sliding_window_size` 与模型元信息的一致性。  
4. **并发安全**：`decode_batch_idx` 在 `EagleDraftInput.prepare_for_decode`、`_draft_preprocess_decode` 中都被递增，确保没有重复递增的路径（如同时走两条 decode 分支）。可以在 `Req` 中加入调试计数或在 `maybe_evict_swa` 前后打印日志以验证。  
5. **性能评估**：新实现把所有 evict 逻辑集中到调度层，理论上可以减少跨模块的锁竞争，但实际开销取决于 `free_swa` 的实现。建议对大型连续生成（> 10k tokens）进行基准，比较旧版和新版的 KV‑Cache 回收时间。  
6. **文档更新**：说明 `swa_evicted_seqlen` 的语义、何时会增长、以及在混合（radix+chunk）模式下的行为差异，帮助后续调试。  

总体而言，此次改动显著提升了 Sliding‑Window Attention 的 KV‑Cache 回收一致性，代码结构更聚合，风险点主要在页面对齐断言和 `decode_batch_idx` 计数，请重点覆盖上述场景的自动化测试。

---

### Fix v32 continue_final_message not work (#16567)
**SHA**: `ebca587` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ebca5879a18ef7852945ec6f16380a7badd9e489)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中（修复 v32 版本中 `continue_final_message` 无效的问题，影响使用该特性的用户）  

**📋 变更摘要**  
1. 在 `serving_chat.py` 中新增 `_handle_last_assistant_message` 与 `_append_assistant_prefix_to_prompt_ids` 两个私有方法，实现对 `continue_final_message` 标记的统一处理。  
2. 当请求开启 `continue_final_message` 且最后一条消息为 `assistant` 时，提取该条内容作为 “assistant 前缀”，从消息列表中剔除；若未开启则将其转为 `user` 消息，以保证聊天始终以用户角色结尾。  
3. 对所有使用 Jinja 模板或 `apply_chat_template` 的路径统一追加提取的前缀 token，且在 BOS token 重复时进行裁剪。  

**🎯 影响范围**  
- `python/sglang/srt/entrypoints/openai/serving_chat.py`（核心聊天路由）  
- 与 `ChatCompletionRequest` 中 `continue_final_message` 标记交互的所有 OpenAI 兼容入口  
- Tokenizer 相关逻辑：`encode`, `apply_chat_template` 的输出会额外拼接前缀 token  

**💡 关注建议**  

1. **功能验证**：  
   - 编写/补充单元测试，覆盖三种场景：  
     a. `continue_final_message=True` 并且最后一条是文本 assistant → 前缀被剔除并拼接。  
     b. `continue_final_message=False` 且最后一条是 assistant → 转为 user。  
     c. 最后消息为 multimodal（list）或空内容 → 不触发任何改动。  
   - 在多模态模型（如视觉/音频）上跑一次完整的 `chat` 流程，确保新逻辑不误删 list‑type 内容。

2. **向后兼容**：  
   - `continue_final_message` 默认值仍为 `False`，新代码对旧请求保持原有行为（assistant 转 user），不应导致异常。  
   - 注意 BOS token 的裁剪逻辑，仅在 `tokenizer.encode` 返回的首 token 与 `bos_token_id` 相同才去除，避免误删合法 token。

3. **性能影响**：  
   - 仅在消息列表末尾做一次 `isinstance` 检查与可能的切片，开销极小；但在高并发入口建议监控 `prompt_ids` 拼接后的长度，防止因前缀过长导致 token 超限。

4. **日志与监控**：  
   - 若项目已有请求审计，可在 `_handle_last_assistant_message` 中加入调试日志（仅在 debug 模式），帮助定位用户是否误用了 `continue_final_message`。

5. **文档更新**：  
   - 在 OpenAI 兼容 API 文档里补充 “continue_final_message” 参数的说明，明确它在 v32 编码下的行为以及对 `assistant` 最后一条消息的处理方式。

通过上述检查和补丁，`continue_final_message` 功能将在 v32 编码下恢复正常，避免因前缀未被正确拼接导致的生成截断或上下文不连贯问题。祝调试顺利！

---

### [Diffusion] Apply qknorm to flux2 and apply lightx2v rms_norm_one_pass kernel(without residual) (#17305)
**SHA**: `cc410a1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cc410a1088355ecfaa71d2b5134ddcfc73e2b8ac)

**🎯 变更类型**：功能增强（CUDA‑加速的 RMSNorm 与 Q‑K 归一化）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `layernorm.py` 中新增对小维度（≤128）的 RMSNorm 调用 `triton_one_pass_rms_norm`，使用全新实现的单遍 Trion 内核。  
2. `triton_ops.py` 中实现了 `_rms_norm_tiled_onepass` 与包装函数 `triton_one_pass_rms_norm`，基于 LightX2V 的实现，对 `seq × dim` 采用块化一次性归一化，避免两次全局归约。  
3. `flux_2.py` 中为查询/键、以及跨模态编码器的 Q‑K 对加入可融合的 `apply_qk_norm`（在满足 head‑dim、dtype 与 eps 相同且 CUDA 环境下），否则回退到原有 `RMSNorm`。  

**🎯 影响范围**  
- `sglang.multimodal_gen.runtime.layers.layernorm`（RMSNorm 前向路径）  
- `sglang.multimodal_gen.runtime.layers.triton_ops`（新增 Trion 内核）  
- `sglang.multimodal_gen.runtime.models.dits.flux_2`（注意力层 Q‑K 归一化逻辑）  

**💡 关注建议**  
1. **准确性验证**：在 CPU 与 CUDA 两条路径上跑同等输入，确保单遍 Trion 实现的数值误差在可接受范围（尤其是 `eps` 较小的情况）。  
2. **兼容性**：`triton_one_pass_rms_norm` 只在 `x.shape[-1] ≤ 128` 时使用，建议在文档中说明该阈值的选取依据，以免用户自行修改导致性能下降或错误。  
3. **回退路径**：`apply_qk_norm` 只有在 `can_use_fused_inplace_qknorm` 为真时才会就地归一化，确保在不满足条件时仍能走原有路径，且对 `allow_inplace=False` 的情况保留测试。  
4. **性能基准**：提供一套基准（如 1k‑seq、128‑dim）对比 `rmsnorm` 与 `triton_one_pass_rms_norm` 的吞吐与显存占用，帮助后续调优。  
5. **跨平台 CI**：加入 CUDA‑only 的单元测试，防止在没有 Triton 环境的机器上出现 import 错误。  

总体来看，此次改动通过引入轻量化 Trion 内核和可融合的 Q‑K 归一化，能够在显存受限且维度较小的注意力场景中显著提升吞吐，但需做好数值与兼容性回退的验证。

---

### [Refactor] Set `fp4-gemm-backend=auto` on SM100 and rename `fp4-gemm-backend` with `flashinfer_` prefix (#17309)
**SHA**: `f374623` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f374623fa9f6156d24b18dd5a7397a61c9e57e53)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中（对使用 FP4 GEMM 的用户有可感知影响）  

**📋 变更摘要**  
1. 将原来的 `--fp4-gemm-backend` 选项统一改为 `flashinfer_` 前缀的命名（`flashinfer_cudnn`、`flashinfer_cutlass`、`flashinfer_trtllm`），并在 SM100（Blackwell）上默认设为 `auto`。  
2. `SGLANG_FLASHINFER_FP4_GEMM_BACKEND` 环境变量标记为已废弃，文档、CLI、测试均同步更新。  
3. `Fp4GemmRunnerBackend` 枚举改名并新增 `get_flashinfer_backend` 用于映射到 FlashInfer 实际接口；相应的 `is_*` 方法、调用处全部改为 `is_flashinfer_*`。  

**🎯 影响范围**  
- `python/sglang/srt/layers/quantization/*`（后端选择、权重布局处理）  
- `python/sglang/srt/server_args.py`（CLI 参数、可选值）  
- 文档 `docs/advanced_features/server_arguments.md`、`docs/references/environment_variables.md`  
- 单元测试 `test/srt/test_nvfp4_gemm.py`（类名与后端标识对应）  
- 任何依赖旧枚举成员（`is_cudnn`、`is_cutlass`、`is_trtllm`）或直接读取废弃环境变量的自定义脚本  

**💡 关注建议**  

1. **用户**：启动 SGLang 时请改为 `--fp4-gemm-backend=auto`（或显式使用 `flashinfer_*`），不要再使用 `SGLANG_FLASHINFER_FP4_GEMM_BACKEND`。在 Blackwell GPU 上默认已自动选取最优实现。  
2. **开发者**：若项目中自行调用 `get_fp4_gemm_runner_backend()`，请使用新的 `is_flashinfer_*` 方法，或通过 `get_flashinfer_backend()` 取得传递给 FlashInfer 的字符串。为兼容旧代码可在本地添加别名映射（如 `Fp4GemmRunnerBackend.CUTLASS = Fp4GemmRunnerBackend.FLASHINFER_CUTLASS`），但建议尽快迁移。  
3. **测试/CI**：确认新测试类名称与后端标识一致，确保在非 Blackwell GPU 上仍能回退到 sgl‑kernel CUTLASS 实现。  
4. **文档/部署**：检查发布说明中是否明确标出该破坏性改动，避免用户在升级后出现 “unknown backend” 错误。  

总体而言，本次重构提升了命名统一性并在 SM100 上实现了自动后端选择，但需要注意 CLI 与环境变量的同步修改以及代码中对旧枚举成员的引用。及时更新相关脚本即可平滑过渡。

---

### support new qwen3_coder_detector (#16744)
**SHA**: `858a4d6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/858a4d659b3e32bbb783a5782dc45f4653cc4e73)

**变更概览**  
本次提交为 `Qwen3CoderDetector` 添加了完整的非流式与流式解析实现，并对工具参数的类型转换做了细化（整数、浮点、布尔、对象、数组等）。核心改动包括：

1. **解析逻辑重构**：原来的基于 `_buf` 的逐字符状态机被改为基于 `self._buffer` 与 `self.parsed_pos` 的游标式解析，结构更清晰，支持跨块增量解析。  
2. **新增正则与标记**：统一了 `<tool_call>、</tool_call>、<function=...>、</function>、<parameter=...>、</parameter>` 的检测与分割，兼容缺失结束标签的容错模式。  
3. **参数配置与类型转换**：通过 `_get_arguments_config` 读取工具 schema，`_convert_param_value` 按 schema 自动转为 `int、float、bool、list、dict` 等 Python 类型，提升了与 OpenAI‑compatible JSON 的一致性。  
4. **测试大幅扩充**：覆盖了普通文本、单/多工具调用、流式增量、特殊字符、数组、空值、布尔、整数等多种场景，确保新实现的正确性。

**影响范围**  
- `sglang/srt/function_call/qwen3_coder_detector.py`（核心解析器）  
- `test/registered/function_call/test_function_call_parser.py`（单元测试）  
- 依赖 `BaseFormatDetector` 的流式处理路径；其他模型（如 GLM4、Qwen4）的检测器保持不变。

**关键风险 & 建议**  

| 风险点 | 说明 | 建议 |
|--------|------|------|
| **未定义变量** `self.current_tool_id`、`self.current_tool_name_sent` 等在新实现中仍被引用，却未在 `__init__` 中初始化。 | 运行时可能触发 `AttributeError`。 | 在构造函数统一声明并赋默认值（如 `-1`），或改用 `self.parsed_tool_id` 并同步更新。 |
| **缓冲区清理** `self._buffer = self._buffer[self.parsed_pos :]` 只在循环结束时执行，若解析过程中提前 `break`，可能导致已处理的字符残留在缓冲区，导致后续解析重复。 | 影响流式增量的准确性。 | 在每次成功消费完一个完整标签后立即截断，或在 `break` 前判定是否已经消费完毕。 |
| **环境变量依赖移除** 原代码使用 `envs.SGLANG_FORWARD_UNKNOWN_TOOLS` 判断未知函数的转发，新的实现直接忽略了该控制开关。 | 部分用户可能依赖该特性。 | 若需保留，加入同等逻辑或在文档中说明已移除。 |
| **正则表达式边界** `self.tool_call_parameter_regex` 采用了“非贪婪 + 前瞻”方式，但在极端嵌套或缺失结束标签时仍可能捕获过长内容。 | 解析异常或 JSON 结构错误。 | 对 `</parameter>`、`<parameter=`、`</function>` 三者取最小正向位置，已有实现可进一步抽象为函数，防止遗漏。 |
| **性能** 流式解析在每次 `parse_streaming_increment` 都对整个缓冲区做 `find`、`startswith` 检查，若连续小块输入（token 级）会产生 O(N²) 的字符遍历。 | 大模型长文本流式输出可能卡顿。 | 将 `self._buffer` 改为 `deque` 或使用索引滑动窗口，尽量在每次只处理新增片段。 |
| **测试覆盖** 当前测试虽丰富，但缺少 **异常标签顺序**（如 `<parameter>` 在 `<function>` 之前）以及 **多层嵌套**（如 `<parameter>` 内部出现 `<parameter>`）的案例。 | 可能隐藏未捕获的解析错误。 | 增加对应的负向测试，用 `assertRaises` 或检查 `normal_text` 正确回退。 |

**对开发者**  
- 请在 `BaseFormatDetector` 文档中补充 `self._buffer` 与 `self.parsed_pos` 的使用约定，防止子类误用旧属性。  
- 合并后务必跑全量 CI，关注 `test_function_call_parser` 的覆盖率是否下降（目前约 95%）。  
- 若计划保留 `SGLANG_FORWARD_UNKNOWN_TOOLS`，在 `detect_and_parse` 与流式路径中统一加入相同的检查，避免行为不一致。

**对使用者**  
- 新版检测器能够直接返回已转换的 Python 类型，降低调用方自行 `json.loads` 的负担。  
- 当模型输出缺失结束标签时仍会尝试解析，但返回的 `normal_text` 可能包含未闭合的原始片段，请在业务层做好容错。  
- 若依赖 “未知函数转发” 功能，请在升级日志中确认已被移除或自行实现相应的前置过滤。  

整体来看，本次增强为 Qwen‑3 的函数调用提供了更完整、类型安全的解析能力，代码结构亦趋于可维护。但需补齐变量初始化、缓冲区管理及性能细节后方可在生产环境安心使用。

---

### Update code sync scripts (#17319)
**SHA**: `fc4b932` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fc4b932f4e5300f1089d7c57611e3badd0fe2d88)

**🎯 变更类型**：功能增强（新增代码同步检查脚本）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 两个 CI workflow 中的 GitHub Token 环境变量名由原来的 `PAT_FOR_CODE_SYNC_FROM_LIANMIN` 改为更语义化的 `GH_PAT_FOR_OPEN_PR_TO_PRIVATE` / `GH_PAT_FOR_OPEN_PR_TO_OSS`。  
2. 新增 `scripts/code_sync/check_commits.py`，用于在私有仓库中自动列出自上一次同步以来、涉及 OSS 同步路径的提交，并生成 Markdown 汇总，写入 GitHub Step Summary。  

**🎯 影响范围**：  
- `.github/workflows/open-pr-copy-from-oss.yml`、`.github/workflows/open-pr-copy-to-oss.yml`（CI 流程）。  
- 新增脚本 `scripts/code_sync/check_commits.py`（依赖 git、Python 3.8+）。  
- 受影响的路径列表在脚本中硬编码，涉及 `3rdparty、assets、benchmark、docker、docs、examples、python/sglang/**、test/**、README.md` 等。

**💡 关注建议**  

1. **Secrets 名称同步**：确保仓库已创建 `GH_PAT_FOR_OPEN_PR_TO_PRIVATE` 与 `GH_PAT_FOR_OPEN_PR_TO_OSS` 两个 secret，并且权限与旧 token 等价，否则 CI 会因 token 不存在/权限不足而失败。  
2. **脚本可移植性**：`check_commits.py` 假设在仓库根目录执行并依赖 `git`，建议在 workflow 中显式 `working-directory: ${{ github.workspace }}`，并在 `steps` 前添加 `git --version` 检查，以防 Runner 环境缺失。  
3. **路径配置**：同步路径写死在脚本，后期若新增或删减文件夹需要手动修改。可以考虑将其抽离到 `config.yaml` 或通过环境变量配置，提高可维护性。  
4. **性能与日志**：在提交量大的仓库中，`git rev-list` 与多次 `git diff-tree` 可能耗时。建议加入 `--since`/`--max-count` 限制或缓存中间结果；同时在 CI 中打开 `set -x` 或将关键变量打印，便于排查。  
5. **错误处理**：脚本在找不到最近同步提交时会返回 `None`，但后续仍会生成 “No sync commit found.”，这在首次同步后是合理的。若 `git` 命令异常（网络、权限），脚本只打印错误并继续，可能导致误报。可将异常抛出并让 CI 步骤失败，以免隐藏实际问题。  
6. **输出兼容**：`GITHUB_STEP_SUMMARY` 只在 GitHub Actions 中可用，若本地调试缺失该环境变量会报错。已使用 `if` 检查，安全性良好。  

总体而言，此次改动提升了同步流程的可视化与可审计性，但请务必同步 secret 配置、验证脚本在 CI 环境的依赖，并考虑后续路径管理的灵活性。

---

### Use dsv3 optimized routing `fused_topk_deepseek` instead of `moe_fused_gate` (#15347)
**SHA**: `84c8390` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/84c8390514d524c251193f405d348d583e809dbb)

**🎯 变更类型**：功能增强（引入 flashinfer 的 `fused_topk_deepseek` 加速 MoE Top‑K 路由）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `sglang/srt/layers/moe/topk.py` 中新增对 `flashinfer.fused_moe.fused_topk_deepseek` 的导入与可选使用，加入一套更严格的前置条件（专家数必须是 2 的幂、top‑k ≤ 8、VPT·topk ≤ 128 等）。  
2. 当满足这些约束且 `fused_topk_deepseek` 可用时，走该 kernel；否则回退到原有的 `moe_fused_gate` 或 AITer 实现。  
3. 新增 `test_fused_topk_deepseek.py`，对该代码路径进行参数化随机测试，并在 CI 中标记为 GPU‑nightly。  
4. 调整 DeepSeek‑V3 多线程推理测试的阈值（去掉 AMD‑CI 条件分支），统一使用 `> 2.8` 作为基准。

**🎯 影响范围**  
- **MoE Top‑K 路由层**：所有使用 `biased_grouped_topk_gpu` 的模型（尤其是 DeepSeek‑V3 配置）将受益于 flashinfer 加速；不满足约束的情况仍保持原行为。  
- **依赖**：新增 `flashinfer`（可选）为运行时必需的 Python 包和对应的 CUDA kernel。  
- **测试套件**：加入新测试后 CI 对 GPU 环境的依赖更强，需确保 CI 镜像预装 flashinfer。  

**💡 关注建议**  

1. **可选依赖安全**：当前 `fused_topk_deepseek` 为 `None` 时会直接走回旧实现，已基本安全。建议在模块文档或 `setup.cfg` 中明确标记 `flashinfer` 为可选依赖，并在导入失败时输出可读提示。  
2. **约束校验**：代码对 `experts_per_group`、`topk`、`topk_group` 等做了多层判断，后续若新增 MoE 配置，请同步更新这些断言；考虑抽成一个统一的 “flashinfer_support” 检查函数，避免条件散落。  
3. **数据类型兼容**：`fused_topk_deepseek` 只接受 `float32`，当前实现已在调用前 `.to(dtype=torch.float32)`，但若上层可能传入 `bfloat16`，请在注释中提醒或在函数入口统一强制转换。  
4. **性能基准**：新 kernel 在 DeepSeek‑V3（256 experts, topk=8）下应显著加速，建议在 `README` 或 benchmark 文档中给出对比表，帮助用户判断是否打开该路径。  
5. **回退验证**：现有 `biased_grouped_topk_impl` 仍作为参考实现，建议在 CI 中加入一次对比两条路径（当 flashinfer 可用时）的一致性测试，以防未来 kernel 行为变更导致数值偏差。  

总体来看，此次改动为 MoE 路由引入了更高效的 CUDA 实现，并兼顾向后兼容，风险可控。只需在文档、依赖声明及约束检查上做少量完善，即可平滑推广。

---

### [Feature] overlap LoRA weight loading with compute (#15512)
**SHA**: `ad1b4e4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ad1b4e4728236269383850ec948124e64fd76926)

**变更类型**：功能增强  
**重要程度**：🟡 中  
**变更摘要**：新增 `--enable-lora-overlap-loading` 参数，实现 LoRA 权重的异步 H2D 迁移，以在计算期间并行加载适配器。实现了 CPU‑pin、CUDA 流/事件管理、调度路径改造以及相应的单元/端到端测试。  
**影响范围**：  
- `sglang/srt/lora/`（LoRA 权重管理、内存池、适配器加载）  
- `sglang/srt/managers/scheduler.py`（调度器对 LoRA 批次的检查与加载路径）  
- `sglang/srt/model_executor/forward_batch_info.py`（前向批次初始化）  
- `sglang/srt/server_args.py`（CLI 参数及校验）  
- 文档 `docs/advanced_features/*`  
- 新增测试 `test/registered/lora/test_lora_overlap_loading.py`、相关测试脚本的参数扩展  

### 关键实现  
1. **CPU‑pin**：在 `LoRAAdapter.pin_weights_in_cpu()` 中对所有 LoRA 权重调用 `pin_memory()`，为异步拷贝做准备。  
2. **异步加载**：`LoRAOverlapLoader` 维护独立 CUDA stream 与事件，利用 `non_blocking=True` 的 `copy_` 实现 H2D 与计算重叠。  
3. **调度改动**：Scheduler 在 `enable_lora_overlap_loading` 为 true 时，改为逐个 LoRA 调用 `try_overlap_load_lora`，而非一次性 batch 验证；成功后将 LoRA ID 加入 `running_loras` 集合。  
4. **内存池**：`load_lora_weight_tensor` 改为 `copy_(..., non_blocking=True)`，兼容异步拷贝。  
5. **约束校验**：在 `ServerArgs.check_lora_server_args` 中加入 `max_loaded_loras ≤ 2 * max_loras_per_batch` 的断言，防止 CPU‑pin 导致的内存膨胀。  

### 潜在风险 & 建议  
- **事件泄漏**：`LoRAOverlapLoader._check_overlap_load_status` 在查询完成后会 `del` 事件，但若异常提前退出可能残留，建议在 `LoRAManager.unload_lora_adapter` 时同步清理对应事件。  
- **并发安全**：当前加载路径仅在单线程的调度器内部使用，如果未来引入多线程/多进程并发调度，需要确认 `self.lora_to_overlap_load_event` 的读写锁安全。  
- **CPU‑pin 内存占用**：对大规模 LoRA（数百 MB）进行 pin，可能导致 CPU 可分页内存紧张。建议在文档中提醒用户根据机器内存评估 `max_loaded_loras`，并在代码中加入 `torch.cuda.memory_reserved` 的检查或警告。  
- **回退兼容**：默认仍是同步加载，保持旧行为不变；但在开启 `enable_lora_overlap_loading` 时，`fetch_new_loras({None})` 在 `LoRAManager.init_memory_pool` 中仍会触发一次同步加载（基模型），这与预期一致。  
- **性能验证**：当前仅通过功能对齐（ROUGE）验证正确性，建议在 CI 中加入基准测试（如“加载时间下降 xx%”），以证明异步加载的实际收益。  

总体来看，改动完整且配套测试覆盖了常见的多 LoRA、TP 场景。若补充上述清理与监控细节，将进一步提升鲁棒性。

---

### [GLM 4.7] Add RTX 6000 Pro aka sm120 (#17235)
**SHA**: `d3eafc7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d3eafc7357f361af4478fb2f4749b6ce2398e7ae)

**🛠️ 变更类型**：功能增强（新增硬件配置）  
**⚡ 重要程度**：🟡 中  

**📋 变更概要**  
- 在 `python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/` 目录下新增了两份 JSON 配置文件，分别对应 RTX 6000 Pro（Blackwell）在两种不同量化方式下的 kernel 参数。  
- 配置内容包括 `BLOCK_SIZE_M/N/K`、`GROUP_SIZE_M`、`num_warps`、`num_stages` 等 Triton‑kernel 调度参数，覆盖从 1 到 4096 的特征维度。  

**🎯 影响范围**  
- **核心模块**：`sglang.srt.layers.moe.fused_moe_triton`（MoE Fusion + Triton）  
- **运行时**：当检测到设备名为 `NVIDIA_RTX_PRO_6000_Blackwell_Max-Q_Workstation_Edition` 时，系统会读取对应配置，以获得最优 kernel 调度。  

**💡 关注建议**  
1. **设备匹配**：确认 `device_name` 与实际 `torch.cuda.get_device_name()` 输出完全一致，防止因命名不符导致回退到默认配置。  
2. **性能验证**：在 RTX 6000 Pro 实机上跑一次典型的 MoE 推理基准（不同 batch/seq 长度），对比旧配置的吞吐与延迟，确保新参数真正提升性能。  
3. **文档同步**：更新 README/硬件支持表，说明新增的 RTX 6000 Pro 支持及对应的 `dtype=fp8_w8a8, per_channel_quant=True` 选项。  
4. **回退机制**：若用户在其他驱动/库版本上出现 kernel 编译错误，建议在代码中捕获异常并回退到最近的通用配置，以免启动失败。  
5. **持续集成**：加入自动化检查，确保新 JSON 文件的键值完整且均为整数，防止手动编辑带来的格式错误。  

总体来看，此次提交为硬件适配层面的配置扩展，对功能没有代码逻辑变动，但需要确认名称匹配和实测性能，以确保在 RTX 6000 Pro 上能够获得预期的加速。

---

### [jit-kernel] Add CuTe DSL GDN Decode Kernel (#15631)
**SHA**: `e00b434` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e00b43442d2fa45975833063e09e37deab8b905a)

**🎯 变更类型**：功能增强（新增 CuTe DSL GDN Decode Kernel 并提供开关）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 `sglang/jit_kernel/cutedsl_gdn.py` 中实现四个 CuTe DSL kernels（小批量/大批量、普通 & 变长），并封装 JIT 启动函数。  
2. 新增环境变量 `SGLANG_USE_CUTEDSL_GDN_DECODE`（默认 False），并在 `HybridLinearAttnBackend` 中依据该标志切换至 `cutedsl_fused_sigmoid_gating_delta_rule_update`。  
3. 添加对应单元测试 `tests/test_cutedsl_gdn.py`，对精度和性能进行对比基准。  
4. 轻微更新 `environ.py`、`hybrid_linear_attn_backend.py`，以及日志输出。

**🎯 影响范围**：  
- `sglang/jit_kernel`（新模块）  
- `sglang/srt/environ.py`（新增 env）  
- `sglang/srt/layers/attention/hybrid_linear_attn_backend.py`（运行时 kernel 选择）  
- `sglang/jit_kernel/tests`（测试套件）  

**💡 关注建议**  

1. **兼容性**：CuTe DSL 依赖 `cutlass`、`cuda.bindings.driver`，仅在支持的 CUDA SM（≥80）上可编译。建议在 CI 中加入 GPU 架构检测，若不满足则自动回退至原 Triton 实现。  
2. **编译时延**：首次调用会触发 JIT 编译，耗时可达数百毫秒。生产环境建议提前热启动或使用 `torch.cuda.Stream` 进行异步预编译。  
3. **内存占用**：SMEM 计算量较大（≈ 4 KB × num stages），在极端 seq‑len / head‑size 时可能触发共享内存不足错误，需监控 `cudaErrorInvalidConfiguration`。  
4. **开关使用**：默认关闭，保持向后兼容。若开启，务必在启动脚本中显式设置 `SGLANG_USE_CUTEDSL_GDN_DECODE=1`，并在日志中确认 `"CuTe DSL GDN decode enabled: True"`。  
5. **性能回归**：新 kernel 在小 batch（B < 32）预期略有提升，大 batch（B ≥ 32）要求 ≥ 1.15× 加速。建议在模型部署前跑 `test_cutedsl_gdn_performance`，对比基准并监控长时运行的抖动。  
6. **错误监控**：新增测试已检查 NaN/Inf；上线后可在 `rank0_log` 中加入对 `torch.isnan`/`torch.isinf` 的运行时监控，以快速捕获数值异常。  

总体而言，此次改动为 GDN 解码提供了潜在更高效的实现，保持了向后兼容和完整的精度/性能校验，需关注 GPU 兼容、共享内存配置以及首次编译开销。

---

### [NPU]Support GPT-OSS for NPU (#14197)
**SHA**: `733de6b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/733de6be31e20242421ea5cbc2a66582711d33f0)

**🎯 变更类型**：功能增强（新增 NPU（Ascend）后端支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交为 GPT‑OSS 在华为 Ascend NPU 上运行提供完整适配。新增 `sgl_kernel_npu` 的 attention‑sinks Triton 实现，完善 NPU 版矩阵乘法的 bias 传递和自定义激活（`npu_swiglu_oai`），在模型、层、后端调度以及服务器参数中加入 NPU 检测与配置，并将 “ascend” 加入支持的注意力后端列表。  

**🎯 影响范围**  
- **硬件后端**：`python/sglang/srt/hardware_backend/npu/attention/ascend_backend.py` 增加 `sinks` 参数、调用 `attention_sinks_*`，以及对 `seq_lens/extend_seq_lens` 的统一管理。  
- **量化层**：`unquant.py` 在 NPU `grouped_matmul` 中加入 bias 输入，并根据配置切换到 `npu_swiglu_oai`。  
- **模型层**：`gpt_oss.py` 引入 `is_npu` 检测，针对 NPU 禁用 rotary‑emb 的 fused KV 缓冲，默认激活函数改为 `"npu_swiglu_oai"`。  
- **服务器参数**：`server_args.py` 将 `"ascend"` 加入注意力后端白名单。  
- **工具函数**：新增 `is_npu`（已在 utils 中）供全局判断。

**💡 关注建议**  
1. **依赖检查**：确保环境已安装 `sgl_kernel_npu`（含 `attention/sinks_attention` 与 `activation/swiglu_oai`），并在 `requirements.txt`/文档中明确说明。  
2. **配置兼容**：`hidden_act` 仅在 NPU 场景被强制为 `"npu_swiglu_oai"`，若用户自行指定其他激活需保持向后兼容或给出警告。  
3. **路径与 dtype**：`forward_metadata.seq_lens` 在 CPU 与 NPU 场景的拷贝逻辑不同，需通过单元测试验证多批次、extend‑mode、draft‑extend‑v2 等分支均能得到正确的长度信息。  
4. **性能回归**：新增的 `attention_sinks_*` 接口为预填和解码两阶段提供加速，建议对比 CUDA、TRT‑LLM、FA3/FA4 三种后端在同等模型规模下的吞吐与延迟，确认 NPU 不出现显著回退。  
5. **错误容错**：在 `forward_extend`、`forward_decode` 等入口若 `sinks` 为 `None`，目前直接返回普通路径，建议加入日志提醒，以免用户误以为已使用 NPU 优化。  
6. **文档与示例**：更新快速启动指南，展示如何在 `server_args.yaml` 中开启 `attention_backend=ascend`，以及如何准备 `sinks` 张量（shape、dtype）。  

总体而言，改动为 Ascend NPU 引入了完整的前向流水线，涉及硬件抽象、算子实现与模型配置的多处联动。请重点关注依赖可用性、配置一致性及跨后端性能基准，以保证新平台的稳定上线。

---

### [Refactor] Split out deepseek v2 weight loader function into mixin (#16649)
**SHA**: `9343372` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/93433726eb72d7e11d0ff692e6101955c99cafbc)

**变更类型**：重构 / 功能增强  
**重要程度**：🟡 中  

**核心变更**  
1. 将 DeepSeek V2/V3 的权重加载逻辑抽取到 `DeepseekV2WeightLoaderMixin`（`deepseek_weight_loader.py`），并在 `DeepseekV2ForCausalLM` 中混入，实现 `do_load_weights` 与统一的 `post_load_weights`。  
2. 原先散落在 `deepseek_v2.py` 中的 `load_weights`、`post_load_weights`、量化‑FP8‑UE8M0 相关代码全部迁移到 mixin，`deepseek_v2.py` 只保留模型结构和前向实现。  
3. `utils.py` 新增 `awq_dequantize_func` 与 `enable_nextn_moe_bf16_cast_to_fp8`，供 mixin 与其它模块统一调用。  
4. `deepseek_nextn.py` 与 `deepseek_v2.py` 相应更新 import，删除大量重复实现。  

**影响范围**  
- `sglang/srt/models/deepseek_v2.py`（模型类）  
- `sglang/srt/models/deepseek_common/deepseek_weight_loader.py`（新 mixin）  
- `sglang/srt/models/deepseek_common/utils.py`（新工具函数）  
- `sglang/srt/models/deepseek_nextn.py`（NextN 支持）  

**潜在风险 & 建议**  
- **兼容性**：Mixin 依赖 `model、config、quant_config、pp_group、num_fused_shared_experts` 成员，确保所有使用该类的实例在创建时已正确初始化这些属性。  
- **并发加载**：原实现使用 `ThreadPoolExecutor`，现在仍在 mixin 中保留，需确认在多进程/分布式环境下 `executor.shutdown` 能及时回收资源。  
- **量化路径**：`_maybe_quant_weights_to_fp8_ue8m0` 与 `post_load_weights` 的行为未变，但被移动后调用链改变，建议跑一次完整的模型加载（包括 FP8、AWQ、INT8、NextN）验证数值一致性。  
- **性能**：拆分后代码更清晰，未引入额外计算。但注意 `deepseek_v2.py` 中去除的 import 可能导致未使用的函数仍被加载，建议在 CI 中检查 import 警告。  
- **测试**：新增单元测试覆盖 `DeepseekV2WeightLoaderMixin.do_load_weights`、`post_load_weights`、以及 `utils.awq_dequantize_func` 在不同设备（CUDA/CPU/ROCM/NPU）的返回情况。  

总体而言，此次重构提升了权重加载的可维护性和复用度，对模型功能无实质性改变，只要保持接口兼容即可平滑迁移。

---

#### 🟢 低重要度变更 (12)

### Inclusion of nvfp4 blockscale in EPLB Rebalance (#17158)
**SHA**: `5c02217` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5c02217746331c9a29351c31eb53d8f1360771be)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `filter_moe_weight_param_global_expert` 中去除对以 `_blockscale_swizzled` 结尾参数的排除判断，简化全局专家过滤逻辑。

---

### Revert "[Perf] fuse q, k norm for Flux2Attention (#17241)" (#17332)
**SHA**: `a3d9a21` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a3d9a218826a1b70e66e8c7908f22d0c3e488044)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：撤销Flux2Attention中q/k融合归一化的实现，改为分别使用RMSNorm；简化 `apply_qk_norm`，去除 contiguity 检查和形状恢复逻辑，保持返回原张量。

---

### [CI] Add partition to stage-b-test-large-1-gpu (11->12) (#17245)
**SHA**: `2d72e16` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2d72e168fdeb9d8233989fad9bcf8083386cec18)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 GitHub Actions `pr-test.yml` 中，将 `stage-b-test-large-1-gpu` 的分区数从 10 扩展至 12，更新 `auto-partition-size` 为 12，以支持更多并行 GPU 测试分片。

---

### [Perf] fuse q, k norm for Flux2Attention (#17241)
**SHA**: `6494667` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/64946679b5db7cca46a1138f0fc8f473086caaea)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `layernorm.py` 中引入 `apply_qk_norm`，实现 Q、K 的 fused RMSNorm（确保张量连续并恢复原始形状），并在 `flux_2.py` 的注意力实现里使用该函数，以提升 CUDA 上的执行效率。

---

### KV Cache Events with Attention DP bug fix (#16030) (#16412)
**SHA**: `5836324` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5836324c55ff60c7257085afc84948d37bb144bf)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复 KV Cache 事件在注意力并行（Attention DP）下的启用逻辑，将判断从 `tp_rank` 改为 `self.attn_tp_rank`，并在对应测试中更新模型名称和资源清理。

---

### Fix kernel selection in biased_grouped_topk_gpu (#17325)
**SHA**: `9fe56cd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9fe56cd0fb77293331b3260af78597b5619ec9f9)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `biased_grouped_topk_gpu` 中去除对 `num_fused_shared_experts == 0` 的检查，使 CUDA 条件更宽松，简化 kernel 选择逻辑。

---

### [Auto Sync] Update tokenizer_manager.py (20260119) (#17317)
**SHA**: `e619f53` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e619f53113b20e2ddff6cc12486ab2478389b366)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `TokenizerManager` 中新增 `req_state_class` 成员，并改为通过该成员实例化 `ReqState`，以支持自定义请求状态类。

---

### [DeepSeek v3.2] Opt MTP decode cuda batch sizes and nsa implementation (#16961)
**SHA**: `d2105d4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d2105d4abda6fd29e10ebc6a80b85db323b4d191)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `nsa_backend.py` 中统一使用 `nsa_impl` 判断实现路径，提升前向模式切换的可维护性；在 `cuda_graph_runner.py` 中引入 `num_tokens_per_bs` 参数并调整捕获 batch size 的计算，确保在多卡注意力分片下 batch 大小合法，同时延迟捕获 batch 列表的初始化以适配 DLLM 场景。

---

### [Minor] Correct sglang version when installing from source (#17315)
**SHA**: `ea879c7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ea879c7739ffcf9acca145021318560cf1e9fdae)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `python/pyproject.toml` 中将 `git_describe_command` 从直接 `git describe` 改为通过 Bash 脚本先获取最新符合 `v*.*.*` 规则的标签再描述，以修正源码安装时的版本号。

---

### [PD] Optimize MHA models pp util calculation logic (#17306)
**SHA**: `0227db8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0227db892662ada1b4ae325c5522958a1c9b2690)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `get_mha_kv_ptrs_with_pp` 中新增 `multiplier_ratio` 与 `v_ptr_offset`，修正解码层数多于预填层数时 KV 指针的计算，防止空值缓存导致错误响应。

---

### Update CODEOWNERS for multimodal_gen (#17308)
**SHA**: `51f147a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/51f147ada3c5a409d1f58bab99380502d445e9dd)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CODEOWNERS` 中新增了 `multimodal_gen/runtime/layers` 与 `multimodal_gen/runtime/models/dits` 两个目录的代码所有者，指派了 @mickqian、@yhyang201 与 @BBuf。

---

### [Diffusion] Apply jit qk_norm to flux1 (#17296)
**SHA**: `330605c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/330605cc888af4c3524fcf05bdc71ccf2e7762a2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `flux.py` 中加入对 JIT 融合 QK 归一化的检测与调用，使用 `can_use_fused_inplace_qknorm` 与 `apply_qk_norm` 对 query/key（及 encoder 部分）进行原位归一化，以提升 CUDA 上的运行效率。

---

