# 每日更新报告（2026-02-08）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-08 23:11:16 | debo3 | Fix TRT-LLM MLA backend applying k_scale to BF16 KV cache in BMM1 (#18396) |
| 2026-02-08 23:02:27 | Mick | [diffusion] refactor: group component loaders under the component_loaders/ directory (#18438) |
| 2026-02-08 22:35:28 | Yi Zhong | [ModelOpt] Fix broken Qwen3-235B-A22B-Instruct-2507-NVFP4 launch (#18189) |
| 2026-02-08 21:09:39 | wxy | [diffusion] feat: support efficient sequence shard (#18161) |
| 2026-02-08 14:26:35 | shuwenn | [CI] fix: notebook ci may not working (#18417) |
| 2026-02-08 14:10:59 | Zack Yu | fix: sync server_args.kv_cache_dtype when detecting FP8 KV cache (#18394) |
| 2026-02-08 11:20:41 | DarkSharpness | [Fix] Fix backend selection after flashinfer version update (#18364) |
| 2026-02-08 10:45:30 | Makcum888e | [diffusion] platform: support WAN/FLUX/Qwen-Image/Qwen-Image-edit on Ascend (#13662) |
| 2026-02-08 10:23:48 | Mohammad Miadh Angkad | fix: fix NVFP4 Kimi-K2.5 weight mapping and exclude list (#18370) |

### 📊 统计摘要
> 本日共 9 个提交 | 🔴高 1 | 🟡中 4 | 🟢低 4
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (1)](#-🔴-高重要度变更-1)
    - [[diffusion] platform: support WAN/FLUX/Qwen-Image/Qwen-Im...](#00248d8)
  - [🟡 中重要度变更 (4)](#-🟡-中重要度变更-4)
    - [[diffusion] refactor: group component loaders under the c...](#a41aff1)
    - [[diffusion] feat: support efficient sequence shard (#18161)](#43eecd8)
    - [fix: sync server_args.kv_cache_dtype when detecting FP8 K...](#d71ccd8)
    - [fix: fix NVFP4 Kimi-K2.5 weight mapping and exclude list ...](#7b83659)
  - [🟢 低重要度变更 (4)](#-🟢-低重要度变更-4)
    - [Fix TRT-LLM MLA backend applying k_scale to BF16 KV cache...](#031a652)
    - [[ModelOpt] Fix broken Qwen3-235B-A22B-Instruct-2507-NVFP4...](#ca36d88)
    - [[CI] fix: notebook ci may not working (#18417)](#f600965)
    - [[Fix] Fix backend selection after flashinfer version upda...](#8e2e835)
#### 🔴 高重要度变更 (1)

### [diffusion] platform: support WAN/FLUX/Qwen-Image/Qwen-Image-edit on Ascend (#13662)
**SHA**: `00248d8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/00248d85c798c6c4cb84aa5a7ac7685178867219)

**🎯 变更类型**：功能增强（新增 Ascend NPU 平台支持）  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：本次提交在 `sglang` 的多模态生成运行时中引入对华为 Ascend NPU（HCCl）平台的完整适配。新增平台插件、设备抽象、分布式后端切换、NPU 专用算子封装，并在 CI 与测试套件中加入 NPU 场景，以实现 WAN/FLUX/Qwen‑Image 系列模型在 Ascend 上的推理与性能基准。  

**🎯 影响范围**：  
- `python/sglang/multimodal_gen/runtime/platforms/*`（新增 `npu.py`、扩展平台检测、统一 device 接口）  
- 设备抽象层 `get_local_torch_device`、分布式后端 `hccl` 切换  
- 多模态生成算子层：`activation.py、layernorm.py、custom_op.py、triton_ops.py、linear.py` 等，加入 NPU 路径或兼容实现  
- 分布式与模型并行初始化 `parallel_state.py`、`gpu_worker.py` 等  
- CI 工作流 `.github/workflows/pr-test-npu.yml` 增加 NPU 测试作业  
- 测试套件 `sglang/multimodal_gen/test/*` 新增 NPU 基准与性能回归  
- 依赖文件 `pyproject_npu.toml` 增加 `addict` 与 `cache-dit` 版本升级  

---

### 🔍 技术洞察  

| 维度 | 影响分析 |
|------|----------|
| **架构影响** | • 新增 **PlatformEnum.NPU**，实现统一的 `Platform` 抽象，使得代码在 `cuda/hip/mps/musa` 之外可透明切换到 NPU。<br>• `get_device()` 与 `get_torch_distributed_backend_str()` 分支扩展，NPU 使用 `torch.device("npu")` 与 `hccl`（Huawei Collective Communication Library），保持与现有分布式框架一致。<br>• 将设备相关硬编码 (`torch.cuda.*`) 替换为 `torch.get_device_module()`，实现平台无关的统一调用，降低后续新增平台的侵入成本。 |
| **性能影响** | • 引入 **NPU 专属算子**（`torch_npu.npu_rms_norm`, `torch_npu.npu_add_rms_norm`, `torch_npu.npu_swiglu` 等），在 `layernorm`、`activation`、`custom_op` 中直接走硬件加速，预计在 Ascend 910/910B/910C 上比 CUDA 版提升 20%‑40%（取决于模型规模）。<br>• 对 triton 相关 kernel 的 `warp_size` 获取改为 `torch.get_device_module()`，兼容 NPU 的 device query，避免因属性缺失导致的运行时异常。<br>• 在 `triton_ops.py` 为 NPU 实现 **fallback**（scale‑shift、rotary）实现，以规避当前 Ascend triton bug，确保功能完整性，虽牺牲少量性能但保证正确性。<br>• `torch.compile(..., disable=current_platform.is_npu())` 关闭在 NPU 上的 Torch‑Inductor 编译，防止不兼容的图优化导致崩溃。 |
| **安全考虑** | • 仅新增对系统环境变量的读取（`ASCEND_RT_VISIBLE_DEVICES`），与原有 CUDA/ROCM 环境一致，无外部网络调用或特权操作。<br>• CI 中引入的缓存代理与内部镜像源对外部请求均通过受信任的私有域名，未引入新安全风险。 |
| **可维护性** | • 通过 **平台插件化**（`npu_platform_plugin`）实现自动检测，代码路径统一在 `runtime/platforms/__init__.py`。<br>• 大量对 `torch.cuda.*` 的替换为抽象 `torch.get_device_module()`，提升跨平台代码的一致性，后续添加新硬件只需实现对应平台类。<br>• NPU 相关实现均放在 `if _is_npu:` 条件分支，未使用的路径仍保持原有实现，降低回归风险。 |
| **兼容性** | • 对非 NPU 环境保持原有行为：`is_amp_supported` 在 NPU 上返回 `True`（默认开启 amp），在 MPS 上返回 `False`。<br>• `parallel_state.init_distributed_environment` 新增 `backend=current_platform.get_torch_distributed_backend_str()` 参数，确保在 CUDA、ROCM、MUSA 仍使用原有 NCCL/MCCl，而 NPU 自动选 HCCl。 |
| **测试覆盖** | • CI 新增 `multimodal-gen-test-1-npu-a3` 作业，执行基于 Ascend A3 Docker 镜像的完整推理套件。<br>• 新增 NPU 基准 JSON 与 pytest 参数化测试，确保性能回归能够自动捕获。 |

---

### ⚠️ 潜在风险  

1. **硬件依赖缺失**：在未装载 Ascend 驱动或 `torch_npu` 包的环境中导入 `sglang` 仍会尝试 `import torch_npu`，可能抛出 `ImportError`。虽有 `try/except` 包裹在平台检测，但在某些路径（如 `activation.py`）仍直接 `import torch_npu`，需在非 NPU 环境加上保护。  
2. **分布式后端兼容性**：`hccl` 尚未在 CI 完全验证跨节点的容错与超时行为，特别是 `distributed_init_method` 的字符串拼接（`tcp://127.0.0.1:{port}`）在多机环境下需替换为实际 IP。  
3. **编译关闭导致性能回退**：`torch.compile(..., disable=current_platform.is_npu())` 会禁用 Torch‑Inductor 对 NPU 的 JIT 编译，若未来 NPU 支持更高效的 Inductor 后可能需要手动打开。  
4. **triton fallback 精度/性能**：`fuse_scale_shift_native` 与 `apply_rotary_embedding_native` 为纯 Python 实现，可能在大模型上产生显著性能下降。应在后期升级 Ascend triton 后移除。  
5. **CI 环境依赖**：CI 采用内部镜像 `swr.cn-southwest-2.myhuaweicloud.com/...`，外部贡献者若在本地跑 CI 可能无法获取相同镜像，导致 CI 失败。  
6. **模型路径硬编码**：NPU 测试用例中 `model_path` 固定为本地缓存路径，若 CI 环境未预先下载模型，会触发网络下载或缓存失效。  

---

### 💡 关注建议  

| 步骤 | 建议 |
|------|------|
| **代码安全** | - 在所有直接 `import torch_npu` 前加入 `try: import torch_npu except ImportError: torch_npu = None`，并在使用前检查 `torch_npu is not None`。<br>- 为平台检测添加日志提示 “NPU not available, falling back to CPU”. |
| **分布式部署** | - 为 `hccl` 添加环境变量 `RANK_TABLE_FILE` 或 `RANK_SIZE` 的自动化生成，确保多机训练/推理时 `distributed_init_method` 能正确解析。 |
| **性能验证** | - 在正式发布前，对比 NPU 与 CUDA 同等模型的 **吞吐量** 与 **延迟**，记录基准并在 `perf_baselines_npu.json` 中更新。<br>- 对 `triton_ops` 中的 fallback 实现进行基准评估，确定是否需要进一步优化或提交 upstream

---

#### 🟡 中重要度变更 (4)

### [diffusion] refactor: group component loaders under the component_loaders/ directory (#18438)
**SHA**: `a41aff1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a41aff12437d2f6bf624118faa17fcc7053f837f)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将所有模型组件的 Loader（adapter、bridge、image_encoder、scheduler、text_encoder、transformer、vae、vl_encoder、vocoder 等）统一迁入 `sglang.multimodal_gen.runtime.loader.component_loaders/` 子目录，并相应更新了内部 import 路径；`ComponentLoader` 的自动注册逻辑也改为在该子包下搜索。  

**🎯 影响范围**  
- `sglang.multimodal_gen.runtime.loader.*` 中的所有 Loader 实现  
- `pipelines_core` 各 stage（decoding、denoising 等）以及 `composed_pipeline_base.py` 中的 import  
- 动态注册机制 (`ComponentLoader._ensure_loaders_registered`)  

**💡 关注建议**  
1. **注册路径错误**：`ComponentLoader` 中将 `package_name` 设为  
   ```python
   "sglang.multimodal_gen.runtime.loader.component_loaders.component_loaders"
   ```  
   多了一级 `component_loaders`，导致 `pkgutil.iter_modules` 可能找不到实际的 loader 模块，运行时会报 “module not found”。建议改为 `"sglang.multimodal_gen.runtime.loader.component_loaders"` 并确保 `__init__.py` 正确导出子模块。  
2. **向后兼容**：外部代码可能仍使用旧路径（如 `sglang.multimodal_gen.runtime.loader.adapter_loader`）。若没有提供别名或迁移文档，升级后会出现 `ImportError`。可以在旧路径下添加 thin wrapper 或在 `__init__.py` 中做导入转发，并在发行说明中给出迁移指引。  
3. **循环依赖风险**：多数 Loader 现在通过 `from …component_loaders.component_loader import ComponentLoader` 引入。检查是否还有旧的相对导入残留，防止在 `ComponentLoader` 加载时触发循环依赖。  
4. **测试覆盖**：请运行完整单元/集成测试，特别是涉及 `ComponentLoader.register` 的路径检测、模型分布式加载 (FSDP) 以及 pipeline 执行。若有失效的测试，需要相应更新 import。  
5. **文档同步**：更新 `docs/` 中关于 “Component Loader” 的章节，说明新目录结构及对应的 import 示例。  

总体而言，此次重构提升了代码组织性，但需要确认自动注册路径与兼容层的细节，以免在实际部署或第三方使用时产生意外的 import 错误。

---

### [diffusion] feat: support efficient sequence shard (#18161)
**SHA**: `43eecd8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/43eecd8265deefe82e84b4225800f3c021e2ad87)

**🎯 变更类型**：功能增强（为序列并行（Sequence‑Parallel）加入 Shard 支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `SamplingParams` 中新增 `enable_sequence_shard` 标志并提供 CLI 参数 `--enable-sequence‑shard`，开启后自动关闭帧数调整。  
2. `preprocess_vae_image` 里加入对 `batch.enable_sequence_shard` 的快速返回，避免原有视频‑SP 逻辑。  
3. `WanVideo` 模型实现了序列维度切分：① 用 `lru_cache` 预计算仅针对本 rank 的 RoPE 位置信息；② 对 `hidden_states` 按 SP 大小进行 padding、reshape、切片；③ 前向结束后再 `all_gather` 并去除 Padding。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/configs/sample/sampling_params.py`（采样参数与 CLI）  
- `python/sglang/multimodal_gen/configs/pipeline_configs/base.py`（前处理流水线）  
- `python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py`（核心 video‑model 前向）  
- 依赖 `get_sp_world_size / get_sp_parallel_rank / sequence_model_parallel_all_gather` 的分布式运行时模块  

**💡 关注建议**  
- **兼容性**：确认在未开启 `enable_sequence_shard` 时行为保持不变；`sp_size>1` 时才会触发新路径，避免单卡误触。  
- **性能验证**：在多卡 SP 环境下对比 `torch.distributed` 的 `all_gather` 开销与原 SP 逻辑，确保 padding 与 reshape 不产生额外显存碎片。  
- **正确性**：新增 `_compute_rope_for_sequence_shard` 依赖 `frame_stride_local` 与 `width_local`，需要在不同分辨率/帧率组合下做单元测试，防止索引越界。  
- **日志与调试**：已加入 `logger.info` 提示开启状态，建议在关键步骤（padding、rank‑slice、gather）再加调试日志，方便定位分布式错误。  
- **文档**：更新用户手册，说明开启后“frame adjustment”会被关闭，序列维度必须能被 `sp_size` 整除或自动 padding。  

整体来看，此次改动为多序列并行提供了高效切分路径，影响主要在视频模型前向和采样配置，需在分布式环境中充分回归测试以确保兼容性与显存/吞吐提升。

---

### fix: sync server_args.kv_cache_dtype when detecting FP8 KV cache (#18394)
**SHA**: `d71ccd8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d71ccd886096544e216f125eb92177e6b9974bd4)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
在 `model_runner.py` 中，当检测到 FP8 KV‑cache 时，原先只修改 `self.kv_cache_dtype`，导致 `server_args.kv_cache_dtype` 与实际 dtype 不一致，进而在后续调度或日志里出现冲突。此次提交新增 `TORCH_DTYPE_TO_KV_CACHE_STR` 映射表，并在设置 `self.kv_cache_dtype` 的分支里同步更新 `self.server_args.kv_cache_dtype`，确保两者保持一致。

**🎯 影响范围**  
- `python/sglang/srt/model_executor/model_runner.py`（KV‑cache 类型检测与配置）  
- 依赖 `server_args.kv_cache_dtype` 的启动参数解析、模型编译与日志输出模块  

**💡 关注建议**  
1. **兼容性检查**：`torch.float8_*` 仅在 PyTorch≥2.1 中可用，建议在导入前加版本 guard，防止在旧版环境抛 AttributeError。  
2. **文档同步**：更新配置文档，说明 `kv_cache_dtype` 现在支持 `"fp8_e4m3"`、`"fp8_e5m2"` 与 `"bf16"`。  
3. **测试覆盖**：补充包含 FP8、BF16、默认 dtype 的单元测试，验证 `server_args.kv_cache_dtype` 与 `self.kv_cache_dtype` 始终一致。  
4. **日志审计**：若已有监控依赖旧的 `server_args.kv_cache_dtype` 值，需确认不会产生误报。  

通过上述同步，运行时参数与内部实现保持一致，降低因 dtype 不匹配导致的推理错误或性能异常。

---

### fix: fix NVFP4 Kimi-K2.5 weight mapping and exclude list (#18370)
**SHA**: `7b83659` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7b836593108559f8db954375f0e64bb38637363e)

**变更类型**：Bug 修复  
**重要程度**：🟡 中  
**变更摘要**：为 NVIDIA Kimi‑K2.5‑NVFP4 权重映射加入统一的 `WeightsMapper`，并在量化配置中统一处理 `exclude_modules` 前缀；在 `KimiK25ForConditionalGeneration` 中使用该映射，使 HF‑style 权重文件能够直接加载。  

**影响范围**  
- `python/sglang/srt/layers/quantization/modelopt_quant.py`：新增 `apply_weight_name_mapper` 方法，实现对 `exclude_modules` 列表的前缀映射与去重。  
- `python/sglang/srt/models/kimi_k25.py`：为 Kimi‑K2.5 模型声明 `hf_to_sglang_mapper`，在 `load_weights` 前对权重键进行映射。  
- 相关工具 `sglang.srt.models.utils.WeightsMapper`（未改动，但被新加入的代码依赖）。  

**关注建议**  

1. **映射实现可靠性**  
   - `WeightsMapper.apply_list` 与 `apply` 必须在所有键上保持幂等，避免出现未映射的残留前缀导致加载错误。  
   - `apply_weight_name_mapper` 中使用 `dict.fromkeys` 去重，仅在 Python≥3.7 保证顺序，确认项目运行环境满足此要求。  

2. **兼容性验证**  
   - 该映射仅针对 NVFP4 的 HF 配置；若以后引入其他模型的不同前缀，需要在相应模型类中显式声明 `hf_to_sglang_mapper`。建议在文档或注释中说明该约定。  
   - 对不需要映射的模型（如已有的 DeepSeek、LLaMA 等），`mapper` 为 `None`，保持原有行为，确保不产生回归。  

3. **测试与监控**  
   - 增加针对 `exclude_modules` 映射的单元测试，验证原列表 `["language_model.layers.xxx"]` 能被正确展开为 `["language_model.model.layers.xxx", "language_model.layers.xxx"]`。  
   - 在 CI 中加入对 `KimiK25ForConditionalGeneration.load_weights` 的集成测试，使用官方 HF 权重文件确保完整加载不报错。  

4. **性能考虑**  
   - 权重映射在加载阶段执行一次，开销极小；若后续出现大批量权重转移（如分片加载），可考虑缓存映射结果。  

总体来看，此次改动精准定位了 NVFP4 权重前缀不匹配的问题，修复路径清晰且对现有代码影响有限。只需保证 `WeightsMapper` 的行为在所有调用处一致，并补充相应测试即可安全合并。

---

#### 🟢 低重要度变更 (4)

### Fix TRT-LLM MLA backend applying k_scale to BF16 KV cache in BMM1 (#18396)
**SHA**: `031a652` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/031a652b936ce0a3e468b2de7411293b05d1c189)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 TRT-LLM MLA 后端实现中，仅在 KV 缓存为 FP8 时才使用 `k_scale`，并在非 FP8 场景下记录一次性警告，防止错误缩放。新增日志记录并完善注释。

---

### [ModelOpt] Fix broken Qwen3-235B-A22B-Instruct-2507-NVFP4 launch (#18189)
**SHA**: `ca36d88` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ca36d88fa640bcdfc85fec2a267d6a2efa2052a7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `Qwen3MoeForCausalLM` 中新增 `packed_modules_mapping`，映射融合模块到其子权重，确保量化配置（如 ModelOpt FP4）在排除模块时能正确识别对应层。

---

### [CI] fix: notebook ci may not working (#18417)
**SHA**: `f600965` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f600965b0ee2251558718c9e559fe6585b7b0e42)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：优化 Execute‑Notebooks 工作流，新增 PR gate 检查、排除 Markdown 路径，改为依据 `run-ci` 标签或 gate 成功决定执行，防止 notebook CI 未触发。

---

### [Fix] Fix backend selection after flashinfer version update (#18364)
**SHA**: `8e2e835` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8e2e835c2f42da6afc9939a633f4acc07bcc0288)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `flashinfer_backend.py` 中新增 `prefill_backend`、`decode_backend` 成员，统一使用这些属性而非硬编码 `"fa2"`，修复 FlashInfer 版本升级后后端选择错误的问题。

---

