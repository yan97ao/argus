# 每日更新报告（2026-01-18）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-18 23:37:58 | Ch3ngY1 | [Bugfix] Fix PD accuracy when MTP is not configured on the prefill node (#17212) |
| 2026-01-18 23:25:46 | b8zhong | [Refactor] Add `-fp4-gemm-backend` to replace `SGLANG_FLASHINFER_FP4_GEMM_BACKEND` (#16534) |
| 2026-01-18 17:47:32 | Ke Bao | Move radix cache related tests (#17295) |
| 2026-01-18 17:20:04 | Xinyuan Tong | fix: Handle multiple named chat templates in HuggingFace tokenizers (#17236) |
| 2026-01-18 16:19:57 | Ke Bao | Add kl test for swa radix cache (#17281) |
| 2026-01-18 15:56:21 | YAMY | [SPEC_V2] Enable cudagraph draft_extend for trtllm_mla_backend and Acclen Fix for DP under cudagraph mode (#16974) |
| 2026-01-18 15:06:33 | Ke Bao | Tiny fix comment typo (#17287) |
| 2026-01-18 15:04:11 | Changyi Yang | [diffusion] fix: set guidance_scale default to None (#17182) |
| 2026-01-18 14:57:01 | Mohammad Miadh Angkad | [Tiny] Improve docs (#17264) |
| 2026-01-18 14:11:17 | Yuan Luo | [VLM][Reland] Refactor load_mm_data to improve performance (#16152) |
| 2026-01-18 13:47:07 | Ke Bao | Use swa radix cache and memory pool for gpt-oss model (#17261) |
| 2026-01-18 13:20:24 | Mick | cli: support sglang version (#17250) |
| 2026-01-18 11:28:05 | Alison Shao | Add runner utilization report workflow (#17234) |
| 2026-01-18 10:15:50 | Lancer | [diffusion] feat: support default 4-step inference for Flux2-Klein distilled models (#17225) |
| 2026-01-18 09:32:01 | Hudson Xing | fix(ci): recover from corrupted MMMU parquet cache (#17256) |

### 📊 统计摘要
> 本日共 15 个提交 | 🔴高 2 | 🟡中 9 | 🟢低 4
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [fix: Handle multiple named chat templates in HuggingFace ...](#2069050)
    - [fix(ci): recover from corrupted MMMU parquet cache (#17256)](#90399cb)
  - [🟡 中重要度变更 (9)](#-🟡-中重要度变更-9)
    - [[Refactor] Add `-fp4-gemm-backend` to replace `SGLANG_FLA...](#4df74eb)
    - [Move radix cache related tests (#17295)](#f3a7c7d)
    - [Add kl test for swa radix cache (#17281)](#1fe0c82)
    - [[diffusion] fix: set guidance_scale default to None (#17182)](#8fd3399)
    - [[VLM][Reland] Refactor load_mm_data to improve performanc...](#6d29d8a)
    - [Use swa radix cache and memory pool for gpt-oss model (#1...](#e499258)
    - [cli: support sglang version (#17250)](#09491a9)
    - [Add runner utilization report workflow (#17234)](#7edb061)
    - [[diffusion] feat: support default 4-step inference for Fl...](#e486a4d)
  - [🟢 低重要度变更 (4)](#-🟢-低重要度变更-4)
    - [[Bugfix] Fix PD accuracy when MTP is not configured on th...](#bb6055b)
    - [[SPEC_V2] Enable cudagraph draft_extend for trtllm_mla_ba...](#a45e0e5)
    - [Tiny fix comment typo (#17287)](#f78201f)
    - [[Tiny] Improve docs (#17264)](#088758c)
#### 🔴 高重要度变更 (2)

### fix: Handle multiple named chat templates in HuggingFace tokenizers (#17236)
**SHA**: `2069050` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2069050d3f44e55d1952efab750dc730a43423a0)

**🎯 变更类型**：Bug修复 / 功能增强（新增对 HuggingFace 多命名 chat template 的选择支持）  
**⚡ 重要程度**：🔴 高  
**📋 变更摘要**：  
1. 在服务器启动参数中加入 `--hf-chat-template-name`，允许用户在 HuggingFace Tokenizer 提供多个命名 chat template 时明确指定使用哪一个。  
2. `TemplateManager` 重构：能够识别 tokenizer/processor 中的 `chat_template` 为 `dict`（多模板），并依据 `ServerArgs.hf_chat_template_name` 进行选择，未指定时默认使用首个模板。  
3. 文档同步更新，说明新参数的使用场景。  

**🎯 影响范围**：  
- `sglang/srt/managers/template_manager.py`（模板加载逻辑）  
- `sglang/srt/server_args.py`（CLI 参数结构）  
- `docs/advanced_features/server_arguments.md`（用户文档）  
- 相关的启动脚本 `sgl.launch_server`（间接受新参数影响）  

**🔍 技术洞察**：

- **架构影响**  
  - **模块耦合度**：仅在 `TemplateManager` 与 `ServerArgs` 之间新增了一个字段 `hf_chat_template_name` 的传递路径，未改变已有层级或组件的职责；保持了单一职责原则。  
  - **可扩展性**：通过统一在 `TokenizerManager` 读取参数，使以后若需要在其他子系统（如推理服务）直接访问此信息时，可以复用现有结构，增强了统一配置的能力。  
  - **向后兼容**：默认行为仍保持原有“使用第一个可用模板”，因此旧版本模型和脚本不受影响。  

- **性能影响**  
  - **运行时开销**：新增的字典查找和一次日志打印的成本极低，基本可以忽略（O(1) 查找）。  
  - **加载速度**：模板仍在模型启动阶段一次性读取，未引入额外 IO 或计算。  

- **安全考虑**  
  - **输入校验**：对 `hf_chat_template_name` 进行存在性检查，若指定名称不存在会抛出明确异常，防止意外使用错误的模板导致意外行为。  
  - **模板来源**：模板来源仍然是模型仓库自带的 JSON/Jinja 文件，属于可信路径。若用户自行提供自定义模板文件（通过 `--chat-template`），安全风险与原有实现相同，需自行确保模板安全。  

**⚠️ 潜在风险**：

1. **错误提示不够友好**：当前在 `TemplateManager._select_named_template` 抛出的 `ValueError` 可能在高并发服务器中被捕获为通用错误，导致 API 返回 500 而非明确的“模板名称不存在”。  
2. **配置冲突**：当同时使用 `--chat-template`（自定义模板）与 `--hf-chat-template-name` 时，后者仍会被尝试解析，可能导致逻辑混乱。  
3. **文档同步遗漏**：如果其他文档（如 CLI 帮助或 OpenAPI 说明）未同步更新 `hf_chat_template_name`，用户可能不知道该参数的存在。  

**💡 关注建议**：

- **单元/集成测试**：加入针对以下场景的测试：  
  - tokenizer.chat_template 为 `dict` 且未提供 `--hf-chat-template-name`（使用首模板）  
  - 提供合法的 `--hf-chat-template-name`（使用对应模板）  
  - 提供不存在的模板名称，检查抛出异常并返回友好错误信息。  
- **错误处理优化**：在 `TemplateManager.initialize_templates` 中捕获 `ValueError` 并转化为 `ServerArgs` 的错误返回码或清晰的日志，防止服务因启动参数错误直接崩溃。  
- **参数冲突检测**：在 `server_args.add_cli_args` 或 `TemplateManager` 初始化阶段加入互斥检查，若两者同时出现则提示用户选择其一。  
- **文档与示例**：在官方示例脚本中补充使用 `--hf-chat-template-name` 的案例（如 `tool_use`），并在 OpenAPI schema 中标注该字段。  
- **监控与日志**：建议在生产环境开启 `logger.info`（已实现）来记录实际加载的模板名称，方便排查后期因模板变更导致的对话行为差异。  

通过上述改动，sglang 在支持多模板模型（如支持 tool‑use、RAG 等场景）的兼容性和可配置性上得到显著提升，同时风险可通过上述建议得到有效管控。

---

### fix(ci): recover from corrupted MMMU parquet cache (#17256)
**SHA**: `90399cb` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/90399cbc07b005494e17b43c54af9d4553989507)

**🎯 变更类型**：Bug修复  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
1. 在 CI 环境下，当 MMMU 数据集的 Parquet 缓存损坏导致 `lmms_eval` 运行异常时，新增自动检测、清理缓存并重新下载的恢复流程。  
2. 所有直接调用 `subprocess.run` 执行 `lmms_eval` 的位置改为封装后的 `_run_lmms_eval_with_retry`，确保在出现 “ArrowInvalid … Parquet magic bytes not found” 错误时自动重试。

**🎯 影响范围**：  
- `python/sglang/test/kits/mmmu_vlm_kit.py`（测试套件）  
- 依赖该套件的 CI 流水线与本地测试脚本  
- 与 MMMU 数据集缓存 (`datasets--lmms-lab--MMMU`) 交互的所有代码路径  

**🔍 技术洞察**：  
- **架构影响**  
  - 引入了 **错误检测 + 自动恢复** 的横切逻辑，属于 **测试层** 的辅助设施，不会影响运行时核心库（`sglang`）的业务路径。  
  - 使用 `temp_set_env` 动态修改环境变量 (`HF_HUB_OFFLINE=0`, `HF_DATASETS_DOWNLOAD_MODE=force_redownload`) 只在重试阶段生效，保持原有环境不受污染。  
  - 新增的路径解析逻辑兼容 CI（`/hf_home/hub/...`）和本地（`HF_HOME` 或默认 `~/.cache/huggingface`），提升跨环境的可移植性。  

- **性能影响**  
  - 正常路径下仅是一次函数包装，几乎没有额外开销（一次异常捕获与字符串拼接）。  
  - 当出现缓存损坏时，会额外执行一次 `shutil.rmtree`（删除约几百 MB）以及一次重新下载数据集，耗时取决于网络带宽与数据集大小（约 1‑2 GB），但这比 CI 完全失败的代价要小得多。  

- **安全考虑**  
  - `shutil.rmtree` 对 **确定的** MMMU 数据集缓存目录进行删除，风险主要在于 **误删**：如果环境变量 `HF_HOME` 指向了共享的缓存目录，可能会影响同一目录下的其他数据集。  
  - 删除操作未做二次确认或备份，属于 **破坏性** 操作，建议在生产/开发者机器上使用前确认路径准确性。  
  - 代码仅在检测到特定错误信息后触发，误判概率低，但仍建议对错误信息做更严格的模式匹配（正则或完整异常类型）。  

**⚠️ 潜在风险**：  
1. **误删除非目标缓存**：`HF_HOME` 环境变量被手动设置为自定义路径时，若该路径下恰好包含 `datasets--lmms-lab--MMMU` 之外的子目录，`rmtree` 仍会完整删除目录，导致其他数据集失效。  
2. **并发冲突**：在同一 CI 实例中可能有多个测试并行执行 `lmms_eval`，第一次检测到腐败并删除缓存后，后续仍在使用已被删除的路径，可能造成 “文件不存在” 错误。  
3. **权限问题**：CI 环境或本地机器对缓存目录缺少写权限时，`rmtree` 会抛出 `OSError`，导致恢复逻辑中断，需手动干预。  
4. **网络波动**：强制重新下载依赖网络，若网络不稳定，重试仍可能失败，导致 CI 再次报错。  

**💡 关注建议**：  
- **安全防护**：在 `_cleanup_mmmu_dataset_cache` 中加入 **路径白名单** 检查（仅当路径名严格匹配 `datasets--lmms-lab--MMMU`）以及 **递归深度限制**，避免误删。  
- **并发防护**：使用文件锁（如 `fasteners.InterProcessLock`）确保在同一机器上只会有一个进程执行删除与重新下载操作。  
- **错误检测加强**：改为使用正则 `re.search(r'ArrowInvalid.*Parquet magic bytes not found.*MMMU', error_output)`，降低误判。  
- **日志与可观测性**：将清理和重试过程的关键信息（路径、是否成功、耗时）写入 CI 日志或监控系统，便于后期问题追溯。  
- **可配置开关**：提供环境变量（如 `MMMU_RETRY_ON_CORRUPTION=0/1`）让用户在需要时自行关闭自动重试，防止在不希望自动删除缓存的环境中触发。  

通过上述改进，CI 的稳健性将显著提升， flaky test 大幅降低，同时也需要在安全与并发层面做好防护，以免引入新的风险。

---

#### 🟡 中重要度变更 (9)

### [Refactor] Add `-fp4-gemm-backend` to replace `SGLANG_FLASHINFER_FP4_GEMM_BACKEND` (#16534)
**SHA**: `4df74eb` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4df74eb57623977cd344fd732388f71fc25166cc)

**变更类型**：重构（引入 `--fp4-gemm-backend` CLI 参数取代旧环境变量）

**核心改动**  
1. 新增 `fp4_utils.py`，封装 FP4 GEMM 后端枚举、初始化与查询逻辑。  
2. `ServerArgs` 与 CLI 参数加入 `fp4_gemm_runner_backend`，默认 `auto`，并提供四个可选后端（auto、cudnn、cutlass、trtllm）。  
3. `environ.py` 增加对废弃环境变量 `SGLANG_FLASHINFER_FP4_GEMM_BACKEND` 的警告并提示使用新 flag。  
4. 在启动流程（`bench_one_batch.py`、`scheduler.py`）以及 FP4 相关算子（`modelopt_quant.py`、`compressed_tensors_w4a4_nvfp4.py`）中统一调用 `initialize_fp4_gemm_config` 与 `get_fp4_gemm_runner_backend`。  
5. 测试改为新模型路径并通过基类分别验证四种后端，原 `test_llama31_fp4` 替换为 `test_nvfp4_gemm.py`。

**影响范围**  
- 量化路径（FP4、FP8）及所有使用 GEMM 的调度/推理代码。  
- CLI/启动脚本、文档及 CI 测试。  
- 旧环境变量将在 0.5.9 前被移除。

**关注建议**  
- 开发者需确认所有旧 `FLASHINFER_FP4_GEMM_BACKEND` 读取已改为 `get_fp4_gemm_runner_backend()`，防止遗漏导致运行时错误。  
- 建议在 `initialize_fp4_gemm_config` 中加入对非法值的显式报错，并在日志中打印最终使用的后端。  
- 用户在升级时应把 `SGLANG_FLASHINFER_FP4_GEMM_BACKEND=...` 替换为 `--fp4-gemm-backend ...`，并留意 `auto` 可能因 CUDA/cuDNN 版本差异选择不同实现。  
- 文档和 Release Note 需同步更新，标明该变量将在 0.5.9 移除。  

整体改动提升了参数统一性和可扩展性，影响主要在启动配置和测试层面，兼容性已通过警告和默认回退处理。

---

### Move radix cache related tests (#17295)
**SHA**: `f3a7c7d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f3a7c7dcd99b0cdd9a40984b6bc231b5906c8006)

**🎯 变更类型**：重构（测试组织）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将与 Radix Cache 相关的单元测试文件统一迁移到 `test/registered/radix_cache/` 目录下，涉及四个测试文件的路径重命名，但本身代码未变更。  
**🎯 影响范围**：  
- `test/registered/` 目录下的测试发现机制  
- CI / 本地运行 `pytest` 时的测试收集路径  
- 可能的文档或脚本中硬编码的相对路径（如 CI 配置、coverage 报告、例子脚本）  

**💡 关注建议**  
1. **确认测试发现**：在本地与 CI 环境执行 `pytest -q`，确保新路径下的四个文件被正确收集。若项目使用 `pytest.ini` 限定了 `testpaths` 或 `python_files`，需相应更新。  
2. **检查引用**：搜索项目中是否有直接引用这些测试文件路径的脚本（如 `run_tests.sh`、GitHub Actions、coverage 配置），相应改为新路径。  
3. **保持文档同步**：若项目文档（README、CONTRIBUTING）中列举了测试路径或示例命令，及时修改说明，以免新贡献者因路径找不到而产生困惑。  
4. **回归验证**：运行完整测试套件（包括性能基准）确认测试结果未受到路径移动的副作用，尤其关注可能因相对导入而产生的 `ImportError`。  
5. **代码审查**：虽然本次改动仅是文件移动，但建议在 PR 中保留旧路径的简要说明，避免后续因误删导致历史 commit 无法追溯。  

总体来看，此次重构提升了 Radix Cache 相关测试的组织结构，对功能没有直接影响，但必须确保 CI 与本地测试工具能够正确发现并执行这些迁移后的测试文件。若上述检查全部通过，风险极低，可安全合并。

---

### Add kl test for swa radix cache (#17281)
**SHA**: `1fe0c82` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1fe0c82f6730889c863533507dfaf02523a6e398)

**🎯 变更类型**：功能增强（新增 KL 效度测试）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `test/registered/radix_cache/` 目录下新增 `test_swa_radix_cache_kl.py`，通过 Kullback‑Leibler（KL）散度对 **SW‑A（Sliding Window）Radix Cache** 在 prefill 与 decode 两阶段的缓存命中情况进行准确性回归测试，使用 `openai/gpt-oss-20b` 作为基准模型，并将阈值限制在 `0.002`。  

**🎯 影响范围**：  
- `sglang.srt.utils.kill_process_tree`（用于清理服务器进程）  
- `sglang.test.ci.ci_register.register_cuda_ci`（CI 注册）  
- `sglang.test.kl_test_utils`（提供 KL 效度校验助手）  
- `sglang.test.test_utils`（服务器启动/超时等通用工具）  
- 新增的测试模块 `test_swa_radix_cache_kl.py` 本身。

**💡 关注建议**  

1. **测试依赖**  
   - 该文件会在 CI 中以 `stage-b-test-large-1-gpu` 运行，要求 GPU、CUDA 环境以及足够显存（`--mem-fraction-static 0.75`）。请确认 CI 机器满足这些条件，否则会导致测试卡死或 OOM。  
   - `MODEL = "openai/gpt-oss-20b"` 需要从 huggingface 拉取大模型，网络带宽和磁盘空间也要足够。建议在 CI 缓存模型或使用轻量化的 mock‑model 进行快速校验。

2. **阈值选择**  
   - 目前仅为 `0.002` 的 KL 散度阈值，若后续引入其他模型或硬件（如不同 GPU 架构）可能需要放宽或自行调节。建议在 `ACC_THRESHOLDS` 中加入注释，说明阈值的来源与调参方法。  

3. **资源清理**  
   - `tearDownClass` 通过 `kill_process_tree` 结束服务器进程，确保不会残留子进程。若后续引入多进程或子线程，需验证该函数是否能够递归杀死所有子进程，以免 CI 产生僵尸进程。  

4. **测试覆盖**  
   - 只覆盖了 `prefill` 与 `decode` 两种缓存命中路径，未检测 **缓存未命中** 或 **混合模式**（如半缓存）场景。后续可考虑添加对应的负向测试，以提升鲁棒性。  

5. **代码风格**  
   - `register_cuda_ci` 调用放在模块顶层，符合项目现有做法；但若以后有多个 test 文件共享相同的 CI 配置，建议抽取为公共装饰器或 fixture，避免重复。  

**总结**：此次提交为 SWA Radix Cache 引入了量化的 KL 效度回归检测，增强了对大模型缓存一致性的信心。开发者需关注 CI 资源、模型下载与阈值配置；测试维护者可以进一步扩展负向案例并统一 CI 配置写法。整体风险低，建议合并后在 CI 上观察资源占用与运行时长。

---

### [diffusion] fix: set guidance_scale default to None (#17182)
**SHA**: `8fd3399` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8fd33998803a1f3da57fbe8fb4446b8b38b2ba3e)

**🎯 变更类型**：功能增强 / 兼容性调整  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `VideoGenerationsRequest` 中将 `guidance_scale` 的默认值从 `1.0` 改为 `None`，使其不再隐式启用 Guidance。  
2. 更新了 `perf_baselines.json` 中多个模型的基准时延数据，以匹配新的默认行为和实际测得的性能。

**🎯 影响范围**  
- **API 层**：`openai/protocol.py` 中的请求模型，所有通过 OpenAI 接口调用视频生成的业务都会感知到默认值的变化。  
- **推理逻辑**：后端在读取 `guidance_scale` 时必须判断 `None` 并跳过 Guidance 相关的计算路径。  
- **测试/CI**：`perf_baselines.json` 被基准测试脚本读取，阈值变化会影响 CI 的性能回归检测。

**💡 关注建议**  
1. **兼容性检查**：确认服务器端在 `guidance_scale is None` 时不会因除以零或未初始化的变量崩溃；若需要保持旧行为，建议在上层加入明确的默认填充（如 `if guidance_scale is None: guidance_scale = 1.0`）。  
2. **文档同步**：更新 OpenAI 接口文档，说明 `guidance_scale` 现在为可选且默认不生效。  
3. **回归测试**：在本地或 CI 中执行完整的 `multimodal_gen` 测试套件，确保所有调用路径（有/无 Guidance）均能正常运行。  
4. **基准阈值**：检查 CI 中使用 `perf_baselines.json` 的断言逻辑，确保新的 `expected_*` 值已被正确加载，防止误报性能回退。  
5. **用户迁移**：若已有用户依赖默认 1.0 行为，建议在发行说明中提供迁移指南或通过环境变量/配置项保留旧默认。  

总体而言，此次修改提升了 API 的显式性，但需在推理实现与 CI 基准两侧做好空值处理和阈值更新，避免潜在的兼容性回归。

---

### [VLM][Reland] Refactor load_mm_data to improve performance (#16152)
**SHA**: `6d29d8a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6d29d8ab16d28702989af5ee4b518369b1384e7a)

**变更类型**：功能增强 / 重构  
**重要程度**：🟡 中  

**变更摘要**  
- 将 `BaseMultimodalProcessor.load_mm_data` 拆分为 **fast** 与 **legacy** 两条路径，依据 token‑data 是否完全对齐决定走哪条分支，以避免不必要的正则扫描和多余的任务调度。  
- 新增 `_submit_mm_data_loading_tasks_simple` 与 `fast_load_mm_data`，采用统一的 `io_executor` 并行加载 Image/Video/Audio，返回顺序化的结果。  
- 在 `MiniCPMMultimodalProcessor` 中添加 `support_dynamic_frame_expansion = True` 标记，配合 `load_mm_data` 的判定逻辑。  
- 将输入合法性检查提前到 `load_mm_data` 开头，保持 legacy 分支的行为不变。

**影响范围**  
- `python/sglang/srt/multimodal/processors/base_processor.py`（核心加载逻辑）  
- `python/sglang/srt/multimodal/processors/minicpm.py`（模型专属标记）  
- 依赖 `BaseMultimodalProcessor.load_mm_data` 的所有上层调用（如推理服务、CLI、测试脚本）。

**关注建议**  
- **开发者**：验证 `fast_load_mm_data` 在 MiniCPMO/MiniCPMV 等模型上能够得到与 legacy 完全一致的输出；关注 `io_executor` 的并发上限，防止在大批量请求时出现线程泄漏或资源争用；为新 `support_dynamic_frame_expansion` 标记编写单元测试。  
- **用户**：在使用 MiniCPM 系列模型时，确保提供的 `image_data/video_data/audio_data` 与提示中的 multimodal token 完全 1:1 对齐，以获得性能提升；若出现不匹配，系统会自动回退至 legacy 路径，行为保持不变。  
- **文档**：补充 fast‑path 使用说明与性能基准，注明何种情况会触发 legacy 分支。  

整体来看，此次重构在保持向后兼容的前提下显著简化了加载流程，并通过并行 IO 提升了多模态推理的吞吐，建议在 CI 中加入两条路径的对比测试，以防回退逻辑误判。

---

### Use swa radix cache and memory pool for gpt-oss model (#17261)
**SHA**: `e499258` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e499258e97ec45a92e631a455bf1cd4000409c10)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `model_config.py` 中加入对 `GptOssForCausalLM` 的混合 SWA（Sliding‑Window Attention）层识别，改为直接使用 `hf_text_config`，并依据 `layer_types` 将 sliding 与 full attention 层分别划分。  
2. `utils.py` 引入 `SWAKVPool`，在 `enable_fused_set_kv_buffer` 中排除该池，以防止对 SWA KV 池使用不适配的 fused set‑kv 路径。  
3. `server_args.py` 当预填或解码注意力后端为 `trtllm_mha` 时，强制 `swa_full_tokens_ratio=1.0` 并给出警告，避免与 GPT‑OSS 的 KV 索引转换冲突。  

**🎯 影响范围**  
- 配置解析：`sglang/srt/configs/model_config.py`  
- KV 缓存与 fused 设置：`sglang/srt/models/utils.py`  
- 启动参数与模型特化：`sglang/srt/server_args.py`  
- 相关依赖：`sglang/srt/mem_cache/swa_memory_pool.py`、`sglang/srt/layers/radix_attention.py`  

**💡 关注建议**  
- **模型兼容性**：确认 HuggingFace 配置中 `layer_types` 字段在所有 GPT‑OSS 发行版均存在，否则会触发 `AttributeError`。  
- **性能回退**：在使用 `trtllm_mha` 后端时，`swa_full_tokens_ratio` 被强制为 1.0，可能导致 SWA 优化失效，请在实验阶段评估实际吞吐。  
- **测试覆盖**：为新增的 `GptOssForCausalLM` 路径补充单元测试，尤其是 `get_hybrid_layer_ids` 的返回值和 `enable_fused_set_kv_buffer` 对 `SWAKVPool` 的判定。  
- **文档更新**：在模型配置说明中加入 `GptOss` 支持细则及 `layer_types`/`hybrid_layer_pattern` 的使用方法。  

---  
该提交把 GPT‑OSS 纳入混合 SWA 框架，核心改动集中在层划分、KV 池判定与启动参数，建议在多后端环境下做充分回归，避免意外的性能回退或配置错误。

---

### cli: support sglang version (#17250)
**SHA**: `09491a9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/09491a9bcd31fb131edbc76f3647fb6468ce1291)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `sglang` CLI 中新增 `version` 子命令，展示发行版本号及对应的 Git revision；实现 `get_git_commit_hash` 并做一次性缓存；完善 `sglang.version` 的获取方式，兼容 wheel、源码、setuptools‑scm 三种情形；微调模型加载日志信息。  

**🎯 影响范围**  
- `python/sglang/cli/main.py`、`sglang/cli/utils.py`：新增 `version` 命令和 Git 哈希获取逻辑。  
- `sglang/version.py`：改为先尝试读取已打包的元数据，再回退到 `setuptools_scm`，最后使用默认 dev 版本。  
- `sglang/multimodal_gen/runtime/loader/component_loader.py`：日志信息加入模型路径。  

**💡 关注建议**  
1. **功能验证**：在本地、CI 环境以及通过 `pip install sglang` 安装后的环境，分别执行 `sglang version`，确认能够正确输出 `__version__` 与短 SHA；若 `SGLANG_GIT_COMMIT` 环境变量已设，应覆盖自动检测。  
2. **缓存行为**：`@lru_cache(maxsize=1)` 使第一次调用后始终返回同一值，确保在长时间进程中不会重复调用 git，兼容无 git 可执行文件的场景。若后期需要动态刷新，可考虑提供清除缓存的接口。  
3. **错误容忍**：当 `git` 不可用或命令失败时返回 `"N/A"`，调用方已打印该值，建议在文档中说明该情况的产生原因（如发行包未包含 .git）。  
4. **版本获取兼容性**：新增的 `importlib.metadata` 与 `setuptools_scm` 逻辑提升了在不同分发方式下的可靠性，但仍需在 `setup.cfg/pyproject.toml` 中确保对应依赖已声明；若项目切换构建后端，需同步更新。  
5. **日志改动**：`component_loader` 现在输出 “Loading X from Y”，有助于调试模型路径错误，建议在相关文档或调试指南中补充示例。  

**总体评价**：本次改动为用户提供了快速查看版本信息的入口，并提升了版本/commit 信息的获取鲁棒性，对 CI、运维与用户支持都有积极作用。请在后续发布说明中补充 `sglang version` 示例，并在单元/集成测试中覆盖该路径。

---

### Add runner utilization report workflow (#17234)
**SHA**: `7edb061` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7edb06158e0fc92df72aa2f2097c81f628ffba1a)

⚠️ LLM分析失败（已重试3次）: API请求失败: HTTPSConnectionPool(host='integrate.api.nvidia.com', port=443): Read timed out. (read timeout=30)

*暂无分析*

---

### [diffusion] feat: support default 4-step inference for Flux2-Klein distilled models (#17225)
**SHA**: `e486a4d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e486a4dac11e70d7bc2e9905a3d25fee90b8abf7)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 Flux‑2‑Klein（4B）模型新增专用采样参数类 `Flux2KleinSamplingParams`，默认推理步数改为 4（distilled 版的步数已被压缩）。在注册表中把该模型的 `sampling_param_cls` 从原 `FluxSamplingParams` 替换为新类，并在性能基准、测试用例中加入对应的基准数据和跑测配置。  

**🎯 影响范围**：  
- `sglang/multimodal_gen/configs/sample/flux.py`（采样参数定义）  
- `sglang/multimodal_gen/registry.py`（模型‑配置映射）  
- 性能基准文件 `perf_baselines.json` 与测试脚本 `testcase_configs.py`（新增 Klein 相关基准和测试）  

**💡 关注建议**  
1. **默认步数变化**：用户若直接使用 `sglang` 生成图像并指定 Klein 模型，默认推理仅 4 步，生成速度快但质量可能低于原 50+ 步的 Flux。建议在文档或 CLI 帮助中明确说明，并提供 `--num-inference-steps` 参数覆盖默认值。  
2. **向后兼容**：其它 Flux 系列模型仍使用 `FluxSamplingParams`，注册表已正确区分，确认未出现误把 Klein 模型映射到旧参数的情况。  
3. **性能监控**：新增的基准数据已写入 `perf_baselines.json`，CI 中应加入与之对比的阈值检查，防止未来回归导致步数或时延异常。  
4. **测试覆盖**：`DiffusionTestCase` 已加入 Klein 测试，用例默认 `T2I_sampling_params`（即原默认 50 步）可能不匹配新 4 步，确保测试框架在读取参数类时能自动使用 `Flux2KleinSamplingParams`，否则可能出现不一致的预期/实际。  
5. **文档更新**：在模型列表、示例代码及 FAQ 中标注 “Flux‑2‑Klein 默认 4 步”，并提示若需更高质量可自行增大 `num_inference_steps`。  

总体来说，此次改动对 Klein 模型的使用体验提升明显，风险主要在默认步数导致的质量差异和文档/参数覆盖的明确度，需要相应更新说明并在 CI 中保持基准监控。

---

#### 🟢 低重要度变更 (4)

### [Bugfix] Fix PD accuracy when MTP is not configured on the prefill node (#17212)
**SHA**: `bb6055b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bb6055b43c146d626b26f300c1b358292dbc92ae)

**🎯 变更类型**：其他  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `get_mha_kv_ptrs_with_pp` 中新增对解码层数多于预填层数且不整除的情况的处理，防止出现空的 Value Cache 导致响应错误。

---

### [SPEC_V2] Enable cudagraph draft_extend for trtllm_mla_backend and Acclen Fix for DP under cudagraph mode (#16974)
**SHA**: `a45e0e5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a45e0e5df4e6e2d765afcaa00b16a67694f5170e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `eagle_draft_extend_cuda_graph_runner` 中加入对 Draft‑Extend V2 的 token 计数逻辑，并在 `eagle_worker_v2` 中支持 TRT‑LLM‑MLA 后端的 CUDA 图初始化。

---

### Tiny fix comment typo (#17287)
**SHA**: `f78201f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f78201f3a93f853617dbfbedb2c284bfe0c148a6)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 GitHub Actions 工作流 `.github/workflows/pr-test.yml` 中，将一行注释的硬件型号从 “H200” 修正为正确的 “H100”，并新增一条注释说明该作业在 5090（32GB, SM120）上运行。  

简洁明了，影响仅限文档/注释，无功能代码改动。

---

### [Tiny] Improve docs (#17264)
**SHA**: `088758c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/088758c1c16bd52bb09669d6891496283ec2c963)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：更新 `install.md` 中 CUDA 13 安装说明，补充 GB300 名称、链接文字、获取 `sgl_kernel` 版本的方式，并修正代码块格式及相关表述。

---

