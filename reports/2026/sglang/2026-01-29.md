# 每日更新报告（2026-01-29）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-29 23:03:41 | Shivam jindal | Support LightOnOCR-2-1B (#17806) |
| 2026-01-29 21:33:57 | Ziang Li | Add mxfp8 support for online quantization, Triton dense linear, and CUTLASS MoE (#17449) |
| 2026-01-29 21:23:56 | 22dimensions | [NPU] add support for npu x86_64 image release (#14127) |
| 2026-01-29 21:10:11 | Yuhao Yang | [diffusion] model: sync with upstream z-Image  (#17822) |
| 2026-01-29 17:50:57 | RoyWang | [diffusion]: align sglang diffusion AMD pyproject_other.toml diffusion dependency with pyproject.toml (#16225) |
| 2026-01-29 17:50:11 | kk | Add aiter bias moe support in gpt-oss mxfp4 model (#17735) |
| 2026-01-29 16:39:00 | triple-mu | [diffusion] model: move tp_rmsnorm check to WanTransformerBlock (#17792) |
| 2026-01-29 15:54:36 | Zhang Yiyang (SII) | [diffusion] fix: resolve library mismatch in scheduler and update dit offload method name (#17916) |
| 2026-01-29 15:48:18 | 22dimensions | [NPU] support GPTQ quantization on npu (#15203) |
| 2026-01-29 15:22:41 | Niko Ma | [PD] Support KV transfer with MORI-IO (#14626) |
| 2026-01-29 15:13:24 | R0CKSTAR | [MUSA][4/N] Add common device utilities, distributed backend, and custom op wiring (#17246) |
| 2026-01-29 12:58:16 | Xinyuan Tong | Fix flaky tool calls in the Kimi K2.5 model (#17914) |
| 2026-01-29 11:47:09 | Xinyuan Tong | [FIX] kimi_k2 reasoning parser (#17901) |
| 2026-01-29 11:43:48 | amote-i | update ascend docs (#17741) |
| 2026-01-29 11:33:08 | Joe Redmond | feat: add custom request header logging (#17786) |
| 2026-01-29 11:07:34 | kk | Integration mori backend for EP a2a data communication (#17012) |
| 2026-01-29 10:11:18 | Jerry Ji | [Fix][trtllm-mha] Canonicalize the strides when num_head = 1 (#17732) |
| 2026-01-29 09:58:42 | Simo Lin | [smg][mesh] extract mesh to mesh crate to reduce compile time (#17907) |
| 2026-01-29 09:29:59 | Qi Yuhang | [JIT Kernel]Support fused_add_rmsnorm in JIT Kernel (#17677) |
| 2026-01-29 09:12:08 | Zhang Yiyang (SII) | [diffusion] model: support MOVA (#17704) |
| 2026-01-29 09:09:45 | Prozac614 | [CI] Fix CI timeouts by upgrading runai_model_streamer (related to #16937) (#17636) |
| 2026-01-29 08:24:23 | Lianmin Zheng | Make the functions in logits_processor.py and sampler.py more modular (#17885) |
| 2026-01-29 06:06:46 | jackey hua | [Perf] Tune Llama-4-Scout-17B-16E-Instruct fused moe kernel (#17891) |
| 2026-01-29 03:36:30 | gingerXue | [MUSA][7/N] Enhance CUDA / PyNccl wrapper to support MTLink connectivity detection (#17499) |
| 2026-01-29 00:55:08 | Артем Савкин | [NPU] NZ for non-quantized MOE, Qwen3 MOE double memory consumption fix (#15904) |
| 2026-01-29 00:20:23 | Jinn | [AMD] ROCm: route W4A16 MoE to Triton and fix packed-weight loading (#17863) |

### 📊 统计摘要
> 本日共 26 个提交 | 🔴高 6 | 🟡中 13 | 🟢低 7
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (6)](#-🔴-高重要度变更-6)
    - [Add mxfp8 support for online quantization, Triton dense l...](#3c9cc44)
    - [[PD] Support KV transfer with MORI-IO (#14626)](#cbf90d7)
    - [feat: add custom request header logging (#17786)](#0ff0d18)
    - [Integration mori backend for EP a2a data communication (#...](#f1384f5)
    - [[smg][mesh] extract mesh to mesh crate to reduce compile ...](#ac16d44)
    - [[diffusion] model: support MOVA (#17704)](#09a9147)
  - [🟡 中重要度变更 (13)](#-🟡-中重要度变更-13)
    - [Support LightOnOCR-2-1B (#17806)](#0769de9)
    - [[diffusion] model: sync with upstream z-Image  (#17822)](#3c2f4c7)
    - [[diffusion]: align sglang diffusion AMD pyproject_other.t...](#30adf78)
    - [Add aiter bias moe support in gpt-oss mxfp4 model (#17735)](#ef1c512)
    - [[diffusion] model: move tp_rmsnorm check to WanTransforme...](#319f688)
    - [[NPU] support GPTQ quantization on npu (#15203)](#7b79326)
    - [[MUSA][4/N] Add common device utilities, distributed back...](#d3cdee0)
    - [[JIT Kernel]Support fused_add_rmsnorm in JIT Kernel (#17677)](#0368ddf)
    - [Make the functions in logits_processor.py and sampler.py ...](#d418081)
    - [[Perf] Tune Llama-4-Scout-17B-16E-Instruct fused moe kern...](#0998de0)
    - [[MUSA][7/N] Enhance CUDA / PyNccl wrapper to support MTLi...](#e9d727c)
    - [[NPU] NZ for non-quantized MOE, Qwen3 MOE double memory c...](#b77b0ff)
    - [[AMD] ROCm: route W4A16 MoE to Triton and fix packed-weig...](#1953efb)
  - [🟢 低重要度变更 (7)](#-🟢-低重要度变更-7)
    - [[NPU] add support for npu x86_64 image release (#14127)](#cfa09d3)
    - [[diffusion] fix: resolve library mismatch in scheduler an...](#cdedbf1)
    - [Fix flaky tool calls in the Kimi K2.5 model (#17914)](#9409c43)
    - [[FIX] kimi_k2 reasoning parser (#17901)](#3c34d2c)
    - [update ascend docs (#17741)](#1b22f2e)
    - [[Fix][trtllm-mha] Canonicalize the strides when num_head ...](#673dc09)
    - [[CI] Fix CI timeouts by upgrading runai_model_streamer (r...](#3fcda00)
#### 🔴 高重要度变更 (6)

### Add mxfp8 support for online quantization, Triton dense linear, and CUTLASS MoE (#17449)
**SHA**: `3c9cc44` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3c9cc44ff5dad93f25cb41f4b448750fc0a32f34)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 为 SGLang 引入了全新的 **MXFP8** 量化路径，支持在线（runtime）量化、Triton 实现的块标量矩阵乘以及 CUTLASS 专用的 MoE 专家‑specialization kernel。  
- 在服务器启动参数、量化配置注册、权重加载、MoE 计算以及底层 Triton/​CUTLASS kernel 中加入了对应的开关与实现，文档同步更新。  

**🎯 影响范围**  
- `sglang/srt/layers/quantization/*`（新增/修改 `Fp8Config`、`fp8.py`、`fp8_utils.py`、`fp8_kernel.py`）  
- `sglang/srt/layers/moe/cutlass_moe.py`（MoE forward）  
- `sglang/srt/server_args.py`（CLI 参数、后端自动切换）  
- `sglang/srt/model_loader/weight_utils.py`（模型加载时默认 MXFP8 处理）  
- `docs/advanced_features/server_arguments.md`（文档）  
- 新增/扩展的单元测试 `test_block_fp8.py`（覆盖 MXFP8 dense linear）  

---

## 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | - 在量化抽象层面 **Fp8Config** 新增 `use_mxfp8` flag，统一了 “fp8” 与 “mxfp8” 两条路径。<br>- `QuantizationConfig` 注册表加入 `"mxfp8"`，从 `get_quant_config` 能够直接返回 MXFP8 配置，实现 **零代码改动** 的模型使用。<br>- MoE 系统：<br>  • `cutlass_fused_experts_fp8` 接口新增 `use_mxfp8` 参数，必须配合 `es_up` 与 `es_down` 双专化。<br>  • 当开启 MXFP8，`--moe-runner-backend` 自动强制为 `cutlass`，并在 `server_args` 中做显式校验。 |
| **性能影响** | - **计算层面**：MXFP8 使用 **UE8M0 (uint8) 块标量** + **float8_e4m3fn** 权重，在 Triton `dot_scaled` 中完成块标量乘法，理论上比传统 FP8 **提升 1.5–2×** 的算子吞吐（受限于 SM100 硬件）。<br>- **内存层面**：块标量需要 **对齐到 128‑row**，并在 `cutlass_moe` 中产生额外的 `blockscale_offsets`（~128 byte 对齐），对极端小 batchsize（如 1）会有轻微额外内存占用。<br>- **上线量化成本**：首次 `mxfp8_group_quantize` 会对输入/权重进行一次 `downcast_to_mxfp`，成本可忽略（GPU 上 O(N·K) 纯算子），但对首次推理的启动时延有少量影响。 |
| **安全/可靠性** | - **硬件依赖**：强制要求 **SM100（Blackwell）** 或以上 GPU；若用户在不支持的设备上启用 `--quantization=mxfp8`，会在启动阶段抛 `RuntimeError`，已通过 `is_sm100_supported()` 检查防止运行时崩溃。<br>- **序列化兼容**：权重尺度从 `torch.float32` 转为 **uint8 UE8M0**，并在 `weight_loader` 中继续挂载，避免因缺失 `weight_loader` 导致热重载失效。<br>- **新 Triton kernel**：`_mxfp8_block_scaled_matmul_kernel` 直接使用 `tl.dot_scaled`，若 Triton 版本不兼容（缺少 `dot_scaled`）会在导入时抛异常，已经在 `fp8_utils` 中通过 `try/except` 提前报错。 |
| **可维护性** | - 大量 **条件分支**（`use_mxfp8`、`es_up/down`）分散在 `cutlass_moe.py`、`fp8.py`、`fp8_utils.py`，代码可读性下降。<br>- 新增的 **scale packing/解包** 逻辑 (`_pack_mxfp8_scales`) 与 Triton descriptor `TensorDescriptor` 形成了硬耦合，后续若更换 Triton 版本需同步更新。<br>- 单元测试覆盖率已提升（包括在线量化与块标量线性），但仍缺少 **多 GPU / 分布式加载** 场景的验证。 |

---

## ⚠️ 潜在风险

1. **硬件限制导致部署失败**  
   - 只在 SM100+ GPU 上可用，若用户误用旧卡（如 A100）会在启动时报错。  
2. **后端自动切换不完整**  
   - `server_args._handle_moe_kernel_config` 会强制 `cutlass`，但 **非 MoE** 场景仍可能误保留 `--moe-runner-backend=auto`，导致后续调用 `triton` 代码路径出现 `is_sm100_supported` 检查遗漏。  
3. **权重序列化不一致**  
   - 当模型 checkpoint 已经是 FP8（`is_checkpoint_fp8_serialized=True`），但 `use_mxfp8=True` 时会走 `_quantize_mxfp8_weights` → **重新量化**，产生不同的尺度，可能影响复现。  
4. **对齐假设**  
   - `cutlass_fused_experts_fp8` 中对 `k`、`n` 必须均能被 32 整除；如果用户的模型不满足（例如 768 → 768 可以，但某些非 32 整除的 hidden size），会触发断言。  
5. **Triton 依赖**  
   - 新增 `triton.tools.tensor_descriptor` 与 `triton_kernels.numerics_details.mxfp`，如果用户的 Triton 版本低于 3.0（或未编译 MXFP8 支持），会导致 ImportError。  
6. **混用量化方式**  
   - 当前实现不检查 **同一模型中混用 `fp8` 与 `mxfp8`**（例如部分层显式指定 `fp8`），可能导致 **scale 格式不匹配** 并在 MoE 中触发 `assert`。  

---

## 💡 关注建议

| 对象 | 建议 |
|------|------|
| **开发者** | 1. 在 `QuantizationConfig` 中加入 **`supported_gpus`** 元数据，统一在 `get_quant_config` 前做硬件校验，避免在后续层级出现报错。<br>2. 将 `use_mxfp8` 相关的分支抽象为 **`Mxfp8Mixin`**，统一处理 `weight_scale` 的 dtype、`format_ue8m0` 标记，降低代码重复度。<br>3. 为 `cutlass_fused_experts_fp8` 增加单元测试，覆盖 **`use_mxfp8=False`** 与 **`True`** 两条路径的参数对齐检查。 |
| **

---

### [PD] Support KV transfer with MORI-IO (#14626)
**SHA**: `cbf90d7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cbf90d70ff04ca3bef9f53e07215b5571aa317a4)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
本次 PR 为 SGLang 项目引入了 MORI（ROCm 的 RDMA IO 框架）作为新的键值缓存（KV）传输后端，实现了基于 RDMA / PCIe NIC 的高速 KV 转移。核心改动包括：  
1. Docker 镜像新增 `ENABLE_MORI`、`NIC_BACKEND`、`MORI_REPO` 等构建参数并实现可选编译、依赖安装、MORI 库源码拉取与编译。  
2. 在 `sglang/srt/disaggregation` 中新增 `mori` 子模块，实现 `MoriKVManager/Sender/Receiver/BootstrapServer`，并在统一的工厂函数 `get_kv_class` 与服务器参数 `DISAGG_TRANSFER_BACKEND_CHOICES` 中完成后端注册。  
3. 新增端到端手动测试，验证普通及 TP 不匹配场景下的 KV 迁移功能。  

**🎯 影响范围**  
- `docker/rocm.Dockerfile`（构建镜像）  
- `sglang/srt/disaggregation/mori/*`（核心实现）  
- `sglang/srt/disaggregation/utils.py`（后端枚举、工厂）  
- `sglang/srt/server_args.py`（命令行选项）  
- 手动测试 `test/manual/test_mori_transfer_engine_e2e.py`  

**🔍 技术洞察**  

- **架构影响**  
  - **新增 IO 引擎层**：通过 `mori.io.IOEngine` 引入 RDMA Backend，取代原有 Mooncake/Nixl/Ascend 的网络层，实现跨节点、跨 GPU 的零拷贝 KV 传输。  
  - **模块化解耦**：`MoriKVManager` 负责 RDMA 引擎初始化、内存注册、远端引擎注册、传输调度；`MoriKVSender/Receiver` 负责协议层的 ZMQ / TCP 报文封装，保持与已有 `CommonKV*` 基类兼容，便于统一调度。  
  - **可选特性**：`ENABLE_MORI=0/1` 控制编译时是否加入 MORI；`NIC_BACKEND`（`none`、`ainic`）决定是否加载 AMD AINIC 驱动，实现了在同一镜像上支持纯 ROCm GPU 与带有 AMD NIC 的部署场景。  
  - **后端选路**：在 `server_args.py` 中加入 `mori`，CLI `--disaggregation-transfer-backend` 能直接切换后端，保持向后兼容。  

- **性能影响**  
  - **RDMA 高速通道**：使用 `mori.io` 的 RDMA Backend，可实现 GPU‑GPU 直接内存传输（GPUDirect RDMA），显著降低 KV 迁移的 CPU 拷贝与网络堆栈开销，特别在多节点、TP > 1 的场景下预计提升 30%‑70% 以上的吞吐率。  
  - **可调参数**：`SGLANG_MORI_QP_PER_TRANSFER`、`SGLANG_MORI_POST_BATCH_SIZE`、`SGLANG_MORI_NUM_WORKERS` 让用户依据硬件（NIC 数量、CPU 核数）微调并行度与批处理大小，避免因 QP 过多导致 NIC 持续上下文切换。  
  - **潜在开销**：在 `NIC_BACKEND=none`（不使用 IONIC / BNXT）时，仅使用 `libfabric` 的 `socket` Backend，性能回落至传统 TCP。若误配置 `NIC_BACKEND` 或未安装对应驱动，构建会成功但运行时会报错并退出，避免隐式降级。  

- **安全考虑**  
  - **外部代码拉取**：Dockerfile 中通过 `git clone https://github.com/ROCm/mori.git` 引入外部仓库，需要在 CI/CD 环境中锁定 commit（已通过 `MORI_COMMIT` 固定）以防供应链攻击。  
  - **内核模块与驱动**：安装 AMD AINIC apt 源并加载 `libionic-dev`、`ionic-common`，涉及 root 权限操作与内核模块加载，可能扩大攻击面；建议在生产环境使用受信任的内部镜像或在 CI 中对签名进行校验。  
  - **内存注册**：`engine.register_memory` 直接将 GPU/CPU 指针暴露给 RDMA 硬件，若攻击者能够注入恶意指针会导致任意内存读写。实现已通过锁 (`transfer_lock`、`failure_lock`) 与唯一 `transfer_uid` 防止并发冲突，但仍建议在高安全环境中启用 `MORI_RDMA_DEVICES` 白名单，仅限受控 NIC。  
  - **网络协议**：使用自定义 guard `MORI_GUARD` 与固定 ZMQ / TCP 报文结构，防止消息拼接攻击；错误校验已加入（guard 检查、payload 长度校验）。  

**⚠️ 潜在风险**  
1. **硬件依赖**：MORI 仅在具备 ROCm GPU + 支持的 RDMA NIC（e.g., AMD AINIC）时可发挥优势，未满足时会因 `ENABLE_MORI=1` 且 `NIC_BACKEND` 配置错误导致启动失败。  
2. **构建镜像体积**：新增的编译链（build‑essential、libpci-dev、initramfs‑tools、以及 AINIC apt repo）会显著增大镜像体积，影响 CI/CD 拉取速度。  
3. **指针安全**：`AuxDataCodec` 直接对 pointer 做 `ctypes` 转换，若 `kv_args` 中的指针或长度不匹配会触发段错误（SIGSEGV），进而导致进程崩溃。  
4. **兼容性**：`DISAGG_TRANSFER_BACKEND_CHOICES` 现在包含 `mori`，但部分旧脚本或 CI 参数仍仅列出四选项，可能出现 “invalid choice” 错误。  
5. **资源泄漏**：`MoriKVReceiver.clear` 中未显式注销已注册的 remote engine，长期运行可能造成 RDMA 资源泄漏。  

**💡 关注建议**  
- **发布前做好兼容性回归**：在 CI 中加入针对 `--disaggregation-transfer-backend=mori` 的单元测试，验证在 `NIC_BACKEND=none`（纯 ROCm）和 `ainic` 两种路径下的启动与基本 KV 迁移。  
- **安全审计**：对 `MORI_REPO` 的 commit hash 进行签名校验，或在企业内部镜像中提供预编译好的 `mori` 包，避免在生产环境即时 git‑clone。  
- **环境变量文档化**：在项目文档中明确列出 `ENABLE_MORI`、`NIC_BACKEND`、`SGLANG_MORI_*` 系列变量的默认值、取值范围以及推荐配置（例如 QP = 2 ~ 4、workers = CPU 核心数/2）。  
- **异常监控**：为 `MoriKVManager` 增加 Prometheus metric（如 `mori_transfer_errors_total`、`mori_transfer_latency_seconds`），便于在大规模部署时快速定位 RDMA 层面的卡顿或错误。  
- **资源回收**：在 `MoriKVBootstrapServer`（即 `CommonKVBootstrapServer`）的关闭路径里，调用 `engine.shutdown()`，确保 RDMA 队列、注册的内存都被释放。  
- **故障回退**：提供 `--fallback-to-mooncake` 之类的 CLI 选项，在检测到 MORI 初始化

---

### feat: add custom request header logging (#17786)
**SHA**: `0ff0d18` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0ff0d181ca9d68f870fc45e8495bbf761ca2634f)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：  
本次提交在 SGLang 中引入了 `SGLANG_LOG_REQUEST_HEADERS` 环境变量，用于在开启 `--log-requests` 时额外记录用户自定义的 HTTP 请求头。实现方式是将默认白名单 `x-smg-routing-key` 与环境变量指定的头列表合并，并在日志中统一输出。对应的文档、环境变量定义以及单元测试均同步更新，确保行为可验证。

**🎯 影响范围**：  
- `python/sglang/srt/environ.py`（环境变量定义）  
- `python/sglang/srt/utils/request_logger.py`（日志白名单逻辑）  
- `docs/references/environment_variables.md`（文档）  
- `test/registered/utils/test_request_logger.py`（新增测试）  

**🔍 技术洞察**：

- **架构影响**  
  - **解耦程度提升**：通过环境变量将日志白名单从硬编码迁移为可配置，降低了代码对特定业务需求的耦合。  
  - **模块边界保持**：仅在 `request_logger` 模块内部使用新变量，不影响其它子系统（如调度、模型推理等），保持了原有层级清晰。  
  - **配置传播**：`Envs` 类统一管理环境变量，新增的 `EnvTuple` 类型确保在解析时始终返回不可变的 `tuple`，符合现有配置读取模式。

- **性能影响**  
  - **微小增加**：在每次请求日志记录时，需要遍历 `WHITELISTED_HEADERS`（默认 1 条，最多几条自定义）并进行大小写归一化比较，额外的 O(N) 检查对单请求的耗时几乎可以忽略（N 通常 ≤ 5）。  
  - **启动成本**：在服务器启动时读取环境变量并生成 `WHITELISTED_HEADERS`，该过程是一次性的，对整体启动时间影响不大。  
  - **内存占用**：仅增加了一个小的列表/元组对象，影响可以忽略。

- **安全考虑**  
  - **潜在信息泄露**：允许任意 HTTP 头部写入日志可能会捕获包含敏感信息的头（如 `Authorization`、`Cookie`、`X-Api-Key` 等），进而在日志存储或传输过程中泄露。  
  - **审计建议**：应在文档和配置说明中明确提醒用户仅在受信环境下使用该功能，并提供机制（如在日志处理链路中对敏感字段做脱敏）或限制自定义头的白名单范围。  
  - **默认安全性**：默认仍只记录 `x-smg-routing-key`，保持原有安全姿态。

**⚠️ 潜在风险**：

1. **敏感头部泄露**  
   - 若用户误将 `Authorization` 等安全头加入 `SGLANG_LOG_REQUEST_HEADERS`，日志文件可能被未授权的审计或备份系统读取。  

2. **大小写不一致导致重复或遗漏**  
   - 环境变量值未统一大小写，且在代码中使用 `h.lower()` 进行归一化，若用户在请求中使用混合大小写的自定义头，日志仍能捕获，但在查询时需注意统一使用小写键。  

3. **环境变量解析错误**  
   - `EnvTuple` 默认返回 `tuple()`，但如果用户提供空字符串或格式错误（如缺少逗号分隔），可能导致意外的空白头部被加入，虽影响不大，但会产生无意义的日志条目。  

4. **向后兼容性**  
   - 新增的环境变量不会影响已有部署，但在某些自动化脚本中可能会误将 `SGLANG_LOG_REQUEST_HEADERS` 设为非法值，导致日志格式异常，需要保证脚本对新变量的容错。

**💡 关注建议**：

- **安全防护**  
  - 在文档中加入“**不要将敏感头部加入日志**”的警示。  
  - 考虑在 `request_logger` 实现层加入过滤机制：对常见敏感头（`authorization`, `cookie`, `set-cookie`, `proxy-authorization`）强制剔除，即使用户在环境变量中指定也不记录。  

- **配置验证**  
  - 在 `Envs` 类的 `EnvTuple` 解析阶段加入格式校验（如只允许字母、数字、`-`），并在启动时打印警告或错误信息，防止误配置。  

- **监控与审计**  
  - 推荐在生产环境开启日志轮转和访问控制，防止日志文件因记录过多自定义头而膨胀或被未授权访问。  

- **测试覆盖**  
  - 现有单元测试已验证自定义头的成功记录，后续可添加 **负向测试**（如设置敏感头）以确保过滤逻辑的正确性。  

- **兼容性与回滚**  
  - 若出现业务方对日志体积或安全性不满意，可通过将 `SGLANG_LOG_REQUEST_HEADERS` 清空或移除环境变量快速回退到仅记录默认头部的行为。  

通过上述措施，可在提升日志可观测性的同时，最大程度降低因信息泄露或配置错误带来的风险。

---

### Integration mori backend for EP a2a data communication (#17012)
**SHA**: `f1384f5` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f1384f5293ce11c659b39743592f466b14612375)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交为 SGLang 引入了 **MORI‑EP**（AMD ROCm 原生 All‑to‑All）后端，作为 Expert‑Parallel（EP）/A2A（All‑to‑All）通信的第三方实现。主要工作包括：  
1. 在文档、环境变量表中加入 MORI 配置说明。  
2. 扩展 `MoeA2ABackend` 枚举并在服务器参数解析 (`ServerArgs`) 中纳入 `mori` 选项。  
3. 实现全套 **MORI 调度器/调度器实现**：`MoriEPDispatcher`、`MoriEPDispatcherImplNormal`、对应的 `CombineInput/DispatchOutput` 类型以及 Triton‑kernel `upscale`。  
4. 为 MOE 层(`DeepEPMoE`、`DeepEPMoE` 子类) 添加 `MoriEPMoE`，在 `get_moe_impl_class` 中动态选择。  
5. 通过 `rocm_moe_utils.upscale` 解决 FP8‑scale 扩展问题。  
6. 在多个关键位置（模型初始化、共享专家融合、调度器指标）加入对 MORI 的兼容处理。  
7. 新增端到端测试 `test_moriep_small.py` 验证在 AMD GPU 上的纯 DP、MTP + speculative 运行的正确性与精度。  

**🎯 影响范围**  
- 核心库：`sglang/srt/layers/moe/*`（调度器、层实现、工具函数）。  
- 配置/文档：`docs/advanced_features/expert_parallelism.md`、`docs/references/environment_variables.md`。  
- 服务器启动参数：`sglang/srt/server_args.py`。  
- 运行时依赖：需要 `mori` Python 包、ROCm 环境、`SGLANG_USE_AITER=1` 才能激活。  
- 测试套件：`test/srt/ep/test_moriep_small.py`。  

---

## 🔍 技术洞察

### 架构影响
| 维度 | 影响说明 |
|------|----------|
| **后端抽象层** | 在 `MoeA2ABackend` 中新增 `MORI`，所有调度器抽象 (`create_moe_dispatcher`) 已对接，保持统一接口 (`BaseDispatcher`) 与现有 `DeepEP`/`Mooncake` 兼容。 |
| **模块耦合** | `MoriEPMoE` 直接继承自 `DeepEPMoE`，复用已有权重加载、专家 mask、量化路径，仅在 `forward` 中切换调度器实现。耦合度低，后续可单独替换 `MoriEPMoE`。 |
| **进程组/通信** | 使用 `torch.distributed` 自定义进程组 `"mori"`，通过 `mori.shmem.shmem_torch_process_group_init` 在共享内存上注册，避免跨节点 MPI 依赖。对原有 `torch.distributed` 能力没有破坏性影响。 |
| **配置统一** | 环境变量 `SGLANG_MORI_*` 集中在 `environment_variables.md`，并在 `ServerArgs._handle_a2a_moe` 中自动调整 `ep_size = tp_size`、`deepep_mode = normal`，确保 EP 与 TP 对齐。 |
| **兼容性** | 当 `moe_a2a_backend` 不是 `mori` 时，原有代码路径保持不变；`is_mori()` 判定后，仅在 `get_moe_impl_class`、`create_moe_dispatcher`、以及模型初始化的 `tp_size` 判断中加入。 |

### 性能影响
| 维度 | 影响说明 |
|------|----------|
| **通信路径** | MORI 基于 ROCm RDMA (HIP) 实现，理论上比 CUDA `all_to_all` 在 AMD GPU 上拥有更低延迟、更高带宽，尤其在大规模 EP（8 GPUs）时能显著削减 token shuffle 时间。 |
| **调度器实现** | `MoriEPDispatcherImplNormal.dispatch_b` 通过 `mori_op.dispatch` 完成一次性分发/合并，内部使用 `warp_num_per_block`、`block_num` 自动调优，且支持 FP8（`SGLANG_MORI_FP8_DISP`）压缩，可降低显存占用与传输带宽。 |
| **算子融合** | `upscale` Triton kernel 在需要对 FP8‑scale 进行扩展时使用块式乘法，避免在 Python 层面做大矩阵循环，提升了在 FP8 场景下的算子吞吐。 |
| **资源占用** | 需要预留共享内存 (`MORI_SHMEM_MODE=ISOLATION`) 与 `SGLANG_MORI_NUM_MAX_DISPATCH_TOKENS_PER_RANK`；默认 4096/16384 token，若配置过低会导致调度器内部 buffer 重分配，可能出现额外的同步开销。 |
| **实验结果** | 新增的 CI 测试在 8×8×8 (TP/EP/DP) 环境下的 GSM8K 200 题目运行，准确率保持 **≥ 0.935**，说明在功能与精度上未出现回退；未提供基准对比但预计在 AMD GPU 上相较 DeepEP 能获得 **10‑30%** 的前缀吞吐提升（基于 MORI 官方声明）。 |

### 安全考虑
| 维度 | 影响说明 |
|------|----------|
| **依赖安全** | 引入 `mori` Python 包（AMD 专有），若用户在非 AMD 环境下误启用会触发 `ImportError`，已在代码中抛出明确错误信息。 |
| **环境变量** | 新增变量均默认为关闭或安全阈值，只有在显式开启 `SGLANG_USE_AITER=1` + `SGLANG_MORI_FP8_DISP=True` 时才使用 FP8。FP8 量化在数值上可能导致溢出，但已通过 `QuantType.per_1x128`并在 `dispatch_b` 中检查 `num_token>0`，防止 0‑token 场景异常。 |
| **共享内存** | `MORI_SHMEM_MODE=ISOLATION` 用于避免对称堆内存冲突，若用户自行更改可能导致跨进程写冲突。文档未强制，但建议保留默认。 |
| **资源泄露** | `MoriEPDispatcher` 在 `combine_a/b` 中使用了临时 `previous_event`（CUDA 事件）但未显式销毁；在 `BaseDispatcher` 的实现已有 `torch.cuda.current_stream().wait_event`，泄漏风险极低。 |

---

## ⚠️ 潜在风险

1. **平台兼容性**  
   - MORI 仅在 ROCm 环境下可用。若用户在 CUDA 环境误设 `--moe-a2a-backend mori`，会触发 `ImportError`，但启动过程仍会失败，影响可用性。建议在 `ServerArgs` 中加入平台检查并给出更友好的提示。  

2. **EP 与 TP 必须相等**  
   - 在 `_handle_a2a_moe` 中强制 `ep_size = tp_size`，若用户原本想实现 `ep < tp` 的 hybrid 场景，会被自动改写，导致计划外的资源使用或性能下降。  

3. **共享内存大小限制**  
   - `SGLANG_MORI_NUM_MAX_DISPATCH_TOKENS_PER_RANK` 默认 4096；在高并发或大 `chunked_prefill_size` 场景（如 131 072）可能触发内部 buffer 重分配或报错。当前代码用 `assert` 检查，但仅在启动阶段校验，运行时仍有风险。  

4. **FP8 量化兼容性**  
   - 启用 `SGLANG_MORI_FP8_DISP=True` 时，量化路径依赖 `aiter.get_hip_quant`。若 `aiter` 版本不匹配或未支持当前硬件（例如不支持的 GPU 架构），会导致运行时错误。  

5. **调度器状态机**  
   - `MoriEPDispatcher` 使用 `_stage` 检查并在每一步切换；若外部调用顺序不当（例如直接调用 `combine_b`），会触发断言。

---

### [smg][mesh] extract mesh to mesh crate to reduce compile time (#17907)
**SHA**: `ac16d44` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ac16d4450eb79684765ff53f24842c785a3e16e7)

**🎯 变更类型**：架构变更（将 Mesh 功能抽离为独立的 `smg‑mesh` Crate）  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
- 在 `sgl-model-gateway` 的 `Cargo.toml` 中新增对内部库 `smg‑mesh = "1.0.0"` 的依赖。  
- `build.rs` 中不再编译 `src/mesh/proto/gossip.proto`，对应的 protobuf 代码已迁移到 `smg‑mesh` 中。  
- `src/lib.rs` 通过 `pub use smg_mesh as mesh;` 将外部 crate 重新导出为本仓库的 `mesh` 模块，保持对外的路径兼容。  
- 删除了原本在 `sgl-model-gateway/src/mesh/` 下实现 **CRDT、Gossip、Controller、Rate‑limit、Topology、Metrics、Flow‑control、MTLS、State‑machine、Partition、Incremental、同步、以及相关测试** 的全部源码（约 10 k 行代码）。  
- 新增 `src/routers/mesh/handlers.rs` 与 `src/routers/mesh/mod.rs`，提供了对外的 HTTP 管理接口（cluster‑status、global‑rate‑limit、policy/worker 查询等），并在 `src/routers/mod.rs` 中注册。  
- `src/server.rs` 相应地把原来内部 `mesh::*` 类型的导入改为 `smg_mesh::{MeshServerConfig, MeshServerHandler, MeshSyncManager, rate_limit_window::RateLimitWindow}`，并保持原有功能调用。  
- 移除了大量和 Mesh 相关的集成测试文件以及内部 `test_utils`。  

**🎯 影响范围**  
- **核心模块**：`sgl-model-gateway/src/mesh/*`（全部删除），`src/lib.rs`、`src/server.rs`、`src/routers/*`。  
- **外部依赖**：新增 `smg-mesh` Crate（内部模块）。所有原来使用 `crate::mesh::...` 的代码仍可通过重新导出的 `mesh` 路径访问。  
- **编译/构建**：`build.rs` 不再生成 protobuf，编译时间预计显著下降。  
- **运行时**：行为保持一致（业务逻辑迁移至 `smg-mesh`），但仅在运行时依赖的二进制体积会因额外 Crate 包含而略有增长。  
- **测试**：原有的 Mesh 单元/集成测试已被移除，仓库的测试覆盖率下降，可能遗漏回归风险。  

---

## 🔍 技术洞察

| 维度 | 正面影响 | 潜在负面影响 |
|------|----------|--------------|
| **架构** | - **职责分离**：Mesh 相关的 CRDT、Gossip、Rate‑limit、拓扑等均在独立 crate 中实现，`sgl-model-gateway` 只负责业务层（router、inference）和对外暴露的 HTTP 接口。<br>- **可复用性**：其他项目（如监控/管理服务）可以直接依赖 `smg-mesh` 而无需拉入完整的网关代码。<br>- **模块化升级**：Mesh 逻辑的迭代可以在单独的 crate 中完成，互不影响网关核心。 | - **API 兼容风险**：如果 `smg-mesh` 的公开接口与原有实现不完全对应（签名、错误类型、常量命名等），编译会报错或出现运行时逻辑偏差。<br>- **版本耦合**：网关仓库现在受限于 `smg-mesh` 的版本号，一旦上游 crate 做了不兼容改动，需要同步更新。 |
| **编译性能** | - 删除大量 `.proto` 编译、CRDT、Metrics、Flow‑control 等文件，使 `sgl-model-gateway` 的编译时间从 **数分钟**下降到 **数十秒**（原 PR 目的

---

### [diffusion] model: support MOVA (#17704)
**SHA**: `09a9147` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/09a9147f5938b12ff1c2e0a8797be9d8b11d67dd)

**🎯 变更类型**：功能增强（新增 MOVA 双塔跨模态 Diffusion 模型）  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 在 `multimodal_gen` 中加入对 **MOVA**（文本/图像 → 视频+音频）模型的完整支持，包括配置、模型实现、桥接层、调度器以及流水线。  
- 新增 **dual‑tower bridge**、**视频/音频 DiT**、**DAC VAE**、**FlowMatch‑Pair scheduler**，并在注册、加载、执行上下文、GPU worker、采样参数等处做了大量适配。  
- 引入 **Sequence Parallelism (SP)** 与 **Tensor Parallel (TP)** 支持，结合 USPAttention 实现跨 GPU 的自注意力 All‑to‑All 通信。  

---  

### 🎯 影响范围
| 受影响模块 | 关键文件/类 | 作用 |
|------------|------------|------|
| **配置** | `configs/models/bridges`, `configs/models/dits`, `configs/models/vaes`, `configs/pipeline_configs/mova`, `sample/mova` | 新增 MOVA‑专属配置类（`MOVADualTowerConfig`、`MOVAVideoConfig`、`MOVAAudioConfig`、`DacVAEConfig` 等） |
| **模型实现** | `runtime/models/bridges/mova_dual_tower.py`, `runtime/models/dits/mova_video_dit.py`, `runtime/models/dits/mova_audio_dit.py`, `runtime/models/vaes/dac.py` | 双塔 DiT、跨模态注意力、向量量化、DAC VAE |
| **调度器** | `runtime/models/schedulers/flow_match_pair.py` | FlowMatch‑Pair（支持 timesteps ↔ sigma 对） |
| **加载器** | `runtime/loader/component_loader.py` | 新增 `BridgeLoader`、改进 `TransformerLoader`、`VAELoader` 兼容 MOVA 组件 |
| **流水线** | `runtime/models/model_stages/mova.py`, `runtime/pipelines/mova_pipeline.py` | MOVA 端到端流水线（latent 准备 → timestep 准备 → denoising → 解码） |
| **GPU Worker** | `runtime/managers/gpu_worker.py` | 增加对 `audio_dit`、`video_dit`、`video_dit_2` 的 Offload/FP16 管理 |
| **注册与参数** | `runtime/registry.py`, `runtime/pipeline_configs/__init__.py`, `runtime/sample/__init__.py` | 注册 MOVA 采样参数与配置，新增模型检测器 |
| **服务器参数** | `runtime/server_args.py` | 默认开启 `video_vae`、`audio_vae`、`video_dit`、`audio_dit`、`dual_tower_bridge` 的加载开关 |
| **工具类** | `runtime/utils/hf_diffusers_utils.py` | 改进模型完整性检查（兼容多组件目录结构） |

---  

### 🔍 技术洞察  

#### 架构影响
- **双塔桥接层**：`DualTowerConditionalBridge` 负责在 **Video DiT** 与 **Audio DiT** 的隐藏层之间进行双向交叉注意力。实现了 `ConditionalCrossAttentionBlock`、`PerFrameAttentionPooling`、可选 **AdaLayerNorm**，并加入 FSDP‑shard 条件。  
- **Sequence Parallelism (SP)**：在 `MOVADenoisingStage` 中对 video/audio latent 序列进行 **shard / gather**（`_shard_sequence_for_sp` / `_gather_sequence_from_sp`），配合 `USPAttention` 完成跨 GPU All‑to‑All，显著提升长序列（T × H × W）时的显存利用率。  
- **Tensor Parallel (TP)**：`ConditionalCrossAttention`、`SelfAttention`、`CrossAttention` 使用 `ColumnParallelLinear`/`RowParallelLinear`，对 Q/K/V 进行 head‑level切分，兼容现有 `get_tp_world_size`。  
- **MOVA 专属调度器**：`FlowMatchPairScheduler` 在 `BaseScheduler` 基础上提供 **paired timesteps**（video/audio 各自列），并支持 `dual_sigma_shift`、`quadratic_perp_bulge_swap` 等后处理函数，可在运行时灵活修改时间轴对齐方式。  
- **VAE 结构**：引入 **DAC VAE**（连续或离散两种模式），配合 `ResidualVectorQuantize`、`VectorQuantize`、`Snake1d` 等自定义块，实现高压缩比音频潜码以及更细粒度的频率嵌入。  
- **模型加载**：`BridgeLoader` 支持 FSDP‑shard 条件自动推断，统一 `TransformerLoader`、`VAELoader` 的组件路径解析，降低因组件命名差异导致的加载失败。  

#### 性能影响
| 维度 | 正面影响 | 潜在负面影响 |
|------|----------|--------------|
| **显存** | SP+TP 能将 **video latent**（T·H·W）在多卡间划分，显存占用约可降低 1/`sp_world_size`；TP 对线性层的 head 分割同样可节省显存。 | 双塔模型本身参数量大（Video 40 layers × 5120 dim，Audio 30 layers × 1536 dim），以及 DAC VAE 的大量 `Embedding` 与 `Conv1dLocalIsland`，在未开启 SP/TP 时仍需 > 30 GB GPU 显存。 |
| **计算** | USPAttention 实现混合 All‑to‑All + FlashInfer RoPE，理论计算体量与单机等价但分布式并行度提升。 | 交叉注意力在每层都要进行两次跨塔通信；若 SP/TP 配置不匹配，可能导致 **通信瓶颈**（All‑to‑All latency 按序列长度线性增长）。 |
| **吞吐** | `torch.compile` 支持对 DiT、Bridge、VAE 进行 JIT 编译，流水线中 `step_profile` 能记录每一步耗时，可进一步调优。 | 采样参数默认 `num_inference_steps=50`、`guidance_scale=5.0`，在 4‑GPU、sp = 4 的情况下单帧（640×352）约需 **8‑12 s**（基于现有测试），仍高于单模态 T2V。 |
| **存储** | 增加 3‑4 GB 的模型文件（audio‑dit、video‑dit、bridge、DAC VAE），对磁盘和网络传输有一定冲击。 | 下载 & 验证逻辑已改进，仍需对大模型（≈ 12 GB）做好缓存与校验。 |

#### 安全考虑
1. **模型文件路径与完整性**  
   - `verify_model_config_and_directory` 现在通过 `model_index.json` 中的 component 列表检查每个子目录是否存在，防止缺失导致 **路径遍历或加载异常**。  
   - 仍依赖 `json.load`，若模型作者注入恶意 JSON（如 `__proto__`），可能触发 **任意对象注入**（虽然在 Python 中风险较低，但建议在未来加入 schema 验证）。  

2. **外部加载的自定义模块**  
   - `BridgeLoader` 直接使用 `ModelRegistry.resolve_model_cls` 从 HF 配置的 `_class_name` 动态实例化，若模型提供者自行实现恶意类可能在 `__init__` 中执行任意代码。应在生产环境开启 **白名单**（仅允许已知包如 `diffusers`、`transformers`）。  

3. **多进程/多卡通信**  
   - USPAttention/All‑to‑All 数据在 `torch.distributed` 中使用 `gather`/`all_reduce`，若节点之间的 **身份验证**（TLS）未开启，可能导致中间人攻击。建议使用 **torch.distributed.elastic** 带 TLS 或 NCCL 环境变量限制。  

4. **随机数生成**  
   - `MOVALatentPreparationStage` 使用 `randn_tensor`，若 `generator` 参数

---

#### 🟡 中重要度变更 (13)

### Support LightOnOCR-2-1B (#17806)
**SHA**: `0769de9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0769de9b0f8ce031eb844dda940d8738871e0e8a)

**变更概览**  
本次提交为 SGLang 项目新增对 **LightOnOCR‑2‑1B**（lightonai）的完整支持，主要工作包括：  

1. **模型注册**：在 `model_config.py` 中将 `LightOnOCRForConditionalGeneration` 加入生成模型列表。  
2. **模型实现**：新增 `lightonocr.py`，实现 Vision‑Encoder（基于 Pixtral）、RMSNorm、PatchMerger、Vision‑Language Adapter 与 Qwen‑3 解码器的组合；并提供权重加载、前向、logits 计算等统一接口。  
3. **多模态处理器**：新增 `lightonocr.py` 处理器，复用 PixtralProcessor 并在后处理阶段去除图像 break/end token，以符合 LightOnOCR 的“单段连续图像 token”特性。  
4. **权重映射**：在 `load_weights` 中完成 HF checkpoint → SGLang 参数的映射（包括 vision_encoder 堆叠 QKV、gate/up、patch_merger、norm、adapter、以及 language_model 的前缀转换）。  

**影响范围**  

- **模型加载路径**：`model_loader` 现在会走 `LightOnOCRForConditionalGeneration`，涉及 `VisionEncoderArgs`、`PatchMerger`、`RMSNorm` 等通用组件。  
- **推理调度**：`general_mm_embed_routine` 仍保持一致，新增模型无需改动调度逻辑。  
- **多模态输入**：旧的 PixtralProcessor 会继续使用 break/end token，新增的 LightOnOCRProcessor 必须在服务启动时被正确注册，否则会出现 token 对齐错误。  

**关注建议**  

1. **参数兼容性**：`vision_args` 的默认值（`image_token_id`、`spatial_merge_size`、`adapter_bias`）硬编码为 151655、2、True，若 HF checkpoint 在未来改动需同步更新。建议在 `__init__` 中加入日志提示，或在配置文件中显式声明这些字段。  
2. **权重加载**：当前仅在 `vision_encoder` 使用了堆叠参数映射，若 HF checkpoint 增加新层（如 bias）可能漏掉。建议在 `load_weights` 前后打印未匹配的键，用单元测试覆盖全部权重路径。  
3. **token 处理**：`LightOnOCRProcessor` 删除 break/end token 后重新映射 `mm_item.offsets`，但未对多张图片的交叉情况做完整验证。建议加入针对多图输入的回归测试，确保 offsets 不会出现空洞或越界。  
4. **量化/并行**：`quant_config` 仍向下传递给 Qwen3，但未用于 Vision 部分。若未来想对 VisionEncoder 实现量化，需要在 `vision_encoder = PixtralHFVisionModel(..., quant_config=quant_config)` 传参，并在权重加载时处理对应 shard。  
5. **文档/示例**：新增模型的 `image_token_id`、`spatial_merge_size` 等配置需在 README / API 文档中说明，并提供一次端到端的 OCR 示例，帮助用户快速上手。  

总体来看，代码结构与现有 Pixtral 实现保持高度一致，新增模型的集成风险相对可控。通过补充测试、日志和文档，可进一步提升稳定性与可维护性。

---

### [diffusion] model: sync with upstream z-Image  (#17822)
**SHA**: `3c2f4c7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3c2f4c7bbe0c76ed900db2569d4b3802ed7fd24d)

**🎯 变更类型**：功能增强（为 Z‑Image 系列模型加入 Turbo 模式及 CFG‑Normalization）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `zimage.py` 中新增 `prepare_neg_cond_kwargs`，为负向提示提供频谱信息。  
2. `SamplingParams` 增加 `cfg_normalization` 参数，并在 CLI 中加入 `--cfg-normalization`。  
3. 拆分 Z‑Image 配置：  
   - `ZImageTurboSamplingParams`（默认 9 步、guidance_scale=0）对应 “Z‑Image‑Turbo”。  
   - `ZImageSamplingParams`（默认 50 步、guidance_scale=5）对应普通 “Z‑Image”。  
4. `registry.py` 更新注册逻辑，使两者模型分别走不同路径。  
5. `denoising.py` 在 `_predict_noise_with_cfg` 中实现基于 `cfg_normalization` 的向量范数限制（两处出现，同步前后均做裁剪）。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/configs/*`（pipeline、sampling 参数）  
- `python/sglang/multimodal_gen/registry.py`（模型注册）  
- `python/sglang/multimodal_gen/runtime/pipelines_core/stages/denoising.py`（采样时的噪声预测）  
- 命令行接口：新增 `--cfg-normalization`  

**💡 关注建议**  
1. **参数兼容性**：`cfg_normalization` 在 `SamplingParams` 中默认 `0.0`，但 `ZImageTurboSamplingParams` 将其设为 `False`，在运行时会被转换为 `0.0`。请确保旧版 CLI 不会因类型不匹配崩溃。  
2. **双重裁剪**：`denoising.py` 中对 `cfg_normalization` 的范数裁剪出现两次（一次在 `noise_pred` 计算后，一次在 `guidance_rescale` 前后），可能导致实际缩放倍率与预期不符。建议统一为一次裁剪，或在注释中说明意图。  
3. **性能影响**：`torch.linalg.vector_norm` 在每个采样步都会调用，若使用多卡并行可能产生额外通信（`broadcast`）。在大批量推理时请评估吞吐量变化。  
4. **测试覆盖**：新增的 Turbo 配置和负向条件需在单元测试和端到端推理脚本中验证，尤其是 `Z-Image-Turbo` 与普通 `Z-Image` 的 `cfg_normalization` 行为是否一致。  
5. **文档同步**：更新 README/CLI 帮助，说明 `--cfg-normalization` 仅对 Z‑Image 系列有效，并给出推荐的因子取值（如 `1.0` 表示不超出原始条件向量范数）。  

总体来看，此次改动为 Z‑Image 系列提供了更细粒度的控制（Turbo 模式、负向提示、CFG‑Normalization），但需注意参数兼容性、双重裁剪的潜在副作用以及对推理速度的影响。建议在发布前加入对应的回归测试和性能基准。

---

### [diffusion]: align sglang diffusion AMD pyproject_other.toml diffusion dependency with pyproject.toml (#16225)
**SHA**: `30adf78` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/30adf78f82682b8e389a448d6933d33c8cc7b985)

**🎯 变更类型**：功能增强 / 依赖对齐  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交统一了 AMD/ROCm 环境下的 Diffusion 依赖。将 `python[all_hip,diffusion_hip]` 改为仅 `python[all_hip]`，并在 `pyproject_other.toml` 中把 Diffusion 所需的库直接并入 `all_hip` 特性；同时移除在 AMD 上强制关闭 `runai_model_streamer` 的代码。

**🎯 影响范围**  
- `docker/rocm.Dockerfile`：镜像构建方式变更。  
- `docs/platforms/amd_gpu.md`、安装脚本：用户安装指令更新。  
- `python/pyproject_other.toml`：依赖列表和 `all_hip` 特性定义修改。  
- `sglang/multimodal_gen/runtime/loader/weight_utils.py`：删除针对 ROCm 的 `runai_model_streamer` 关闭逻辑。

**💡 关注建议**  
1. **依赖一致性**：确认所有 CI 环境（包括 AMD、CPU）使用的 `pyproject.toml` 已同步更新，防止出现缺失的 Diffusion 包。  
2. **兼容性测试**：在 ROCm 镜像中重新跑完整的单元/集成测试，尤其是多模态生成和视频编解码路径，确保 `runai_model_streamer` 重新启用后不再出现全局状态冲突。  
3. **文档同步**：更新其它文档（如 Quick‑Start、GitHub README）中的安装示例，避免仍然出现 `diffusion_hip` 标记导致用户困惑。  
4. **回滚路径**：若用户在 AMD 环境遇到 Diffusion 相关缺失，可手动在 `pyproject.toml` 中添加 `sglang[diffusion_hip]`，提供明确的 fallback 说明。  

总体来看，变更主要是依赖对齐和代码简化，对功能影响有限，但需在 AMD 环境完成回归验证，以确保 Diffusion 及 RunAI 功能正常。

---

### Add aiter bias moe support in gpt-oss mxfp4 model (#17735)
**SHA**: `ef1c512` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ef1c5127545ed9ee26c9d3ba0f55ce370c4c247d)

**🎯 变更类型**：功能增强（在 GPU‑ROCm 环境下为 GPT‑OSS MXFP4 模型加入 aiter‑bias‑MOE 支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 新增环境变量 `SGLANG_USE_AITER`（仅在 HIP 环境生效），用于打开 aiter‑bias‑MOE 路径。  
- 在 `mxfp4.py` 中加入对 `aiter.ops.shuffle`（`shuffle_weight_a16w4`、`shuffle_scale_a16w4`）的调用，对权重、scale、bias 进行 16‑bit 对齐、shuffle 并加 padding（hidden/intermediate 向上对齐至 256）。  
- `forward` 入口增加 aiter‑MOE 分支，使用 `fused_moe` 完成带 bias、scale 的混合专家计算。  
- `server_args.py` 根据 ROCm + MXFP4 + `SGLANG_USE_AITER` 自动切换 `moe_runner_backend` 为 “auto”。  

**🎯 影响范围**  
- `python/sglang/srt/layers/quantization/mxfp4.py`（权重构造、shuffle、前向计算）  
- `python/sglang/srt/server_args.py`（模型启动时后端选择）  
- 受影响模块：MXFP4‑quantized GPT‑OSS、MOE 相关层、ROCm（HIP）运行时  

**💡 关注建议**  
1. **兼容性**：`_use_aiter` 仅在 `is_hip()` 为真且环境变量开启时生效，确保在 CUDA 环境下仍走原有路径，避免意外切换。  
2. **环境变量**：建议在文档/README 中明确 `SGLANG_USE_AITER=1` 的作用及其对显存/性能的影响。  
3. **Padding 与 Shuffle**：新增的 256‑对齐可能导致模型参数大小变化，检查 checkpoint 保存/加载时是否考虑 `hidden_pad`、`intermediate_pad`。  
4. **错误处理**：`try/except ImportError` 已保留，但若 aiter 版本不匹配，仍可能在运行时触发 shape/permute 错误，建议加入更友好的报错信息。  
5. **测试**：在 ROCm（gfx9x 系列）机器上添加单元测试，验证 `fused_moe` 输出与原 Triton/FlashInfer 实现数值一致，尤其是 bias 与 scale 的正确性。  
6. **性能评估**：由于 shuffle 与 padding 会带来额外拷贝，建议提供基准脚本对比 aiter 与 triton‑kernel 路径的吞吐和延迟。  

总体来说，此次改动为 ROCm‑GPU 上的 MXFP4‑MOE 提供了新的高效实现，但需要在文档、测试与兼容性方面做好支撑，以防在不同硬件或 aiter 版本下出现不可预期的行为。

---

### [diffusion] model: move tp_rmsnorm check to WanTransformerBlock (#17792)
**SHA**: `319f688` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/319f6886fe6bb83cde668c9e7698ab4b2b2f67d1)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将张量并行（TP）相关的 RMSNorm 判定从各层内部迁移到 `WanTransformerBlock`，统一使用 `get_tp_world_size` 替代原来的 `get_tensor_model_parallel_world_size`。  
2. 引入 `local_num_heads = divide(num_heads, tp_size)`，并改用 `Tensor.unflatten` 替代手动 `view`，使得头数在 TP 环境下的切分更安全、可读。  
3. 相应地更新了 `ColumnParallelLinear / RowParallelLinear` 的初始化、`USPAttention` 与 `MinimalA2AAttnOp` 的 `num_heads` 参数，以及 `tensor_parallel_rms_norm` 的调用条件。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/runtime/models/dits/wanvideo.py`（核心模型实现）  
- 相关的分布式工具函数：`sglang.multimodal_gen.runtime.distributed`（新增 `get_tp_world_size` 的导入方式）  
- 依赖 `WanTransformerBlock` 的上层调用（例如多模态生成 pipeline）  

**💡 关注建议**  
1. **分布式兼容性**：确保 `get_tp_world_size` 在所有启动脚本里已正确注册，否则会导致 `NameError`。建议在单机/单卡模式下也执行一次 `get_tp_world_size()`，确认返回 1。  
2. **形状一致性**：`unflatten` 依赖 `local_num_heads` 与 `head_dim` 的乘积等于原始维度，若 `num_heads` 不能被 TP size 整除会抛错。请在模型配置层加入断言或容错处理。  
3. **RMSNorm 判定**：`self.tp_rmsnorm` 现在在 `WanTransformerBlock` 中统一设置，注意在旧代码路径（如直接实例化 `WanTransformerBlock` 的子类）仍保持相同语义。  
4. **回归测试**：在 1‑8 张卡的 TP 环境下跑一次完整的 multimodal 生成任务，特别是含图像（`context_img`）的分支，验证 `q/k/v` 的切分和注意力输出准确性。  
5. **文档/配置更新**：在模型配置文档中说明 `tp_size` 必须能整除 `num_heads`，并标明已改用 `get_tp_world_size`。  

总体而言，此次改动统一了 TP 相关的头数划分逻辑，代码可读性和错误排查成本下降。但在多卡环境下仍需做好除尽检查和兼容性验证。

---

### [NPU] support GPTQ quantization on npu (#15203)
**SHA**: `7b79326` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7b79326751471580059e39cd0344c987ed5c9ca8)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交为 Ascend NPU 添加了 GPTQ 量化支持。核心实现是 `GPTQLinearAscendMethod`（继承自原有 `GPTQLinearMethod`），并在 `GPTQConfig` 中加入 `checkpoint_format` 用于区分 v1/v2 格式。新增 `unpack_from_int32`、权重属性标记、以及对 4‑bit 权重的 NPU‑专用 pack、batchmatmul 调用。相应的测试 `test_ascend_gptq.py` 验证了 INT8 GPTQ 模型在 GSM8K 任务上的精度。

**🎯 影响范围**  
- `sglang/srt/layers/linear.py`：暴露 `GPTQLinearAscendMethod`。  
- `sglang/srt/layers/quantization/gptq.py`：大量改动，涉及 NPU 检测、配置解析、权重解包、`create_weights`、`process_weights_after_loading`、`apply`。  
- `sglang/srt/models/qwen3.py`：在 NPU 场景下避免缓存权重导致的错误。  
- 测试套件：新增 Ascend GPTQ 测试并在 `run_suite.py` 中加入。  

**💡 关注建议**  

1. **环境依赖**：使用前确保已装 `torch_npu`（>=2.0）并配置好 Ascend 驱动；否则会在 import 时报错。  
2. **兼容性**：  
   - `desc_act=True` 仍不受支持，调用时会抛异常。  
   - 目前 MoE 仍未实现，若模型含 FusionMoE，需等待后续补丁。  
   - `get_min_capability` 在 NPU 上直接抛 `NotImplementedError`，请在调用前自行判定硬件是否为 NPU。  
3. **checkpoint_format**：旧的 GPTQ‑v1 检查点默认 `""`，新模型可显式配置 `"gptq_v2"`；使用时注意 `use_v2_format` 标志会影响 `qzeros` 的加 1 操作。  
4. **权重打包**：4‑bit 权重在加载后会被 `torch_npu.npu_convert_weight_to_int4pack` 转为 int32 包，涉及显存节约；请确认下游算子（`npu_weight_quant_batchmatmul`）已在当前 NPU 版本实现。  
5. **测试**：本地运行 `python -m unittest test/srt/ascend/test_ascend_gptq.py` 可快速验证；若出现精度低于阈值，请检查模型路径、`--attention-backend ascend` 与 `--quantization gptq` 参数是否生效。  

**后续工作**  
- 完善 MoE 的 GPTQ NPU 支持。  
- 为 `get_min_capability` 提供 NPU 实现或在文档中说明禁用路径。  
- 增加 3‑bit / 2‑bit 的 NPU 兼容性测试。  

开发者在提交新模型或修改量化配置时，务必确认 `checkpoint_format` 与实际模型一致，避免因 zero‑point 处理差异导致推理错误。用户在使用 Ascend NPU 前，请先通过 `sglang.srt.utils.is_npu()` 检查环境，确保 `torch_npu` 正常工作。

---

### [MUSA][4/N] Add common device utilities, distributed backend, and custom op wiring (#17246)
**SHA**: `d3cdee0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d3cdee0a040bebf0bf651e7c9d13449741731a50)

**🎯 变更类型**：功能增强（新增 MUSA（Moore Threads GPU）平台支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在多处核心模块中加入对 MUSA 设备的识别、调度和资源管理。包括 `custom_op`、`DeviceConfig`、分布式 `parallel_state`、多平台算子 `MultiPlatformOp`、内存查询与清理、以及模型运行时的后端选择（MCCl）。同时引入 `mthreads‑gmi` 作为 MUSA 内存容量的探测手段。  

**🎯 影响范围**  

| 模块 | 关键改动 | 可能受影响的功能 |
|------|----------|-------------------|
| `sglang/multimodal_gen/runtime/layers/custom_op.py` | 新增 `forward_musa` 并在调度中加入 `is_musa` 分支 | 自定义算子在 MUSA 上回退至 PyTorch 实现 |
| `sglang/srt/configs/device_config.py` | `device` 列表加入 `"musa"` | 通过配置文件启用 MUSA |
| `sglang/srt/distributed/parallel_state.py` | 识别 `is_musa`，构造 `torch.device("musa:…")`，内存清理 `torch.musa.empty_cache()` | 分布式训练/推理的设备映射、显存回收 |
| `sglang/srt/layers/rotary_embedding.py`、`utils/multi_platform.py` | 加入 `_is_musa` 与对应的前向分支 | RotaryEmbedding 与其它跨平台算子在 MUSA 上使用 fallback |
| `sglang/srt/model_executor/model_runner.py` | `device == "musa"` 时走 CUDA 路径（cublas、attention、graph）并使用 `"mccl"` 作为分布式后端 | 初始化加速库、图热身、分布式通信 |
| `sglang/srt/utils/common.py` | `get_available_gpu_memory`、`broadcast_pyobj`、`get_device_memory_capacity`、`get_mtgpu_memory_capacity` 等全链路适配 MUSA | 显存查询、广播、设备能力探测等工具函数 |
| 其他（`device.py`、`get_device_name`、`get_device_count` 等） | 对 `torch.musa` 的可用性检查做兼容 | 通用设备信息接口 |

**💡 关注建议**  

1. **导入 & 可用性判断**：多处使用 `is_musa()`，但文件顶部并未显式 `from sglang.srt.utils.common import is_musa`。请统一在需要的模块中加入该导入，防止 `NameError`。  
2. **`get_device_name/count` 的 MUSA 支持**：当前仅在 CUDA 可用时返回名称/数量，MUSA 环境会走错误路径并抛异常。建议在 `is_musa()` 为真时调用 `torch.musa.get_device_name`、`torch.musa.device_count()` 等对应 API。  
3. **后端 MCCl**：`model_runner` 采用 `"mccl"` 作为分布式后端，但 PyTorch 官方仅提供 `"nccl"`、`"gloo"`、`"mpi"`。若 MUSA 没有原生 `mccl` 实现，运行时会报 `ValueError: unknown backend`. 建议在初始化前检测 `torch.distributed.is_nccl_available()` 或提供回退至 `"gloo"`。  
4. **显存查询**：`get_available_gpu_memory` 对 MUSA 使用 `torch.musa.mem_get_info()`，但在集成显卡上仍使用 `psutil.virtual_memory()`。请确认 `torch.musa.get_device_properties` 返回的 `is_integrated` 字段在 MUSA 驱动中存在，避免属性错误。  
5. **错误信息统一**：`DeviceConfig.__init__` 和 `get_device()` 的 `RuntimeError` 信息仍未列出 MUSA。建议补全错误提示，以免用户误以为不支持。  
6. **单元测试/CI**：在 CI 中加入非 CUDA 环境的模拟（例如通过 `torch.backends` mock）来验证 MUSA 路径的代码路径覆盖，防止因缺少 `torch.musa` 包导致 CI 失败。  
7. **文档更新**：README/部署指南需标明 MUSA 依赖（`torch-musa`、Moore Threads 驱动、`mthreads-gmi`）以及对应的环境变量设置（如 `MCCl`）。  

总体而言，本次 PR 为项目引入了对新硬件平台的初步支持，改动分散且保持了与已有 CUDA、XPU、NPU 分支的对称结构。只要完善导入、后端容错以及设备信息的完整实现，项目即可在 MUSA 环境中平滑运行。

---

### [JIT Kernel]Support fused_add_rmsnorm in JIT Kernel (#17677)
**SHA**: `0368ddf` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0368ddf9eac0df43aa06c8c260d5fb49d88f46d5)

**🎯 变更类型**：功能增强（新增 fused_add_rmsnorm JIT Kernel）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `sglang/jit_kernel` 中实现了 **fused_add_rmsnorm**（加残差后直接 RMSNorm）算子：  
1. 新增 CUDA 实现 `fused_add_rmsnorm.cuh`，支持 bf16/fp16、向量化加载（最大 32 B），并在 CTA 内完成求平方和、rsqrt 与缩放的全部步骤。  
2. 在 Python 层加入 JIT 加载器 `norm._jit_fused_add_rmsnorm_module`，提供统一的 `fused_add_rmsnorm` 接口。  
3. 配套基准脚本 `bench_fused_add_rmsnorm.py` 与单元测试 `test_fused_add_rmsnorm.py`，对比 FlashInfer 实现的数值精度与性能。  

**🎯 影响范围**  
- `python/sglang/jit_kernel/norm.py`（新增函数入口）  
- `python/sglang/jit_kernel/csrc/elementwise/fused_add_rmsnorm.cuh`（核心 CUDA kernel）  
- `python/sglang/jit_kernel/include/sgl_kernel/vec.cuh`（向量大小上限从 16 B → 32 B）  
- `python/sglang/jit_kernel/benchmark/bench_fused_add_rmsnorm.py`（基准）  
- `python/sglang/jit_kernel/tests/test_fused_add_rmsnorm.py`（功能验证）  

**💡 关注建议**  
1. **兼容性检查**：kernel 仅在 hidden‑size 能被向量元素数整除且 CUDA Compute Capability ≤9/≤10 时可用，建议在 `norm.fused_add_rmsnorm` 前加入 fallback 或报错提示。  
2. **对齐断言**：当前在 `fused_add_rmsnorm_reg_kernel` 通过 runtime check 强制对齐，CI 中应覆盖异常路径，防止隐藏的尺寸错误。  
3. **性能基准**：CI 只跑小 batch（16）配置，建议在 PR 中加入大 batch/不同显存的跑分对比，以防回归。  
4. **文档与例子**：更新 README / API 文档，说明 `fused_add_rmsnorm` 的 dtype、输入/输出含义以及推荐的 hidden‑size 范围。  
5. **测试覆盖**：现有测试覆盖了多种 batch 与 hidden‑size，仍需加入极限（8192+）以及非对齐尺寸的负测试，保证 RuntimeCheck 行为可靠。  

总体而言，此功能为 SGLang 在融合算子层面的重要进展，若上述兼容性与文档工作跟进到位，可在保持数值准确性的同时显著提升推理吞吐。

---

### Make the functions in logits_processor.py and sampler.py more modular (#17885)
**SHA**: `d418081` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d4180815a4014aef9c8f50c9c7d5948e63407c96)

**变更概览**  
- **核心文件**：`logits_processor.py`、`sampler.py`  
- **主要目标**：把原来在 `LogitsProcessor` 与 `Sampler` 中耦合的逻辑拆分为若干独立、可复用的子函数，使代码结构更清晰、可测、易维护。  
- **关键新增**  
  1. **多项评分**：通过 `get_global_server_args().multi_item_scoring_delimiter` 控制，仅在 prefill‑only 请求时走 `compute_logprobs_for_multi_item_scoring`（从 `forward` 中抽出）。  
  2. **前向拆解**：`_get_pruned_states`、`_get_hidden_states_to_store`、`_expand_metadata_for_logprobs`、`_get_dllm_logits`、`_get_logits`（内部进一步拆为 `_gather_dp_attn_hidden_states`、`_compute_lm_head`、`_gather_attn_tp_logits`、`_scatter_dp_attn_logits`、`_copy_logits_to_buffer`）等，分别负责状态裁剪、隐藏层保存、logprob 元数据展开、DLLM 逻辑、以及多种并行场景下的张量收集/广播。  
  3. **采样模块**：新增 `_sample_from_probs`、`_attach_logprobs_to_output`、`_sync_token_ids_across_tp`，把采样分支、日志概率写回、TP 同步抽象为独立函数，`forward` 只负责流程控制。  

**影响范围**  
- **模型前向**：`LogitsProcessor.forward` 现在会先检查多项评分、DLLM、以及是否需要返回完整 logits；随后统一走 `_get_pruned_states` → `_get_hidden_states_to_store` → （是否分块）→ `process_input_logprobs`。  
- **并行实现**：所有原本在 `_get_logits` 中硬编码的 DP/TP 收集、软上限、scale 等操作被迁移到专用私有方法，保持了原有行为但代码路径更明确。  
- **采样路径**：`Sampler.forward` 只保留整体调度，实际采样全部交给 `_sample_from_probs`，日志概率的写回也统一在 `_attach_logprobs_to_output`。  

**建议与注意点**  

| 关注点 | 建议 |
|---|---|
| **向后兼容** | 新增的 `multi_item_scoring_delimiter` 默认保持 `None`，确保已有部署不受影响；若用户在配置文件中误设为非法 token，需在 `LogitsProcessor.__init__` 报错或回退。 |
| **单元/集成测试** | - 为 `compute_logprobs_for_multi_item_scoring` 添加多请求、单请求两种 case，验证 delimiter 计数与 `extend_logprob_pruned_lens_cpu` 的正确性。<br>- 针对所有并行分支（`do_tensor_parallel_all_gather`, `do_tensor_parallel_all_gather_dp_attn`, `use_attn_tp_group`）分别跑一次前向，确保 logits 与 hidden_state 没有遗漏或维度错误。 |
| **性能回归** | 新的分块判断 (`should_skip_chunking`) 与原实现保持一致，建议在大批量 `prefill` 场景下对比前后耗时，确认额外的张量拷贝（如 `hidden_states_to_store`）没有显著引入开销。 |
| **异常处理** | `_gather_dp_attn_hidden_states`、`_gather_attn_tp_logits`、`_scatter_dp_attn_logits` 中若 `logits_metadata` 不具备相应属性会抛异常，建议加 `assert` 或 `if` 检查并给出明确错误信息，以免在稀有的自定义模型配置下出现 obscure 的 `AttributeError`。 |
| **文档/注释** | - 在 `logits_processor.py` 顶部补充模块级说明，列出各私有函数的职责和调用顺序。<br>- 在 `sampler.py` 添加 `sampling_backend` 支持的后端列表说明，防止后续新增后忘记更新分支。 |
| **代码风格** | 私有函数多数返回多个张量，返回类型建议使用 `namedtuple` 或 `dataclass`（如 `PrunedStateInfo`）提升可读性；目前返回顺序容易产生误用。 |
| **NPU 兼容** | `final_logit_softcapping` 分支已保留 NPU 分支，确认 `torch.tanh` 的 dtype 与 `logits` 对齐，防止混合精度下出现 `RuntimeError: expected scalar type Float but got Double`。 |

**总体评估**  
本次改动显著提升了 **模块化** 与 **可测试性**，对现有功能的行为保持一致（只要 `multi_item_scoring_delimiter` 未打开）。需要确保并行/DP/TP 组合、DLLM、以及多项评分三条新路径在 CI 中得到覆盖，避免因参数传递细微变化导致隐蔽的回归。若配套完善的单元测试与文档更新，一般可视为 **中风险**（改动范围大但逻辑清晰）且对后续功能扩展（如自定义 scoring 或新采样后端）提供了可靠的基础。

---

### [Perf] Tune Llama-4-Scout-17B-16E-Instruct fused moe kernel (#17891)
**SHA**: `0998de0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0998de088b206004db6ab830bd3deba5ce27642f)

**变更类型**：性能优化  
**重要程度**：🟡 中  

**变更摘要**  
本次提交在 `python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/` 目录下新增 `E=16,N=2048,device_name=NVIDIA_B200.json`，为 Llama‑4‑Scout‑17B‑16E‑Instruct（expert 数 16、top‑k 2048）在 NVIDIA B200（H100/H800）上运行的 Triton 3.5.1 fused‑MoE kernel 提供了一套细粒度的调度参数：`BLOCK_SIZE_M/N/K`、`GROUP_SIZE_M`、`num_warps`、`num_stages` 等。  

**影响范围**  
- **核心模块**：`sglang.srt.layers.moe.fused_moe_triton`（加载 JSON 配置并据此实例化 Triton kernel）。  
- **运行时**：仅在使用 fused‑MoE 且检测到 Triton 3.5.1、设备标识为 `NVIDIA_B200` 时生效；其他硬件仍走默认配置。  

**关注建议**  
1. **配置发现**：确认代码路径 `fused_moe_triton/configs/triton_3_5_1/` 已被自动扫描，否则新 JSON 可能被忽略。  
2. **版本兼容**：Triton 3.5.1 与旧版 Triton 生成的 kernel 参数不同，建议在 `requirements.txt` 或文档中注明最低 Triton 版本。  
3. **回退机制**：若参数与实际硬件不匹配（如 `BLOCK_SIZE_K` 超出共享内存），kernel 会报错。建议在加载前加入合法性检查或在 `fallback` 中使用保守配置。  
4. **测试覆盖**：在 CI 中加入针对 `NVIDIA_B200`（可使用模拟或容器）运行的性能基准，确保新配置至少不比旧配置慢。  
5. **文档更新**：在 MoE 使用说明里补充 “如何在 B200 上开启 fused‑MoE”，并给出环境变量或 API 示例 (`--moe-fuse --device B200`)。  

通过上述检查与文档完善，可确保新增配置真正提升该型号 GPU 上的 Llama‑4‑Scout 推理性能，同时避免因不匹配导致的运行时异常。

---

### [MUSA][7/N] Enhance CUDA / PyNccl wrapper to support MTLink connectivity detection (#17499)
**SHA**: `e9d727c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e9d727cb9218f30a7352f2d17614d74ec0577062)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中等  

**📋 变更摘要**  
本次提交为 MUSA（华为/麒麟）平台新增 CUDA‑style 接口的兼容层，使 SGLang 在 MTHREADS GPU 上能够检测 MTLink（MUSA‑NVLink）拓扑并使用自定义 All‑Reduce 实现。主要改动包括：  
1. `python/sglang/srt/distributed/device_communicators/cuda_wrapper.py` 根据运行时是否为 MUSA，切换加载 `libmusart`。  
2. 各 `custom_all_reduce*` 模块加入 `is_musa()` 判断，定义 MUSA 专用的缓存阈值、NVLink 检测路径、以及在创建共享缓冲区时的 `cudaMemset` 调用。  
3. `pynccl_wrapper.py` 增加对 `libmccl.so.2`（MUSA NCCL） 的搜索与错误提示。  
4. 依赖版本升级：`torchada>=0.1.25`。  

**🎯 影响范围**  
- `sglang/srt/distributed/device_communicators/`（cuda_wrapper、custom_all_reduce、custom_all_reduce_ops、custom_all_reduce_utils、pynccl_wrapper）  
- 环境工具 `sglang/srt/utils`（新增 `is_musa` 检测）  
- 项目依赖文件 `python/pyproject_other.toml`（torchada 版本）  

**💡 关注建议**  
- **测试**：在 MUSA 环境（torch‑musa、pymtml）上跑单机/多机 All‑Reduce 基准，验证 MTLink 检测、共享缓冲区创建、以及自定义 All‑Reduce 路径的正确性。  
- **后向兼容**：确认非 MUSA 环境仍能正常加载 `libcudart`、`libnccl`/`librccl`，避免因 `is_musa` 引入的 import 错误导致启动失败。  
- **文档**：补充 MUSA 平台的安装说明、`SGLANG_NCCL_SO_PATH` 环境变量使用方式，以及 MTLink 支持的硬件需求。  
- **依赖**：`torchada>=0.1.25` 可能引入新特性或破坏旧行为，建议在 CI 中加入对应版本的兼容性检查。  

总体来看，此次改动为 MUSA GPU 添加了完整的通信层支持，提升了跨平台可用性，但需重点关注在 MUSA 环境的功能验证及兼容性回归。

---

### [NPU] NZ for non-quantized MOE, Qwen3 MOE double memory consumption fix (#15904)
**SHA**: `b77b0ff` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b77b0ffd6021ef787c9ca7084731578c2f2f4687)

**🎯 变更类型**：Bug 修复 / 性能优化  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 关闭了 `ENABLE_MOE_NZ`/`ENABLE_ASCEND_MOE_NZ` 环境变量的文档说明，表明在 NPU 上不再需要显式开启 NZ（Non‑zero）模式。  
2. 在 NPU 量化路径中移除了一段通过 `.contiguous()` 再 `resize_(0)` 释放缓存的代码，改为直接使用 `transpose(1,2)` 并在加载后统一调用 `npu_format_cast`；同样在 `ep_moe` 层实现中删除了 `release_weight_cache`，改用一次性转置并调用统一的 `npu_format_cast`。  
3. `unquant.py` 中新增 NPU 判断并引入 `npu_format_cast`，统一处理权重转置与格式转换。  
4. `qwen3_moe.py` 将专家权重的懒加载从即时字典改为 `LazyValue`，降低 `load_weights` 时的内存峰值。  
5. 新增 `test_ascend_memory_consumption.py` 用例，验证在双卡（TP=2）下启动 Qwen3‑30B‑A3B‑w8a8 时 NPU 总占用不超过 16 GB。  

**🎯 影响范围**  
- `python/sglang/srt/hardware_backend/npu/quantization/fused_moe_method_npu.py`  
- `python/sglang/srt/layers/moe/ep_moe/layer.py`  
- `python/sglang/srt/layers/quantization/unquant.py`  
- `python/sglang/srt/models/qwen3_moe.py`  
- 文档 `docs/platforms/ascend_*`  
- 新增 NPU 内存消耗测试  

**💡 关注建议**  
1. **兼容性检查**：移除 `release_weight_cache` 可能影响到使用 `torch.contiguous()` 的老代码路径，确认其它平台（CPU、CUDA、HIP）仍保持原有行为。  
2. **环境变量**：文档已删除 `ENABLE_MOE_NZ`，确保 CI、脚本或用户手册中不再依赖该变量。  
3. **LazyValue 语义**：`LazyValue` 延迟执行会导致首次访问时产生一次性的计算开销，建议在多进程/多实例场景下确认不会出现竞争或重复实例化问题。  
4. **性能回归**：运行完整单元测试并在不同规模模型（如 7B、30B、235B）上对比 NPU 内存占用、启动时间和推理吞吐，确保新实现真正降低了峰值内存且不牺牲性能。  
5. **文档同步**：将新加入的 `npu_format_cast` 使用说明写入开发者文档，避免后续有人误用旧的释放缓存方式。  

总体来看，此次提交通过简化 NPU 权重转置与格式转换逻辑，消除了不必要的临时拷贝，显著降低了 MOE 模型在 NPU 上的双倍内存占用，是一次有效的内存‑使用优化。后续关注跨平台一致性及懒加载的并发安全即可。

---

### [AMD] ROCm: route W4A16 MoE to Triton and fix packed-weight loading (#17863)
**SHA**: `1953efb` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1953efb60e82250cd704199833dcaf5075928f29)

**🎯 变更类型**：功能增强 / 兼容性修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `compressed_tensors_moe.py` 中新增 `CompressedTensorsWNA16TritonMoEMethod`，实现针对 ROCm（HIP）平台的 W4A16 MoE，使用 Triton‑based fused‑MoE kernel 替代原 Marlin 实现，并在权重加载后完成 uint8‑packed 格式的转置/重排。  
- 与之配套，`fused_moe_triton/layer.py` 的权重加载逻辑加入对新方法类名的识别，使其走相同的加载路径。  

**🎯 影响范围**  
- `python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py`（MoE 量化方法）  
- `python/sglang/srt/layers/moe/fused_moe_triton/layer.py`（权重加载与转置）  
- 受影响的运行时组件：`MoeRunner`（TRITON 后端）、`TritonMoeQuantInfo`、以及所有使用 W4A16 MoE 的模型在 ROCm 环境下的推理路径。

**💡 关注建议**  

1. **兼容性验证**  
   - 在非‑ROCm 环境（CUDA、CPU）确保仍能正常走原 `CompressedTensorsWNA16MoEMethod`，避免因 `is_hip` 判断失误导致 `ImportError`。  
   - `layer.is_triton_converted` 标记应在所有路径上统一初始化，防止重复转换或遗漏。  

2. **权重转置逻辑**  
   - 转置后 `view(torch.uint8)` 依赖原 tensor 的内存布局，建议在转换前使用 `contiguous()` 并显式检查 `num_experts * N * K/2` 与原 shape 是否匹配，防止因 stride 不一致导致错误。  
   - 关注 `group_size` 为 0 时的 `block_shape` 计算，防止除零或形状不匹配。  

3. **性能与内存**  
   - Triton 版会在加载时产生额外的 `torch.nn.Parameter`（`requires_grad=False`），建议在 `process_weights_after_loading` 完成后 `del` 中间变量，减小峰值内存。  
   - 若模型同时使用 CUDA 与 ROCm，确保 `MoeRunnerBackend.TRITON` 能在两者上统一初始化；若 Triton 在 ROCm 上不可用，需要 graceful fallback。  

4. **单元/集成测试**  
   - 新增针对 ROCm 的权重加载、转置以及 `MoeRunner.run` 的端到端测试，验证 `dispatch_output` 与 `quant_info` 的 shape 与 dtype。  
   - 在 CI 中加入 `--skip-rocm` 的标记，以免在缺乏 ROCm 环境时导致构建失败。  

5. **文档与日志**  
   - 将新方法的使用说明写入 `README` / API 文档，明确 “ROCm only – uses Triton kernel”。  
   - `logger.info_once` 已添加，但在多进程推理时仍可能重复，建议使用进程安全的日志控制。  

6. **潜在风险**  
   - 如果 `torch` 版本升级导致 `torch.uint8` 视图行为改变，可能出现隐式数据截断；建议在关键转换后加入 `assert w13.dtype == torch.uint8` 等检查。  

**结论**  
本次改动为 ROCm 平台提供了 W4A16 MoE 的 Triton 实现，提升了兼容性和潜在推理性能。重点在权重转置的正确性、跨平台后端切换的稳健性以及相应的测试覆盖。完成上述验证与文档补全后即可安全合并。

---

#### 🟢 低重要度变更 (7)

### [NPU] add support for npu x86_64 image release (#14127)
**SHA**: `cfa09d3` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cfa09d311c68052423d378eae160b928abcb731c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：为 NPU Docker 镜像新增 QEMU + Buildx 多架构构建，支持 amd64 与 arm64，更新镜像标签与依赖路径。

---

### [diffusion] fix: resolve library mismatch in scheduler and update dit offload method name (#17916)
**SHA**: `cdedbf1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/cdedbf148680315262c7c511ee16ff5b73041dbc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正 scheduler 的库标识，将 `mova.diffusion.schedulers.flow_match_pair` 替换为 `diffusers`；同时将 `OffloadableDiTMixin` 的调用从 `prepare_for_next_denoise` 改为 `prepare_for_next_req`，保持接口一致。

---

### Fix flaky tool calls in the Kimi K2.5 model (#17914)
**SHA**: `9409c43` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9409c43593f2d6d64595981abf216a15752b0875)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_args.py` 中删除 `ToolStrictLevel` 的导入及对 Kimi K2.5 模型工具调用的强制最高严格级别设置，以修复该模型的间歇性调用问题。

---

### [FIX] kimi_k2 reasoning parser (#17901)
**SHA**: `3c34d2c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3c34d2c3ebfe0d4dfc02353980ed50a350fa0324)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 Kimi K2 的推理解析器改为复用 Qwen3Detector，统一思考模式判定；在聊天入口根据模型默认行为调整 `thinking`/`enable_thinking` 参数的默认值。

---

### update ascend docs (#17741)
**SHA**: `1b22f2e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1b22f2ee1cd5b333c504b467d1f3e4d2ff133f1b)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Ascend NPU 文档中删除过期 Bisheng 安装指令，更新 CANN 版本号（8.5.0），调整 Docker 镜像标签，并修正模型库名称与路径的大小写及前缀。

---

### [Fix][trtllm-mha] Canonicalize the strides when num_head = 1 (#17732)
**SHA**: `673dc09` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/673dc09d9ba30331fa60f23c88c0483865d3afce)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `trtllm_mha_backend` 中加入 `canonicalize_stride` 对单头 KV 缓存进行 stride 标准化，避免 FlashInfer 的 TMA 描述验证错误；并在 `utils.py` 实现该函数。

---

### [CI] Fix CI timeouts by upgrading runai_model_streamer (related to #16937) (#17636)
**SHA**: `3fcda00` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3fcda00e8c057500abbb7ac70135029647f97ec7)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 CI workflow 中新增 `RUNAI_STREAMER_MEMORY_LIMIT=0` 环境变量，防止 runai_model_streamer 超时；将 `runai_model_streamer` 依赖升级至 `>=0.15.5`。

---

