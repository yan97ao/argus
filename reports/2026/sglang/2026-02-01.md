# 每日更新报告（2026-02-01）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-02-01 23:40:07 | Lianmin Zheng | [Auto Sync] Update test_deterministic.py (20260131) (#18034) |
| 2026-02-01 23:39:37 | Lianmin Zheng | [Auto Sync] Update elementwise.py (20260131) (#18033) |
| 2026-02-01 18:59:31 | Yuan Luo | Optimize custom-all-reduce (#17674) |
| 2026-02-01 18:59:15 | Yuan Luo | [VLM] Optimize get_rope_index for GLM4v (#17420) |
| 2026-02-01 18:31:39 | jiashaokun-1 | [NPU] support the Enable return routed experts (#17025) |
| 2026-02-01 18:14:26 | Estrella-xx | [NPU] disaggregation_decode_enable_fake_auto parameter adaptation (#17811) |
| 2026-02-01 17:17:37 | Ke Bao | Reset evict swa status when retract (#18059) |
| 2026-02-01 16:21:56 | Alison Shao | Move deleted 8-GPU tests to test/manual/ (#18060) |
| 2026-02-01 16:20:57 | sunxxuns | [CI] Fix AMD CI by inlining dummy_grok config (#18044) |
| 2026-02-01 15:57:45 | yefei12 | feat: Add Ling Flash v2.0 support for Eagle3 (#15119) |
| 2026-02-01 15:45:23 | lukec | support qwen3-next eagle3 (#14607) |
| 2026-02-01 15:44:44 | Praneth Paruchuri | [Feature] Support file:// URL format for multimodal inputs (#14490) |
| 2026-02-01 15:41:21 | ZhenshengWu | Fix CUDA 12 dependency when importing Mooncake in official CUDA 13.x image (#17540) |
| 2026-02-01 15:39:37 | Roger Young | Optimizing all_reduce in RMSNormTP in minimax_m2 (#16483) |
| 2026-02-01 15:38:40 | Kangyan-Zhou | Fix Diffusion Request Validation to allow missing input artifacts if the input only contains text (#16610) |
| 2026-02-01 15:37:36 | tc-mb | [model] Support MiniCPM-V 4.5 (#9610) |
| 2026-02-01 15:35:11 | linhaifeng | [Bugfix] fix the display error (inconsistent context) (#17699) |
| 2026-02-01 15:30:51 | Kangyan-Zhou | Fix installation script for H200 runners (#18050) |
| 2026-02-01 14:37:22 | Alison Shao | Migrate 4-GPU/8-GPU workflow jobs to stage-c and add CI registry decorators (#17299) |
| 2026-02-01 14:33:43 | Alison Shao | Disable test_mla_int8_deepseek_v3.py temporarily (#18057) |
| 2026-02-01 14:26:51 | Ke Bao | Fix swa kv cache memory allocation (#18039) |
| 2026-02-01 14:26:26 | Minglei Zhu | [BugFix] fix gpt-oss accuracy issue when enabling piecewise cuda graph (#18013) |
| 2026-02-01 14:25:59 | Yinghai Lu | [metric] Optional extra metric labels (#18049) |
| 2026-02-01 13:23:52 | Baizhou Zhang | Set torch url index in pyproject.toml (#16802) |
| 2026-02-01 12:45:36 | khalilzhk | [BugFix] Fix draft model specified config file (#17815) |
| 2026-02-01 12:37:55 | Kangyan-Zhou | Fix rerun stage command with merged commit history (#17960) |
| 2026-02-01 12:21:03 | Kaixi | Skipped warning on sm100 (#18000) |
| 2026-02-01 11:26:19 | Kangyan-Zhou | Improve error output in tnightly tets (#18053) |
| 2026-02-01 09:50:42 | Yingchun Lai | feat: validate ib devices in server args (#17598) |
| 2026-02-01 08:56:23 | b8zhong | [Perf] Add Flashinfer DeepGEMM SM90 for SwapAB Optimization (#15514) |
| 2026-02-01 08:04:08 | Chongchong Tian | Fix: Remove duplicate assignment for use_w4afp8 (#17858) |
| 2026-02-01 08:02:57 | Yi Zhong | [Performance] Optimize Mllama LayerNorm -> Upd (#9725) |
| 2026-02-01 05:20:41 | Hao Jin | Update python/sglang/README.md (#18045) |
| 2026-02-01 05:16:06 | Zaili Wang | [CPU] toml file update (#17861) |
| 2026-02-01 05:14:25 | Mohammad Miadh Angkad | [Tiny] Fix grammar in shared experts fusion log messages (#18043) |
| 2026-02-01 04:02:39 | R0CKSTAR | [MUSA] Update 3rd party dir to build/_deps (#18035) |

### 📊 统计摘要
> 本日共 36 个提交 | 🔴高 2 | 🟡中 12 | 🟢低 22
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (2)](#-🔴-高重要度变更-2)
    - [feat: Add Ling Flash v2.0 support for Eagle3 (#15119)](#855dd05)
    - [feat: validate ib devices in server args (#17598)](#0f2df93)
  - [🟡 中重要度变更 (12)](#-🟡-中重要度变更-12)
    - [[VLM] Optimize get_rope_index for GLM4v (#17420)](#4ea4f2a)
    - [[NPU] disaggregation_decode_enable_fake_auto parameter ad...](#27bec34)
    - [Move deleted 8-GPU tests to test/manual/ (#18060)](#56907cb)
    - [support qwen3-next eagle3 (#14607)](#3ca29df)
    - [Fix Diffusion Request Validation to allow missing input a...](#9c168fc)
    - [[model] Support MiniCPM-V 4.5 (#9610)](#4d28cda)
    - [Fix installation script for H200 runners (#18050)](#e5ac622)
    - [Migrate 4-GPU/8-GPU workflow jobs to stage-c and add CI r...](#a0bae4c)
    - [Fix swa kv cache memory allocation (#18039)](#d396650)
    - [[metric] Optional extra metric labels (#18049)](#1189259)
    - [Fix rerun stage command with merged commit history (#17960)](#e884b17)
    - [[Perf] Add Flashinfer DeepGEMM SM90 for SwapAB Optimizati...](#398d13a)
  - [🟢 低重要度变更 (22)](#-🟢-低重要度变更-22)
    - [[Auto Sync] Update test_deterministic.py (20260131) (#18034)](#1acae30)
    - [[Auto Sync] Update elementwise.py (20260131) (#18033)](#fb60966)
    - [Optimize custom-all-reduce (#17674)](#afebb7a)
    - [[NPU] support the Enable return routed experts (#17025)](#0fe2825)
    - [Reset evict swa status when retract (#18059)](#d9050b4)
    - [[CI] Fix AMD CI by inlining dummy_grok config (#18044)](#47592a2)
    - [[Feature] Support file:// URL format for multimodal input...](#9bb1260)
    - [Fix CUDA 12 dependency when importing Mooncake in officia...](#71babde)
    - [Optimizing all_reduce in RMSNormTP in minimax_m2 (#16483)](#486c7de)
    - [[Bugfix] fix the display error (inconsistent context) (#1...](#2c036f1)
    - [Disable test_mla_int8_deepseek_v3.py temporarily (#18057)](#9518048)
    - [[BugFix] fix gpt-oss accuracy issue when enabling piecewi...](#38d275a)
    - [Set torch url index in pyproject.toml (#16802)](#c7d53fa)
    - [[BugFix] Fix draft model specified config file (#17815)](#429ef98)
    - [Skipped warning on sm100 (#18000)](#2b25154)
    - [Improve error output in tnightly tets (#18053)](#d443d2d)
    - [Fix: Remove duplicate assignment for use_w4afp8 (#17858)](#9951a1a)
    - [[Performance] Optimize Mllama LayerNorm -> Upd (#9725)](#c2ab371)
    - [Update python/sglang/README.md (#18045)](#6aaea09)
    - [[CPU] toml file update (#17861)](#97593c9)
    - [[Tiny] Fix grammar in shared experts fusion log messages ...](#9ac4dca)
    - [[MUSA] Update 3rd party dir to build/_deps (#18035)](#46095f0)
#### 🔴 高重要度变更 (2)

### feat: Add Ling Flash v2.0 support for Eagle3 (#15119)
**SHA**: `855dd05` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/855dd0546ce7ba460f23bed08c469e63955228e2)

**🎯 变更类型**：功能增强 / 架构变更  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：在 `BailingMoE` 相关模型中加入了对 **Ling Flash v2.0 Eagle3** 的支持。新增了捕获指定层的 auxiliary hidden states 的机制，并通过 `capture_aux_hidden_states` 标记向下游 `LogitsProcessor` 传递这些中间特征，以供后续的 Flash‑Attn 计算使用。  

**🎯 影响范围**：  
- `python/sglang/srt/models/bailing_moe.py` 中的 `BailingMoE`、`BailingMoEForCausalLM`、`BailingMoeForCausalLM` 类。  
- 触发这些模型的推理管线（尤其是使用 pipeline parallelism 的场景）。  
- 与 `LogitsProcessor` 交互的代码路径。  

**🔍 技术洞察**  

- **架构影响**  
  - 引入了 **auxiliary hidden‑state 捕获层** (`layers_to_capture`) 以及全局开关 `capture_aux_hidden_states`，使模型在 **最后一个 pipeline rank** 时会返回 `(hidden_states, aux_hidden_states)` 双值。  
  - `LogitsProcessor` 的接口被扩展，需要接受 `aux_hidden_states` 参数；这对上层调用者形成了 **向后兼容的破坏**（原先只返回 logits）。  
  - 新增的 `set_eagle3_layers_to_capture` 方法在模型实例化后可动态配置捕获层，提升灵活性但也要求调用方在正确的进程（last rank）上执行。  

- **性能影响**  
  - **内存**：每捕获一次中间特征会额外保留一份 `hidden_states`（或其 residual 加和），在大模型、长序列下可能导致显存提升 10%~30%（取决于捕获层数）。  
  - **计算**：捕获本身几乎是零开销，但返回的 aux hidden‑states 将被 `LogitsProcessor` 用于 Flash‑Attn 的 “kv‑cache” 计算，可能提升整体吞吐或降低延迟（这正是 Ling Flash 的目标）。  
  - **分布式影响**：仅在 **最后一个 rank** 开启捕获，避免在前置 rank 中额外通信；但若用户在非最后 rank 调用 `set_eagle3_layers_to_capture`，会 silently 被忽略，可能导致调试困难。  

- **安全考虑**  
  - 代码未引入外部输入校验，仅在内部使用层索引列表。若 `layer_ids` 超出合法范围，会导致 `layers_to_capture` 包含非法层号，进而在 `forward` 循环中出现 `IndexError`。应在 API 中加入边界检查以防止潜在的 **拒绝服务**（进程崩溃）。  

**⚠️ 潜在风险**  

1. **向后兼容性破坏**：`forward` 现在在特定条件下返回二元 tuple，未适配的调用方会抛出 `TypeError` 或误解析返回值。  
2. **显存激增**：捕获多个层的特征会显著提升显存占用，尤其在多卡并行时可能导致 OOM。  
3. **错误的层索引**：`set_eagle3_layers_to_capture` 对传入的 `layer_ids` 未做范围检查，错误索引会导致运行时异常。  
4. **分布式不一致**：如果仅在部分 worker 上打开 `capture_aux_hidden_states`，`LogitsProcessor` 可能收到 `None`，导致逻辑错误或不确定行为。  
5. **调试难度**：新增的全局记录器 `get_global_expert_distribution_recorder()` 仍保持原样，但捕获路径的加入可能影响 profiler 的统计结果，需要更新相应的监控脚本。  

**💡 关注建议**  

- **兼容层**：在 `BailingMoEForCausalLM.forward` 中加入向后兼容包装：如果调用方未期望 aux hidden‑states，仍返回单一 tensor，以免破坏已有客户端。  
- **显存预警**：在 `set_eagle3_layers_to_capture` 中加入显存估算并抛出警告；或者提供 `max_aux_layers` 参数，限制最多捕获的层数。  
- **输入校验**：在 `set_eagle3_layers_to_capture` 对 `layer_ids` 做边界检查（`0 <= id < num_hidden_layers`），并在非法时抛出明确异常。  
- **文档与示例**：在项目 README 或 API 文档中说明如何在 Eagle3 环境下启用该特性、需要在最后 rank 调用的时机、以及典型的层索引配置（如 `[2, N//2, N-3]`）。  
- **单元/集成测试**：添加覆盖以下情形的测试：  
  1. `capture_aux_hidden_states=False` 时仍返回单一 hidden_states。  
  2. 捕获 1/3/全部层的 aux hidden‑states，验证返回结构和数值一致性。  
  3. 在多卡 pipeline 环境中，仅在最后 rank 开启捕获，确保其他 rank 正常返回。  
- **监控与回滚**：在发布新版本时，提供环境变量或配置开关（如 `SGLANG_EAGLE3_CAPTURE=0|1`）以便快速回滚。  

通过以上措施，可最大化新功能的业务价值（支持 Ling Flash v2.0 提升推理效率），同时将潜在的兼容性、性能和安全风险降到最低。

---

### feat: validate ib devices in server args (#17598)
**SHA**: `0f2df93` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0f2df9370a1de1b4fb11b071d39ab3ce2287a350)

**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**：  
在 `server_args.py` 中为 Mooncake（基于 InfiniBand 的 RDMA）后端新增 IB 设备校验逻辑，统一在启动参数解析阶段验证设备名称、去重并检查系统实际可用的 IB 设备；同时在单元测试夹具中实现自动检测可用 IB 设备并根据 GPU 数量智能映射 RDMA 设备，提升 CI 与实际部署的可靠性和可维护性。  

**🎯 影响范围**：  
- `python/sglang/srt/server_args.py`（服务器启动参数处理、Elastic EP、Disaggregation 相关）  
- `python/sglang/test/server_fixtures/disaggregation_fixture.py`（CI 测试用例、RDMA 设备自动发现）  
- 与 Mooncake 后端交互的所有模块（如 `elastic_ep_backend`, `disaggregation_transfer_backend`, `encoder_transfer_backend`）  

**🔍 技术洞察**：

- **架构影响**  
  - 引入了 **参数验证层**（`_validate_ib_devices`），在服务器启动前即对 IB 设备进行合法性校验，防止非法或不存在的设备进入 Mooncake 后端。  
  - 与原有的 “硬编码/环境变量” 方式形成互补，增强了 **配置即验证**（Config‑as‑Code）的设计理念。  
  - 测试夹具中加入了 `\_get_available_ib_devices` 自动发现逻辑，使 CI 能够在不同硬件环境下自适应，降低了对固定硬件的耦合。

- **性能影响**  
  - 校验仅在 **启动阶段**执行一次，涉及一次 `os.listdir` 与若干文件读取（`state`、`rate`），对整体运行时性能几乎没有影响。  
  - 在没有 InfiniBand 驱动或 `/sys/class/infiniband` 不存在的机器上，会快速抛出异常或返回 `None`，不会产生运行时循环开销。

- **安全考虑**  
  - 对用户传入的设备字符串进行 **严格解析、去空格、去重**，防止因意外的特殊字符导致路径遍历或注入风险。  
  - 依赖系统文件 `/sys/class/infiniband`，若进程权限受限导致读取失败，会在验证阶段抛出 `RuntimeError`，从而 **阻止不安全的默认回退**（如误将普通以太网卡当作 RDMA 设备）。

**⚠️ 潜在风险**：

1. **非 InfiniBand 环境启动失败**  
   - 当系统缺少 `/sys/class/infiniband`（如普通云服务器）且用户仍指定 `mooncake`，验证会抛 `RuntimeError`，导致服务无法启动。  
2. **CI 环境依赖系统路径**  
   - 自动检测函数在 CI 中可能因为缺乏 `/sys` 或权限不足而返回 `None`，进而使用硬编码回退；但如果硬编码与实际硬件不匹配，可能出现 **设备映射错误**。  
3. **权限问题**  
   - 读取 `/sys/class/infiniband/*/ports/1/state`、`rate` 需要 **读取权限**；在受限容器或沙箱中可能被阻断，引发异常。  
4. **异常处理不够细粒度**  
   - 当前在发现无效设备或重复设备时直接抛 `ValueError`，缺少可恢复的降级方案（如自动过滤或使用 “auto discovery”），对用户体验有一定冲击。  

**💡 关注建议**：

- **可配置的容错开关**：在 `ServerArgs` 中加入 `--mooncake-allow-invalid-devices`（默认关闭）或 `--skip-ib-validation`，在特殊测试/容器环境下可绕过校验，避免启动崩溃。  
- **更健壮的权限检查**：在 `_validate_ib_devices` 前先检测对 `/sys/class/infiniband` 的读取权限，若不足给出明确提示而不是直接抛异常。  
- **日志级别统一**：将验证阶段的 `logger.warning` 替换为 `logger.info`/`logger.debug`，避免在生产环境产生过多警告；错误仍保留 `logger.error`。  
- **文档补全**：在项目 README 或部署手册中注明 Mooncake 后端对 IB 设备的需求、如何通过环境变量或自动检测配置。  
- **单元测试覆盖**：增加对异常路径的测试（如无 `/sys`、重复设备、未知设备），确保在缺失硬件时能够给出友好的错误信息。  

通过以上措施，可在保持功能增强价值的同时，降低对特定硬件依赖的风险，提升项目在多样化部署环境中的鲁棒性。

---

#### 🟡 中重要度变更 (12)

### [VLM] Optimize get_rope_index for GLM4v (#17420)
**SHA**: `4ea4f2a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4ea4f2a20c4b7d6d78220ac5e1c80aa1d288c9fb)

**🎯 变更类型**：性能优化 / 重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 新增 `benchmark/bench_rope/benchmark_rope_index.py`，用于评估 `MRotaryEmbedding.get_rope_index_glm4v` 在不同 token 长度、图片/视频数目等配置下的跑时。  
2. 大幅重写 `rotary_embedding.py` 中的 `get_rope_index` / `get_rope_index_glm4v`：去除频繁的 `.item()`，改用 `int()`；在 `torch.arange` 中显式指定 `device`，改用 `reshape` 替代 `flatten`；利用一次性 `torch.cat`、高级索引与就地（in‑place）操作降低循环开销；提前把 `attention_mask` 转至目标设备，减少跨设备拷贝。  

**🎯 影响范围**：  
- `sglang/srt/layers/rotary_embedding.py`（核心位置编码逻辑）  
- 新增的 benchmark 脚本仅用于内部性能测试，不影响库的运行时 API。  

**💡 关注建议**：  
1. **功能等价性**：请补充单元测试，验证在相同输入下优化前后的 `position_ids` 与 `mrope_position_deltas` 完全一致，尤其是多模态（image/video）分支的索引拼接。  
2. **设备兼容**：`int(t)` 等 CPU 转换在 GPU 环境下仍然安全，但若后续改为 `torch.int64` 张量传递，需要确保不再误用 CPU 标量。建议在注释中说明此转换的意图。  
3. **内存占用**：在极大 batch / token 场景下仍会在循环中累计 `llm_pos_ids_list`，可考虑改为预分配大张量或分块写入，以进一步降低峰值内存。  
4. **代码可读性**：部分逻辑（如 `video_check_flg`→`input_token_type` 再 `groupby`）虽已优化，但仍可抽成小函数，提升可维护性。  
5. **Benchmark 脚本**：该脚本依赖 `torch.cuda.synchronize`，在 CPU 环境下会报错；建议在入口处添加 `if device.type == "cuda"` 的 guard，或提供 `--device cpu` 时的安全路径。  

总体来看，改动显著提升了位置索引的构建速度（尤其在多模态分支），对现有 API 没有破坏性影响。只需补齐等价性测试并注意上述细节，即可安全合入主线。

---

### [NPU] disaggregation_decode_enable_fake_auto parameter adaptation (#17811)
**SHA**: `27bec34` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/27bec34203c981ed90e4f1c6bfd3ea75cc5bf346)

**🎯 变更类型**：功能增强 & Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 Ascend 侧的 disaggregation 实现中新增 `disaggregation_decode_enable_fake_auto` 参数，使得在 “decode‑fake‑auto” 模式下可以跳过 TransferEngine 的真实初始化，避免不必要的资源占用和潜在错误。相关构造函数、连接对象以及初始化流程同步传递并使用该标志。  

**🎯 影响范围**  
- `python/sglang/srt/disaggregation/ascend/conn.py` – 连接对象新增成员。  
- `python/sglang/srt/disaggregation/ascend/transfer_engine.py` – 构造函数和 `initialize` 增加参数、逻辑分支。  
- `python/sglang/srt/disaggregation/common/conn.py` – 将服务器端参数向上透传。  

**💡 关注建议**  
1. **向后兼容**：目前 `AscendTransferEngine.__init__` 必须显式传入 `disaggregation_decode_enable_fake_auto`，但旧的调用点可能未更新。建议为该参数提供默认值（如 `False`），或在调用方加入兼容层，防止运行时 `TypeError`。  
2. **参数来源**：`server_args` 中的 `disaggregation_decode_enable_fake_auto` 必须在 CLI/配置文件中声明并有合理默认，防止未定义导致属性错误。  
3. **日志与可观测性**：在 fake 模式下仅打印 info，建议在关键路径加入 `debug` 级别日志，帮助排查为何跳过初始化。  
4. **单元测试**：补充两类测试：① 在 `disaggregation_decode_enable_fake_auto=True` 时，`initialize` 直接返回且不调用底层 TransferEngine；② 在 `False` 时保持原有行为。  
5. **文档更新**：在部署指南和参数说明中加入 `disaggregation_decode_enable_fake_auto` 的用途、默认值及使用场景。  

整体改动规模适中，核心逻辑清晰，但需注意向后兼容和测试覆盖，以免在未升级的节点上导致启动失败。

---

### Move deleted 8-GPU tests to test/manual/ (#18060)
**SHA**: `56907cb` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/56907cbcb190affa4dfa3473cbe7534622f2f8b1)

**🎯 变更类型**：其他（将原 8‑GPU 手工测试移入 `test/manual/`）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：新增了两个手动回归测试（`test_kimi_k2_models.py`、`test_mistral_large3_basic.py`），分别对 Kimi‑K2‑Thinking 和 Mistral‑Large‑3‑675B 在 8‑GPU 环境下执行 GSM8K few‑shot 评估以及单提示吞吐速率。测试使用 `popen_launch_server` 启动 SGLang 服务，并在 `tearDownClass` 中通过 `kill_process_tree` 结束进程。  

**🎯 影响范围**  
- **测试体系**：`test/manual/` 下的新增文件会被 CI 中的手动/慢速测试入口加载；若默认跑全部测试，可能导致 CI 超时或在资源不足的机器上失败。  
- **运行时环境**：需要 8 张 GPU、足够显存（数百 GB）以及网络访问 `moonshotai/Kimi‑K2‑Thinking`、`mistralai/Mistral‑Large‑3‑675B‑Instruct‑2512`。  
- **全局配置**：为 Mistral‑Large‑3 暂时关闭 JIT DeepGemm (`SGLANG_ENABLE_JIT_DEEPGEMM=0`) 并在 `tearDownClass` 中清理。  

**💡 关注建议**  

1. **标记为慢测/手动**  
   - 在文件或类上使用 `@unittest.skipIf(not is_in_ci() or not enough_gpus(), "requires 8‑GPU manual run")`，或添加 pytest `@pytest.mark.slow`、`@pytest.mark.manual`，防止误跑到普通 CI。  

2. **资源检查**  
   - 在 `setUpClass` 前检测 CUDA 可见设备数 (`torch.cuda.device_count()`) 与显存是否满足模型加载，否则提前报错并跳过。  

3. **超时与清理**  
   - `popen_launch_server` 的 `timeout` 已放宽至 `*5`，但仍建议在测试入口捕获 `TimeoutError`，并在异常路径同样调用 `kill_process_tree` 防止残留进程。  

4. **网络依赖**  
   - `requests.get(.../flush_cache)` 以及模型下载可能因网络波动导致 flaky，考虑加入重试或在 CI 环境下提前缓存模型。  

5. **结果阈值**  
   - 断言的 `accuracy > 0.95 / 0.90` 与 `speed > 50 token/s` 在不同硬件上波动大，建议改为 `self.assertGreaterEqual` 并在 `README` 中注明参考硬件（A100‑80GB, 8‑way TP）。  

6. **文档同步**  
   - 更新项目根目录的 `test/README.md` 或 CI 手动测试说明，列出新增的 8‑GPU 手工测试及其启动方式，防止新贡献者误以为它们是普通单机测试。  

整体来看，此次改动仅在 **测试层面** 引入了对超大模型的手动回归，代码本身无功能变化。但若未做好 “仅在 8‑GPU 环境下运行、及时跳过” 的防护，可能导致 CI 资源耗尽或在普通开发机器上出现异常。按上述建议补全检测、标记与文档，即可安全合并。

---

### support qwen3-next eagle3 (#14607)
**SHA**: `3ca29df` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3ca29dffc72ac8a1000d3dad988fd96ab3339b9d)

**🎯 变更类型**：功能增强（新增对 EAGLE3 方案的支持）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `qwen3_next.py` 中为模型前向传播加入 `captured_last_layer_outputs` 参数，用于在指定层捕获中间隐藏状态。  
- 新增 `set_eagle3_layers_to_capture`、`capture_aux_hidden_states`、`get_embed`/`set_embed` 等接口，并在 `ForwardBatch` 调用链中传递捕获列表。  
- `Qwen3NextForCausalLM.forward` 现在可返回 `(hidden_states, aux_hidden_states)`，并将其传递给 `LogitsProcessor`。  

**🎯 影响范围**  
- `python/sglang/srt/models/qwen3_next.py`（模型层、前向、权重加载、嵌入管理）。  
- 可能波及到所有使用 `Qwen3NextForCausalLM` 的推理/微调脚本以及测试用例。  

**💡 关注建议**  
1. **API 兼容性**：`forward` 的返回值由单一 Tensor 变为 Tuple，调用方需检查并适配。  
2. **层捕获配置**：默认捕获第 2、mid、倒数第 3 层；如需自定义，请在 PP 最后 rank 调用 `set_eagle3_layers_to_capture`。  
3. **内存/性能**：捕获中间层会额外占用显存，建议在显存宽裕的节点开启。  
4. **权重共享**：`set_embed` 对目标隐藏尺寸不同的模型会被跳过，确保 `config.target_hidden_size` 与 `config.hidden_size` 一致。  
5. **测试覆盖**：添加针对 `capture_aux_hidden_states=True` 场景的单元测试，验证 `aux_hidden_states` 与期望层对应。  

保持上述注意点，可顺利迁移到支持 EAGLE3 的新版本。

---

### Fix Diffusion Request Validation to allow missing input artifacts if the input only contains text (#16610)
**SHA**: `9c168fc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9c168fcac7f6200d9584ea9a60ee45ce3ac0cc14)

**🎯 变更类型**：功能增强 / Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 为 `VideoGenerationsRequest` 增加 `reference_url` 字段，使 API 可直接通过 URL 提供参考图像。  
2. `create_video` 接口的输入校验逻辑改为依据模型任务类型 (`task_type.requires_image_input()`) 判断是否必须提供图像；当任务不需要图像时，允许仅提交文本 `prompt`。  
3. 统一了 `server_args` 的获取位置，避免重复调用 `get_global_server_args()`；并对非 multipart 场景增加同样的图像必填校验。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/protocol.py`（请求模型）  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/video_api.py`（视频生成入口）  
- 相关的任务配置 (`pipeline_config.task_type`) 与验证函数 `requires_image_input()`  

**💡 关注建议**  
1. **任务类型安全**：`requires_image_input()` 的实现需确保对所有已注册任务返回正确布尔值，避免误判导致图像必填或误放行。  
2. **向后兼容**：旧版客户端若仍使用 `input_reference` 而不提供 `reference_url`，逻辑已保留兼容，但建议在文档中明确两者等价。  
3. **异常信息统一**：两处抛出 “input_reference or reference_url is required for image‑to‑video generation” 的路径保持一致，建议抽成常量或工具函数以防未来文字不一致。  
4. **路径创建**：`os.makedirs(..., exist_ok=True)` 在每次调用都会检查一次，若并发请求频繁可考虑在服务启动时预创建 `outputs/uploads`，减少 I/O。  
5. **单元测试**：补充 `task_type.requires_image_input()` 为 `False` 时，仅文本请求通过的测试，以及 `True` 时缺图像返回 400 的边界 case，确保新校验不引入回归。  

总体来看，此次修改提升了 API 灵活性并修复了对纯文本任务的误拦截，风险主要集中在任务类型判定的正确性和异常信息的一致性。将上述建议落地后，可进一步保证功能可靠性。

---

### [model] Support MiniCPM-V 4.5 (#9610)
**SHA**: `4d28cda` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4d28cda007b0ff34e9f936c4ad1b8ec08ed0574b)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 MiniCPM‑V 系列新增 **4.5 版** 支持。  
- 实现了 `Resampler4_5`，在原 2D 位置嵌入基础上加入 **时间维度** 的 sin‑cos 编码，并对位置信息缓存进行动态扩展。  
- 新增 `MiniCPMV4_5` 类，使用 Qwen‑3 语言模型、Idefics2 视觉编码器以及新的 Resampler。  
- 扩展 LoRA、bitsandbytes 参数映射；更新 `_SUPPORT_VERSION` 与错误提示，加入 `Qwen3Config/ForCausalLM` 的导入。  

**🎯 影响范围**  
- **核心模型代码**：`python/sglang/srt/models/minicpmv.py`（新增类、函数、映射表）。  
- **依赖**：引入 `sglang.srt.models.qwen3`，需要项目中已经提供 Qwen‑3 配置/实现。  
- **运行时**：`Resampler4_5.forward` 中加入 `temporal_ids` 逻辑，可能影响推理时的张量形状、mask 处理以及显存占用。  

**💡 关注建议**  
1. **兼容性检查**：确认旧 2.6 / 4.0 代码路径未受影响，尤其 `MiniCPMV.__init__` 对 `_SUPPORT_VERSION` 的更新不会破坏已有导入。  
2. **依赖管理**：在 `requirements.txt` 或文档中明确声明 Qwen‑3 相关包的最低版本，防止 `ImportError`。  
3. **显存与设备**：`Resampler4_5` 默认在 `torch.float16` 并强制 `cuda`，建议在非 GPU 环境下提供回退或抛出友好错误。  
4. **单元/集成测试**：加入对 `temporal_ids` 为 `[-1]`、多帧序列以及超出缓存大小的情况的覆盖，验证缓存自动扩展不会导致梯度泄漏或不一致。  
5. **量化/LoRA 支持**：检查 `bitsandbytes_stacked_params_mapping` 与新参数名是否在模型权重加载阶段正确映射，尤其在使用 `bitsandbytes` 8‑bit 量化时。  
6. **文档更新**：在模型列表和使用示例中加入 “MiniCPM‑V 4.5” 的说明，解释如何在 `MultimodalInputs` 中提供 `temporal_ids`。  

通过上述检查并补全测试，可确保新版本在多模态（包括时间序列）场景下平稳运行。

---

### Fix installation script for H200 runners (#18050)
**SHA**: `e5ac622` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e5ac6229e18662dff4bef85582d8c71dcd885fff)

**🎯 变更类型**：功能增强（CI 安装脚本的容错改进）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `scripts/ci/cuda/ci_install_deepep.sh` 与 `scripts/ci/cuda/ci_install_dependency.sh` 添加了 **apt‑get 安装失败容错**：使用 `--no‑install‑recommends`，在 `apt-get install` 失败时检查对应包是否已在系统中，若已安装则继续，否则报错退出。  
- 为多处 `apt-get update` 加上 `|| true`，避免因无关破损依赖导致脚本整体中止。  

**🎯 影响范围**  
- CI/CD 环境（GitHub Actions、内部 runner）  
- 任何直接使用上述脚本在裸机/容器中手动部署 DeepEP、GDRCopy 等依赖的用户  

**💡 关注建议**  
1. **验证场景**：在全新、干净的 Ubuntu 镜像上跑一遍完整 CI，确保即使 `apt-get update` 失败，所有必需的包仍能通过 `dpkg -l` 检测到并成功继续。  
2. **dpkg 检查的可靠性**：`dpkg -l "$pkg"` 只能判断 “已安装” 状态，若包处于 `rc`（已移除但配置残留）亦会匹配 `^ii` 失效。建议改为 `dpkg -s "$pkg"` 并检查 `Status: install ok installed`。  
3. **日志与可读性**：当前大量 `echo "Warning …"` 可能掩盖真实错误。建议在成功回退后输出 “✅ $pkg already present”。  
4. **权限**：脚本在 CI 环境默认以 `root` 运行，但若在用户态执行会因缺少 `sudo` 而失败。可以在开头统一使用 `sudo -E` 或检测 `EUID`.  
5. **依赖组的写法**：`GDRCOPY_DEPS_1="nvidia-dkms-580"` 等在 `for deps_group in "$GDRCOPY_DEPS_1" …` 中会把整个字符串当作一个元素，后面再循环 `for pkg in $deps_group` 时会正确展开。然而若未来加入多包组合，需要确保变量不被意外引用。  
6. **`apt-get update || true`**: 只在更新失败时继续，但可能导致后续包版本过旧。若关键包因镜像源失效而未更新，仍会报错。建议在 CI 前置步骤确保镜像源健康。  

总体来看，这次改动显著提升了 CI 脚本在 **“与项目无关的 broken packages”** 环境下的鲁棒性，只要在干净环境多次验证并优化 dpkg 检查逻辑，即可安全合并。

---

### Migrate 4-GPU/8-GPU workflow jobs to stage-c and add CI registry decorators (#17299)
**SHA**: `a0bae4c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a0bae4c34349d09b3ad71db71d4fc3f4f32f221e)

**🎯 变更类型**：功能增强 / CI 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将原 `unit‑test‑backend‑*/…`、`per‑commit‑*` 等 4‑GPU/8‑GPU 测试全部迁移到新的 **stage‑c** 流水线。  
- 在 `.github/workflows/pr-test.yml` 中新增/重命名大量 job，统一使用 `python3 run_suite.py --hw cuda --suite stage‑c‑*`。  
- 引入 `sglang.test.ci.ci_register`，在每个测试文件里通过 `register_cuda_ci(..., suite="stage‑c‑…")`（或 `register_amd_ci`）完成 CI 注册和估算时间。  
- 移除已迁移至 `test/registered/` 的旧测试入口（如 `per‑commit‑*`、`test_kimi_k2_models.py`），并相应更新 `test/run_suite.py` 的 suite 列表。  

**🎯 影响范围**  
- CI 流程：所有依赖旧 job 名称的 PR、调度逻辑以及手动触发的 workflow 需要使用新的 stage‑c 名称。  
- 测试注册系统：新增 `ci_register` 装饰器，所有 CUDA/AMD 测试必须在对应文件中调用，否则会被 CI 过滤掉。  
- 代码库：约 200 行 workflow 改动、数十个测试文件新增 `register_*` 调用、若干老测试被删除。  

**💡 关注建议**  
1. **CI 同步**：确认 `pr-test.yml` 中 `needs`、`if` 条件与 `ci_register` 中的 suite 名称完全一致，防止 job 被错误跳过。  
2. **本地验证**：在本地或临时分支运行 `python -m sglang.test.run_suite`，检查所有 `stage‑c` suite 能成功被发现并执行。  
3. **文档更新**：更新 CONTRIBUTING/CI 指南，说明新 suite 命名规则及 `register_cuda_ci/register_amd_ci` 的使用方法。  
4. **回退兼容**：若还有外部脚本依赖旧 job 名称（如 `unit-test-backend-4-gpu`），考虑在 workflow 中添加 alias 或兼容层，防止突发构建失败。  
5. **删除文件审查**：已删除的 `test_kimi_k2_models.py` 等文件是否仍有需求；若未来需要恢复，请确保对应的 CI 注册已补全。  

总体来看，此次改动把多卡测试统一纳入 **stage‑c**，提升了 CI 可维护性和执行时间估算，但也引入了大量名字映射，需要仔细核对，以避免因名称不匹配导致的测试遗漏或 CI 失败。

---

### Fix swa kv cache memory allocation (#18039)
**SHA**: `d396650` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d396650bd29bbf4376a926844194e91ebfcbe679)

**变更概览**  
- **模型配置**：新增 `swa_head_dim`、`swa_v_head_dim` 并在 `get_swa_num_kv_heads` 中加入属性检测，未定义时回退至普通 KV 计算。  
- **KV‑Cache 大小计算**：`ModelRunner.get_cell_size_per_token` 改为区分 Full 与 SWA 层，使用各自的 head‑dim、kv‑head 数量累计；去掉了专门针对 `MiMoV2FlashForCausalLM` 的硬编码。  
- **内存分配算法**：`align_page_size` 重新推导公式，基于 **全层 + SWA 层** 的 per‑token 内存统一求解 `full_max_total_num_tokens` 与 `swa_max_total_num_tokens`，并加入更严格的除数合法性检查。  

**影响范围**  
- `python/sglang/srt/configs/model_config.py`（模型结构元信息）  
- `python/sglang/srt/model_executor/model_runner_kv_cache_mixin.py`（KV‑Cache 大小、内存分配）  

**重点关注**  
1. **属性兼容**：新属性默认回退到普通 `head_dim`/`v_head_dim`，但仍依赖 `hf_text_config` 中可能缺失的 `swa_head_dim`、`swa_v_head_dim`、`swa_num_key_value_heads`。确认所有 hybrid‑SWA 模型的 HF 配置已同步这些字段，或在加载前做更完善的容错。  
2. **除零风险**：`denominator` 计算加入了 `full_per_token*full_layers_num + swa_full_tokens_ratio*swa_per_token*swa_layers_num`，若 `kv_size` 为 0 或层数为 0 仍可能触发断言。建议在入口处校验 `full_layers_num`、`swa_layers_num` > 0。  
3. **dtype 处理**：`torch._utils._element_size(self.kv_cache_dtype)` 依赖内部 API，若后端改动可能失效；可考虑封装为 helper 并加入 fallback。  
4. **测试覆盖**：新增 hybrid‑SWA 场景的单元测试，验证 `full_max_total_num_tokens`、`swa_max_total_num_tokens` 与预计比例一致，且在非‑SWA 模型下仍保持原有行为。  

**建议**  
- 在 `ModelConfig` 中加入文档注释，说明 `swa_*` 参数的意义与默认行为。  
- 为 `get_cell_size_per_token` 添加注释，解释 Full 与 SWA 层的分摊逻辑，便于后续维护。  
- 考虑将内存分配公式抽象为独立函数，便于单独测试和未来模型扩展（如多‑SWA 级别）。  

总体而言，此次改动消除了原先对特定模型的硬编码，提高了 hybrid‑SWA 的通用性和内存利用率，但需要通过兼容性检查和更完整的单元测试确保在所有配置组合下的稳定运行。

---

### [metric] Optional extra metric labels (#18049)
**SHA**: `1189259` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/11892599f164d1c2da9c489e5f0b922c2ec6a3c4)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 SGLang 的指标体系引入 `extra_metric_labels` 参数，支持在所有调度器、分词器以及缓存相关的 `MetricsCollector` 中注入用户自定义标签；在 CLI 中新增 `--extra-metric-labels` 选项，可通过 JSON 直接指定键值对。  

**🎯 影响范围**  
- `sglang/srt/server_args.py`：新增字段、CLI 参数解析。  
- `sglang/srt/managers/scheduler_metrics_mixin.py`、`tokenizer_manager.py`：在创建对应的 `MetricsCollector` 时合并自定义标签。  
- `sglang/srt/mem_cache/base_prefix_cache.py`、`hiradix_cache.py`：缓存类初始化和存储度量收集器也会附加这些标签。  
- 相关的 `MetricsCollector`（`SchedulerMetricsCollector`, `TokenizerMetricsCollector`, `RadixCacheMetricsCollector`, `StorageMetricsCollector`）间接受影响。  

**💡 关注建议**  
1. **输入校验**：`--extra-metric-labels` 通过 `json.loads` 解析，建议在 `ServerArgs` 中加一次类型检查，防止非 `dict` 或键冲突导致运行时异常。  
2. **向后兼容**：默认值仍为 `None`，确保已有部署不受影响；但若用户误传空字符串等，应安全回退为 `None`。  
3. **文档与示例**：在 README/CLI 文档中补充使用示例，说明标签的可见范围（跨进程/节点均会传播）。  
4. **测试覆盖**：添加单元测试验证：① 正常传入多标签后 `metrics_collector.labels` 包含全部键；② `extra_metric_labels` 为 `None` 时不改变原有标签集合；③ 底层存储度量在 `HiCache` 启用时正确合并标签。  
5. **监控平台适配**：若使用 Prometheus/OTel，确保自定义标签不会超过平台的标签数量限制，必要时在代码中做裁剪或警告。  

总体而言，此次改动提升了指标的可观测性与灵活性，影响主要集中在指标收集器的初始化路径，风险可通过上述校验和测试加以控制。

---

### Fix rerun stage command with merged commit history (#17960)
**SHA**: `e884b17` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e884b176325037a65680d21f19f5cb0e026c808a)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次 PR 修复了在使用 `/rerun‑stage`（`workflow_dispatch` 并指定 `target_stage`）时，文件变更检测失效的问题。原先依赖 `dorny/paths‑filter` 的实现会在合并提交（merge commit）上只能比较“上一次提交”，导致无法正确判断 PR 实际修改的文件。改动新增了基于 GitHub API 的变更检测（`filter‑api`），并在后续步骤统一使用 API 与旧过滤器的结果，以在普通 PR 与 `/rerun‑stage` 两种场景下都能得到准确的 `main_package、sgl_kernel、jit_kernel、multimodal_gen` 标记。同时加入了对 `target_stage` 与 `sgl_kernel` 冲突的显式检查，防止在需要自定义 kernel 的 PR 中误用 `/rerun‑stage`。

**🎯 影响范围**  
- `.github/workflows/pr-test.yml`（CI 工作流）  
- 受影响的构建/测试步骤：文件变更过滤、B200 runner 选择、并行度设置、CI 摘要展示  
- 与 `sgl‑kernel` 相关的 CI 路径及 `target_stage` 逻辑  

**💡 关注建议**  
1. **保持 API Token 有效**：`filter‑api` 依赖 `github.token`，确保仓库的 token 权限未被限制。  
2. **注意 `target_stage` 限制**：如果 PR 包含 `sgl‑kernel/` 改动，CI 会直接报错并提示使用 `/tag-and-rerun-ci` 或 `/rerun-ci`，请开发者遵循该流程。  
3. **验证兼容性**：在本地或临时分支运行一次 `workflow_dispatch`（含 `target_stage`）确认变更检测结果与预期一致，防止误判导致不必要的全量测试。  
4. **监控运行日志**：CI Summary 中已新增 “detection_method” 与 “sgl_kernel (raw/used)” 列，建议在后续审查时关注这些字段，快速定位检测逻辑是否按期望工作。  

总体而言，此次改动提升了 `/rerun‑stage` 的可靠性，避免了因合并提交导致的误判，同时通过明确的错误提示防止用户在不适用的场景下使用该指令。后续如需添加新的变更过滤规则，只需同步更新 API 检测的 `grep` 条件和 `paths‑filter` 的 filter 列表即可。

---

### [Perf] Add Flashinfer DeepGEMM SM90 for SwapAB Optimization (#15514)
**SHA**: `398d13a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/398d13a1897d5c883e8aceb5531a656af67f6023)

**🎯 变更类型**：功能增强（新增 FlashInfer DeepGEMM SM90 后端）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 在 `Fp8GemmRunnerBackend` 中加入 `FLASHINFER_DEEPGEMM`，并提供对应的判定方法。  
2. 检测 Hopper（SM90）并在可用时导入 `flashinfer.gemm.fp8_blockscale_gemm_sm90`。  
3. 为该后端实现 `flashinfer_deepgemm_w8a8_block_fp8_linear_with_fallback`，在满足形状/ dtype 条件时走 DeepGEMM‑swapAB 路径，否则回退到 Triton 实现。  
4. CLI 参数列表及帮助信息同步新增 `flashinfer_deepgemm` 选项。  
5. 为 SM90 环境补充单元测试。

**🎯 影响范围**  
- `python/sglang/srt/layers/quantization/fp8_utils.py`（FP8 GEMM 调度核心）  
- `python/sglang/srt/server_args.py`（命令行参数解析）  
- 单元测试 `test/srt/test_fp8_blockwise_gemm.py`  

**💡 关注建议**  

1. **后端判定统一**：`is_flashinfer` 仍仅检查 `FLASHINFER_TRTLLM`，若后续有其他 FlashInfer 变体（如 DeepGEMM），建议改为 `return self.name.startswith("FLASHINFER")`，防止遗漏。  
2. **导入防护**：`fp8_blockscale_gemm_sm90` 在 `is_sm90_supported()` 为真且 `flashinfer` 可用时才导入，已做好保护；但若用户在非 SM90 环境却手动指定 `--fp8-gemm-backend=flashinfer_deepgemm`，会抛出 RuntimeError——这与现有行为一致，建议在 CLI 参数校验阶段提前给出更友好的提示。  
3. **fallback 条件**：目前仅检查 `N % 64 == 0` 与 `K % 128 == 0`、dtype 为 `bfloat16`。如果未来出现其他合法形状（例如 N 可被 32 整除），需同步更新判断逻辑。  
4. **权重量化处理**：在回退到 Triton 时使用 `_unpack_ue8m0_scale_for_triton`，但该分支仍假设 `input_scale is None`。若后续加入 BF16‑to‑FP8 输入量化，需要重新审视这段断言。  
5. **文档与示例**：README/CHANGELOG 中应补充 “flashinfer_deepgemm” 适用场景（Hopper、低‑M 解码），并给出 `--fp8-gemm-backend=flashinfer_deepgemm` 的使用示例。  
6. **CI/测试覆盖**：新增测试仅在 SM90 机器上执行，CI 环境若缺少此硬件会跳过。建议在 CI 中加入 “GPU‑sm90” 标记或在公共 CI 上保持该测试可被忽略，以免误报。  

总体来看，此次改动为 Hopper GPU 引入了更高效的 swapAB 路径，提升低批量解码性能。关注上述细节可进一步提升可维护性和用户体验。

---

#### 🟢 低重要度变更 (22)

### [Auto Sync] Update test_deterministic.py (20260131) (#18034)
**SHA**: `1acae30` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1acae308069104b40821ea52c5c9eda1630c5663)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_deterministic.py` 中新增 `DIVERGENCE_EPS` 常量，仅在 logprob 差异超过该阈值时计算 KL、最大值和平均差异，避免因极小差异导致误判，打印更精确的发散信息。

---

### [Auto Sync] Update elementwise.py (20260131) (#18033)
**SHA**: `fb60966` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fb609669ca1288f15fcbf9dcd563f61ea9521811)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `elementwise.py` 中将 `tl.program_id` 的返回值显式转为 `int64`，修正类型不匹配，提升 Triton kernel 的兼容性。

---

### Optimize custom-all-reduce (#17674)
**SHA**: `afebb7a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/afebb7ab7893869ec8cf7ed6dd6f06981bc8ccef)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在文档中新增 `SGLANG_CUSTOM_ALLREDUCE_ALGO` 环境变量说明；在 `custom_all_reduce.cuh` 中读取该变量并支持强制使用单阶段或双阶段 All-Reduce，实现参数校验并相应调整调度逻辑。

---

### [NPU] support the Enable return routed experts (#17025)
**SHA**: `0fe2825` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0fe282543fbb36eaa3066bdf0555bc882d3ce0bc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 NPU Top‑K 实现中新增 `layer_id` 参数，并在选专家后调用 `get_global_experts_capturer().capture`，实现对路由专家的捕获；相应地在 `topk.py` 的调用处传递 `layer_id`。

---

### Reset evict swa status when retract (#18059)
**SHA**: `d9050b4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d9050b4a9c0c6d7035e4b5c8fcb67da7b161a01c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `reset_for_retract` 中新增 `swa_evicted_seqlen`、`extend_batch_idx`、`decode_batch_idx` 三个状态字段的重置，确保在撤回时能够恢复 SWA（Sliding Window Attention）相关的批次索引和序列长度。

---

### [CI] Fix AMD CI by inlining dummy_grok config (#18044)
**SHA**: `47592a2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/47592a23c74b1cc1974a9f0166396e811d0a9971)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 AMD CI 脚本中内联 dummy_grok 配置，避免访问 Azure Blob 导致的权限问题；同时在 `sglang/multimodal_gen/__init__.py` 添加注释以触发多模态 CI 测试。

---

### [Feature] Support file:// URL format for multimodal inputs (#14490)
**SHA**: `9bb1260` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9bb1260558315c03a6b772e82a6bb146efd7ca6f)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `python/sglang/srt/utils/common.py` 中新增对 `file://` URL 的解析与解码，分别在音频、图像、视频以及获取二进制数据的函数中加入 `unquote(urlparse(...).path)` 处理，使本地文件可通过 `file://` 方式读取。

---

### Fix CUDA 12 dependency when importing Mooncake in official CUDA 13.x image (#17540)
**SHA**: `71babde` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/71babdef510ef3d95ea829ec61e5f7c3f9ae7232)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Dockerfile 中新增 `MOONCAKE_VERSION` 与编译参数，依据 CUDA 主版本号判断：CUDA ≥ 13 时从源码编译 Mooncake，CUDA < 13 时使用 pip 安装，并移除原有的直接 pip 安装行。

---

### Optimizing all_reduce in RMSNormTP in minimax_m2 (#16483)
**SHA**: `486c7de` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/486c7de39f5c6ec8d651c5c0c662e940fccb63e0)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `rms_sumsq_serial` 中新增对齐填充 `B_padded`，使用自定义跨设备 reduce 以提升 RMSNormTP 的 all‑reduce 性能；同时移除 `forward_qk` 的 `torch.compile` 装饰，简化编译路径。

---

### [Bugfix] fix the display error (inconsistent context) (#17699)
**SHA**: `2c036f1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2c036f1eb120b3e175380ef15feef5a49aaea19b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：统一错误提示文本，分别将 `in_rate_thr` → `img_rate_thr` 的断言信息以及 `sm_group_num` 的阈值异常信息改为更准确的表述，消除显示不一致的 bug。

---

### Disable test_mla_int8_deepseek_v3.py temporarily (#18057)
**SHA**: `9518048` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/95180484e9a63ff4ceffc3454126dfb16f618b89)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_mla_int8_deepseek_v3.py` 中暂时禁用 DeepSeek‑V3 INT8 量化测试，说明原因是 HF token 无私有仓库访问权限。

---

### [BugFix] fix gpt-oss accuracy issue when enabling piecewise cuda graph (#18013)
**SHA**: `38d275a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/38d275a9fd7c42353839b88ce83359444fb3cb6c)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 decode 模式且开启 `enable_piecewise_cuda_graph` 时，新增条件提前返回，防止对非 chunk cache 的树缓存错误执行逐块 CUDA 图的 evict 操作，从而修复了 GPT‑OSS 的精度问题。

---

### Set torch url index in pyproject.toml (#16802)
**SHA**: `c7d53fa` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c7d53fa26a6d86d749e35596192456f0bab2a8b1)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `pyproject.toml` 为 `torch` 添加 CUDA cu129/cu130 的额外索引，并相应更新文档中的安装说明。

---

### [BugFix] Fix draft model specified config file (#17815)
**SHA**: `429ef98` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/429ef988bcdfa95e70a1c8f1b12b50cd876a39a5)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `ModelConfig.from_server_args` 中，根据是否为草稿模型选择对应的解密配置文件，修复草稿模型读取错误的配置文件的问题。

---

### Skipped warning on sm100 (#18000)
**SHA**: `2b25154` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2b2515423a6482bc8e0f2e9e1a7f4c33146da1bd)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `model_config.py` 中新增 `is_sm100_supported` 导入并调整量化警告逻辑，避免在 SM100 GPU 上对 MXFP4 量化输出不必要的警告。

---

### Improve error output in tnightly tets (#18053)
**SHA**: `d443d2d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d443d2d2ae1f79be8ef84aa80fba4776b0f6f902)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：完善 `run_combined_tests` 的错误输出，新增对性能、准确率、工具调用等子测试的失败检测与汇总，提供更详细的模型失败信息和错误描述。

---

### Fix: Remove duplicate assignment for use_w4afp8 (#17858)
**SHA**: `9951a1a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9951a1ae074d172cb66558b54df05a8160dae05d)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `python/sglang/srt/layers/moe/ep_moe/layer.py` 中删除了重复的 `self.use_w4afp8 = False` 赋值，避免不必要的代码冗余。

---

### [Performance] Optimize Mllama LayerNorm -> Upd (#9725)
**SHA**: `c2ab371` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c2ab3713e95a8d2afc2222f8684f722648a185b9)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除自实现的 `MllamaTextRMSNorm`，改用 `RMSNorm` 并在 Q/K 归一化时加入 reshape，以提升 LayerNorm 的数值精度和执行效率。

---

### Update python/sglang/README.md (#18045)
**SHA**: `6aaea09` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6aaea09b3d7e2e03c76e65295a2a665eaf610707)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `python/sglang/README.md` 中新增 `multimodal_gen` 项目说明，介绍其为加速图像/视频生成的推理框架。

---

### [CPU] toml file update (#17861)
**SHA**: `97593c9` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/97593c9f41e358739ba3515ed181825ca5091d3e)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：更新 CPU 镜像 Dockerfile、文档及 pyproject.toml，统一依赖版本并移除手动 `torch` 安装；修正 kernel 包名。

---

### [Tiny] Fix grammar in shared experts fusion log messages (#18043)
**SHA**: `9ac4dca` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9ac4dcada4bd77a18e6f3d3efcd1906f705b235d)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修正了 `deepseek_v2.py` 和 `glm4_moe_lite.py` 中关于共享专家融合的日志信息的语法错误，使表述更准确、符合英文习惯。

---

### [MUSA] Update 3rd party dir to build/_deps (#18035)
**SHA**: `46095f0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/46095f0551ec99d579402a65ad7a7c6b595f6a37)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 MUSA 第三方库路径改为 `build/_deps`，并支持通过环境变量 `SGLANG_MUSA_THIRD_PARTY_DIR`/`SGLANG_MUSA_SKIP_THIRD_PARTY` 自定义；同步更新 `.gitignore` 和目录创建逻辑。

---

