# 每日更新报告（2026-01-30）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-30 22:04:54 | Xiaoyu Zhang | [Diffusion] Fix lora default lora_scale bug (#17982) |
| 2026-01-30 20:44:25 | Zheng Li | [BUGFIX] Fix dp size > 1 for qwen3 vl model (#17624) |
| 2026-01-30 20:01:14 | Changhun Lee | [Model] Add K-EXAONE model support (#16294) |
| 2026-01-30 18:03:55 | amote-i | update ascend docs (#17987) |
| 2026-01-30 17:03:30 | Fan Yin | fix: fix SHM pointer re-serialization in DP attention (#17930) |
| 2026-01-30 16:56:53 | Ke Bao | Add cuda graph status to prefill log (#17836) |
| 2026-01-30 16:02:05 | Haotong Zhang | SGLang Tracing: Improve root span attributes (#17008) |
| 2026-01-30 15:55:25 | Kangyan-Zhou | Increase install dependency for gb200 (#17977) |
| 2026-01-30 15:26:54 | jianzhao-xu | adapt MODELSCOPE download (#17922) |
| 2026-01-30 15:19:42 | McZyWu | [NPU] enhance accuracy for model kimi-vl-a3b-instruct (#17480) |
| 2026-01-30 15:05:42 | cswuyg | fix(benchmark): add missing args for speculative decoding benchmark (#17974) |
| 2026-01-30 14:30:13 | jianan-gu | [CPU][INT4] Add INT4 kernels for CPU  (#8226) |
| 2026-01-30 14:07:05 | Ma Mingfei | [CPU] optimize flash_attn_varlen_func (#15708) |
| 2026-01-30 14:03:58 | jianan-gu | [CPU] Optimize Qwen3-next model on CPU (#12525) |
| 2026-01-30 13:53:53 | Polisetty V R K Jyothendra Varma | [Intel GPU] fix import error to run DeepSeek-V2-Lite model with BF16 on XPU (#10858) |
| 2026-01-30 13:28:17 | gaopengff | Fix prefill latency performance drop of bench serving (#14592) |
| 2026-01-30 13:21:38 | Polisetty V R K Jyothendra Varma | [Intel GPU] fix device in DeepseekScalingRotaryEmbedding to run DeepSeek-V2-Lite BF16 on XPU (#10021) |
| 2026-01-30 13:11:47 | LHXuuu | Fix the scenario where eh_proj is quantized in the bailing moe nextn weights (#17808) |
| 2026-01-30 13:11:11 | Kangyan-Zhou | [Fix] Remove unused Type import in gpt_j.py (#17975) |
| 2026-01-30 13:01:57 | Koushik Dutta | [Fix] add block size logic for sm120 smem size (#14311) |
| 2026-01-30 13:00:04 | Wenchen Lo | GPTJForCausalLM Support (#7839) |
| 2026-01-30 11:09:55 | b8zhong | add weightless qk norm to RMSNorm interface for Llama 4 (#12813) |
| 2026-01-30 09:49:51 | baonudesifeizhai | model: support DeepSeek-OCR-2 (#17897) |
| 2026-01-30 09:31:55 | Kangyan-Zhou | Add concurrency tracking to runner utilization report (#17963) |
| 2026-01-30 09:19:57 | YC Tseng | [AMD] fix pip sglang version (#17950) |
| 2026-01-30 08:52:29 | StonyPort | feat: add forward timeout (#17831) |
| 2026-01-30 07:14:43 | Cheng Wan | Fix logprob_start_len handling for prefill-only requests (#17395) |
| 2026-01-30 05:00:42 | Kangyan-Zhou | Fix ci weight validation logic to check the safetensor completeness (#17917) |
| 2026-01-30 04:54:30 | Alison Shao | Fix /tag-and-rerun-ci to do full rerun when PR has sgl-kernel changes (#17729) |
| 2026-01-30 04:46:35 | Cheng Wan | Fix capture_sizes range for pcg (#17956) |
| 2026-01-30 03:55:13 | EduardDurech | Fix `torch.__version__` for PEP440 (#15682) |
| 2026-01-30 01:58:54 | R0CKSTAR | [MUSA] Add labeler config (#17923) |
| 2026-01-30 01:50:54 | Hudson Xing | Add tool call tests for DeepSeek V3.2 in nightly CI (#17951) |
| 2026-01-30 01:35:12 | Ratish P | [diffusion]: add dummy device attribute to fix AttributeError (#17949) |

### 📊 统计摘要
> 本日共 34 个提交 | 🔴高 5 | 🟡中 10 | 🟢低 19
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (5)](#-🔴-高重要度变更-5)
    - [fix: fix SHM pointer re-serialization in DP attention (#1...](#8ce9609)
    - [[CPU][INT4] Add INT4 kernels for CPU  (#8226)](#c35aa02)
    - [[CPU] optimize flash_attn_varlen_func (#15708)](#88f7759)
    - [model: support DeepSeek-OCR-2 (#17897)](#84ab611)
    - [feat: add forward timeout (#17831)](#2b3408f)
  - [🟡 中重要度变更 (10)](#-🟡-中重要度变更-10)
    - [[BUGFIX] Fix dp size > 1 for qwen3 vl model (#17624)](#0c5a81a)
    - [[Model] Add K-EXAONE model support (#16294)](#c04efe0)
    - [Add cuda graph status to prefill log (#17836)](#77a27e7)
    - [SGLang Tracing: Improve root span attributes (#17008)](#c8dc543)
    - [fix(benchmark): add missing args for speculative decoding...](#33c053c)
    - [[CPU] Optimize Qwen3-next model on CPU (#12525)](#336dc45)
    - [GPTJForCausalLM Support (#7839)](#046b29b)
    - [Add concurrency tracking to runner utilization report (#1...](#2cd2c31)
    - [Fix `torch.__version__` for PEP440 (#15682)](#1b6798a)
    - [Add tool call tests for DeepSeek V3.2 in nightly CI (#17951)](#d417c68)
  - [🟢 低重要度变更 (19)](#-🟢-低重要度变更-19)
    - [[Diffusion] Fix lora default lora_scale bug (#17982)](#abf13cc)
    - [update ascend docs (#17987)](#9c2a468)
    - [Increase install dependency for gb200 (#17977)](#a16aba8)
    - [adapt MODELSCOPE download (#17922)](#96584ab)
    - [[NPU] enhance accuracy for model kimi-vl-a3b-instruct (#1...](#70db339)
    - [[Intel GPU] fix import error to run DeepSeek-V2-Lite mode...](#71e4d3b)
    - [Fix prefill latency performance drop of bench serving (#1...](#7541da1)
    - [[Intel GPU] fix device in DeepseekScalingRotaryEmbedding ...](#858dc80)
    - [Fix the scenario where eh_proj is quantized in the bailin...](#0e4d9dd)
    - [[Fix] Remove unused Type import in gpt_j.py (#17975)](#606ff09)
    - [[Fix] add block size logic for sm120 smem size (#14311)](#632c7af)
    - [add weightless qk norm to RMSNorm interface for Llama 4 (...](#22df62d)
    - [[AMD] fix pip sglang version (#17950)](#b7b1ba3)
    - [Fix logprob_start_len handling for prefill-only requests ...](#6a6b363)
    - [Fix ci weight validation logic to check the safetensor co...](#c3bf53c)
    - [Fix /tag-and-rerun-ci to do full rerun when PR has sgl-ke...](#1f75c2a)
    - [Fix capture_sizes range for pcg (#17956)](#a416af4)
    - [[MUSA] Add labeler config (#17923)](#4f2b73b)
    - [[diffusion]: add dummy device attribute to fix AttributeE...](#88fb927)
#### 🔴 高重要度变更 (5)

### fix: fix SHM pointer re-serialization in DP attention (#17930)
**SHA**: `8ce9609` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8ce9609fa2083a5ddd4fbb2e22d3d710fc7e105b)

**🎯 变更类型**：Bug修复 / 功能增强  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 修复了 `ShmPointerMMData` 在对象序列化(`__getstate__`)时未能重新创建共享内存导致的崩溃问题；在缺失 `shm` 时会基于当前张量重新分配共享内存并写入数据。  
2. 同步更新 `__setstate__` 使用持久化的 `shape` 与 `dtype`，并确保反序列化后得到正确的 `torch.Tensor`。  
3. 新增分布式 DP‑Attention 场景下的 VLM（视觉语言模型）端到端测试，验证 `--enable-dp-attention` 与 `--tp 2` 组合在图像输入下的生成能力。  

**🎯 影响范围**  
- `python/sglang/srt/managers/mm_utils.py` 中的 `ShmPointerMMData`（共享内存指针包装类）。  
- 与分布式推理、DP‑Attention 相关的序列化/反序列化路径。  
- 测试套件 `test/registered/distributed/test_dp_attention.py`（新增 VLM DP‑Attention 测试）。  

**🔍 技术洞察**  

- **架构影响**  
  - **共享内存管理层**：原实现假设在序列化前一定已有 `self.shm`，在某些路径（如跨进程复制后）会缺失导致 `AttributeError`。本次改动在缺失时动态创建共享内存并写入 CPU‑tensor 数据，使得 `ShmPointerMMData` 在任意进程间均可安全序列化/反序列化，提升了分布式算子（DP‑Attention）对共享内存的鲁棒性。  
  - **分布式调度**：新增的 VLM 测试检验了 **DP+TP 双并行** 场景下的图像 token 处理流程，确保模型路径、ChatTemplate 与图像 token 的协同工作，这对未来扩展多模态模型的分布式部署具有示例价值。  

- **性能影响**  
  - **额外拷贝**：在缺失 `shm` 时会把 GPU 张量转到 CPU、转成 `np.ndarray` 再写入共享内存，这会产生一次 CPU‑GPU 拷贝及一次内存拷贝。该路径仅在 **首次序列化**（如跨进程首次传递）触发，后续使用共享内存直接映射，整体性能影响可忽略。  
  - **资源利用**：通过 `self.shm.close()` 在写入后立即释放文件描述符，避免文件描述符泄漏，间接提升长时间运行的服务稳定性。  

- **安全考虑**  
  - **共享内存大小校验**：创建共享内存时使用 `nbytes = cpu_tensor.numel() * cpu_tensor.element_size()`，确保分配大小与实际数据匹配，防止溢出。  
  - **异常处理**：若在没有可用张量的情况下尝试序列化，会抛出明确的 `RuntimeError`，避免产生未定义的共享内存对象。  
  - **关闭句柄**：`finally` 中保证 `self.shm.close()` 被调用，防止因异常导致句柄残留，降低潜在的 DoS 风险。  

**⚠️ 潜在风险**  
1. **资源泄漏**：虽然已在 `finally` 中关闭共享内存句柄，但若 `cpu_tensor` 极大（数 GB）且频繁触发重新创建，可能导致短时间内大量共享内存块占用系统资源。  
2. **并发竞争**：在极端并发场景下，多进程可能几乎同时尝试创建同名共享内存（虽然默认会生成唯一名称），若手动指定 `shm_name` 可能会产生冲突。  
3. **测试依赖外部模型**：新增的 VLM 测试依赖外部模型 `moonshotai/Kimi-VL-A3B-Instruct` 与网络图片。CI 环境若无法访问这些资源，将导致测试失败，影响回归速度。  

**💡 关注建议**  
- **监控共享内存使用**：在生产环境中加入监控（如 `shm` 包的使用量、文件描述符数）以捕获异常增长。  
- **限制重新创建频率**：可以在 `ShmPointerMMData` 内部记录一次成功的共享内存创建，后续序列化直接复用，避免在同一对象多次跨进程传递时重复拷贝。  
- **CI 稳定性**：为 VLM 测试提供本地缓存的模型权重和图片（或使用 mock），确保 CI 不依赖外部网络。  
- **文档更新**：在项目文档中补充 “DP‑Attention 与共享内存序列化” 的使用约束，提示用户在自定义序列化路径（如自定义 `__reduce__`）时保持 `shm` 可用。  

---  
此更改提升了分布式推理中共享内存的可靠性，修复了关键的崩溃路径，并通过实际 VLM 场景的测试验证了功能的完整性。适当的资源监控与 CI 稳定性措施将帮助团队在后续迭代中保持高可用性。

---

### [CPU][INT4] Add INT4 kernels for CPU  (#8226)
**SHA**: `c35aa02` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c35aa0238c730e68810b422bcc91d2a0790538ea)

**🎯 变更类型**：功能增强 / 架构变更 / 性能优化  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. **CPU 端全新 INT4 量化路径**：在 `sglang` 中加入 `CPUQuantMethod::INT4_W4A8`，并实现从权重量化、打包到 GEMM 再到 MoE 完整的流水线。  
2. **统一量化 API**：将原来的 `use_int8_w8a8 / use_fp8_w8a16` 布尔开关全部替换为 `CPUQuantMethod` 枚举，实现 `UNQUANT、INT8_W8A8、FP8_W8A16、INT4_W4A8` 四种统一调度。  
3. **新增 C++ 核心实现**：  
   - `gemm.h`/`gemm_int4.cpp`：INT4‑VNNI 打包、去量化、BRGEMM‑加速、tinygemm‑fallback。  
   - `moe_int4.cpp`：INT4 版 MoE 核心 `fused_experts_int4_w4a8_kernel_impl`。  
   - `convert_int4_weight_packed_with_compensation` 与 `autoawq_to_int4pack`：权重预打包与尺度/零点打包。  
   - `int4_scaled_mm_cpu`：对外提供的 INT4 GEMM 接口。  
4. **Python 侧适配**：在 `amx_utils.py`、各量化层、模型实现、MoE 代码中引入 `CPUQuantMethod`，并把新的 `moe_comp_method` 参数向下传递。  
5. **PyTorch C++ Extension**：暴露 `int4_scaled_mm_cpu` 与 `convert_weight_packed_scale_zp`。  
6. **测试覆盖**：新增 INT4‑AWQ GEMM 与 MoE 单元测试，验证与参考实现的数值一致性。  

---

## 🔍 技术洞察

| 维度 | 影响 |
|------|------|
| **架构影响** | - **枚举化量化方式**：原有的布尔开关被统一为 `CPUQuantMethod`，代码路径更清晰，后续若再加入新量化（如 `INT2`）只需在枚举和分支里扩展。<br>- **新增模块**：`sgl-kernel/csrc/cpu/gemm_int4.cpp`、`sgl-kernel/csrc/cpu/moe_int4.cpp` 以及对应的头文件，形成独立的 INT4 计算子模块。<br>- **接口向后兼容**：旧的 `int8_scaled_mm_with_quant` / `fp8_scaled_mm` 仍保留，仅在内部统一使用枚举；对外 API 兼容已有调用。 |
| **性能影响** | - **内存占用**：INT4 权重量化从 8‑bit 降至 4‑bit，理论可减少 50% 体积，显著降低缓存压力。<br>- **算子加速**：使用 AVX‑512 VNNI‑int8/​int4 以及 **BRGEMM**（`cpublas::brgemm`），在支持的 CPU（≥ Ice Lake）上可以获得 **2–3×** 的 GEMM 加速（与 INT8 近似，且在同等算子上更快）。<br>- **MoE 串行‑并行**：`fused_experts_int4_w4a8_kernel_impl` 复用了 `tinygemm_kernel` 的分块 → BRGEMM 自动切换逻辑，使得在小 batch（M≤48）仍保持单线程高效。<br>- **缓存友好**：权重打包后的布局 `[Nc, Kc, block_n * _block_k/2 + block_n*sizeof(int32)]` 与原 INT8‑W8A8‑VNNI 布局保持一致，避免额外的转置开销。 |
| **安全考虑** | - **CPU特性检测**：代码在 `gemm.h` 中直接使用 AVX‑512 指令 (`_mm512_*`)；若在不支持 AVX‑512 的机器上加载该库会触发 **非法指令**（SIGILL）。目前没有 runtime guard（如 `cpuinfo::IsSupported(AVX512_VNNI)`），因此部署前必须确保硬件兼容或在 CI 中加入特性检测。<br>- **指针算术**：大量手动偏移（如 `packed_w1 + expert_id * stride_e`）在整数乘法溢出时可能导致越界读取。已使用 `int64_t` 统一大小，但仍建议在 Release‑Debug 对比检查。<br>- **线程局部缓冲**：`dqB_tmp`、`C_tmp` 等通过 `at::get_thread_num()` 分配，若线程数在运行时被重新配置（如 `torch.set_num_threads`）可能产生竞争；建议在函数入口固定 `omp_set_num_threads`。 |
| **可维护性** | - **代码重复性**：量化层（fp8/int8/unquant）现在只有一套分支逻辑，改动后不必同步多处布尔标记。<br>- **模板/宏**：大量 `AT_DISPATCH_…` 与 `ATEN_OPS` 包装，保持与 PyTorch 代码风格一致。<br>- **测试覆盖率**：新增的 150+ 行单元测试覆盖了 **权重打包 → GEMM → MoE** 全链路，大幅提升回归安全性。 |

---

## ⚠️ 潜在风险

| 风险 | 说明 | 缓解措施 |
|------|------|----------|
| **硬件兼容性** | 仅在支持 AVX‑512 VNNI 的 CPU（Xeon Scalable、IceLake+）上可运行；其它 CPU 会因非法指令崩溃。 | 在 `torch.ops.sgl_kernel` 加载前检测 `cpuinfo::IsSupported(cpuinfo::AVX512_VNNI)`，若不支持自动回退到 INT8/FP8 或抛出友好错误。 |
| **未实现的 fallback** | `tinygemm_kernel` 在 `#else` 分支（非 AVX‑512）直接 `TORCH_CHECK(false)`. | 实现纯 C++ scalar 路径或使用 OpenBLAS/OneDNN 作为后备。 |
| **打包/解包不对齐** | `convert_int4_weight_packed_with_compensation` 假设 `K`、`N` 均能被 `block_n`、`_block_k` 整除。若模型维度不满足会触发越界或错误结果。 | 在入口加入 `TORCH_CHECK(K % block_k == 0 && N % block_n == 0, …)`，并在 `test` 中加入非对齐边界测试。 |
| **多线程资源竞争** | `c_temp_buffer` 大小取决于 `at::get_num_threads()`；若外部调度更改线程池大小（例如 `torch.set_num_threads`）而 buffer 已分配，可能出现内存不足或竞争。 | 在函数入口使用 `int num_threads = std::min(at::get_num_threads(), max_supported_threads);` 并在 `buffer2` 分配前重新计算。 |
| **量化误差回归

---

### [CPU] optimize flash_attn_varlen_func (#15708)
**SHA**: `88f7759` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/88f77594023d6a0c9833c450eddc73b4a146541c)

**🎯 变更类型**：功能增强 / 性能优化 / 重构 / 架构变更  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
1. 在 `sgl‑kernel/csrc/cpu` 中实现 **Flash‑Attention（变长）** 的完整 CPU 版 kernel（`flash_attn.cpp/h`），并在 Torch 扩展层公开 `flash_attn_varlen_func`。  
2. 将原有 **extend_attention** 逻辑拆解并加入 **BLOCK_M / BLOCK_N 动态调度**（根据 `max_len_extend` 选取 32‑128‑256‑512‑768 的块），显著提升大块 extend 的吞吐。  
3. 引入 **向量化 Softmax / exp 实现**（`_mm512_fexp_u20_ps`）以及 **BFloat16/Float16 Pack‑VNNI** 的通用实现（`vec_pack.h`），并使用 AVX‑512/AMX 加速 BRGEMM、转置与 pack。  
4. 为 **可变序列长度** 添加索引/前缀‑后缀结构、缓冲区自动伸缩 (`resize_buffer`、`resize_indices`) 并实现 `has_varlen_sequences` 判别。  
5. 大量重构：统一 `sm_scale` 命名、移除 `logit_cap`、简化 `fill_stub / copy_stub`，并在 `extend_attention` 中使用新实现的 `flash_attn_softmax`。  
6. 新增单元测试 `test_flash_attn.py`、在 CI 中加入执行，增加 `extend_attention` 的极端长度测试。

---

## 🔍 技术洞察  

| 维度 | 影响说明 |
|------|----------|
| **架构影响** | - **新增 CPU Flash‑Attention 路径**：在原有的 `decode` / `extend` 流程之外，提供独立的 `flash_attn_varlen_func` API。<br>- 使用 **VNNI（bfloat16/float16）** 数据布局以适配 AVX‑512/AMX，显著提升 `K·V` 的读写带宽。<br>- 通过 **块划分 (BLOCK_M, BLOCK_N)**，每个线程持有私有 `s_i / v_prime / Btmp`，避免跨线程共享导致的竞争和 false sharing。<br>- `flash_attn_softmax` 被抽象为模板，可在不同 `scalar_t`（float、bfloat16）上自动特化；bfloat16 采用专门的 SIMD‑exp 实现，避免多次转型。 |
| **性能影响** | - **extend_attention**：原始固定 `BLOCK_M = 32 / BLOCK_N = 32` 改为根据 `max_len_extend` 动态选取 4 套块大小（32‑128‑256‑512‑768），在 `max_len_extend > 1024` 时使用 512×768，降低循环次数、提升 L2/L3 利用率。<br>- **Flash‑Attention**：<br>  • 使用 **BRGEMM（INTEL native cpublas)** 进行 Q·K、sΔ·V 两次矩阵乘，避免手写 inner‑product loops，天然利用 AVX‑512/AMX 的矩阵乘子块。<br>  • 软化 `exp` 采用 **近似 20‑bit 精度函数** (`_mm512_fexp_u20_ps`) 替换 `std::exp`，在 AVX‑512 上可并行 16‑float 计算，提升 2‑3×。<br>  • `flash_attn_softmax` 合并了 **scale、max、exp、累加** 四步，单遍遍历 `s_i`，降低访存次数。<br>  • 只在 `causal` 场景下写入 mask，避免不必要的 memset。<br>- **整体吞吐**：在内部 benchmark（参考作者提交的 PR 说明），对 8‑head, 128‑dim, seq=4k 的情况，CPU（AVX‑512）相较于旧实现提升约 **1.8‑2.3×**；对 9000 长度的 `extend_attention`（新块 512×768）提升约 **3.5×**（受缓存命中提升主导）。 |
| **内存/带宽** | - `fill_stub` 与 `copy_stub` 使用 **Vectorized** 存储，填充/拷贝一次完成 256‑bit（bfloat16）或 512‑bit（float）写入，提升 L1/L2 利用率。<br>- `pack_vnni` 系列函数统一使用 **mask load/store**，在 `N`/`K` 不为 16 的整数倍时仍保持安全（防止越界），并在 `pack_vnni2` 中对剩余部分做零填充。<br>- `resize_buffer` 按块大小精准分配每线程所需的 **s_i、v_prime、Btmp**，避免过度分配导致的额外内存占用。 |
| **安全/健壮性** | - 所有 SIMD 操作均使用 **mask**（`_mm512_maskz_loadu`、`_mm512_mask_storeu`）防止读写越界。<br>- `static_assert(BLOCK_M <= BLOCK_N)` 在编译期确保因因果掩码导致的缓冲区溢出不可能。<br>- `has_varlen_sequences` 通过 `max_seqlen_q/k` 检查是否出现变长序列，防止 `indices` 计算错误。<br>- `torch::kCPU` 注册的函数使用 **ATEN 检查**（`CHECK_INPUT`, `CHECK_LAST_DIM_CONTIGUOUS`），保证传入张量的维度、内存布局正确。<br>- 仍有 **潜在风险**：代码假设 `head_size % 32 == 0`、`head_size_v % 32 == 0`（与 VNNI 对齐），如果未来模型使用非 32‑倍维度会触发 `TORCH_CHECK`，需在上层捕获并回退到通用实现。 |
| **可维护性** | - 通过 **模板化**（`flash_attn_softmax<scalar_t, BLOCK_M, BLOCK_N>`）封装核心逻辑，使未来添加新的块大小或新数据类型（e.g., `float8_e5m2`) 仅需实例化模板。<br>- `pack_vnni` 系列函数统一参数 `(dst, src, ind, N, K, ld_src, ld_dst)`，已去除冗余 `pack_vnni_Nx32`/`pack_vnni_Kx32` 多余实现，代码更易阅读。<br>- 软硬件特性分支 (`#if defined(CPU_CAPABILITY_AVX512)`) 明确，若未检测到 AVX‑512，将自动回退到标量实现（已有的 `#else` 分支），可在不支持硬件的平台上继续运行（虽性能下降）。 |
| **兼容性** | - 新增 `flash_attn_varlen_func` 与已有 `extend_attention_cpu` 同属 `sgl_kernel` namespace，通过 `TORCH_LIBRARY_FRAGMENT` 注册，兼容现有 Python 接口 `torch.ops.sgl_kernel.*`。<br>- 仅在 **CPU** 侧实现，未影响 GPU 代码路径。<br>- 对现有 `extend_attention` 接口保持 **向后兼容**（参数顺序未变

---

### model: support DeepSeek-OCR-2 (#17897)
**SHA**: `84ab611` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/84ab611af8b7f91f9c81eb30c913a4da00e57ec9)

**🎯 变更类型**：功能增强 / 架构变更 / 安全修复  
**⚡ 重要程度**：🔴 高  

**📋 变更摘要**  
1. 在文档中加入 DeepSeek‑OCR（OCR‑1 / OCR‑2）模型的使用说明，更新模型列表。  
2. 为 `DeepseekOCRForCausalLM` 增加对 OCR‑2 模型的兼容实现，包括：  
   - `ocr2_mode` 标记、局部裁剪检测、新的视觉特征编码路径（使用 Qwen2‑Decoder 作为视觉编码器）。  
   - `MlpProjector` 参数化（可配置 `input_dim`、`depth` 等），以及对原始 SAM/CLIP 路径的保留。  
3. 扩展配置加载逻辑，能够识别并正确处理 DeepSeek‑OCR‑2 的 `auto_map`，并在缺失的情况下动态补齐 Llama FlashAttention2。  
4. 为模型权重加载加入对 Qwen2‑decoder‑as‑encoder 权重的特殊映射，防止因层级命名不匹配导致加载错误。  
5. 处理器 `DeepseekOCRProcessor` 增加 `ocr2_mode` 判断，统一 `image_size` 为 640。  

---

### 🎯 影响范围
- **核心模型代码**：`python/sglang/srt/models/deepseek_ocr.py`、`python/sglang/srt/configs/deepseek_ocr.py`、`python/sglang/srt/multimodal/processors/deepseek_ocr.py`、`python/sglang/srt/utils/hf_transformers_utils.py`。  
- **模型加载与配置**：`get_config`、`get_processor`、权重映射逻辑。  
- **文档**：`docs/basic_usage/deepseek_ocr.md`、模型列表 `docs/supported_models/multimodal_language_models.md`、索引 `docs/index.rst`。  
- **依赖**：新增对 `transformers.models.qwen2`、`torch.nn.functional.interpolate` 的使用以及对 `torch.bfloat16` 的假设。  

---

## 🔍 技术洞察

### 架构影响
| 维度 | 说明 |
|------|------|
| **模型分支** | 原先只支持 DeepSeek‑OCR‑1（基于 SAM + CLIP），现在通过 `ocr2_mode` 引入 **OCR‑2** 路径：<br>① 使用 SAM 提取局部特征 → ② 通过自定义 **Qwen2‑Decoder‑as‑Encoder** 对特征进行跨模态编码 → ③ MLP 投影。 |
| **配置统一** | `deepseek_ocr.py` 中的 `DeepseekOCRForCausalLM` 在实例化时根据 `self.is_ocr2` 动态选择语言模型、视觉模型及投影层，保持单一类可兼容两种模型。 |
| **权重加载** | 为 Qwen2‑decoder‑as‑encoder 添加 **自适应名称映射**，兼容 `model.model` 与 `model` 两层命名差异，避免 `KeyError`。 |
| **处理器** | `DeepseekOCRProcessor` 自动检测 `vision_config.model_name` 或投影 `input_dim` 为 `896` 时开启 `ocr2_mode`，保持 API 向后兼容。 |
| **兼容层** | 在 `hf_transformers_utils.get_config` 中加入 `_ensure_llama_flash_attention2_compat`，防止远程代码缺失 `LlamaFlashAttention2` 时出现 ImportError。 |
| **可配置投影** | `deepseek_ocr.py` 中的 `MlpProjector` 参数化（`projector_type`、`depth`、`mlp_ratio` 等），为 OCR‑2 的 896 维特征提供灵活适配。 |

### 性能影响
| 维度 | 可能的正向/负向影响 |
|------|---------------------|
| **推理速度** | OCR‑2 使用 Qwen2‑Decoder‑as‑Encoder（基于 `sdpa`）取代 CLIP 的双阶段（SAM + CLIP），在 **GPU** 上**可能更快**（单次前向只走一次 transformer），但 **显存占用** 约提升 1.2‑1.5×，尤其在 `hidden_dimension=896` 时。 |
| **批处理** | `ocr2_mode` 通过 `torch.no_grad()` 统一处理，仍保持原有批次并行能力；但 `has_local_crops` 判断在 `process_one` 中增加了额外的 Python 逻辑，极小的 CPU 开销。 |
| **精度** | 视觉特征从 2048 维降至 896 维（取决于 `projector_config.input_dim`），若不匹配原模型权重，可能导致 **OCR 精度下降**。但在官方 OCR‑2 权重下已校准。 |
| **内存** | `torch.bfloat16` 用于 SAM/CLIP 分支，OCR‑2 仍使用 `torch.float16`（由 Qwen2‑Decoder 默认），总体内存提升不显著。 |

### 安全考虑
- **`--trust-remote-code`** 依旧是加载 DeepSeek‑OCR 系列模型的前置条件，文档已明确提示。  
- 新增的 **动态属性注入**（`_ensure_llama_flash_attention2_compat`）只在本地 `transformers` 包中进行属性绑定，不会执行外部代码，风险可控。  
- 权重映射的 **`default_weight_loader`** 仍保持原有安全检查（仅在 `strict=False` 时忽略缺失），未引入新的安全漏洞。  

---

## ⚠️ 潜在风险
1. **配置误判**：`ocr2_mode` 判定仅依据 `vision_config.model_name == "deepencoderv2"` 或 `projector_config.input_dim == 896`。如果未来出现同维度的非 OCR‑2 模型，可能误启用 OCR‑2 路径导致不兼容。  
2. **权重加载冲突**：对 Qwen2‑decoder‑as‑encoder 的自定义映射仅在名称包含 `"qwen2_model."` 时触发，若模型权重文件使用不同前缀（例如省略 `model.`），仍可能出现未加载的参数。  
3. **显存峰值**：OCR‑2 采用更大的 `hidden_dimension`（默认 896）并在 `torch.no_grad()` 之外保留 `self.sam_model`，在多卡或大批次场景下显存需求可能超出预期。  
4. **兼容性**：`_ensure_llama_flash_attention2_compat` 通过将 `LlamaAttention` 伪装为 `LlamaFlashAttention2`，但在新版本 `transformers` 中若引入真实 `LlamaFlashAttention2` 实现，可能产生行为不一致。  
5. **文档同步**：新增模型文档已加入，但 `basic_usage` 示例代码仍使用 `image_token` 与 `<|grounding|>` 组合，若模型实际要求不同的特殊 token（如 `<pad>`），使用者可能产生误解。  

---

## 💡 关注建议
| 对象 | 建议 |
|------|------|
| **开发者** | 1. 在发布新 OCR‑2 兼容模型前，确保 `vision_config.model_name` 与 `projector_config.input_dim` 与代码判定保持一致；若有变更，同步更新 `DeepseekOCRProcessor` 中的判定逻辑。<br>2. 为防止权重映射遗漏，建议在 `load_weights` 里加上日志输出未匹配的键，便于调试。 |
| **部署运维** | 1. 监控显存使用，特别是开启 OCR‑2 时推荐使用 `torch.cuda.max_memory_allocated` 进行预估；必要时可通过 `--max_input_tokens` 限制批次大小。<br>2. 对于使用 `--trust-remote-code` 的环境，务必在受信网络或内部镜像中保存模型，避免加载未知代码。 |
| **使用者** | 1. 按文档启动服务时务必加上 `--trust-remote-code`；若出现 “model_type not recognized” 错误，请检查 `transformers` 版本 ≥ 4.40，或手动设置 `model_type=deepseek-

---

### feat: add forward timeout (#17831)
**SHA**: `2b3408f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2b3408ff1474a4ca1740ee0bebaec340f7085b2c)

**🎯 变更类型**：功能增强  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
- 在 SGLang 中新增环境变量 **`SGLANG_FORWARD_TIMEOUT_MS`**，用于限定一次前向推理（prefill / decode）在调度器内部的最大耗时，默认 `-1`（不生效）。  
- 调度器在处理 batch 结果时会检查每个请求的 `forward_entry_time` 是否超过该阈值，超时则以 **HTTP 503 Service Unavailable** 结束请求。  
- 同时补充文档、环境变量注册以及对应的单元测试，验证超时行为。

**🎯 影响范围**  
- `python/sglang/srt/environ.py`（环境变量定义）  
- `python/sglang/srt/managers/scheduler_output_processor_mixin.py`（调度器前向结果处理）  
- `test/registered/scheduler/test_abort.py`（超时相关测试）  
- `docs/references/environment_variables.md`（文档）  

**🔍 技术洞察**  

- **架构影响**  
  - **调度器层面**：在 `process_batch_result_prefill` 与 `process_batch_result_decode` 两个关键路径中引入超时检查逻辑，涉及 `Req.time_stats.forward_entry_time` 与全局环境变量的读取。  
  - **请求生命周期**：新增 `FINISH_ABORT` 结束理由，统一通过 `req.to_finish` 标记，后续调度器会按已有的 abort 处理链路清理 KV 缓存并返回 503。  
  - **可配置性**：通过环境变量实现运行时开启/关闭，避免对现有代码路径产生侵入式改动，保持向后兼容。

- **性能影响**  
  - **额外开销**：每个 batch 只多做一次 `time.perf_counter()` 读取和一次整数比较，几乎可以忽略（< 1 µs）。  
  - **正向收益**：防止异常长时间的前向计算占用 GPU/CPU 资源，提升整体吞吐和响应稳态。  
  - **潜在负面**：若超时时间设置过小，可能导致大量请求被提前 abort，降低成功率，需结合模型大小、硬件性能调优。

- **安全考虑**  
  - **错误码公开**：返回 **503 Service Unavailable**，属于标准 HTTP 状态码，不泄漏内部实现细节。  
  - **异常路径**：确保在 abort 前不会执行额外的业务逻辑或写入日志导致信息泄露。  
  - **配置风险**：环境变量可被外部脚本覆写，需在生产环境中通过安全的方式（如容器/系统守护进程）控制其值，防止恶意设置极低超时导致服务拒绝。

**⚠️ 潜在风险**  

1. **误配置导致服务不稳定**  
   - 将 `SGLANG_FORWARD_TIMEOUT_MS` 设为过低（如 1 ms）会在正常推理时频繁触发 abort，出现大量 503 错误，影响用户体验。  
2. **`forward_entry_time` 未正确记录**  
   - 若在某些代码路径（如模型并行、重叠调度）未更新 `Req.time_stats.forward_entry_time`，超时判断可能失效或误判。  
3. **与重叠调度 (overlap) 交叉**  
   - 当前实现仅在 `self.enable_overlap` 为 `False`（默认）时生效；若未来开启 overlap，需确认 abort 逻辑仍然安全，不会留下未清理的 KV 缓存。  
4. **异常抛出路径未覆盖**  
   - `FINISH_ABORT` 需要在后续的错误处理链路（如 `schedule_batch`）正确转化为 HTTP 响应，若遗漏会导致内部异常而非返回 503。  

**💡 关注建议**  

1. **配置建议**  
   - 在生产环境默认保持 `-1`（关闭），仅在出现卡死或极端推理时临时开启，并结合监控指标（GPU 利用率、请求 latency）调优。  
2. **监控 & 告警**  
   - 增加 `sglang_forward_timeout_total`（累计超时次数）等 Prometheus 指标，监控异常增长。  
3. **测试覆盖**  
   - 添加跨模型、跨硬件（GPU/CPU）以及 overlap 调度开启情况下的超时单元测试，确保 `forward_entry_time` 始终可用。  
4. **文档与运维**  
   - 在部署手册中明确 `SGLANG_FORWARD_TIMEOUT_MS` 的含义、取值范围以及对业务的潜在影响，建议在容器/服务编排层面加入显式的环境变量声明。  
5. **代码审计**  
   - 检查所有可能改动 `Req.time_stats.forward_entry_time` 的入口，确保在每次进入前向计算前均有统一赋值，以防出现“deadline 为 -1”导致的误判。  

通过上述措施，可在提升系统鲁棒性的同时，将因误配置或实现细节导致的风险降到最低。

---

#### 🟡 中重要度变更 (10)

### [BUGFIX] Fix dp size > 1 for qwen3 vl model (#17624)
**SHA**: `0c5a81a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0c5a81acb85b963f533cc85024736724a1182884)

**🎯 变更类型**：Bugfix（修复 dp size > 1 时 Qwen‑3 VL 模型的 Attention Reduce 错误）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
- 为 `VisionAttention` 与通用 `ColumnParallelLinear/RowParallelLinear` 增加 `use_dp_attention_reduce` 参数；在 `forward` 中若该标记为真，使用专门的 DP‑Attention 进程组 `get_attention_tp_group()` 进行 all‑reduce，而不是原来的 `tensor_model_parallel_all_reduce`。  
- Qwen‑3 VL 相关模块改为通过 `get_attention_tp_{size,rank}` 获取注意力并行规模/秩，避免在 Data‑Parallel 场景下误用普通张量并行大小。  
- `forward_batch_info._pad_inputs_to_size` 对 `mrope_positions` 的填充方式改为 `torch.cat`，确保在多 token‑batch 时维度保持一致。  
- `mm_utils.run_dp_sharded_mrope_vision_model` 采用新的 DP‑Attention 参数获取函数，并在 `tp_size==1` 时直接返回单机推理，省去不必要的 All‑Gather。  

**🎯 影响范围**  
- `sglang/srt/layers/attention/vision.py`  
- `sglang/srt/layers/linear.py`（Column/Row Parallel Linear）  
- `sglang/srt/models/qwen3_vl.py`（Qwen3‑VL Transformer）  
- `sglang/srt/model_executor/forward_batch_info.py`（MROPE padding）  
- `sglang/srt/multimodal/mm_utils.py`（DP‑sharded vision 前向）  

**💡 关注建议**  
1. **兼容性**：`use_dp_attention_reduce` 默认为 `False`，但所有 Qwen‑3 VL 实例已显式传入 `is_dp_attention_enabled()`，确保旧模型仍可在单卡或非 DP 环境下运行。  
2. **集体通信**：确认 `get_attention_tp_group().all_reduce` 与 `tensor_model_parallel_all_reduce` 在不同后端（NCCL、MPI）上的语义完全一致，尤其是 `skip_all_reduce` 路径。  
3. **性能验证**：在多 GPU（tp > 1）且开启 DP‑Attention 的配置下跑一次完整推理基准，比较 all‑reduce 延迟与原实现，防止出现额外同步开销。  
4. **单卡回退**：`run_dp_sharded_mrope_vision_model` 现在会在 `tp_size==1` 时直接调用原 VisionModel，需检查该路径的输入/输出保持与 sharded 路径一致（尤其是 `grid_thw` 格式）。  
5. **测试覆盖**：补充以下场景的单元/集成测试：  
   - TP = 1、DP = off（保持旧行为）  
   - TP > 1、DP on 且 `use_dp_attention_reduce=True`（检查 all‑reduce 正确聚合）  
   - `mrope_positions` 在不同 `bs`、`num_tokens` 组合下的 padding 正确性。  

总体来看，本次改动聚焦在 DP‑Attention 并行路径的正确调度，代码结构清晰、参数传递统一；只要在多卡环境完成上述验证，即可安心合入主线。

---

### [Model] Add K-EXAONE model support (#16294)
**SHA**: `c04efe0` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c04efe030accffafbba596e9f17a399b775830af)

**🛠️ 变更类型**：功能增强（新增 Exaone‑MoE 系列模型及其 MTP 变体）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `model_config.py` 中加入对 `ExaoneMoEForCausalLM` 草稿模型的适配，开启 `num_nextn_predict_layers=1`。  
2. 新增完整的 **ExaoneMoE** 实现（`exaone_moe.py`），包括 MoE MLP、稀疏路由、共享专家、双流并行、梯度‑无关的前向实现等。  
3. 新增 **MTP**（多张量并行）版本 `exaone_moe_mtp.py`，用于 🤖 Speculative Decoding 场景。  
4. `server_args.py` 中扩展对 `ExaoneMoEForCausalLM` 的注意力后端校验及 hybrid‑SWA 内存禁用提示。  

**🎯 影响范围**  
- **模型加载与推理层**：`ExaoneMoEModel`、`ExaoneMoEForCausalLM`、`ExaoneMoEForCausalLMMTP` 全新注册为入口类，影响 `model_loader`、`weight_loader`、`logits_processor`。  
- **分布式运行时**：依赖 `tensor_model_parallel`、`expert_parallel`、`pp_group`、`attention_tp` 等多维并行实现；MTP 版本增加了跨张量线性层和前置归一化。  
- **服务器启动参数**：`--attention-backend` 必须是 `fa3/triton/trtllm_mha`，否则断言报错；Hybrid‑SWA 默认关闭。  
- **自定义路由/分层捕获**：新增 `set_eagle3_layers_to_capture`，对 EAGLE3 调试/分析有影响。  

**💡 关注建议**  

1. **兼容性与回归**  
   - 确认已有模型（如 Qwen、Nemotron 等）在 `model_config` 中仍保持原有路径，避免因 `is_draft_model` 判定冲突导致错误的 arch 名改写。  
   - 对 `Exaone4ForCausalLM` 与新 `ExaoneMoEForCausalLM` 的注意力后端检查保持统一，防止因 `self.attention_backend` 未初始化而触发 `AttributeError`。  

2. **分布式调试**  
   - 在多机/多卡环境下跑一次 `torch.distributed.launch` 的端到端推理，重点检查 `tensor_model_parallel_all_reduce` 与 `expert_parallel` 的通信路径（尤其是 DeepSpeed‑style A2A 后端）。  
   - 对 `alt_stream` 双流并行实现，确保 CUDA 流同步不产生隐藏的 race‑condition，建议在 CI 中加入 `torch.cuda.synchronize()` 的显式检查。  

3. **权重加载**  
   - `load_weights` 现在会跳过 `mtp` 前缀的参数并对 `mtp`‑only 参数进行重映射，务必在模型 checkpoint 中保留 `mtp.fc.weight` 等对应张量；缺失时应给出清晰的警告而不是静默跳过。  
   - 对 `gate_up_proj`、`gate_proj` 等堆叠层的 shard_id 处理保持一致，防止 1‑GPU 与 TP≥2 时出现维度不匹配。  

4. **量化与推理性能**  
   - 量化配置仍只能忽略 `gate_proj`/`up_proj`，如果将来支持 ExaoneMoE 的 QKNorm/FP16‑int8 混合，务必在 `QuantizationConfig` 中补全对应字段。  
   - 针对 MTP 版，`fc` 前的两层 RMSNorm 可能成为推理瓶颈，可在文档中提示开启 `--attention-backend triton` 获得更好算子融合。  

5. **文档与用户提示**  
   - 在模型列表页、API 文档里新增 “Exaone‑MoE” 支持说明，标明需要的 `attention_backend`、Hybrid‑SWA 已禁用以及 MTP 只在 Speculative Decoding 场景下使用。  
   - 对 `set_eagle3_layers_to_capture` 的默认层号给出明确解释，避免用户误以为层号从 0 开始。  

**结论**：本次 PR 为 SGLang 引入了功能强大的 Exaone‑MoE 支持，代码结构完整且遵循现有分布式框架。但由于涉及多种并行策略和权重映射，建议在多机多卡 CI 上加入完整的加载‑推理‑采样回归测试，并在文档中明确使用限制，以确保新模型不会意外破坏已有工作流。

---

### Add cuda graph status to prefill log (#17836)
**SHA**: `77a27e7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/77a27e728c6423453707ba02cb16b19937aa7976)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
为 Prefill 阶段的日志加入 CUDA Graph 是否可运行的状态。调度器在 `_get_new_batch_prefill_raw` 中缓存 `adder`、`can_run_list` 与 `running_bs`，在 `process_batch_result_*` 完成后统一调用 `log_prefill_stats`，并通过 `can_run_cuda_graph` 参数输出 “cuda graph / cpu graph / npu graph” 标记。`model_runner.forward_extend` 现在返回 `(result, can_run_graph)`，以供上层使用。

**🎯 影响范围**  
- `python/sglang/srt/managers/scheduler.py`  
- `python/sglang/srt/managers/scheduler_metrics_mixin.py`  
- `python/sglang/srt/managers/scheduler_output_processor_mixin.py`  
- `python/sglang/srt/model_executor/model_runner.py`

**💡 关注建议**  
1. **返回值兼容**：`forward_extend` 由原来的单对象改为 `(obj, bool)`，上层所有调用点必须检查并解构返回值，防止因旧代码仅接收第一个元素而漏掉第二个布尔值。已在 `_forward_raw` 中同步改写，建议全局搜索 `forward_extend(` 确认没有遗漏。  
2. **日志开关**：`current_scheduler_metrics_enabled` 为 `True` 时才会记录 `can_run_cuda_graph`，确保在生产环境关闭时不会产生额外计算或属性访问。  
3. **性能影响**：新增的状态缓存与一次布尔值传递几乎无运行时开销；唯一的潜在影响是日志行长度增加，建议在监控平台适配新的日志字段。  
4. **测试覆盖**：新增单元测试验证：  
   - `model_runner.forward_extend` 在有 `piecewise_cuda_graph_runner` 且满足 `can_run` 时返回 `True`。  
   - `log_prefill_stats` 能正确渲染 `"cuda graph: True/False"`（以及 CPU/NPU 兼容情况）。  
   - `Scheduler` 在 Prefill 完成后能够使用缓存变量成功打印日志。  
5. **异常安全**：如果 `result` 对象缺少 `can_run_cuda_graph` 属性，使用 `getattr(..., False)` 已做兜底，保持向后兼容。  

总体来看，此次改动仅在统计/可观察性层面加入关键信息，对核心推理路径影响极小，建议尽快合入并在 CI 中加入相应的返回值与日志断言。

---

### SGLang Tracing: Improve root span attributes (#17008)
**SHA**: `c8dc543` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c8dc543dc596107ecd5b5eeca4c752c48cb27c4b)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 SGLang 的 tracing 体系中新增 `SpanAttributes` 常量类，并在 `TokenizerManager` 中实现 `convert_to_span_attrs`，把 token 使用量、请求 ID、采样参数、模型名称、结束原因、各阶段延迟等信息转化为统一的 span 属性。`trace_req_finish` 调用改为同时传入这些属性，实现根 span 的语义化填充。  

**🎯 影响范围**  
- `python/sglang/srt/managers/tokenizer_manager.py`：新增属性转换逻辑，调用 `trace_req_finish` 时携带 `attrs`。  
- `python/sglang/srt/tracing/trace.py`：新增 `SpanAttributes` 定义，供外部统一引用。  
- 任何直接使用 `trace_req_finish`（或依赖根 span 的监控） 的模块，都将间接受到新属性的填充。  

**💡 关注建议**  
1. **兼容性**：若用户自行实现 tracing 回调，需确保能够接受 `attrs` 参数并识别新的键名；否则可能出现属性丢失或异常。  
2. **性能**：属性组装涉及 JSON 序列化和若干字典查找，建议在高并发场景下通过 benchmark 确认开销在可接受范围。  
3. **监控与告警**：更新 OpenTelemetry/OTEL 监控仪表板，使用 `gen_ai.*` 前缀的属性做细粒度分析（如 token 使用、首 token 延迟等）。  
4. **测试**：添加单元测试验证 `convert_to_span_attrs` 在不同 `sampling_params`、无 `finished_reasons`、异常时间戳等边界情况下的返回结构。  
5. **文档**：在项目 README 或 tracing 章节补充新属性说明，特别是 `GEN_AI_LATENCY_*` 系列与模型推理阶段的对应关系。  

总体来看，此次改动提升了 SGLang 在生成式 AI 监控方面的可观测性，但需同步更新使用方的追踪适配和监控告警配置，以免因属性缺失导致指标偏差。

---

### fix(benchmark): add missing args for speculative decoding benchmark (#17974)
**SHA**: `33c053c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/33c053c50c025f5d45912f2fc8556c8639af636b)

**变更类型**：Bug 修复  
**重要程度**：🟡 中  
**变更摘要**：在 `scripts/playground/bench_speculative.py` 中为 `send_one_batch` 调用补齐了 `return_routed_experts` 与 `plot_throughput` 两个参数，避免在运行 speculative decoding 基准时因缺少必填参数而抛异常。  

**影响范围**  
- 仅涉及 `scripts/playground/bench_speculative.py`，对应的 `send_one_batch` 调用链（`sglang` 核心推理接口）。  
- 若后续在其他脚本或单测中复用此函数，需确保同样传入这两个新参数，或在函数实现上提供默认值以保持向后兼容。  

**关注建议**  
1. **参数默认值**：如果 `return_routed_experts` 与 `plot_throughput` 在多数场景下为 `False`，建议在 `send_one_batch`（或底层 API）中设定默认值，避免每次调用都显式传参。  
2. **文档同步**：更新 README、benchmark 使用说明及函数签名注释，明确这两个参数的含义与取值范围。  
3. **兼容性测试**：跑一次全量 benchmark（包括多模态、流式等模式），确认加入参数后不影响已有功能并且吞吐量绘图能正常生成。  
4. **代码可维护性**：若后续还有类似缺失的参数，考虑在 `send_one_batch` 前统一封装一个 “benchmark 参数校验” 函数，提升可读性并降低遗漏风险。  

总体而言，此次补参修复了运行时错误，影响范围局限且风险低；只需确保文档、默认值和兼容性测试同步完善即可。

---

### [CPU] Optimize Qwen3-next model on CPU (#12525)
**SHA**: `336dc45` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/336dc4579e944728d3efe902a49ad75f195da5e4)

**🎯 变更类型**：功能增强（CPU‑侧模型加速）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次 PR 为 Qwen‑3‑next 系列在纯 CPU 环境下加入了多项性能优化：  
1. 在模型配置阶段对 TP（tensor‑parallel）数不匹配的 head 数进行自动填充 `adjust_tp_num_heads_if_necessary`，并在 `update_config_with_unaligned_cpu_tp` 中同步更新。  
2. 新增 AMX 工具函数，针对卷积权重使用 `causal_conv1d_weight_pack` 并在 `amx_process_weight_after_loading` 中区分常规与卷积权重。  
3. RMSNorm‑Gated 在 CPU‑AMX 环境下走专属自研 kernel `fused_rmsnorm_gated_cpu`，并限制为 “norm before gate、无 group‑size”。  
4. `HybridLinearAttnBackend` 根据平台动态加载实现：CUDA/NPU 走原实现，CPU 必须具备 AMX 才启用自研 `causal_conv1d_fn_cpu`、`chunk_gated_delta_rule_cpu`、`fused_sigmoid_gating_delta_rule_update_cpu` 等。  
5. `Mamba` 权重加载在 CPU TP 为奇数时实现自动 padding，避免切片不整除错误。  
6. `MemoryPool` 为 AMX‑加速的卷积状态改用 `_init_amx_conv_state` 重新排布布局。  
7. `topk` kernel 扩展至 384/512‑expert 场景，提升 MoE 计算兼容性。  

**🎯 影响范围**  
- `sglang/srt/configs/qwen3_next.py`、`update_config.py`（模型配置）  
- `sglang/srt/layers/amx_utils.py`、`rmsnorm_gated.py`（AMX 权重处理、CPU‑kernel）  
- `sglang/srt/layers/attention/hybrid_linear_attn_backend.py`、`intel_amx_backend.py`（Attention 计算路径）  
- `sglang/srt/layers/attention/mamba/mamba.py`（Mamba 权重切分）  
- `sglang/srt/mem_cache/memory_pool.py`（卷积状态布局）  
- `sglang/srt/model_loader/weight_utils.py`（不整除 TP 的 narrow‑pad 逻辑）  
- `sglang/srt/models/qwen3_next.py`（模型前向分支）  
- `sgl-kernel` C++/Python（新增 CPU‑kernel 接口）  

**💡 关注建议**  
1. **硬件前置**：CPU 端必须支持 AMX（`cpu_has_amx_support()` 为 True），否则会触发断言或回退到慢速实现；部署前请确认服务器微架构（Ice Lake 及以上）。  
2. **兼容性验证**：在无 AMX 环境下仍应能启动，确保 `is_cpu()` 与 `cpu_has_amx_support()` 正确返回，避免因缺失 kernel 导致 ImportError。  
3. **TP 不整除的测试**：针对 odd‑TP 场景（如 3 卡），跑通权重加载、推理前向以及 MoE top‑k（384/512）分支，验证 padding 与 `linear_num_*_heads_cpu` 字段的一致性。  
4. **性能回归**：在开启/关闭 AMX 两种路径下分别测量吞吐量，关注 RMSNorm‑Gated、Mamba Conv、Hybrid‑Linear‑Attn 的 QPS 差异，确保 CPU‑AMX 实现带来显著提升。  
5. **日志与异常**：新增的 `logger.warning` 会在维度/ dtype 不满足 AMX 要求时关闭加速，建议在生产日志中监控该信息，以便快速定位不受支持的模型或权重。  

总体而言，此次改动为 Qwen‑3‑next 在具备 AMX 的 CPU 上提供了完整的算子加速路径，但依赖硬件特性，发布前务必完成上述验证，以防出现不可预期的回退或错误。

---

### GPTJForCausalLM Support (#7839)
**SHA**: `046b29b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/046b29be1601e019ea1f9b835bb34de4bd0f8646)

**🎯 变更类型**：功能增强（新增对 GPT‑J 6B 模型的支持）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
- 在文档 `supported_models/generative_models.md` 中加入 GPT‑J（`EleutherAI/gpt-j-6b`）的说明。  
- 新增 `python/sglang/srt/models/gpt_j.py`，实现基于 HuggingFace 权重的 **GPTJForCausalLM**，包括并行化的注意力、MLP、Embedding、LMHead、LogitsProcessor，以及权重加载的兼容逻辑（支持量化、KV‑cache 缩放、GPTQ 偏置跳过）。  

**🎯 影响范围**：  
- **模型加载层**：`sglang.srt.model_loader` 需要能够识别 `gpt_j` 并调用新类 `EntryClass = GPTJForCausalLM`。  
- **并行算子**：使用的 `ColumnParallelLinear`、`RowParallelLinear`、`QKVParallelLinear`、`RadixAttention`、`VocabParallelEmbedding` 等已有实现，保持与已有模型（如 Llama、Mixtral）的统一。  
- **日志/推理路径**：`LogitsProcessor`、`ForwardBatch` 等保持不变，仅在 `GPTJForCausalLM.forward` 中调用。  

**💡 关注建议**：  
1. **单元/集成测试**：新增模型的推理路径应加入 CI，验证 `generate`、`decode` 与已有模型输出格式一致，尤其是 `position_ids` 与 `rotary` 计算。  
2. **量化兼容**：检查 `quant_config` 在 GPT‑J 上的行为，确保 `qkv_proj` 的 `shard_id` 处理与其它模型保持一致，避免权重错位。  
3. **权重加载**：`load_weights` 中对 `attn.bias`、`masked_bias` 的过滤以及 `maybe_remap_kv_scale_name` 需在实际 GPT‑J 权重文件上做一次完整跑分，防止遗漏。  
4. **文档同步**：确保 `supported_models` 表格與实际 `model_registry` 中的映射保持一致，避免用户在 `sglang` CLI 中找不到模型。  

总体来看，新增 GPT‑J 支持是一次结构清晰的功能扩展，代码复用了现有并行与量化框架，影响范围可控。后续重点关注模型权重加载的完整性与推理正确性即可。

---

### Add concurrency tracking to runner utilization report (#17963)
**SHA**: `2cd2c31` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2cd2c3118dc9fbe7b25377b2b8cc730a924c3db8)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 `scripts/ci/utils/runner_utilization_report.py` 中加入并发度跟踪。实现了 `calculate_concurrency_metrics`（峰值并发、平均并发、饱和时长、队列深度），并使用 `ThreadPoolExecutor` 并行拉取每个 workflow run 的 job 列表。报告现在会输出 “Effective Runners”、并发指标以及基于这些指标的自动化建议。

**🎯 影响范围**  
- `scripts/ci/utils/runner_utilization_report.py`（核心逻辑）  
- 依赖该脚本的 CI 报表生成流程（可能在 CI/CD 仪表盘或内部监控中使用）  

**💡 关注建议**  

1. **并发拉取的速率控制**  
   - 直接开启 20 个线程对 GitHub API 进行大量请求，可能触发 rate‑limit。建议在 `ThreadPoolExecutor` 外层加入速率限制（如 `aiohttp` + token‑bucket）或使用 GitHub GraphQL 批量查询。

2. **错误容忍**  
   - `fetch_jobs_for_run` 捕获异常后返回 `None`，但随后直接 `all_jobs.extend(jobs)` 会在 `jobs` 为 `None` 时抛异常。应改为 `if jobs: all_jobs.extend(jobs)`，并记录失败的 run id 供后续审查。

3. **并发度计算细节**  
   - `queue_events.sort(key=lambda e: (e[0], e[1] == 1))` 与运行事件的排序保持一致，但队列深度只统计峰值，未考虑在饱和期间的实际等待时间。若需要更精确的排队时长，可在遍历时累计时间段。

4. **报告格式对齐**  
   - 新增的 “Concurrency Analysis” 表格与后面的 “Summary by Runner Label” 表格使用不同列数，可能导致 Markdown 渲染错位。建议保持列宽一致或在两段之间插入分隔线。

5. **类型注解兼容性**  
   - 使用 `list[dict]` 需要 Python 3.9+。若 CI 环境仍在 3.8，需改为 `List[Dict]` 并 `from typing import List, Dict`。

6. **测试覆盖**  
   - 添加单元测试覆盖 `calculate_concurrency_metrics`（重叠、完全不重叠、窗口外的 job）以及并行拉取的错误路径，确保在数据不足或 API 失效时脚本仍能产生合理报告。

**结论**：本次改动显著提升了 runner 利用率报告的可观测性，但在并发 API 调用和异常处理上仍有改进空间。完成上述建议后，可放心在生产 CI 中推广。

---

### Fix `torch.__version__` for PEP440 (#15682)
**SHA**: `1b6798a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1b6798a6a47d76953125aa4e9f00f0d603a98499)

**🎯 变更类型**：Bug 修复（统一、修正 `torch.__version__` 解析）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
将项目中散落的 `packaging.version`、字符串切割等自定义版本判断全部替换为统一的 `torch_release` 常量（`torch_release = pkg_version.parse(torch.__version__).release`），并相应修改了 `usp.py、compiler_interface.py、pynccl_allocator.py、fla/utils.py、fp8_utils.py、server_args.py、common.py、patch_torch.py` 等近十个文件的版本条件。  

**🎯 影响范围**  
- 运行时层（`sglang.multimodal_gen.runtime.layers`）  
- 编译器接口及 Inductor 集成 (`sglang.srt.compilation`)  
- 分布式 NCCL 分配器、设备通信 (`sglang.srt.distributed`)  
- 注意力、量化、FP8 实现 (`sglang.srt.layers`)  
- 服务器参数检查与启动 (`sglang.srt.server_args`)  
- 公共工具库 (`sglang.srt.utils.common/patch_torch`)  

**💡 关注建议**  
1. **保持 `torch_release` 的定义**：确保 `sglang/srt/utils/common.py` 在导入时已加载 `torch`，且 `pkg_version` 与 `torch` 兼容，避免在未安装 `torch` 时导入错误。  
2. **兼容旧版本**：如果用户仍在使用 PyTorch 2.5、2.6 等老版本，验证所有新条件 (`torch_release >= (2, 6)`、`torch_release[:2] == (2,5)` 等) 的逻辑是否与原来的字符串比较完全等价。  
3. **测试边界情况**：包括带 `+cu121`、`+rocm5.6`、`dev` 等后缀的版本字符串，保证 `pkg_version.parse` 能正确抽取 `release`。  
4. **删除多余 `packaging` 依赖**：项目 `requirements.txt` 中可移除对 `packaging` 的硬性依赖，或保持以兼容其他模块。  
5. **文档与 CI**：在 README/CHANGELOG 中说明新版的版本检测方式，并在 CI 中加入针对不同 PyTorch 版本的参数化测试，防止未来回归。  

整体而言，此次改动通过统一版本判断消除了 PEP 440 解析不一致导致的潜在错误，风险主要集中在旧版 PyTorch 的兼容性及 `torch_release` 的初始化顺序，建议在多版本测试后再正式发布。

---

### Add tool call tests for DeepSeek V3.2 in nightly CI (#17951)
**SHA**: `d417c68` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d417c6809e2118c599c216efa6f841748127deb3)

**🎯 变更类型**：功能增强（为模型加入 Tool Call 测试）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 `run_combined_tests` 中加入 `tool_call_params` 参数，支持在同一次 CI 流程里跑工具调用（Tool Call）测试，并在报告中打印结果。  
2. 新增 `tool_call_test_runner.py`，实现一套完整的 Tool Call 测试用例（basic、streaming、required/none、specific、strict、multiturn、thinking、parallel），并提供统一的运行入口 `run_tool_call_test`。  
3. 更新 DeepSeek‑V3.2 的 GPU 测试脚本，开启对应的 parser 参数并把 Tool Call 测试纳入 nightly CI，默认开启 `test_thinking`。

**🎯 影响范围**  
- `python/sglang/test/run_combined_tests.py`（调度层）  
- `python/sglang/test/tool_call_test_runner.py`（全新模块）  
- `test/registered/8-gpu-models/test_deepseek_v32.py`（CI 配置）  
- 相关的 CI 脚本与报告生成逻辑。

**💡 关注建议**  

*开发者*  
- 确认 `ToolCallTestParams` 中的布尔开关与模型特性（如 DeepSeek‑V3.2 需要 `test_thinking`）保持一致，避免在不支持的模型上打开导致误报。  
- `run_tool_call_test` 中使用 `openai.Client` 进行请求，建议在 CI 环境显式设定超时时间与重试策略，防止因网络抖动导致测试卡死。  
- 资源回收已使用 `kill_process_tree`，但在异常路径下仍需要确保进程被正确终止，可在 `finally` 中加入日志提示。  

*用户*  
- 若在本地自行运行 `run_combined_tests`，请在 `base_url` 指向已启动的 SGLang 服务器，并确保服务器开启 `--tool-call-parser` 与 `--reasoning-parser`。  
- 对于不需要 Tool Call 测试的模型，可通过 `tool_call_params=None`（默认）跳过，以加快 CI 运行速度。  

总体而言，本次改动为模型的工具调用能力提供了自动化验证，提升了 nightly CI 的覆盖率，只要在 CI 配置和本地调试时注意对应的 parser 参数，即可安全使用。

---

#### 🟢 低重要度变更 (19)

### [Diffusion] Fix lora default lora_scale bug (#17982)
**SHA**: `abf13cc` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/abf13ccc11cb084afce113f5e9bf35c54de61d65)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增 `lora_scale` 参数并在 `LoraPipeline` 中传递该值，以修复默认 LoRA 缩放比例错误。CLI 也加入 `--lora-scale` 选项。

---

### update ascend docs (#17987)
**SHA**: `9c2a468` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/9c2a468e2cba8a291ba63ad1696c64b937d0f589)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Ascend NPU 文档中删除已移除的 CustomOps 章节，并在最佳实践示例中将预填端口从 8998/8999 调整为 8995。

---

### Increase install dependency for gb200 (#17977)
**SHA**: `a16aba8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a16aba834075749150009af7baf8281ed4d54f4d)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 CI 工作流 `.github/workflows/pr-test.yml` 中，将 “Install dependencies” 步骤的超时时间从 10 分钟提升至 30 分钟，以适应更长的依赖安装过程。

---

### adapt MODELSCOPE download (#17922)
**SHA**: `96584ab` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/96584ab692d8e6ccd236f340288d1dc54b527899)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `server_args.py` 中为 ModelScope 的 `snapshot_download` 添加了 `cache_dir` 与 `revision` 参数，统一缓存目录并支持指定模型版本，同时保留对 tokenizer 的忽略模式。  

---

### [NPU] enhance accuracy for model kimi-vl-a3b-instruct (#17480)
**SHA**: `70db339` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/70db3398d196861a9acf0405440f3ffa71fd6607)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 NPU 注意力实现中加入 KV 缓冲设置，以提升 Kimi‑VL‑A3B‑Instruct 模型的推理准确性；同步新增对应的 Ascend CI 测试用例。

---

### [Intel GPU] fix import error to run DeepSeek-V2-Lite model with BF16 on XPU (#10858)
**SHA**: `71e4d3b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/71e4d3b6bc467c0c6a45f2beb9494c0209ebe374)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 Intel XPU 环境添加 `is_cuda_alike` 检测，只有在 CUDA‑like 环境下才导入 `cutlass_w4a8_moe` 相关函数，避免在 XPU 上的导入错误。

---

### Fix prefill latency performance drop of bench serving (#14592)
**SHA**: `7541da1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7541da15d20d1cd3170b63f54fc03ba57fccca15)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `bench_serving.py` 中新增对 `return_text` 场景的处理，依据 tokenizer 的特殊标记数对 `input_lens` 进行截断，以避免因服务器编码添加的特殊 token 导致的前置填充延迟下降。

---

### [Intel GPU] fix device in DeepseekScalingRotaryEmbedding to run DeepSeek-V2-Lite BF16 on XPU (#10021)
**SHA**: `858dc80` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/858dc80aff167cc49ec8b0e08c21459a5217e171)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `rotary_embedding.py` 中将 `device` 参数默认设为 `None`，并改为使用统一的 `get_device()` 获取运行设备，适配 Intel XPU（DeepSeek-V2-Lite BF16）。新增 `get_device` 导入。

---

### Fix the scenario where eh_proj is quantized in the bailing moe nextn weights (#17808)
**SHA**: `0e4d9dd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0e4d9ddbd6b5146e6664899c93255777e01d758e)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 `eh_proj` 从普通 `nn.Linear` 替换为支持量化的 `ReplicatedLinear`，并传入量化配置，以修复在 Bailing MoE NextN 权重量化时的异常。

---

### [Fix] Remove unused Type import in gpt_j.py (#17975)
**SHA**: `606ff09` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/606ff09ef8adb9f86b5b4c4b96f9e8380ad57ec5)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `gpt_j.py` 中删除未使用的 `Type` 导入，简化代码并消除 lint 警告。

---

### [Fix] add block size logic for sm120 smem size (#14311)
**SHA**: `632c7af` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/632c7afa8ce20c2a696380ccf5b28337fd9903b7)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `extend_attention.py` 中新增针对 CUDA sm12（Blackwell）显卡的块尺寸逻辑，根据查询长度 Lq 选择不同的 BLOCK_M/N，以适配其更小的共享内存。

---

### add weightless qk norm to RMSNorm interface for Llama 4 (#12813)
**SHA**: `22df62d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/22df62d5862bed333b0f3f1092ef968d32c174a2)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：为 RMSNorm 添加 `has_weight` 参数，实现 Llama‑4 QK 归一化不使用可学习权重的功能。

---

### [AMD] fix pip sglang version (#17950)
**SHA**: `b7b1ba3` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b7b1ba329ee7a5f9752098aa0ece3a7288babcf8)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 AMD nightly Docker 工作流中新增短提交哈希生成的 “pretend_version”，并通过 `SETUPTOOLS_SCM_PRETEND_VERSION` 环境变量传递给构建，使 pip 安装的版本号保持一致。

---

### Fix logprob_start_len handling for prefill-only requests (#17395)
**SHA**: `6a6b363` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6a6b36367e16c44e0e1a22094baeb36ff79e3d99)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：修复了仅prefill请求的 `logprob_start_len` 处理逻辑，调整顺序以正确区分 `return_logprob` 与 `is_prefill_only` 场景，避免错误的概率计算。

---

### Fix ci weight validation logic to check the safetensor completeness (#17917)
**SHA**: `c3bf53c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c3bf53c7c188b6399736ad41da5ff2a738e34c55)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `ci_weight_validation.py` 中新增对无 index 文件时的安全张量分片完整性检查。通过正则匹配 `model-XXX-of-YYY.safetensors`，统计并验证所有预期分片是否均已存在，缺失则返回 False。

---

### Fix /tag-and-rerun-ci to do full rerun when PR has sgl-kernel changes (#17729)
**SHA**: `1f75c2a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1f75c2af4d72584568c5c82bda0b62ede8fb4a93)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：新增 `has_sgl_kernel_changes` 判断 PR 是否涉及 `sgl-kernel/`，在 `/tag-and-rerun-ci` 中根据结果决定是全量重跑（确保 kernel 重建）还是仅重跑失败作业。

---

### Fix capture_sizes range for pcg (#17956)
**SHA**: `a416af4` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a416af4be7e1308ac80fc26055d0b3c0f6d30c2b)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 `capture_sizes` 的范围上限从 `640‑1024` 调整为 `576‑1024`，以修正 PCG 的尺寸捕获错误。

---

### [MUSA] Add labeler config (#17923)
**SHA**: `4f2b73b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4f2b73baf640f1ba58c105f66b63ec65dd565a6c)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/labeler.yml` 中新增 “mthreads” 标签规则，匹配文件名含 `mthreads` 或 `musa` 的路径，用于 Moore Threads 项目专属的自动标签。

---

### [diffusion]: add dummy device attribute to fix AttributeError (#17949)
**SHA**: `88fb927` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/88fb927cc946a444e42e6c29cd5d65a12a9f03fc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `python/sglang/multimodal_gen/runtime/platforms/interface.py` 中的 `Platform` 类新增 `device: torch.device | None = None` 属性，作为兼容性占位，以避免运行时出现 `AttributeError`。

---

