# 每日更新报告（2026-01-08）

## sgl-project/sglang

| 提交时间 | 作者 | 提交信息 |
|----------|------|----------|
| 2026-01-08 23:52:22 | Mohammad Miadh Angkad | [Performance] Force split_k=1 for MXFP4 Triton kernels on Hopper (#16014) |
| 2026-01-08 22:57:45 | Simo Lin | [smg][ci] migrate enable_thinking tests to new infrastructure (#16739) |
| 2026-01-08 22:47:21 | cheng peng | [Diffusion] model: fix zimage tp (#16719) |
| 2026-01-08 22:44:38 | Xiaoyu Zhang | [Diffusion] Avoid cpu2gpu sync in flashinfer rope and apply flashinfer rope to wanvideo (#16668) |
| 2026-01-08 22:30:45 | Simo Lin | Remove migrated e2e_grpc/basic tests (#16738) |
| 2026-01-08 22:29:23 | Simo Lin | [smg][ci] migrate chat completions tests to new infrastructure and build wheel once and share via artifact (#16709) |
| 2026-01-08 22:28:52 | fzyzcjy | Add file size hints in bitwise model file verifier (#16735) |
| 2026-01-08 22:24:12 | luoyuyan | Fix FP8 MoE NaN with DeepGEMM on Blackwell (#16622) |
| 2026-01-08 22:12:01 | yuhao | VLM: enhance VL embedding model with video input support and revise warm-up strategy (#16635) |
| 2026-01-08 21:50:42 | Hexq0210 | [NPU] update model and features supported (#16733) |
| 2026-01-08 20:26:13 | fzyzcjy | Support pre-generating and using expected checksums (#16730) |
| 2026-01-08 20:19:25 | fzyzcjy | Support bitwise weight checksum verifier (#16729) |
| 2026-01-08 18:35:46 | Lianmin Zheng | [Auto Sync] Update schedule_batch.py, common.py, eagle_info... (20260105) (#16519) |
| 2026-01-08 16:30:41 | Douglas Yang | fix: adding timeout for install dependencies (#16706) |
| 2026-01-08 15:15:43 | chenxu214 | [NPU][Bugfix] Fix qwen3 error when enable-dp-lm-head (#16115) |
| 2026-01-08 14:51:53 | YC Tseng | [AMD] Turn on AMD CI if rocm.Dockerfile changed (#16634) |
| 2026-01-08 14:43:48 | Bingxu Chen | [AMD CI] re-enable testcases missed when migrating ci test files (#16535) |
| 2026-01-08 13:54:33 | 陈一涵 | [jit kernel] support dtype as a cpp template parameter (#16452) |
| 2026-01-08 13:51:26 | Fan Lin | [diffusion] endpoint: add API endpoint to query loaded LoRA adapters information (#16533) |
| 2026-01-08 13:14:32 | Hubert Lu | [AMD] Fix aiter page-size handling, DeepSeek MLA tuple inputs, and HiCache/FA3 decode-backend override (#16531) |
| 2026-01-08 12:28:16 | Alison Shao | ci: migrate 2-GPU tests to test/registered/ (#16529) |
| 2026-01-08 12:18:29 | Alan Kao | [AMD] Add pip install / wheel build support for ROCm sgl-kernel (#15627) |
| 2026-01-08 12:00:18 | hw-csong | [NPU][Bugfix] move free_page logics to cpu (#16608) |
| 2026-01-08 11:59:47 | Yuwei An | Disable PCG TP Unittest (#16693) |
| 2026-01-08 11:40:03 | MarcoDWei | [Build] Enable full kernel in aarch64 wheel (#16155) |
| 2026-01-08 11:14:03 | Yuhao Yang | [diffusion] refactor: eliminate redundant parameters in req (#16505) |
| 2026-01-08 11:13:46 | Liangsheng Yin | Tiny adjust cancel PR workflow. (#16697) |
| 2026-01-08 11:10:00 | siyu | vlm: support SGLANG_MM_SKIP_COMPUTE_HASH for bypassing multimodal feature hashing (#16555) |
| 2026-01-08 11:08:34 | Xiaoyu Zhang | [Diffusion] clean useless and buggy set_seq_parallel_pg in yunchang (#16669) |
| 2026-01-08 11:01:32 | Teng Ma | Add sufeng-buaa into CI_PERMISSION (#16625) |
| 2026-01-08 10:24:12 | Simo Lin | [smg][ci] delete old responses api ci (#16695) |
| 2026-01-08 10:23:06 | hlu1 | Revert "[sgl-kernel] Update flashmla to include fp8 sparse_mla optimizations" (#16678) |
| 2026-01-08 10:19:44 | Simo Lin | [smg][ci] rename 3rd models from cloud backend and delete dead code (#16692) |
| 2026-01-08 09:45:21 | Junrong Lin | remove redundant max_running_reqs calculation in r3 (#16629) |
| 2026-01-08 09:40:03 | Simo Lin | [smg][ci] Migrate Response API e2e tests to shared infrastructure (#16680) |
| 2026-01-08 09:32:21 | Changyi Yang | [diffusion] fix: reduce default text length for Qwen-Image from 1024 to 512 (#16445) |
| 2026-01-08 09:22:31 | Baizhou Zhang | [1/n]deepseek_v2.py Refactor: attention backend handlers and forward method definition (#16306) |
| 2026-01-08 09:13:30 | Hexq0210 | [NPU] Update model and features supported (#16652) |
| 2026-01-08 08:54:45 | gongwei-130 | Add google-cloud-storage into Dockerfile (#15343) |
| 2026-01-08 07:49:02 | Harish | Fix KeyError when logprobs=false in completions endpoint (#16095) |
| 2026-01-08 07:35:00 | b8zhong | MoE Refactor: Refactor `fp8.py` -> `flashinfer_trllm.py` (#15151) |
| 2026-01-08 07:20:46 | Alison Shao | Fix pytest tests to exit with proper exit code (#16681) |
| 2026-01-08 05:25:41 | Simo Lin | [smg][ci] Add thread safety to ModelPool and GPUAllocator (#16674) |
| 2026-01-08 05:21:16 | Alison Shao | Migrate tokenizer tests to test/registered/tokenizer/ (#16457) |
| 2026-01-08 04:58:32 | Alison Shao | Fix gpt_oss_common import path and migrate core tests (#16426) |
| 2026-01-08 03:24:47 | Michael | Revert "Add SwapAB Optimization for triton fused_moe_kernel on SM90." (#16676) |
| 2026-01-08 01:45:55 | Douglas Yang | fix: 8-gpu-b200 increase timeout length (#16658) |
| 2026-01-08 00:28:15 | Simo Lin | Add reference counting to ModelInstance for parallel test safety (#16672) |
| 2026-01-08 00:15:23 | Ziwen Zhao | [model-gateway] Fix IGW routing for external OpenAI workers (#16633) |

### 📊 统计摘要
> 本日共 49 个提交 | 🔴高 3 | 🟡中 26 | 🟢低 20
## 📋 目录

- [sgl-project/sglang](#sgl-project-sglang)
  - [📊 统计摘要](#-统计摘要)
  - [🔴 高重要度变更 (3)](#-🔴-高重要度变更-3)
    - [[smg][ci] delete old responses api ci (#16695)](#4c46ecd)
    - [[smg][ci] Migrate Response API e2e tests to shared infras...](#109fe03)
    - [[smg][ci] Add thread safety to ModelPool and GPUAllocator...](#6037267)
  - [🟡 中重要度变更 (26)](#-🟡-中重要度变更-26)
    - [[smg][ci] migrate enable_thinking tests to new infrastruc...](#c8dc4d2)
    - [Remove migrated e2e_grpc/basic tests (#16738)](#f52ae58)
    - [[smg][ci] migrate chat completions tests to new infrastru...](#2f8a363)
    - [Add file size hints in bitwise model file verifier (#16735)](#b6e8a0d)
    - [VLM: enhance VL embedding model with video input support ...](#d2ea44f)
    - [Support pre-generating and using expected checksums (#16730)](#83abecd)
    - [Support bitwise weight checksum verifier (#16729)](#d54f0a1)
    - [[Auto Sync] Update schedule_batch.py, common.py, eagle_in...](#fb04e7e)
    - [fix: adding timeout for install dependencies (#16706)](#1e5de05)
    - [[NPU][Bugfix] Fix qwen3 error when enable-dp-lm-head (#16...](#7dd679c)
    - [[AMD CI] re-enable testcases missed when migrating ci tes...](#f9c0426)
    - [[jit kernel] support dtype as a cpp template parameter (#...](#48b8dcd)
    - [[diffusion] endpoint: add API endpoint to query loaded Lo...](#41b434a)
    - [[AMD] Fix aiter page-size handling, DeepSeek MLA tuple in...](#4935344)
    - [ci: migrate 2-GPU tests to test/registered/ (#16529)](#63cc97f)
    - [[AMD] Add pip install / wheel build support for ROCm sgl-...](#ab7d582)
    - [[diffusion] refactor: eliminate redundant parameters in r...](#e14f5ec)
    - [[Diffusion] clean useless and buggy set_seq_parallel_pg i...](#5a5cece)
    - [[smg][ci] rename 3rd models from cloud backend and delete...](#a08dc5a)
    - [[1/n]deepseek_v2.py Refactor: attention backend handlers ...](#38dc583)
    - [MoE Refactor: Refactor `fp8.py` -> `flashinfer_trllm.py` ...](#24b30f7)
    - [Fix pytest tests to exit with proper exit code (#16681)](#3a4767d)
    - [Migrate tokenizer tests to test/registered/tokenizer/ (#1...](#0241e04)
    - [Fix gpt_oss_common import path and migrate core tests (#1...](#0c47427)
    - [fix: 8-gpu-b200 increase timeout length (#16658)](#f474255)
    - [Add reference counting to ModelInstance for parallel test...](#7385834)
  - [🟢 低重要度变更 (20)](#-🟢-低重要度变更-20)
    - [[Performance] Force split_k=1 for MXFP4 Triton kernels on...](#05ab110)
    - [[Diffusion] model: fix zimage tp (#16719)](#82a8d77)
    - [[Diffusion] Avoid cpu2gpu sync in flashinfer rope and app...](#294ff71)
    - [Fix FP8 MoE NaN with DeepGEMM on Blackwell (#16622)](#fb7609f)
    - [[NPU] update model and features supported (#16733)](#20ca2c6)
    - [[AMD] Turn on AMD CI if rocm.Dockerfile changed (#16634)](#3d51ae1)
    - [[NPU][Bugfix] move free_page logics to cpu (#16608)](#261860e)
    - [Disable PCG TP Unittest (#16693)](#154740b)
    - [[Build] Enable full kernel in aarch64 wheel (#16155)](#1c09cbe)
    - [Tiny adjust cancel PR workflow. (#16697)](#8867d24)
    - [vlm: support SGLANG_MM_SKIP_COMPUTE_HASH for bypassing mu...](#6b3f93c)
    - [Add sufeng-buaa into CI_PERMISSION (#16625)](#d566739)
    - [Revert "[sgl-kernel] Update flashmla to include fp8 spars...](#12a0292)
    - [remove redundant max_running_reqs calculation in r3 (#16629)](#eec7dbd)
    - [[diffusion] fix: reduce default text length for Qwen-Imag...](#bb798a1)
    - [[NPU] Update model and features supported (#16652)](#5e867f6)
    - [Add google-cloud-storage into Dockerfile (#15343)](#65bed83)
    - [Fix KeyError when logprobs=false in completions endpoint ...](#156d97b)
    - [Revert "Add SwapAB Optimization for triton fused_moe_kern...](#3e73e12)
    - [[model-gateway] Fix IGW routing for external OpenAI worke...](#b5a94f8)
#### 🔴 高重要度变更 (3)

### [smg][ci] delete old responses api ci (#16695)
**SHA**: `4c46ecd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4c46ecde8022b786a810b28c260773c9c789e9ef)

**🎯 变更类型**：重构（清理废弃的 Response API 端到端测试及对应 CI 配置）  
**⚡ 重要程度**：🔴高  
**📋 变更摘要**：在 CI 工作流中删除了 `response-api` 测试步骤，并同步移除了 `sgl-model-gateway/e2e_test/e2e_response_api` 目录下全部旧的 e2e 测试文件（约 2 278 行）。此次提交旨在剔除已不再维护或已被新实现取代的 Response API 测试套件，缩短 CI 时长并简化代码库。  

**🎯 影响范围**：  
- `.github/workflows/pr-test-rust.yml`（CI 运行阶段）  
- `sgl-model-gateway/e2e_test/e2e_response_api/` 及其子目录（所有旧的响应 API 测试、fixture、工具函数）  

**🔍 技术洞察**：  
- **架构影响**：  
  - 仅涉及 CI 配置和测试代码，未触及核心业务库或运行时组件，系统架构保持不变。  
  - 删除的 fixture（`router_fixtures.py`、`util.py`）以及大量测试用例，不再在 CI 中启动 OpenAI/XAI、gRPC 等后端的真实路由器进程。  

- **性能影响**：  
  - **CI 运行时间显著下降**：原本的 `response-api` 测试需要启动多个进程、进行网络健康检查，通常耗时 5‑10 min。删除后 CI 总耗时预计缩短 30%‑50%。  
  - **仓库体积**：删除约 2 KB 的 Python 源码，对构建或运行时无直接性能提升。  

- **安全考虑**：  
  - 删除的测试代码中会读取环境变量中的 API Key（`OPENAI_API_KEY`、`XAI_API_KEY`），在 CI 环境下可能导致泄露风险。清理后降低了此类隐私泄露的可能性。  
  - 其它业务代码未受影响，无新增安全风险。  

- **可维护性**：  
  - 移除未维护或已失效的测试套件，降低了代码噪声，提升了代码库可读性。  
  - 同时，也削减了对 Response API 的回归检测，若未在其他位置补充等价测试，长期来看可能增加隐藏缺陷的风险。  

**⚠️ 潜在风险**：  
1. **回归覆盖缺失**：原有的 CRUD、状态管理、流式、结构化输出、工具调用等关键业务路径的 e2e 测试全部被删除，若项目未在别处提供相同覆盖，可能导致功能回退未被捕获。  
2. **CI 依赖变更破坏**：某些 CI 脚本或本地开发指南可能仍引用已删除的测试路径或 fixture，导致本地 `pytest` 运行错误。  
3. **文档不一致**：项目 README / CONTRIBUTING 中若仍描述 “Response API e2e tests”，用户会产生困惑。  
4. **误删风险**：如果这些测试实际上仍在使用（例如外部 CI、内部 QA 环境），删除会导致这些环境的测试失效。  

**💡 关注建议**：  
- **补充等价测试**：在 `e2e_test/responses` 或其他已保留的目录下，加入覆盖相同业务场景（CRUD、工具调用、流式等）的新测试，确保关键路径仍受 e2e 验证。  
- **更新文档**：立即同步更新项目文档，说明已移除 `response-api` 相关测试，并指向新的测试入口或说明替代方案。  
- **CI 监控**：在 CI 中加入简单的健康检查或单元测试覆盖率阈值（如 `coverage > 80%`），防止因测试削减导致整体质量下降。  
- **审计依赖**：全局搜索仓库（包括 CI、脚本、README）中对 `e2e_response_api` 的引用，全部清理或改写。  
- **迭代回滚策略**：保留该分支的完整历史，在必要时可以快速恢复被删除的测试文件，以应对突发的回归问题。  

---  

*总体结论*：此次提交是一次大幅度的代码清理，能够显著提升 CI 效率并降低潜在的凭证泄露风险，但必须确保在其他位置补齐相同或更好的测试覆盖，避免因测试缺失导致功能回退未被检测。及时更新文档并监控质量指标是降低风险的关键。

---

### [smg][ci] Migrate Response API e2e tests to shared infrastructure (#16680)
**SHA**: `109fe03` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/109fe03ad13140c26c53716f7fbff9b2eece5033)

**🎯 变更类型**：功能增强 / 重构 / 架构变更  
**⚡ 重要程度**：🔴高  

**📋 变更摘要**  
本次提交将 **Response API** 的 End‑to‑End（e2e）测试迁移至项目统一的共享测试基础设施，并在 CI 工作流中新增 `responses` 测试组。代码层面完成了以下关键改动：  
1. **CI 配置**：在 `.github/workflows/pr-test-rust.yml` 中加入 `responses` job，统一使用 `e2e_test/responses` 目录；启用重试、环境变量及 Cloud 后端准备。  
2. **后端抽象**：将 `CLOUD_BACKENDS` 简化为 `CLOUD_RUNTIMES`，并通过别名保持兼容；默认模型从 `gpt‑4o‑mini` 升级为 `gpt‑5‑nano`（示例模型）。  
3. **存储后端标记**：在 pytest fixture 中新增 `storage(backend)` 标记，实现对 `memory` 与 `oracle` 两种历史存储的灵活切换。  
4. **模型规格**：为 `qwen‑14b` 增加 `worker_args`（`--context-length=1000`），避免大模型启动时的显存/上下文长度瓶颈。  
5. **全量新增 e2e 测试**：包括 CRUD、状态管理、流式事件、结构化输出、工具调用等 5 大场景，分别覆盖 Cloud（OpenAI、xAI）与本地 gRPC（Qwen、Harmony）后端。  

**🎯 影响范围**  
- **CI/CD 流水线**：所有 Pull Request 将额外执行约 45 分钟的 Response API 测试，影响 PR 检查时长。  
- **sgl‑model‑gateway/e2e_test**：新增 `responses` 子目录及相关 fixtures，影响测试套件的执行顺序与依赖。  
- **后端启动脚本**：`launch_cloud_backend` → `launch_cloud_gateway` 别名保持向后兼容；`CLOUD_BACKENDS` 别名即将被移除。  
- **模型调度**：`model_pool._launch_model` 现在会解析 `worker_args`，影响在本地机器或 CI 中的模型实例化过程。  
- **历史存储**：`storage` 标记引入 Oracle 后端，涉及到数据库凭证的配置（`ORACLE_*` 环境变量）。  

**🔍 技术洞察**  

| 维度 | 影响描述 |
|------|----------|
| **架构影响** | • 引入统一的 **e2e shared infra**，使不同模块（responses、e2e_grpc、e2e_test）能够共享 `setup_backend`、fixture 逻辑，提升可维护性。<br>• 将 `CLOUD_BACKENDS` 统一为 `CLOUD_RUNTIMES`，简化后端映射表，降低配置冗余，但别名保留期间可能产生两套命名不一致的风险。 |
| **性能影响** | • 新增 `--context-length=1000` 参数对 `qwen‑14b` 可显著缩短模型加载时间与显存占用，利于 CI 中并行启动多个实例。<br>• 测试套件规模激增（≈ 2.3k 行新增代码），CI 运行时间预计提升 30%–45%。若并行度受限（`parallel_opts` 为 `""`），会进一步拉长总耗时。 |
| **安全考虑** | • 仍通过环境变量注入 `OPENAI_API_KEY`、`ORACLE_*` 等敏感凭证，未出现硬编码泄露。<br>• 新增 `storage` 标记可能导致在本地运行时意外使用 Oracle 后端，需确保测试机器网络与权限安全。<br>• 通过别名保留的兼容层避免了突发的 API 使用错误。 |
| **可维护性** | • 大量业务逻辑搬到了测试目录，使用统一的 fixture 与 marker，降低了各子模块之间的重复代码。<br>• `launch_cloud_backend` 已被声明为别名，后期可安全移除，以统一调用 `launch_cloud_gateway`。<br>• 新增的 Marker (`storage`) 与 CI 环境变量统一管理，文档需同步更新。 |
| **可靠性/稳健性** | • 多数新增测试依赖外部云服务（OpenAI、xAI），在 CI 中可能出现网络波动、配额耗尽等 **flaky** 情况。<br>• 引入 `--reruns 2 --reruns-delay 5` 对 `responses` 测试进行自动重试，缓解短暂的云端异常。 |

**⚠️ 潜在风险**  
1. **CI 超时/成本**：新增约 45 min 的测试且未开启并行，可能导致 PR 检查超过 GitHub Actions 默认超时或导致付费额度提升。  
2. **云服务不稳定**：依赖 OpenAI/X‑AI API 的测试若频繁触发配额或网络超时，会导致测试失真、回滚难度增大。  
3. **别名残留**：`CLOUD_BACKENDS` → `CLOUD_RUNTIMES` 的别名在迁移完毕后若未彻底清理，可能出现文档/示例与实际字段不匹配的混淆。  
4. **Oracle 后端泄露**：若 CI 环境误配置 `ORACLE_*` 环境变量，测试会写入真实数据库，产生数据污染或费用。  
5. **资源竞争**：`worker_args` 只在 `qwen‑14b` 上使用，若 CI 机器显存不足仍会导致 OOM，尤其在并发启动多个模型实例时。  
6. **兼容性破坏**：外部脚本或第三方插件仍使用旧函数 `launch_cloud_backend`，在别名被移除前需保持兼容，否则会出现 `ImportError`。  

**💡 关注建议**  
- **CI 优化**：考虑对 `responses` 测试开启 **并行执行**（若后端支持），或使用 **缓存/模拟**（如 `pytest-recording`）来降低对真实云端的依赖。  
- **配额管理**：在 CI 中使用专用的 **低配额** OpenAI 账户或 **Mock Server**，并在测试前检查配额剩余。  
- **别名清理计划**：在 `README` 与代码注

---

### [smg][ci] Add thread safety to ModelPool and GPUAllocator (#16674)
**SHA**: `6037267` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6037267f5bc14105965d00ecd712e8aa5e7aaff4)

**🎯 变更类型**：功能增强 / 重构 / 性能优化  

**⚡ 重要程度**：🔴高  

**📋 变更摘要**：  
本次提交为 **sgl‑project/sglang** 引入了完整的线程安全机制，使得 `ModelPool`、`GPUAllocator` 以及相关测试设施能够在 **pytest‑parallel** 的多线程并发模式下安全共享资源。核心改动包括：  
1. 在 `ModelPool`、`GPUAllocator`、`ModelPool` 的实例管理以及 `GPUAllocator` 的分配/释放逻辑上加入 **RLock**，实现对共享状态的同步保护。  
2. 将模型实例获取 (`ModelPool.get`) 与工作线程启动 (`ModelPool.launch_workers`) 改为 **循环等待** 直至 GPU 资源可用，避免因资源争夺导致的异常退出。  
3. 在测试层面加入 **pytest‑parallel** 支持、`thread_unsafe` 标记、并行执行默认配置、日志中加入线程名，以便在并发环境下调试。  
4. 将模型池的销毁迁移到 **atexit**，防止在并行测试中因 fixture `finalizer` 过早触发导致资源泄露。  

**🎯 影响范围**：  
- `sgl-model-gateway/e2e_test/infra/*`（GPUAllocator、ModelPool、hooks、setup_backend、fixtures）  
- `sgl-model-gateway/e2e_test/conftest.py`、`e2e_test/fixtures/*`  
- CI 工作流文件 `.github/workflows/pr-test-rust.yml`（并行选项、依赖）  
- 项目根目录 `pyproject.toml`（新增 `pytest-parallel`、`py` 依赖，新增 marker）  

**🔍 技术洞察**  

| 维度 | 影响 |
|------|------|
| **架构影响** | - **ModelPool 成为全局单例**，但其内部使用 **线程锁** 保护 `instances` 与 `GPUAllocator`，实现 **共享会话级**（session‑scoped）资源在多线程测试中的安全访问。<br>- 通过 `atexit` 注册统一的关闭流程，避免在并行模式下因 fixture `finalizer` 多次执行导致二次 shutdown。<br>- 新增 `is_parallel_execution` 辅助函数，使得其他钩子可以感知 pytest‑parallel 的运行模式，从而在并行时自动跳过 `thread_unsafe` 测试。 |
| **性能影响** | - **并行执行** 使用 `--workers 1 --tests-per-worker N`，在单进程内以线程方式并行测试，显著提升 CI 中 E2E 测试吞吐（尤其是大量轻量路由测试）。<br>- 由于 GPU 资源仍然稀缺，`ModelPool.get` 与 `launch_workers` 采用 **轮询等待**（2 s 间隔）机制，可能在资源竞争极端情况下略增测试时长，但提升了整体稳定性，避免因 `cuda`/`fork` 冲突导致的崩溃。<br>- 引入 `threading.RLock` 的开销极小（锁竞争主要在获取/释放模型实例的路径），对单线程路径几乎没有影响。 |
| **安全考虑** | - 代码未涉及网络协议或身份验证层面的变更，安全风险主要来自 **并发竞争**。通过 `RLock` 与 **原子化的 acquire/release** 机制，已消除竞态条件与资源泄露的风险。<br>- `GPUAllocator` 仍使用 `nvidia‑smi` 检测 GPU，避免在 fork 后初始化 CUDA，防止 **CUDA 初始化后 fork** 引发的不确定行为，这是对安全性的提升。 |
| **可维护性** | - 代码结构更清晰：`ModelPool` 的内部逻辑被拆分为 `_startup_unlocked`、`_get_unlocked`、`_launch_workers_unlocked` 等私有方法，职责单一，易于单元测试。<br>- 新增的 `thread_unsafe` 标记与自动跳过逻辑，使开发者可以显式声明不适合并行的测试，降低未来因误用并行导致的 flaky 测试风险。 |
| **兼容性** | - 对现有 **单进程、单线程** 测试向后兼容：默认 `--workers 1 --tests-per-worker 1` 与原有行为等价。<br>- 依赖 `pytest-parallel` 只在 CI 中显式开启，普通本地开发不受影响。<br>- 环境变量 `ENV_SKIP_MODEL_POOL`、`ENV_MODELS`、`ENV_BACKENDS` 仍保持原有语义。 |

**⚠️ 潜在风险**  

1. **GPU 等待超时**  
   - 在极端 GPU 资源紧张（例如多 job 同时占用同一机器）时，`ModelPool.get` 与 `launch_workers` 将循环等待直至 `gpu_wait_timeout`（默认 300 s）超时，可能导致部分测试因超时而失败。  
2. **锁竞争导致的性能波动**  
   - 虽然 `RLock` 开销低，但在大量并发获取/释放模型实例的场景（例如 10+ 并行测试线程）下，可能出现短暂的锁争用，导致每个请求的延迟上升。  
3. **资源泄露风险**  
   - 如果测试代码在异常路径中忘记 `release()`（尤其是通过 `get_workers_by_type` 获得的实例），会导致模型实例引用计数不降至 0，进而阻塞后续测试的 GPU 分配。虽已在 `launch_workers` 中加入等待重试，但仍可能导致整体 CI 运行时间膨胀。  
4. **对外部脚本的兼容性**  
   - `atexit` 注册的 shutdown 只在进程结束时执行。如果用户在测试之外手动调用 `model_pool.shutdown()` 再次结束进程，可能出现 “double shutdown” 警告（虽然当前实现对 `None` 做了检查，但日志中仍会出现异常信息）。  

**💡 关注建议**  

| 建议 | 目的 | 操作 |
|------|------|------|
| **监控 GPU 等待时间** | 防止因资源争夺导致的超时 | 在 CI 中加入 `--durations=0` 或自定义日志，收集 `ModelPool` 等待日志，若出现频繁 “All GPUs in use… waiting” 需考虑扩容或调低并发度。 |
| **显式 `release()`** | 保证引用计数正确，避免资源泄漏 | 在自定义 fixture或工具函数中使用 `try/finally` 或 `with` 上下文管理器包装 `instance.acquire()` → `instance.release()`。可考虑在 `ModelInstance` 实现 `__enter__/__exit__` 提供语法糖。 |
| **调优 `gpu_wait_timeout`** | 根据机器资源配置合理设置超时阈值 | 对于 GPU 较少的节点（1‑2 张卡），可将 `gpu_wait_timeout` 调低（如 120 s）以快速反馈资源不足；对资源充足的机器保持默认 300 s。 |
| **限制并行度** | 防止锁竞争放大 | 在 CI 中根据 GPU 数量动态设置 `--tests-per-worker`（如每张 GPU 最多 2‑3 并发测试），在 `pyproject.toml` 中加入注释说明。 |
| **新增单元/集成测试覆盖锁路径** | 防止回归导致竞态 | 为 `GPUAllocator`、`ModelPool` 编写多线程单元测试，使用 `threading.Barrier` 同时触发 `get`/`launch_workers`，验证不会出现死锁或资源泄漏。 |
| **文档与示例更新** | 降低团队使用门槛 | 在项目 README 或 CONTRIBUTING 中补充 “并行测试指南”，演示如何使用 `@pytest.mark.thread_unsafe` 跳过不兼容的测试。 |
| **审计 `atexit`** | 防止在某些异常退出场景下资源未释放 | 在 `sgl-model-gateway/e2e_test/fixtures/pool.py` 中加入 `atexit.unregister(_shutdown_model_pool)` 的安全检查，或在 `shutdown` 时捕获异常记录。 |

> **整体结论**：此次改动为 **sglang** 引入了真正的多线程安全模型，使得在单进程内的并行测试成为可能，显

---

#### 🟡 中重要度变更 (26)

### [smg][ci] migrate enable_thinking tests to new infrastructure (#16739)
**SHA**: `c8dc4d2` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/c8dc4d2d1f1ab32626f43a04accc67917078b22d)

**🎯 变更类型**：功能验证 / CI 增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 新增 `sgl-model-gateway/e2e_test/chat_completions/test_enable_thinking.py`，完整覆盖 `enable_thinking`（Qwen‑3 推理）在非流式、流式两种模式下的正向/负向路径。  
2. 在 `sgl-model-gateway/e2e_test/infra/model_specs.py` 中将 `qwen-30b` 的 tensor‑parallel (`tp`) 从 **2** 升至 **4**，以匹配测试对更高并行度的需求。  

**🎯 影响范围**  
- **e2e_test**：新增 168 行测试，用到 `grpc` 后端、`--reasoning-parser qwen3`、`--history-backend memory`。  
- **model_specs**：模型资源配置变更，影响所有依赖 `qwen-30b` 的 CI 与本地跑分/部署脚本。  

**💡 关注建议**  
1. **资源检查**：`tp=4` 需要至少 4 张 GPU，且对应的显存需求约 60 GB（已在 spec 中标注）。CI 环境若未满足，会导致该套 test 直接卡 / OOM，建议在 CI 配置中加入显存/卡数校验或 fallback 到 `tp=2`。  
2. **测试可靠性**：流式读取 `iter_lines` 时对 “data: [DONE]” 的过滤写得恰当，但若网关在极短间隔多次发送空行，可能导致 `has_content` 为 false。可在循环结束前加入超时/最小行数校验，防止假阴性。  
3. **向后兼容**：仅在 `enable_thinking=True` 时返回 `reasoning_content`，而在 `False` 时保持字段可为 `null` 或缺失。若后续对 OpenAI‑compatible schema 作更严的校验，需要同步更新本地 mock/文档。  
4. **文档同步**：README/CI 说明中应补充 “启用 Qwen‑3 推理需要 4‑卡 GPU，且已开启 `--reasoning-parser qwen3`”。  
5. **代码审查**：测试代码中重复的 `if line.startswith("data:") and not line.startswith("data: [DONE]")` 可抽取为工具函数，保持测试文件简洁，便于后续新增更多模型的类似用例。  

总体而言，此次迁移把原有 `enable_thinking` 的单元/集成测试迁入统一的 E2E 基础设施，并提升了模型并行度，以更真实地检验生产环境行为。只要 CI 环境满足 GPU 与显存要求，变更对现有功能无副作用。若资源不足，请提前做好 fallback 或在 CI 配置中排除该套测试。

---

### Remove migrated e2e_grpc/basic tests (#16738)
**SHA**: `f52ae58` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f52ae586b681e577084530b62709cd5a01c54586)

**🎯 变更类型**：重构（删除已迁移的冗余 E2E 测试）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：本次提交删除了 `sgl-model-gateway/e2e_test/e2e_grpc/basic/test_openai_server.py`，共 343 行。该文件是原 `test/srt/openai_server/basic` 中的复制，仅把启动方式改为 `popen_launch_workers_and_router`。随着对应逻辑已经迁移到新的位置，旧文件被移除以避免重复运行和维护成本。  

**🎯 影响范围**  
- **sgl-model-gateway/e2e_test**：E2E 测试目录结构被缩减。  
- **CI/CD 流程**：若 CI 脚本硬编码了该路径，需同步更新。  
- **测试覆盖**：原先的 OpenAI‑API 兼容性验证仍保留在新位置；否则可能导致覆盖率下降。  

**💡 关注建议**  
1. **确认迁移完成**：确保所有测试用例已安全迁入新的目录（如 `test/srt/openai_server/basic`），并且 `pytest`、`unittest` 能正常发现。  
2. **更新 CI 配置**：检查 `github/workflows/*.yml` 中的 `paths` 或 `run:` 命令，删除对已删除文件的引用，防止构建因找不到文件而报错。  
3. **保持依赖一致**：`popen_launch_workers_and_router`、`util` 等内部 fixture 仍被新测试使用，确保它们的导入路径在项目根目录下保持不变。  
4. **运行全套测试**：在本地或 CI 中执行 `pytest -q`，确认全套单元、集成、E2E 测试均通过，尤其是 `test_openai_server` 相关的兼容性检查。  
5. **文档同步**：如果项目文档中列举了该测试文件或运行说明，请相应更新为新路径或说明已合并。  

总体而言，此次删除属于清理冗余、提升可维护性的合理重构，对功能没有直接影响；只需确保迁移后的测试仍完整覆盖相同场景即可。

---

### [smg][ci] migrate chat completions tests to new infrastructure and build wheel once and share via artifact (#16709)
**SHA**: `2f8a363` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/2f8a36347c9886e483b1e4fe0478807cb91ae651)

**🎯 变更类型**：其他（CI 基础设施升级、E2E 测试扩展）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. CI 改为 **“build‑wheel → artifact → downstream jobs”**，在 GPU runner 上一次性编译 Python wheel 并通过 artifact 共享。  
2. 新增 `chat_completions` E2E 用例，覆盖 OpenAI‑compatible Chat Completion（包括日志概率、并行采样、正则、频率惩罚、前缀续写等）。  
3. `fixtures/hooks.py` 调整模型 marker 解析逻辑，兼容类级 marker（解决继承情况下的模型获取失效）。

**🎯 影响范围**：  
- `.github/workflows/pr-test-rust.yml`（整个 CI 流程）  
- `sgl-model-gateway/bindings/python`（wheel 构建）  
- `sgl-model-gateway/e2e_test/chat_completions/`（新增测试）  
- `sgl-model-gateway/e2e_test/fixtures/hooks.py`（fixture 行为）

**💡 关注建议**  
1. **Artifact 与平台兼容**：wheel 在 `4-gpu-a10`（manylinux）上构建后，后续 `ubuntu-latest` 安装需保持 `manylinux_2_28`（或更高）兼容，建议在 CI 中显式检查 `pip install` 是否出现 “not a supported wheel on this platform”。  
2. **缓存一致性**：`build-wheel` 仍使用 sccache，但下游 jobs（`gateway-e2e`、`python-unit-tests`）不再配置 rust 环境。若后续需要重新编译 Rust extension，请保留 `rust-cache` 步骤或在 artifact 中加入 `Cargo.lock`。  
3. **路径统一**：所有 downstream job 中均使用 `sgl-model-gateway/bindings/python/dist/*.whl` 或下载到 `dist/`、`wheel/`，请确认路径在所有 job 中保持一致，防止因路径拼写错误导致 “artifact not found”。  
4. **Python 版本**：`build-wheel` 直接在 GPU runner上使用系统 Python（默认 3.13），而 `python-unit-tests` 通过 `setup-python@v5` 指定 3.13。若项目对其他版本有兼容需求，考虑在 `build-wheel` 步骤显式使用 `actions/setup-python`。  
5. **fixture 修改影响**：新逻辑在类级别寻找 `@pytest.mark.model`，可能影响已有仅在函数上标记的测试。建议跑一遍全套单元/集成测试，确认没有因 marker 解析变化导致的漏测或误判。  

总体而言，此次改动显著提升 CI 构建复用效率，新增的 Chat Completion 测试让路由功能覆盖更完整。注意上述兼容性与路径细节，即可平滑上线。

---

### Add file size hints in bitwise model file verifier (#16735)
**SHA**: `b6e8a0d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b6e8a0d8514a821629f58ce9db529ce1a7b16722)

**🎯 变更类型**：功能增强（为模型文件校验器加入文件大小信息）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 引入 `FileInfo`、`Manifest` 数据类，实现“文件 → {sha256, size}”的结构。  
2. 旧的 `checksums`（仅 sha256）格式被标记为已废弃，在读取时会抛出 `DeprecationWarning`。  
3. `verify`、`generate_checksums`、内部加载与计算函数全部改为基于 `Manifest`，并在错误报告中同步显示文件大小。  
4. 对 HF 仓库读取逻辑做相应改造，返回 `FileInfo`，在本地计算时额外记录文件字节数。  
5. 测试用例同步更新，包括快照校验、旧格式兼容性警告以及 CLI 输出结构的验证。

**🎯 影响范围**  
- `python/sglang/srt/utils/model_file_verifier.py`（核心实现）  
- 相关单元测试 `test/registered/utils/test_model_file_verifier.py`  
- CLI 调用路径（`sglang srt utils model_file_verifier`）  

**💡 关注建议**  

*对开发者*  
- 确认项目文档已更新，说明新生成的 `checksums.json` 使用 `files` 键并包含 `size`。  
- 对外提供迁移指南：老的 `checksums` 文件仍能使用，但会触发 `DeprecationWarning`，建议重新生成。  
- 注意 `size=-1` 的回退路径（旧数据或 HF 中缺失 size 时），在比较时默认只检测 hash，避免误报。  
- 若后续在网络环境下大量拉取 HF 文件，考虑在 `Manifest.from_dict` 里缓存已解析的 `FileInfo`，提升性能。  

*对用户*  
- 当看到 “deprecated” 警告时，请重新运行 `sglang srt utils model_file_verifier --generate` 生成新版校验文件。  
- 若自行编辑 `checksums.json`，务必保持 `files.{filename}.sha256` 与 `size` 同步，否则校验会报 “mismatch”。  
- 在 CI 中使用 `verify` 时，可通过 `-W ignore::DeprecationWarning` 暂时屏蔽警告，待全部模型迁移后再去除。  

整体来看，此次改动在保持原有功能的同时，提升了完整性检查的可靠性，兼容性处理得当，风险较低。只需确保文档与迁移流程同步更新即可。

---

### VLM: enhance VL embedding model with video input support and revise warm-up strategy (#16635)
**SHA**: `d2ea44f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d2ea44f7756e243ac6f603357496fd2d6a1d29fd)

**🎯 变更类型**：功能增强（VLM 多模态嵌入新增 video 支持）+ 小幅重构（warm‑up 策略调整）  

**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 在 OpenAI 协议层为 `MultimodalEmbeddingInput` 增加 `video` 字段，并在 `/serving_embedding.py` 将其纳入内部请求的 `video_data`。  
2. `tokenizer_manager` 统一把 `video_data` 转为列表，加入多模态限制校验。  
3. `conversation.generate_embedding_convs` 现在接受 `videos` 参数，并在构造 Prompt 时拼接 `conv.video_token`。  
4. `http_server._execute_server_warmup` 的 image‑warmup 条件新增 `model_info["is_generation"]`，仅在生成模型（而非纯嵌入模型）下使用 chat‑completion 格式。  

**🎯 影响范围**  
- `python/sglang/srt/entrypoints/openai/*`（协议、embedding 服务）  
- `python/sglang/srt/managers/tokenizer_manager.py`  
- `python/sglang/srt/parser/conversation.py`  
- `python/sglang/srt/entrypoints/http_server.py`（warm‑up 逻辑）  

**💡 关注建议**  
1. **兼容性**：`MultimodalEmbeddingInput` 新增可选字段，向后兼容性良好。但调用方若使用旧的序列化（如手写 JSON）需确认不误把 `video` 当作非法字段。  
2. **模板要求**：`conv.video_token` 依赖对应 chat‑template 中定义 video token，检查所有已注册模板是否已补全，否则在生成 Prompt 时会抛 `AttributeError`。  
3. **资源与限制**：视频数据通常比图片大，确保 `self._validate_mm_limits` 已加入对视频大小/帧数的限制，否则可能触发 OOM。建议在文档中明确视频尺寸/时长上限。  
4. **warm‑up 行为**：新增的 `model_info["is_generation"]` 判断防止对嵌入模型执行不适用的图像 warm‑up，但如果某生成模型不支持 video，仍会走旧路径。可在未来进一步细化为 “supports_video”。  
5. **测试覆盖**：补充单元/集成测试：  
   - `MultimodalEmbeddingInput` 包含 video 时的请求转换。  
   - `generate_embedding_convs` 正确拼接 video token。  
   - `tokenizer_manager` 对 video list 的处理。  
   - warm‑up 在生成模型与嵌入模型之间的分支行为。  
6. **文档与示例**：更新 API 文档说明 video 字段的格式（base64、URL、或文件路径）以及对应的 tokenizer 行为，提供示例请求。  

总体来看，此提交为 VLM 引入了 video 多模态能力，并对 warm‑up 逻辑做了安全性提升，影响主要集中在请求解析、Prompt 构造以及启动阶段。确保模板完整、资源限制合理并补足测试，即可平稳上线。

---

### Support pre-generating and using expected checksums (#16730)
**SHA**: `83abecd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/83abecd0c27cb9695a4df074705872e781681aaf)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 `model_file_verifier` 引入 `generate` 子命令，支持本地或 HuggingFace 仓库预生成 `checksums.json`。  
- 将校验入口统一为 `verify`，可接受本地 JSON 或 HF 元数据作为校验源。  
- 新增文件发现、忽略模式、并行计算实现，并在测试中覆盖位翻转、缺失文件、CLI 调用以及 HF 场景。  

**🎯 影响范围**  
- `python/sglang/srt/utils/model_file_verifier.py`（核心实现、CLI）  
- 测试目录 `test/registered/utils/test_model_file_verifier.py`（新增大量单元/集成测试）  
- 与模型启动相关的 `sglang` 服务器启动脚本（通过 `--model-checksum` 参数使用新校验逻辑）。  

**💡 关注建议**  
1. `IGNORE_PATTERNS` 变量在本文件未显式定义，需确认其来源或在文件顶部加入默认列表，防止运行时 `NameError`。  
2. `generate_checksums` 返回的字典结构为 `{filename: sha256}`，但写入文件时包装为 `{"checksums": …}`；文档和错误提示应保持一致，防止用户误以为根层就是映射。  
3. 并行计算依赖 `ThreadPoolExecutor`，在极大模型（数千文件）时可能出现打开过多文件句柄的风险，建议在 `max_workers` 上加上上限或使用 `os.setrlimit` 进行保护。  
4. CLI 成功时未输出明确的 “verified successfully” 信息，可在 `verify` 成功路径打印统一提示，便于脚本化使用。  
5. 测试中通过 `subprocess.run` 调用模块，建议在 CI 中加入 `PYTHONPATH` 或使用 `-m sglang.srt.utils.model_file_verifier` 前确保工作目录正确，以免 Windows/macOS 环境出现找不到模块的错误。  
6. 考虑为 `generate` 支持 `--workers`、`--ignore` 等可选参数，提升灵活性。  

总体来说，此次改动显著提升了模型完整性校验的可用性和自动化程度，关键逻辑保持向后兼容，关注上述细节即可平滑上线。

---

### Support bitwise weight checksum verifier (#16729)
**SHA**: `d54f0a1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d54f0a10b4945f5bf4fde3b127ba1b3c40f0cdfd)

**变更类型**：功能增强  
**重要程度**：🟡 中  

**核心变更**  
1. **模型加载阶段加入完整性校验** – 在 `loader._prepare_weights` 中读取全局 `ServerArgs.model_checksum`，若提供则调用新实现的 `model_file_verifier.verify`。  
2. **CLI 与配置新增 `--model-checksum` 参数**，对应 `ServerArgs.model_checksum`，支持三种用法：  
   - 不带值 → 取模型路径作为 HF repo ID；  
   - 指定本地 JSON 文件路径；  
   - 指定 HF repo ID。  
3. **实现 `sglang/srt/utils/model_file_verifier.py`**：  
   - 从 HF Repo 拉取或本地读取 `sha256`（包括 LFS 自动获取），对模型目录内的目标文件并行计算校验值并比对。  
   - 报错时抛出 `IntegrityError`，并在控制台打印错误文件名。  
4. **完整单元 / 集成测试**：  
   - 验证 SHA256 计算函数；  
   - 对真实 HF 模型仓库执行校验；  
   - 在服务器启动阶段验证完整性，故意损坏权重文件时确保启动失败并输出 mismatch 信息。  

**影响范围**  
- `sglang/srt/model_loader/loader.py`（模型加载路径）  
- `sglang/srt/server_args.py`（CLI 参数、全局配置）  
- 新增 `model_file_verifier` 工具模块及对应异常类  
- 测试目录 `test/registered/utils/`（新增 2‑3 类测试）  

**关注建议**  
1. **依赖声明**：`model_file_verifier` 直接使用 `huggingface_hub`，请确认该库已在 `requirements.txt`/`pyproject.toml` 中声明，并在 CI 环境预装。  
2. **容错与兼容**：当前在 `loader` 中只在 `server_args` 存在且 `model_checksum` 非 `None` 时触发校验，建议在异常捕获层面加入提示（如 “model checksum verification failed, aborting”），防止因网络异常导致模型加载不可用。  
3. **性能**：并行计算 SHA256 默认 4 线程，若模型文件数量极多可能产生 I/O 饱和，建议提供 `--checksum-workers` 选项或在 `ServerArgs` 中加入可调参数。  
4. **文档**：更新 README / CLI 帮助，说明 `--model-checksum` 的三种使用方式及何时需要提供。  
5. **测试的网络依赖**：`test_model_file_verifier` 会真实下载 HF 模型，CI 环境应保证网络可达且缓存足够；考虑在离线 CI 场景加入 mock/skip 逻辑。  

总体而言，功能实现完整、错误信息友好，测试覆盖从单元到端到端。但需要注意依赖声明、异常传播和文档同步，以保证用户在启用校验时获得流畅体验。

---

### [Auto Sync] Update schedule_batch.py, common.py, eagle_info... (20260105) (#16519)
**SHA**: `fb04e7e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fb04e7e3c8c51d80ec4c091ed7c34d86a810a081)

**🎯 变更类型**：功能修复 / 细节重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. `ScheduleBatch.pop_committed_kv_cache` 去掉了对 `speculative_eagle_topk` 的分支判断，始终返回已提交的 KV 长度并标记为已释放。  
2. `release_kv_cache` 新增对 `MambaRadixCache` 的特殊处理，防止在前缀缓存命中但尚未分配 KV 时出现异常。  
3. `eagle_info.verify` 在接受 token 后显式更新 `req.kv_committed_len` 与 `req.kv_allocated_len`，并删除了原先在释放阶段的重复计数逻辑。  

**🎯 影响范围**：  
- `python/sglang/srt/managers/schedule_batch.py`（KV 释放逻辑）  
- `python/sglang/srt/mem_cache/common.py`（MambaRadixCache 适配）  
- `python/sglang/srt/speculative/eagle_info.py`（Eagle 采样与 KV 长度维护）  

**💡 关注建议**：  
- 重点验证 `speculative_eagle_topk>1` 场景，确认仍能正确回收 KV，避免旧的分支被误删导致泄漏。  
- 对使用 Mamba 模型的推理路径做回归测试，确保在前缀缓存命中后不再触发空指针异常。  
- 检查所有调用 `pop_committed_kv_cache` 的上层代码，确认不再依赖之前的 `topk` 条件分支。  
- 建议加入单元测试，覆盖 KV 计数的增删过程以及异常 abort 场景，提升后续维护安全性。

---

### fix: adding timeout for install dependencies (#16706)
**SHA**: `1e5de05` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1e5de05e35a8bbd89dace347dc9510b135ff7506)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：为 `.github/workflows/pr-test.yml` 中所有 “Install dependencies” 步骤统一添加 `timeout-minutes: 10`，防止依赖安装环节因网络波动或镜像失效导致 CI 卡死，提升 workflow 的可靠性。  

**🎯 影响范围**：  
- CI/CD 流程（GitHub Actions）  
- 与 `scripts/ci/ci_install_dependency.sh`、`ci_install_deepep.sh` 相关的所有作业  

**💡 关注建议**：  
1. **超时阈值**：10 分钟在多数环境足够，若后续仍出现超时，可考虑调高或将阈值设为可配置的输入参数。  
2. **复用性**：当前在同一文件中多次重复写 `timeout-minutes`，可抽取为一个 reusable step 或使用 `defaults.run.timeout-minutes`，降低维护成本。  
3. **监控**：留意 CI 运行日志，确认新的超时不会误杀仍在正常执行的长时间依赖安装（如大模型下载）。  
4. **文档**：在项目的 CI 文档中注明超时设置的目的和可调节方式，帮助贡献者了解 CI 行为。  

整体而言，此次改动对项目功能无影响，仅提升 CI 稳定性，建议在后续 CI 优化中进一步抽象统一配置。

---

### [NPU][Bugfix] Fix qwen3 error when enable-dp-lm-head (#16115)
**SHA**: `7dd679c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7dd679cbb93abb4526e636be0875f56e867d1794)

**🎯 变更类型**：Bug 修复（NPU 端 Qwen3 在开启 `enable_dp_lm_head` 时出现错误）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 NPU 发行版标签从 `20251206` 更新为 `2025.12.31`，保持 CI 与 Docker 镜像同步。  
2. 在 `rotary_embedding` 中加入对 `bfloat16` 的特殊处理，避免在 NPU 上走不支持的 half‑mode。  
3. 对 `vocab_parallel_embedding` 使用 `torch.compile(...,disable=_is_npu)`，防止编译器在 NPU 环境下产生错误。  
4. 引入 `is_npu` 判定，在 Llama、Qwen3、Qwen3‑MoE 的前向路径中切换到 NPU 专用的 `split_qkv_rmsnorm_rope` 实现，并把归一化权重、eps 参数显式传递。  
5. `qwen3` 的 `lm_head` 在构造时加入 `use_attn_tp_group=get_global_server_args().enable_dp_lm_head`，使数据并行头部能够正确分配。  

**🎯 影响范围**  
- `python/sglang/srt/layers/*`（rotary、vocab 并行 embedding）  
- `python/sglang/srt/models/llama.py`、`qwen3.py`、`qwen3_moe.py`（前向计算、NPU 路径）  
- CI / Docker 工作流脚本及 NPU 依赖安装脚本  

**💡 关注建议**  
1. **兼容性**：`split_qkv_rmsnorm_rope` 的接口在不同 `sgl-kernel-npu` 版本可能变化，建议在 `requirements.txt` 中锁定对应 tag，或在运行时检测函数签名提供回退。  
2. **性能**：关闭 `torch.compile` 仅在 NPU 环境，会影响 CPU/GPU 编译优化，确保 CI 中的基准测试覆盖两种路径。  
3. **测试**：补充 NPU‑only 单元测试，特别是 `enable_dp_lm_head` 开启与关闭的两套前向流程，验证 `bfloat16` 与 `float32` 的数值一致性。  
4. **文档**：在项目的 NPU 使用说明中注明新 tag（`2025.12.31`）与 `enable_dp_lm_head` 对 LM‑head 的影响，帮助用户快速定位潜在配置问题。  

总体而言，本次改动集中在 NPU 侧的算子兼容与并行头部的正确调度，修复后应能恢复 Qwen3 在多卡 DP 场景下的推理功能。请在正式发布前对上述重点模块进行回归验证。

---

### [AMD CI] re-enable testcases missed when migrating ci test files (#16535)
**SHA**: `f9c0426` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f9c04266922c6d3f07dd5b9968ffacda919c062f)

**🎯 变更类型**：功能增强 / CI 扩展  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 为 AMD CI 重新启用并注册了大量原本仅在 CUDA CI 中激活的测试用例（attention、backend、LoRA、模型等），并在 `ci_register` 中新增 `register_amd_ci` 调用。  
- 调整了 AMD 工作流分片大小、超时阈值及部分 GPU 资源配置，以容纳更长的测试时长和更细的并行划分。  

**🎯 影响范围**  
- `.github/workflows/pr-test-amd.yml`（CI 任务矩阵、分片策略、超时）  
- `test/registered/**` 各子目录的注册脚本（attention、backend、LoRA、模型、OpenAI Server 特性等）  
- `test/srt/run_suite.py`（AMD‑specific 测试时长更新）  

**💡 关注建议**  
- **CI 稳定性**：新增的分片（0‑11）和更长的 `timeout-per-file` 可能导致跑满资源或占用更长的 runner 时间，建议监控 AMD runner 的并发使用率，必要时对分片数或资源池进行调优。  
- **测试完整性**：部分 AMD 注册带有 `disabled` 注释（如 `test_triton_attention_kernels.py`），提示仍需在 AMD 环境验证，后续应补全对应实现或移除禁用标记。  
- **资源预算**：超时从 1800s 提升至 3600s，意味着单测耗时翻倍，团队需评估每日 CI 时长与费用是否在可接受范围。  
- **回归风险**：大量测试同时加入 AMD CI，建议在合并前通过单独的 AMD PR 检查跑通，避免因硬件差异（如 ROCm 兼容性）导致隐藏的回归。  

总体而言，此次改动显著提升了 AMD 平台的覆盖率，但也带来了资源消耗和验证工作量的增加，请相应关注 CI 资源调度和未完全适配的禁用测试。

---

### [jit kernel] support dtype as a cpp template parameter (#16452)
**SHA**: `48b8dcd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/48b8dcd42e55a0826fbba4acc36bdc0a84f35bb6)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 将 QK‑Norm JIT kernel 的数据类型抽象为模板参数，新增对 `half`、`nv_bfloat16` 的统一支持。  
- 新增 `PackedDType` 辅助结构体用于 2/4‑pack 类型的映射，并在 CUDA 端引入相应头文件。  
- Python 层包装函数通过 `torch.dtype` 传递数据类型，统一 dtype 检查逻辑。  

**🎯 影响范围**  
- `python/sglang/jit_kernel/csrc/norm.cuh`（核心 kernel 实现）  
- `python/sglang/jit_kernel/include/sgl_kernel/utils.{h,cu}`（类型映射、模板辅助）  
- `python/sglang/jit_kernel/*.py`（加载、可用性检查、调用入口）  
- 多模态/视觉模型的几处 `layernorm`、`attention` 调用路径。  

**💡 关注建议**  
1. **编译兼容性**：模板中使用 `.template with_device<kDLCUDA>` 已解决 NVCC 对 `<` 的歧义，建议在 CI 中加入不同 CUDA 版本的编译测试。  
2. **类型安全**：`static_assert` 限制在 `half`/`bf16`，若后续需扩展 float 等，需要同步更新 `PackedDType`。  
3. **性能基准**：删除原 `bf16_kernel/fp16_kernel` 分支后，请确保 `runtime::get_blocks_per_sm(kernel, …)` 能正确返回两者的占用率，最好补充 BF16 与 FP16 的对比基准。  
4. **文档/注释**：在 `utils.cuh` 中加入 `PackedDType` 的使用说明；在 Python API `can_use_fused_inplace_qknorm` 的 docstring 中标明仅支持 `float16`/`bfloat16`。  
5. **回归测试**：新增包含 `torch.float16` 与 `torch.bfloat16` 的单元测试，验证 `apply_qk_norm` 在两种 dtype 下均走 JIT 路径且数值一致。  

总体而言，此次改动实现了 dtype 参数化，提升了 kernel 的通用性；关注编译兼容、性能基准及相应文档/测试的完善，可避免后续回归。

---

### [diffusion] endpoint: add API endpoint to query loaded LoRA adapters information (#16533)
**SHA**: `41b434a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/41b434a7e69fbaa7ae60851135e46ac5d436808d)

**🎯 变更类型**：功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
新增 OpenAI‑compatible `/v1/list_loras` 接口，用于查询当前进程已加载的 LoRA 适配器及其在各模块（transformer、transformer_2、critic 等）上的激活状态。为此在 `utils.py` 添加 `ListLorasReq` 标记类，在调度器、GPU worker、LoRA pipeline 中实现对应的业务逻辑；同时在客户端 `diffusion_generator.py` 暴露 `list_loras` 方法，并补充了文档与端到端单元测试。  

**🎯 影响范围**  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/*`（新增路由与请求处理）  
- `python/sglang/multimodal_gen/runtime/managers/*`（scheduler、gpu_worker）  
- `python/sglang/multimodal_gen/runtime/pipelines_core/lora_pipeline.py`（状态汇总实现）  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/utils.py`（请求模型）  
- 文档 `openai_api.md` 与测试 `test_server_common.py`  

**💡 关注建议**  
1. **错误信息统一**：当前在 `gpu_worker.list_loras` 若 LoRA 未启用返回 `OutputBatch(error="Lora is not enabled")`，建议返回 HTTP 400/404 并在文档注明错误码。  
2. **JSON 可序列化**：`get_lora_status` 已确保只返回基本类型，但后续若加入 tensor 信息需再次检查。  
3. **并发安全**：`list_loras` 只读取状态，无写操作，基本安全；若以后加入状态修改，应考虑加锁或使用线程安全容器。  
4. **兼容性**：新路由对已有服务没有破坏性影响，但在旧版客户端调用时会得到 404，建议在发行说明中提示。  
5. **性能影响**：查询仅遍历已加载的字典，成本极低，可忽略。  
6. **文档与示例**：已在 `openai_api.md` 增加说明，建议在 README 中同步更新快速启动示例，防止用户误以为该接口默认可用。  

整体来看，变更实现完整、测试覆盖充分，除上述细节外无需额外修改，可直接合入。

---

### [AMD] Fix aiter page-size handling, DeepSeek MLA tuple inputs, and HiCache/FA3 decode-backend override (#16531)
**SHA**: `4935344` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/4935344fcd57ee515148aed23fcfc559300a4509)

**🎯 变更类型**：Bug 修复 / 功能增强  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. `aiter_backend` 中恢复对非‑1 page‑size 的支持，去掉强制断言并改为使用实例化时的 `self.page_size`。  
2. `deepseek_v2` 位置‑编码前的 `hidden_states` 可能是 tuple（量化路径），改为取首个 Tensor 取形状并保持原 tuple 传入后续算子。  
3. `server_args` 对 HiCache 与 FlashAttention‑3（FA3）解码后端的兼容处理更细致：仅在实际使用 FA3 时自动回退 IO 后端，并在用户显式指定 FA3 时给出警告。

**🎯 影响范围**  
- `python/sglang/srt/layers/attention/aiter_backend.py`（页大小、kv 缓存元数据生成）  
- `python/sglang/srt/models/deepseek_v2.py`（MLA/rope 前处理，涉及量化模型）  
- `python/sglang/srt/server_args.py`（启动参数、HiCache 与 FA3 解码的兼容性）  

**💡 关注建议**  
- **测试**：在不同硬件（AMD ROCm、NVIDIA SM100+）和不同 page‑size 设置下跑一次完整的 generate，确认 kv‑cache 正确索引且不会出现 shape 不匹配。  
- **量化路径**：确保后续所有 `Linear`、`Rope` 等算子在接收到 tuple 时仍能正常工作，建议在单元测试里加入 tuple‑input 场景。  
- **配置冲突**：用户如果显式指定 `decode_attention_backend=fa3`，现在会自动改为 `direct` IO 并给出 warning，文档需要更新说明此行为及可能的性能影响。  
- **回退逻辑**：`_handle_hicache` 中对 `is_sm100_supported()` 的调用仅在非‑MLA 场景下使用，确认该分支在所有支持的 GPU 上返回正确。  

总体来看，此次提交修复了 page‑size 断言导致的 AMD 环境崩溃，并兼容了 DeepSeek‑V2 量化模型的 tuple 输入，同时提升了 HiCache 与 FA3 的兼容性。若上述测试通过，可视为对多平台稳健性的显著提升。

---

### ci: migrate 2-GPU tests to test/registered/ (#16529)
**SHA**: `63cc97f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/63cc97f4ef7a4641185a7e3655c75a5d039988c2)

**变更类型**：CI 迁移 / 兼容性修复  
**重要程度**：🟡 中  

**核心改动**  
1. **GitHub CI**  
   - 将 2‑GPU 测试改为使用 `stage‑b‑test‑large‑2‑gpu` 并通过 `--auto‑partition-id/size` 并行执行，删除了原 `unit-test-backend-2-gpu` 任务。  
   - 新增矩阵 `partition: [0,1]`，并在 `scripts/ci/slash_command_handler.py` 中同步移除该 stage。  
   - 多个原本在 `test/registered` 目录下的测试文件被重新定位并通过 `register_cuda_ci`、`register_amd_ci` 显式声明套件与预估时长。  

2. **权重加载兼容**  
   - 在 `multimodal_gen/runtime/loader/weight_utils.py` 与 `srt/model_loader/weight_utils.py` 中新增 `_load_pt_file`，先尝试 `torch.load(..., weights_only=True)`，若因 “legacy .tar format” 失败再回退 `weights_only=False` 并给出警告。  
   - `pt_weights_iterator` 与多线程版本改用该函数，避免在 PyTorch 2.6+ 中因默认 `weights_only=True` 而读取老式 `.tar` checkpoint 时报错。  

3. **其他容错**  
   - `pynccl_allocator.is_symmetric_memory_enabled` 加上 `try/except` 防止在未初始化全局参数时抛异常，默认为 `False`。  

**影响范围**  
- CI 流水线（2 GPU、AMD CI、stage‑b‑test‑large‑2‑gpu）以及对应的测试注册脚本。  
- 所有加载 HuggingFace 权重的模型路径，尤其在使用旧 `.tar` checkpoint 时。  
- `pynccl_allocator` 在未配置对称内存的环境下的启动流程。  

**建议**  
- 为 `_load_pt_file` 添加单元测试，覆盖新旧 checkpoint 两种情况，确保回退路径只在可信来源触发（可通过额外来源校验）。  
- 考虑将 `_load_pt_file` 抽取为公共工具，避免在两个模块中重复实现。  
- CI 迁移后，确认 `run_suite.py` 中 `--auto-partition-*` 参数已被测试脚本完整解析，否则可能出现 “unknown arg” 错误。  
- 保留 `unit-test-backend-2-gpu` 的功能覆盖（如有必要），或在文档中说明已合并至 `stage-b-test-large-2-gpu`。  
- 监控新 CI 运行时的分区时长，若出现显著不平衡，可适当调节 `est_time` 或分区策略。  

通过上述检查和对应的回归测试，可确保迁移后的 CI 稳定性以及模型权重加载的向后兼容。

---

### [AMD] Add pip install / wheel build support for ROCm sgl-kernel (#15627)
**SHA**: `ab7d582` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/ab7d5829cde4109ce68154c5fd3af3e857cad928)

**🎯 变更类型**：功能增强（新增 ROCm 构建/发布流水线 & ROCm 支持的 CMake/脚本）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：在 GitHub Actions 中加入 `build-rocm700` 与 `release-rocm700` 两个 Job，实现对 ROCm 7.0（gfx942/gfx950）平台的 `sgl‑kernel` wheel 编译、上传并同步至 `sgl‑project/whl` 仓库；新增 ROCm 专用 CMakeLists、Docker‑内部构建脚本、hipify 辅助脚本以及 wheel 重命名工具；`scripts/update_kernel_whl_index.py` 也扩展为可生成 ROCm 索引。  
**🎯 影响范围**：  
- `sgl‑kernel` 编译链（CMake、hip 编译选项、宏定义）  
- CI 流水线（`.github/workflows/release-whl-kernel.yml`）  
- 包发布脚本 (`update_kernel_whl_index.py`)  
- 3rd‑party/amd 目录下的新增构建脚本与资源  

**💡 关注建议**  
1. **兼容性检查**：当前 CMake 只支持 `gfx942` 或 `gfx950`，若后续出现其它 AMDGPU 架构会导致 `FATAL_ERROR`。建议在 `CMakeLists_rocm.txt` 中加入更宽松的报错或自动降级。  
2. **环境变量 & 路径**：`AMDGPU_TARGET` 默认硬编码为 `gfx942;gfx950`，而 CI 通过 `AMDGPU_TARGET` 环境变量传入。确保所有 Runner 都已设置该变量，否则会出现构建错误。  
3. **版本获取**：`build_rocm.sh` 通过 `git tag` 读取版本，CI 中若未拉取完整标签（`git fetch --tags origin` 失败）会回退到硬编码 `v0.5.6`。建议在 CI 中显式 `git fetch --tags --depth=0`，或在失败时直接 abort 以免误发布旧版本。  
4. **wheel 重命名**：`rename_wheels_rocm.sh` 依赖 `/opt` 下是否存在 `7.0` 来决定后缀，可能在未来的镜像中路径变化导致命名错误。改为使用传入的 ROCm 版本变量更可靠。  
5. **安全性**：`release-rocm700` 使用 `GH_PAT_FOR_WHL_RELEASE` 推送 `whl` 仓库，请确认该 secret 权限最小化，并在 CI 中添加 `set -o pipefail` 防止意外泄露。  
6. **测试覆盖**：在 PR 中加入针对 `scripts/update_kernel_whl_index.py --rocm` 的单元测试，验证生成的 `index.html` 与 SHA256 是否正确。  
7. **文档更新**：README/CONTRIBUTING 中应说明如何在本地复现 ROCm wheel 构建（Docker 镜像、环境变量等），避免用户在非 CI 环境下卡住。  

总体来看，改动为 AMD GPU 用户打开了官方 pip 包渠道，结构清晰；若对上述细节做适当加固，可进一步提升可靠性与可维护性。

---

### [diffusion] refactor: eliminate redundant parameters in req (#16505)
**SHA**: `e14f5ec` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/e14f5ec8a814e99c959c86600b520242f52b9b66)

**🎯 变更类型**：重构（消除 `Req` 中的冗余参数）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：  
1. 将大部分与采样相关的字段从 `Req` 移至 `SamplingParams`，`Req` 只保留调度、缓存、运行时等核心信息。  
2. 新增 `prepare_request`，统一在入口处构造 `Req(sampling_params=…, VSA_sparsity=…)`，并把 `diffusers_kwargs` 合并到 `extra`。  
3. 为 `Req` 实现 `__getattr__` / `__setattr__` 委托，使访问缺失的属性时自动转到内部的 `sampling_params`，保持向后兼容。  
4. 删除 `image_api` 中的 `_build_req_from_sampling`、冗余 `dataclasses`、`shallow_asdict` 等旧实现。  

**🎯 影响范围**：  
- `python/sglang/multimodal_gen/runtime/pipelines_core/schedule_batch.py`（`Req` 定义）  
- `python/sglang/multimodal_gen/runtime/entrypoints/utils.py`（`prepare_request`）  
- `python/sglang/multimodal_gen/runtime/entrypoints/openai/image_api.py`（调用入口）  
- 可能的自定义 pipeline、测试脚本或插件仍直接创建 `Req` 实例的代码。

**💡 关注建议**  

| 给开发者的建议 | 说明 |
|---|---|
| **检查旧代码** | 项目内部或外部插件若手动构造 `Req`（如 `Req(prompt=…, seed=…)`），需要改为 `Req(sampling_params=SamplingParams(...), …)` 或使用 `prepare_request`。 |
| **验证属性访问** | 由于新增属性委托，运行时仍可通过 `req.seed` 等访问，但属性的真实存放位置已经在 `sampling_params`，单元测试应覆盖这类访问路径。 |
| **更新文档/示例** | API 文档中关于 `image_api`、`process_generation_batch` 等的参数说明应去掉已迁移的字段，强调使用 `SamplingParams`。 |
| **保持向后兼容** | `__getattr__`/`__setattr__` 已实现兼容，但如果有代码依赖 `Req.__dataclass_fields__`（如 `dataclasses.fields(Req)`），会失效，需要调整。 |
| **运行完整回归** | 由于去除了大量字段并改变了构造方式，建议跑一次完整的 CI（包括图像、视频、文本 pipeline）以确认未引入隐藏错误。 |
| **监控性能** | 重构本身不改变算法，但属性委托会产生一次额外的 `getattr`/`setattr` 调用，影响可忽略；仍可在高并发场景下做一次基准对比。 |

总体而言，此次重构提升了 `Req` 的职责单一性，简化了接口并降低了参数维护成本，只要遵循新构造方式并更新可能的手动 `Req` 创建点，即可平滑迁移。

---

### [Diffusion] clean useless and buggy set_seq_parallel_pg in yunchang (#16669)
**SHA**: `5a5cece` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5a5cece5616d930ea9651b831e6b4d2efa074351)

**🎯 变更类型**：重构（清理无用/有 bug 的实现）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
本次提交在 `python/sglang/multimodal_gen/runtime/distributed/yunchang.py` 中删除了 `set_seq_parallel_pg` 函数（62 行），该函数原本负责根据 ulysses 与 ring 的度数手动创建序列并行的进程组。注释指出该实现冗余且存在错误，现改为仅保留 `set_seq_parallel_pg_by_sp_groups` 入口。  

**🎯 影响范围**  
- `sglang/multimodal_gen/runtime/distributed` 模块（尤指 `yunchang.py`）  
- 可能在其他文件或外部脚本中直接调用 `set_seq_parallel_pg` 的代码  

**💡 关注建议**  
1. **搜索调用**：在项目内全局搜索 `set_seq_parallel_pg`，确认是否仍有剩余调用。若有，需要改为使用 `set_seq_parallel_pg_by_sp_groups` 或重新实现。  
2. **单元/集成测试**：确保分布式启动流程仍能正常创建 `ULYSSES_PG`、`RING_PG`（由新实现完成），防止因缺失函数导致运行时 `AttributeError`。  
3. **文档同步**：更新 README、API 文档或注释，去除已删除函数的说明，避免误导用户。  
4. **兼容性**：若该函数曾在旧版部署脚本中被使用，考虑提供一个兼容包装（抛出明确错误提示），以帮助使用者快速定位问题。  

总体而言，此次清理提升了代码可读性并消除了潜在的分布式错误，但请务必确认所有引用已迁移，以免引入运行时异常。

---

### [smg][ci] rename 3rd models from cloud backend and delete dead code (#16692)
**SHA**: `a08dc5a` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/a08dc5aa10b1a43c17fa2827310bde0b33837a58)

**🎯 变更类型**：重构 / 代码清理  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**  
1. 删除了 `sgl-model-gateway/e2e_test/backends.py` 与 `router_manager.py`，清理了已废弃的云后端配置和路由管理实现。  
2. 将原来的 `CLOUD_RUNTIMES / CLOUD_BACKENDS` 替换为 `THIRD_PARTY_MODELS`，并在 `infra/model_specs.py` 中统一声明。  
3. 为新模型集合实现 `launch_cloud_gateway`，并在 `infra/__init__.py` 与 `infra/gateway.py` 中公开。  
4. 相应地更新了 `fixtures/setup_backend.py` 以使用新的常量名称。

**🎯 影响范围**  
- **核心模块**：`sgl-model-gateway/e2e_test/infra/`（`gateway.py`, `model_specs.py`, `__init__.py`）以及测试 fixtures。  
- **依赖方**：任何直接 `import backends`、`CLOUD_RUNTIMES`、`launch_cloud_backend`（旧名）的代码或脚本都会在运行时抛 `ImportError`/`KeyError`。  
- **CI/E2E 测试**：旧的 `RouterManager` 已被移除，所有基于该类的集成测试需要改为使用 `infra.Gateway`。

**💡 关注建议**  
1. **兼容性**：如果仍有外部项目或内部脚本引用旧 API，建议在本仓库保留一个轻量的兼容层（如 `CLOUD_RUNTIMES = THIRD_PARTY_MODELS`、`launch_cloud_backend = launch_cloud_gateway`），或在文档中明确迁移路径。  
2. **循环依赖**：`gateway.py` 在函数内部才导入 `THIRD_PARTY_MODELS`，避免了模块间循环，保持这种懒加载方式即可。  
3. **测试覆盖**：确认所有 E2E/单元测试已切换到新 `launch_cloud_gateway`，并删除对 `RouterManager` 的调用；运行完整 CI 以确保未遗漏的引用。  
4. **文档更新**：在 README 或开发者手册中把 “cloud runtime” 改为 “third‑party model” 并示例 `THIRD_PARTY_MODELS` 的使用方式。  

整体来看，此次改动是一次合理的代码清理与命名统一，风险主要在于旧引用的残留。完成上述迁移检查后即可安全合并。

---

### [1/n]deepseek_v2.py Refactor: attention backend handlers and forward method definition (#16306)
**SHA**: `38dc583` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/38dc5839dd8d185b419be9e5bb2d22c2908db979)

**🎯 变更类型**：重构  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 DeepSeek‑V2 中分散的注意力后端分派、前向实现枚举以及环境标志抽取到统一的 `deepseek_common` 包。新增 `AttentionBackendRegistry`、`AttnForwardMethod` 与若干环境变量封装，并在该包内统一完成后端处理函数的注册；原文件 `deepseek_v2.py` 中对应代码被删除，仅保留对新模块的引用。  

**🎯 影响范围**  
- `python/sglang/srt/models/deepseek_v2.py`（注意力前向调度逻辑）  
- 新增的 `deepseek_common` 子包（`attention_backend_handler.py、forward_methods.py、utils.py、__init__.py`）  
- 可能引用 `AttnForwardMethod`、`AttentionBackendRegistry` 的其他模型或测试代码。  

**💡 关注建议**  
1. **导入路径迁移**：项目外部或内部仍使用 `from …deepseek_v2 import AttnForwardMethod` 的位置需改为 `from …deepseek_common.attention_forward_methods import AttnForwardMethod`（同理 `AttentionBackendRegistry`）。  
2. **注册生效检查**：新模块已在文件末尾完成所有后端的 `register`，确保在模型实例化前已导入该模块，否则默认回退到 `triton`。可在启动日志中打印已注册的 backend 列表作验证。  
3. **环境变量封装**：`deepseek_common.utils` 中的 `_is_*`、`_use_aiter_*` 变量取代了原文件的即时调用，确认这些标志在多进程/CUDA‑graph 场景下仍保持一致；如有自定义环境变量请同步到此处。  
4. **兼容性回退**：若有旧代码直接访问 `deepseek_v2.AttentionBackendRegistry`，需提供兼容层或在文档中标明已迁移。  
5. **测试覆盖**：重点跑包含 flashinfer、FA3、TRITON、AITER、NPU 等后端的单元/集成测试，验证 `handle_attention_*` 返回的 `AttnForwardMethod` 与原行为一致；特别留意 deterministic‑inference、piecewise‑cuda‑graph 场景的分支。  

整体来看，本次改动将注意力调度抽象为统一插件式机制，代码重复度大幅下降，后续新增后端或策略时仅需在 `deepseek_common` 中实现并注册即可。但要注意迁移旧导入及保持注册顺序，以免出现“未找到 handler”或枚举不匹配的回归。

---

### MoE Refactor: Refactor `fp8.py` -> `flashinfer_trllm.py` (#15151)
**SHA**: `24b30f7` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/24b30f7757f8dc650c8e562eef104d8eae7628e6)

**🎯 变更类型**：功能增强 / 重构  
**⚡ 重要程度**：🟡 中  

**📋 变更概要**  
1. 将原 `fp8.py` 中的 FlashInfer‑FP8 逻辑抽离到新模块 `flashinfer_trtllm.py`，并实现 **FlashInfer‑TRT‑LLM** 后端的权重对齐、量化信息封装以及 fused‑moe 调用。  
2. `MoeRunner` 增加对 `flashinfer_trtllm` 后端的识别（`runner_core` 为 `None`，只能走 fused 路径），并在初始化时强制检查 fused func 是否已注册。  
3. `fp8.py` 中的权重预处理改为统一调用 `align_fp8_moe_weights_for_flashinfer_trtllm`，删除了旧的 `apply_with_router_logits` 实现，转而在 `apply` 中直接构造 `FlashInferTrtllmFp8MoeQuantInfo` 并交给 fused func。  
4. `runner.py` 与 `moe_runner/runner.py` 相应更新，以支持新后端的调度逻辑。  

**🎯 影响范围**  
- `sglang/srt/layers/moe/fused_moe_triton/layer.py`（调用 `quant_method.apply` 而非 `apply_with_router_logits`）  
- `sglang/srt/layers/moe/moe_runner/flashinfer_trtllm.py`（新增）  
- `sglang/srt/layers/moe/moe_runner/runner.py`（后端判断、异常检查）  
- `sglang/srt/layers/quantization/fp8.py`（权重对齐、`apply` 分支、后端切换）  
- 依赖 `MoeRunnerBackend.is_flashinfer_trtllm()` 的所有调用点（如模型加载、推理入口）。  

**💡 关注建议**  
1. **兼容性检查**：确认在不满足 `flashinfer_trtllm` 条件（如缺少 FlashInfer 包或不支持的 GPU）时，默认回落到 DeepGemm/Trition；当前在 `runner_core is None && fused_func is None` 抛出 `NotImplementedError`，请在上层捕获并给出友好提示。  
2. **权重对齐路径**：`align_fp8_moe_weights_for_flashinfer_trtllm` 中使用了 `shuffle_matrix_a`、`reorder_rows_for_gated_act_gemm`，这些函数仅在 FlashInfer 1.1+ 可用，建议在 `import` 前进行版本检测，防止因 API 变动导致加载失败。  
3. **内存对称分配**：`use_symmetric_memory` 仍依赖 `is_allocation_symmetric`，若用户自行关闭对称内存，需确保 FlashInfer‑TRT‑LLM kernel 能正常接受非对称分配的张量（建议在文档中注明）。  
4. **单元测试**：增加针对 `flashinfer_trtllm` 后端的 **端到端** 测试，包括：  
   - 权重对齐后 shape/ dtype 正确；  
   - `apply` 调用后返回的 `StandardCombineInput.hidden_states` 与参考 DeepGemm 输出数值误差在容忍范围；  
   - `runner_core` 为 `None` 时 fused func 正常触发。  
5. **文档更新**：在 README/使用手册中说明：  
   - 新后端仅支持 `silu` 激活、`no_combine=False`；  
   - 何时需要手动调用 `align_fp8_moe_weights_for_flashinfer_trtllm`（模型加载后自动完成）；  
   - 已废弃的 `apply_with_router_logits` 接口。  

通过上述检查与测试，可确保新 FlashInfer‑TRT‑LLM 路径在不同硬件/环境下的稳定性，并平滑迁移已有模型。

---

### Fix pytest tests to exit with proper exit code (#16681)
**SHA**: `3a4767d` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3a4767daa344abe42b3c99be66cbb516f6a52793)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将项目中所有直接运行 `pytest.main([__file__])` 的测试脚本改为 `sys.exit(pytest.main([__file__]))`，并在 README 中同步说明。这样可以让 CI 在测试失败时返回非零退出码，从而准确捕获错误。  
**🎯 影响范围**：  
- `test/README.md`（文档）  
- 多个测试入口文件：`test/registered/function_call/test_unknown_tool_name.py`、`test/registered/layers/mamba/*`、`test/srt/test_evs.py`、`test/srt/test_video_utils.py` 等（共 8 余个）。  

**💡 关注建议**  
1. **CI 行为**：此改动仅影响 CI 与本地手动 `python3 test_x.py` 的退出码，业务代码不受影响。确认 CI 脚本仍然以 `python3` 调用测试文件即可。  
2. **兼容性**：`sys.exit` 在 Windows、Linux、macOS 上行为一致，但若某些测试脚本内部自行捕获 `SystemExit`，可能导致退出码被吞掉。建议全局检查是否有 `except SystemExit` 的逻辑。  
3. **统一入口**：后续新加入的测试文件请遵循 “`if __name__ == '__main__': import sys; sys.exit(pytest.main([__file__]))`” 模式，避免再次出现退出码为 0 的假阴性。可以在项目模板或脚本生成工具中固定该写法。  
4. **本地验证**：开发者在本地运行单个测试文件时，应观察返回值，例如 `echo $?`，确保非零错误能够被捕获。  
5. **文档同步**：README 已更新，确保 CI 维护文档的其它章节（如 CONTRIBUTING）同样使用该写法。  

总体来看，此次改动风险极低，主要提升 CI 对测试失败的感知能力，建议尽快合并并在后续的测试文件模板中固化该写法。

---

### Migrate tokenizer tests to test/registered/tokenizer/ (#16457)
**SHA**: `0241e04` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0241e0460fd43bb2f1d6a2b771042b19547e1c90)

**🎯 变更类型**：其他（测试组织结构迁移）  
**⚡ 重要程度**：🟡 中  
**📋 变更摘要**：将 tokenizer 相关的单元测试搬迁到 `test/registered/tokenizer/`，并在每个文件中新增 CI 注册调用（`register_cuda_ci` / `register_amd_ci` / `register_cpu_ci`）。同步修改 `test/srt/run_suite.py`，将对应的 `TestFile` 条目删除，避免重复执行。  
**🎯 影响范围**：  
- `test/registered/function_call/test_jinja_template_utils.py`  
- `test/registered/tokenizer/test_multi_tokenizer.py`  
- `test/registered/tokenizer/test_skip_tokenizer_init.py`  
- CI 注册模块 `sglang.test.ci.ci_register`  
- 测试调度脚本 `test/srt/run_suite.py`  

**💡 关注建议**  

1. **CI 注册准确性**：新增的 `register_*_ci` 调用必须与项目的 CI 配置保持一致。检查 `ci_register` 中对应的标签、估算时间与实际运行时间是否匹配，防止因时间估算不当导致调度失衡。  
2. **测试发现路径**：迁移后，确保 `pytest` 或自定义测试框架仍能正确发现 `test/registered/...` 目录下的文件。可以在本地运行 `pytest -q` 验证。  
3. **run_suite 同步**：`run_suite.py` 已删除对应 `TestFile` 条目，但如果还有其它脚本或 CI 步骤硬编码了旧路径，需要同步更新，避免“找不到文件”或重复执行。  
4. **依赖导入**：新增的 `register_*_ci` 引入了 `sglang.test.ci.ci_register`，确认该模块在所有运行环境（CPU、CUDA、AMD）均可 import，避免在缺少相应依赖的环境中引发 ImportError。  
5. **回归测试**：由于这些测试涉及多卡（CUDA/AMD）和 CPU‑only 场景，建议在本地分别跑一次 `pytest` 的 CUDA/AMD 标记（如 `-m cuda`），确保注册信息不影响测试本身的逻辑。  
6. **文档/注释**：可以在迁移说明或对应目录的 `README` 中补充说明，方便后续维护者了解测试分类和 CI 注册的约定。  

总体来说，此次迁移提升了测试的可分组管理和 CI 调度颗粒度，但需要确认 CI 注册和路径发现的完整性，以防出现隐藏的构建或运行错误。

---

### Fix gpt_oss_common import path and migrate core tests (#16426)
**SHA**: `0c47427` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/0c474273c514e048ea4dc3477e74c8fd085dd2db)

**🎯 变更类型**：其他（测试组织与 CI 注册）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
1. 将 `gpt_oss_common.py`、`test_gpt_oss_*` 等文件的导入路径统一改为 `sglang.test.gpt_oss_common`，消除跨目录相对导入导致的模块找不到问题。  
2. 为核心单元测试（deterministic、hidden_states、input_embeddings、io_struct、model_hooks、page_size、request_queue_validation、score_api、server_args、srt_endpoint、srt_engine 等）统一加入 `ci_register` 调用，声明在 CUDA 与 AMD CI 环境下的估算运行时长及所属测试套件 `stage-b-test-small-1-gpu(-amd)`。  
3. 更新 `test/srt/run_suite.py`：删除上述已迁移至 `registered/core` 的测试在 `per-commit-1-gpu`、`per-commit-amd-mi35x` 等套件中的条目，使 CI 只调度已注册的测试；相应地删除冗余 `test_gpt_oss_common.py` 条目。  

**🎯 影响范围**  
- **sglang.test** 包路径：所有依赖 `test_gpt_oss_common` 的模块需使用新路径。  
- **CI 配置**：新增 `register_cuda_ci` / `register_amd_ci`，影响 CI 调度、超时估算及资源分配。  
- **测试套件定义**：`run_suite.py` 中的 `per-commit-` 套件列表被精简，防止重复执行。  

**💡 关注建议**  
- 开发者在新增或修改测试时，务必通过 `sglang.test.ci.ci_register` 注册对应平台的估算时间，避免 CI 超时。  
- 确认本地运行 `pytest` 时仍能找到 `sglang.test.gpt_oss_common`，若有自定义 `PYTHONPATH` 需要同步更新。  
- CI 负责人检查 `stage-b-test-small-1-gpu` 与 `-amd` 套件的资源配额，确保新加入的估算时间总和不超过节点限制。  

此改动主要是清理导入路径和统一 CI 注册，风险较低，但后续新增测试应遵循同一注册方式。

---

### fix: 8-gpu-b200 increase timeout length (#16658)
**SHA**: `f474255` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/f4742558ac7430ffdcbfedc4dff94476a6cee786)

**🎯 变更类型**：Bug 修复  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
将 GitHub Actions 工作流 `.github/workflows/nightly-test-nvidia.yml` 中 “Run common 8‑GPU model tests” 步骤的超时时间由 200 分钟提升至 300 分钟。目的是避免因模型测试耗时超过原设定而导致的异常中断。

**🎯 影响范围**  
- CI/CD 流程（nightly‑test‑nvidia 工作流）  
- 运行 8‑GPU 相关模型的自动化测试  

**💡 关注建议**  
1. **监控实际耗时**：在后续几次 nightly 运行后统计该步骤的平均耗时，确认 300 分钟仍有余量，防止资源浪费。  
2. **成本控制**：延长超时会导致 GPU 实例占用更久，建议评估是否需要对测试用例进行拆分或并行，以降低单次运行时长。  
3. **队列影响**：若超时延长导致工作流执行时间显著增长，可能影响其他 PR 的 CI 排队，需在项目维护页面提示用户。  
4. **回滚机制**：保留原始 200 分钟配置的提交记录，以便在测试时间恢复正常后快速回退。  

该更改仅涉及 CI 配置，不会影响业务代码或运行时功能。后续可通过观察 workflow run 日志，评估是否需要进一步优化测试套件或资源分配。

---

### Add reference counting to ModelInstance for parallel test safety (#16672)
**SHA**: `7385834` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/7385834c8d984176de126516d55d98249f026962)

**🎯 变更类型**：功能增强（为 `ModelInstance` 引入引用计数，使并行测试安全）  
**⚡ 重要程度**：🟡 中  

**📋 变更摘要**  
- 在 `sgl-model-gateway/e2e_test/infra/model_pool.py` 为 `ModelInstance` 添加 `_ref_count`、`_ref_lock`、`acquire()/release()` 与 `is_in_use`，并在驱逐逻辑中跳过仍被引用的实例。  
- 所有 e2e 测试 fixture（`pool.py`、`setup_backend.py`）在获取实例后显式 `acquire()`，使用完毕后 `release()`，避免在测试期间实例被驱逐。  
- 为并发驱逐提供线程安全的时间戳更新和引用计数管理。  

**🎯 影响范围**  
- **核心模块**：`sgl-model-gateway/e2e_test/infra/model_pool.py`（ModelInstance 与 ModelPool）  
- **测试层**：`sgl-model-gateway/e2e_test/fixtures/*`（pool、setup_backend）  
- 由于引用计数在生产代码里也会生效，任何使用 `ModelPool.get()` 的场景若未手动 `acquire()`，仍会得到实例但可能在并发驱逐时被误删。

**💡 关注建议**  

1. **异常安全**：当前 fixture 在 `yield` 前 `acquire()`，在 `finally` 中 `release()`，但若 `yield` 前的代码抛异常（如 `openai.OpenAI` 初始化失败），`release()` 可能不会执行。建议改为  
   ```python
   instance.acquire()
   try:
       client = openai.OpenAI(...)
       yield client
   finally:
       instance.release()
   ```  
   同理在 `setup_backend` 等处也使用 `try/finally` 包裹 `acquire()`。

2. **上下文管理器**：为 `ModelInstance` 实现 `__enter__/__exit__` 或单独的 `@contextmanager`，让测试代码更简洁、避免遗漏 `release()`。

3. **锁的粒度**：`_ref_lock` 采用 `field(default_factory=threading.Lock)`，每个实例拥有独立锁，已符合预期。但若后续在 `ModelPool` 中对多实例进行批量操作（如一次性驱逐多条记录），可能出现锁竞争，可考虑在 `ModelPool` 层加全局锁来保证一致性。

4. **驱逐策略**：驱逐时仍按 `last_used` 降序（最近使用优先被驱逐），这与多数 MRU/LRU 策略相悖。若业务期望 “最近使用的应保留”，请改为升序排序或使用 LRU。当前实现可能导致频繁使用的实例被错误淘汰。

5. **日志与监控**：`acquire()`/`release()` 已加入 `logger.debug`，建议在 CI 中打开相应日志等级，以便定位测试并发导致的驱逐问题。

6. **兼容性检查**：部分代码仍直接调用 `model_pool.get(model_id)` 后返回 `instance.base_url`，未 `acquire()`。如果有其它非测试路径使用该接口，可能会在高并发下触发驱逐错误。可以在 `ModelPool.get()` 中添加可选 `auto_acquire: bool = False` 参数或在文档中强调使用者必须自行调用 `acquire()`。

总体而言，此次改动为并行 e2e 测试提供了可靠的生命周期管理，代码结构清晰。只要在异常路径加入 `finally` 释放并考虑上面几点细节，即可在生产环境安全部署。

---

#### 🟢 低重要度变更 (20)

### [Performance] Force split_k=1 for MXFP4 Triton kernels on Hopper (#16014)
**SHA**: `05ab110` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/05ab110e02baa5e876de6ff0108526efd8984dc3)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 MXFP4 Triton kernel 的 Hopper（SM90）路径中新增 `is_sm90_supported` 检测，并强制 `split_k=1`，以提升该平台的运行性能。

---

### [Diffusion] model: fix zimage tp (#16719)
**SHA**: `82a8d77` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/82a8d77bc0309712b3b73ac8f91e6a11d1dcb5cc)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `zimage.py` 中引入 Tensor Parallel（TP）支持，改用 `ColumnParallelLinear` 并根据 TP 世界大小划分本地 head 数量，新增 TP 规模校验。

---

### [Diffusion] Avoid cpu2gpu sync in flashinfer rope and apply flashinfer rope to wanvideo (#16668)
**SHA**: `294ff71` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/294ff71d189c918205e29181233c37121e1a5056)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：在 `rotary_embedding.py` 中将位置张量创建在查询所在设备上，去除不必要的 CPU→GPU 同步；在 `wanvideo.py` 中加入对 FlashInfer 的 RoPE 加速支持，默认在 CUDA 环境下使用 `apply_flashinfer_rope_qk_inplace`，否则回退到原有实现。

---

### Fix FP8 MoE NaN with DeepGEMM on Blackwell (#16622)
**SHA**: `fb7609f` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/fb7609f1ddbebe9fd50ba789e69b6a4dead3f1e6)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `fp8.py` 中新增 `is_deepgemm_moe_runner_backend_enabled` 方法，统一判断 MoE 是否使用 DeepGEMM；相应地在权重处理与 MoE runner 创建处改为调用该方法，去除重复导入和判断逻辑。

---

### [NPU] update model and features supported (#16733)
**SHA**: `20ca2c6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/20ca2c6e1ebc8c830e9f92de7345200348ea8e14)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：更新 Ascend NPU 支持文档，新增 `direct` IO 后端，完善自定义权重加载选项，扩充并细化大模型与多模态模型支持列表。

---

### [AMD] Turn on AMD CI if rocm.Dockerfile changed (#16634)
**SHA**: `3d51ae1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3d51ae18a18e72eba75fd90573a2e48d82eee544)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/workflows/pr-test-amd.yml` 中新增 `docker/rocm.Dockerfile` 为触发路径，使 AMD CI 在该 Dockerfile 变更时自动运行。

---

### [NPU][Bugfix] move free_page logics to cpu (#16608)
**SHA**: `261860e` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/261860e17b2727f2de5f5f44166008ffb4d56fc8)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 NPU 分配器 `allocator_npu.py` 中新增 `free` 方法，将释放页的逻辑迁移到 CPU，处理空索引、分组与排序，并在调试模式下验证唯一性。

---

### Disable PCG TP Unittest (#16693)
**SHA**: `154740b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/154740bd4dcd86a24ebe715638ac429ed86bfdc1)

**🎯 变更类型**：测试修改  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `test_piecewise_cuda_graph_2_gpu.py` 中为 CI 注册添加 `disabled="see issue #16691"`，暂时关闭 PCG TP 相关单元测试。

---

### [Build] Enable full kernel in aarch64 wheel (#16155)
**SHA**: `1c09cbe` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/1c09cbe3edd59e85433f470b14d0cf6f3b9f30d9)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在发布 workflow、Dockerfile 与构建脚本中添加 `CMAKE_EXTRA_ARGS` 参数，针对 aarch64 架构传入 `-DENABLE_BELOW_SM90=ON`，实现全 kernel 编译。

---

### Tiny adjust cancel PR workflow. (#16697)
**SHA**: `8867d24` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/8867d248796917a7472df31bcaa32078f4a1f835)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将原 “Cancel All Pending PR Test Runs” 工作流改名并细化，仅取消状态为 queued/waiting 且 event 为 pull_request 的 PR 测试运行；新增独立的 “Cancel All PR Test Runs” 工作流，默认只针对 `pr-test.yml`，并加入对 in_progress 状态的处理。

---

### vlm: support SGLANG_MM_SKIP_COMPUTE_HASH for bypassing multimodal feature hashing (#16555)
**SHA**: `6b3f93c` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/6b3f93c4dd15606d886696ab9327c7395283f80a)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：新增环境变量 `SGLANG_MM_SKIP_COMPUTE_HASH`，在 `schedule_batch` 中可跳过多模态特征哈希计算，直接使用随机 UUID 生成的哈希值。

---

### Add sufeng-buaa into CI_PERMISSION (#16625)
**SHA**: `d566739` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/d566739b65b98dca66bcf101cf9c31d61410185f)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `.github/CI_PERMISSIONS.json` 中新增 `sufeng-buaa` 条目，赋予其标记运行 CI、重新运行失败 CI、阶段重跑等权限，冷却时间设为 0。

---

### Revert "[sgl-kernel] Update flashmla to include fp8 sparse_mla optimizations" (#16678)
**SHA**: `12a0292` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/12a0292bfdde5f807e3a6dba7575df50cf1473bc)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：将 FlashMLA 子模块的 `GIT_TAG` 从 `b1860dc…` 回滚至 `be055fb…`，撤销之前的 fp8 sparse_mla 优化更新。

---

### remove redundant max_running_reqs calculation in r3 (#16629)
**SHA**: `eec7dbd` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/eec7dbd31e804efadd10515d4eacbd35694eb434)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：删除 `max_running_requests` 的冗余计算，直接使用已有的 `self.max_running_requests` 参数。

---

### [diffusion] fix: reduce default text length for Qwen-Image from 1024 to 512 (#16445)
**SHA**: `bb798a1` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/bb798a1c264805030fe43cc243bf40c4fc640e89)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：将 Qwen‑Image 编码器的默认 `text_len` 从 1024 缩减至 512，减小输入文本长度，降低内存和计算开销。

---

### [NPU] Update model and features supported (#16652)
**SHA**: `5e867f6` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/5e867f60cf8ee47c6f1d9553b3c3823ede6bbc17)

**🎯 变更类型**：文档更新  
**⚡ 重要程度**：🟢低  
**📋 摘要**：完善了 Ascend NPU 支持特性文档，修正链接、默认值与参数说明；同步更新了模型支持表，新增并标记了多款模型为已支持，统一了表格格式与选项描述。

---

### Add google-cloud-storage into Dockerfile (#15343)
**SHA**: `65bed83` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/65bed8382b84fc94d5ee31dd1f14f5e86d26e9a1)

**🎯 变更类型**：配置调整  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 Dockerfile 中新增 `google-cloud-storage` 依赖，以支持 Google Cloud 存储的 Python 客户端。

---

### Fix KeyError when logprobs=false in completions endpoint (#16095)
**SHA**: `156d97b` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/156d97b219d9fa034192e5a1e5308c4add2a4aa5)

**🎯 代码重构**  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 completions 接口中，当 `logprobs=False` 时改用 `dict.get(..., [])` 防止缺失 `output_top_logprobs`/`output_token_logprobs` 导致 `KeyError`，并新增单元测试验证非流式响应的正确性。

---

### Revert "Add SwapAB Optimization for triton fused_moe_kernel on SM90." (#16676)
**SHA**: `3e73e12` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/3e73e12458e316f7679a8efa80b556364bd2c823)

**🎯 变更类型**：代码重构（撤销 SwapAB 优化）  
**⚡ 重要程度**：🟢 低  
**📋 摘要**：删除了针对 SM90 GPU 的 SwapAB 逻辑及相关函数、导入和参数，恢复原始的 fused_moe_triton 核心实现。

---

### [model-gateway] Fix IGW routing for external OpenAI workers (#16633)
**SHA**: `b5a94f8` | 🔗 [查看提交](https://github.com/sgl-project/sglang/commit/b5a94f8a8e54fe1f6fc168921c06cc0c6e112426)

**🎯 变更类型**：代码重构  
**⚡ 重要程度**：🟢低  
**📋 摘要**：在 `router_manager.rs` 中引入 `RuntimeType::External`，为外部 OpenAI worker 添加路由入口，调整路由优先级顺序为 external > grpc-pd > http-pd > grpc-regular > http-regular。

---

